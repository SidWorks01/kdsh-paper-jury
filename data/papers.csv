Abstract,Conference
"Semi-supervised learning (SSL) offers a robust framework for harnessing the potential of unannotated data. Traditionally, SSL mandates that all classes possess labeled instances. However, the emergence of open-world SSL (OwSSL) introduces a more practical challenge, wherein unlabeled data may encompass samples from unseen classes. This scenario leads to misclassification of unseen classes as known ones, consequently undermining classification accuracy. To overcome this challenge, this study revisits two methodologies from self-supervised and semi-supervised learning, self-labeling and consistency, tailoring them to address the OwSSL problem. Specifically, we propose an effective framework called OwMatch, combining conditional self-labeling and open-world hierarchical thresholding.  Theoretically, we analyze the estimation of class distribution on unlabeled data through rigorous statistical analysis, thus demonstrating that OwMatch can ensure the unbiasedness of the label assignment estimator with reliability.  Comprehensive empirical analyses demonstrate that our method yields substantial performance enhancements across both known and unknown classes in comparison to previous studies. Code is available at https://github.com/niusj03/OwMatch.",NIPS
"The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparse-that can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparseindices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of longcontext LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. Byevaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",NIPS
"Graph Contrastive Learning (GCL) has emerged as a powerful approach for generating graph representations without the need for manual annotation. Most advanced GCL methods fall into three main frameworks: node discrimination, group discrimination, and bootstrapping schemes, all of which achieve comparable performance. However, the underlying mechanisms and factors that contribute to their effectiveness are not yet fully understood. In this paper, we revisit these frameworks and reveal a common mechanismâ€”representation scatteringâ€”that significantly enhances their performance. Our discovery highlights an essential feature of GCL and unifies these seemingly disparate methods under the concept of representation scattering. To leverage this insight, we introduce Scattering Graph Representation Learning (SGRL), a novel framework that incorporates a new representation scattering mechanism designed to enhance representation diversity through a center-away strategy. Additionally, consider the interconnected nature of graphs, we develop a topology-based constraint  mechanism that integrates graph structural properties with representation scattering to prevent excessive scattering. We extensively evaluate SGRL across various downstream tasks on benchmark datasets, demonstrating its efficacy and superiority over existing GCL methods. Our findings underscore the significance of representation scattering in GCL and provide a structured framework for harnessing this mechanism to advance graph representation learning. The code of SGRL is at https://github.com/hedongxiao-tju/SGRL.",NIPS
"Program synthesis with language models (LMs) has unlocked a large set of reasoning abilities; code-tuned LMs have proven adept at generating programs that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word concatenation). However, not all reasoning tasks are easily expressible as code, e.g. tasks involving commonsense reasoning, moral decision-making, and sarcasm understanding. Our goal is to extend a LMâ€™s program synthesis skills to such tasks and evaluate the results via pseudo-programs, namely Python programs where some leaf function calls are left undefined. To that end, we propose, Code Generation and Emulated EXecution (COGEX). COGEX works by (1) training LMs to generate pseudo-programs and (2) teaching them to emulate their generated programâ€™s execution, including those leaf functions, allowing the LMâ€™s knowledge to fill in the execution gaps; and (3) using them to search over many programs to find an optimal one. To adapt the COGEX model to a new task, we introduce a method for performing program search to find a single program whose pseudo-execution yields optimal performance when applied to all the instances of a given dataset. We show that our approach yields large improvements compared to standard in-context learning approaches on a battery of tasks, both algorithmic and soft reasoning. This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered.",NIPS
"Scalarization is a general, parallizable technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, yet some have dismissed this versatile approach because linear scalarizations cannot explore concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that provably explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights achieves an optimal sublinear hypervolume regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. For the setting of multiobjective stochastic linear bandits, we utilize properties of hypervolume scalarizations to derive a novel non-Euclidean analysis to get regret bounds of $\tilde{O}( d T^{-1/2} + T^{-1/k})$, removing unnecessary $\text{poly}(k)$ dependencies. We support our theory with strong empirical performance of using non-linear scalarizations that outperforms both their linear counterparts and other standard multiobjective algorithms in a variety of natural settings.",NIPS
"The Diffusion Model has not only garnered noteworthy achievements in the realm of image generation but has also demonstrated its potential as an effective pretraining method utilizing unlabeled data. Drawing from the extensive potential unveiled by the Diffusion Model in both semantic correspondence and open vocabulary segmentation, our work initiates an investigation into employing the Latent Diffusion Model for Few-shot Semantic Segmentation.Recently, inspired by the in-context learning ability of large language models, Few-shot Semantic Segmentation has evolved into In-context Segmentation tasks, morphing into a crucial element in assessing generalist segmentation models.In this context, we concentrate on Few-shot Semantic Segmentation, establishing a solid foundation for the future development of a Diffusion-based generalist model for segmentation. Our initial focus lies in understanding how to facilitate interaction between the query image and the support image, resulting in the proposal of a KV fusion method within the self-attention framework.Subsequently, we delve deeper into optimizing the infusion of information from the support mask and simultaneously re-evaluating how to provide reasonable supervision from the query mask.Based on our analysis, we establish a simple and effective framework named DiffewS, maximally retaining the original Latent Diffusion Model's generative framework and effectively utilizing the pre-training prior. Experimental results demonstrate that our method significantly outperforms the previous SOTA models in multiple settings.",NIPS
"Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework. However, these methods still struggle with task ambiguity in in-context segmentation, as not all in-context examples can accurately convey the task information. In order to address this issue, we present SINE, a simple image $\textbf{S}$egmentation framework utilizing $\textbf{in}$-context $\textbf{e}$xamples. Our approach leverages a Transformer encoder-decoder structure, where the encoder provides high-quality image representations, and the decoder is designed to yield multiple task-specific output masks to eliminate task ambiguity effectively. Specifically, we introduce an In-context Interaction module to complement in-context information and produce correlations between the target image and the in-context example and a Matching Transformer that uses fixed matching and a Hungarian algorithm to eliminate differences between different tasks. In addition, we have further perfected the current evaluation system for in-context image segmentation, aiming to facilitate a holistic appraisal of these models. Experiments on various segmentation tasks show the effectiveness of the proposed method.",NIPS
"The Gauss-Newton (GN) matrix plays an important role in machine learning, most evident in its use as a preconditioning matrix for a wide family of popular adaptive methods to speed up optimization. Besides, it can  also provide key insights into the optimization landscape of neural networks. In the context of deep neural networks, understanding the GN matrix involves studying the interaction between different weight matrices as well as the dependencies introduced by the data, thus rendering its analysis challenging.In this work, we take a first step towards theoretically characterizing the conditioning of the GN matrix in neural networks. We establish tight bounds on the condition number of the GN in deep linear networks of arbitrary depth and width, which we also extend to two-layer ReLU networks.We expand the analysis to further architectural components, such as residual connections and convolutional layers. Finally, we empirically validate the bounds and uncover valuable insights into the influence of the analyzed architectural components.",NIPS
"Models of hyperbolic geometry have been successfully used in ML for two main tasks: embedding models in unsupervised learning (e.g. hierarchies) and embedding data. To our knowledge, there are no approaches that provide embeddings for supervised models; even when hyperbolic geometry provides convenient properties for expressing popular hypothesis classes, such as decision trees (and ensembles).In this paper, we propose a full-fledged solution to the problem in three independent contributions. The first linking the theory of losses for class probability estimation to hyperbolic embeddings in Poincar\'e disk model. The second resolving an issue for a clean, unambiguous embedding of (ensembles of) decision trees in this model. The third showing how to smoothly tweak the Poincar\'e hyperbolic distance to improve its encoding and visualization properties near the border of the disk, a crucial region for our application, while keeping hyperbolicity.This last step has substantial independent interest as it is grounded in a generalization of Leibniz-Newton's fundamental Theorem of calculus.",NIPS
"_Graph Neural Tangent Kernel_ (GNTK) fuses graph neural networks and graph kernels, simplifies the process of graph representation learning, interprets the training dynamics of graph neural networks, and serves various applications like protein identification, image segmentation, and social network analysis. In practice, graph data carries complex information among entities that inevitably evolves over time, and previous static graph neural tangent kernel methods may be stuck in the sub-optimal solution in terms of both effectiveness and efficiency. As a result, extending the advantage of GNTK to temporal graphs becomes a critical problem. To this end, we propose the temporal graph neural tangent kernel, which not only extends the simplicity and interpretation ability of GNTK to the temporal setting but also leads to rigorous temporal graph classification error bounds. Furthermore, we prove that when the input temporal graph grows over time in the number of nodes, our temporal graph neural tangent kernel will converge in the limit to the _graphon_ NTK value, which implies the transferability and robustness of the proposed kernel method, named **Temp**oral **G**raph **N**eural **T**angent **K**ernel with **G**raphon-**G**uaranteed or **Temp-G$^3$NTK**. In addition to the theoretical analysis, we also perform extensive experiments, not only demonstrating the superiority of Temp-G$^3$NTK in the temporal graph classification task, but also showing that Temp-G^3NTK can achieve very competitive performance in node-level tasks like node classification compared with various SOTA graph kernel and representation learning baselines. Our code is available at https://github.com/kthrn22/TempGNTK.",NIPS
"Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for tuning large language models.The problem is fairly understood in toy settings with linear target functions or over finite small domains that limits practical interest.Taking the next step, we consider infinite domains and kernelized rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm.We propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game and chooses action pairs that are informative and have favorable reward values. MaxMinLCB consistently outperforms algorithms in the literature and satisfies an anytime-valid rate-optimal regret guarantee. This is owed to our novel preference-based confidence sequences for kernelized logistic estimators, which are of independent interest.",NIPS
"Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks.However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions.Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities.To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents.LogiCity models diverse urban elements using semantic and spatial concepts, such as $\texttt{IsAmbulance}(\texttt{X})$ and $\texttt{IsClose}(\texttt{X}, \texttt{Y})$. These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios.Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning.To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors.Our extensive evaluation reveals the advantage of NeSy frameworks in abstract reasoning. Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data.With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI.All the code and data are open-sourced at our website.",NIPS
"In multi-output regression, we identify a previously neglected challenge that arises from the inability of training distribution to cover all combinations of input features, leading to combinatorial distribution shift (CDS). To the best of our knowledge, this is the first work to formally define and address this problem. We tackle it through a novel tensor decomposition perspective, proposing the Functional t-Singular Value Decomposition (Ft-SVD) theorem which extends the classical tensor SVD to infinite and continuous feature domains, providing a natural tool for representing and analyzing multi-output functions. Within the Ft-SVD framework, we formulate the multi-output regression problem under CDS as a low-rank tensor estimation problem under the missing not at random (MNAR) setting, and introduce a series of assumptions about the true functions, training and testing distributions, and spectral properties of the ground-truth embeddings, making the problem more tractable.To address the challenges posed by CDS in multi-output regression, we develop a tailored Double-Stage Empirical Risk Minimization (ERM-DS) algorithm that leverages the spectral properties of the embeddings and uses specific hypothesis classes in each frequency component to better capture the varying spectral decay patterns. We provide rigorous theoretical analyses that establish performance guarantees for the ERM-DS algorithm. This work lays a preliminary theoretical foundation for multi-output regression under CDS.",NIPS
"Deep learning models like AlphaFold2 have revolutionized protein structure prediction, achieving unprecedented accuracy. However, the dependence on robust multiple sequence alignments (MSAs) continues to pose a challenge, especially for proteins that lack a wealth of homologous sequences. To overcome this limitation, we introduce MSA-Generator, a self-supervised generative protein language model. Trained on a sequence-to-sequence task using an automatically constructed dataset, MSA-Generator employs protein-specific attention mechanisms to harness large-scale protein databases, generating virtual MSAs that enrich existing ones and boost prediction accuracy. Our experiments on CASP14 and CASP15 benchmarks reveal significant improvements in LDDT scores, particularly for complex and challenging sequences, enhancing the performance of both AlphaFold2 and RoseTTAFold. The code is released at \url{https://github.com/lezhang7/MSAGen}.",NIPS
"This work presents a modification of the self-attention dynamics proposed in Geshkovski et al to better reflect the practically relevant, causally masked attention used in transformer architectures for generative AI. This modification translates into an interacting particle system that cannot be interpreted as a mean-field gradient flow. Despite this loss of structure, we significantly strengthen the results of Geshkovski et al in this context: While previous rigorous results focused on cases where all three matrices (key, query, and value) were scaled identities, we prove asymptotic convergence to a single cluster for arbitrary key-query matrices and value matrix equal to the identity.Additionally, we establish a connection to the classical R\'enyi parking problem from combinatorial geometry to make initial theoretical steps towards demonstrating the existence of meta-stable states.",NIPS
"Large language models (LLMs) often lack culture-specific everyday knowledge, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are usually limited to a single language or online sources like Wikipedia, which may not reflect the daily habits, customs, and lifestyles of different regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play or the sports they practice in school is not always explicitly written online. To address this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. The benchmark comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We evaluate LLMs in two formats: short-answer questions, and multiple-choice questions. We show that LLMs perform better in cultures that are more present online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format.Furthermore, we find that LLMs perform better in their local languages for mid-to-high-resource languages. Interestingly, for languages deemed to be low-resource, LLMs provide better answers in English. We make our dataset publicly available at: https://github.com/nlee0212/BLEnD.",NIPS
"We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in the standard alignment process. (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.",NIPS
"Unsupervised representation learning has significantly advanced various machine learning tasks. In the computer vision domain, state-of-the-art approaches utilize transformations like random crop and color jitter to achieve invariant representations, embedding semantically the same inputs despite transformations. However, this can degrade performance in tasks requiring precise features, such as localization or flower classification. To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information. However, current methods depend on transformation labels and thus struggle with interdependency and complex transformations. We propose Self-supervised Transformation Learning (STL), replacing transformation labels with transformation representations derived from image pairs. The proposed method ensures transformation representation is image-invariant and learns corresponding equivariant transformations, enhancing performance without increased batch complexity. We demonstrate the approachâ€™s effectiveness across diverse classification and detection tasks, outperforming existing methods in 7 out of 11 benchmarks and excelling in detection. By integrating complex transformations like AugMix, unusable by prior equivariant methods, this approach enhances performance across tasks, underscoring its adaptability and resilience. Additionally, its compatibility with various base models highlights its flexibility and broad applicability. The code is available at https://github.com/jaemyung-u/stl.",NIPS
"The pursuit of fairness in machine learning (ML), ensuring that the models do not exhibit biases toward protected demographic groups, typically results in a compromise scenario. This compromise can be explained by a Pareto frontier where given certain resources (e.g., data), reducing the fairness violations often comes at the cost of lowering the model accuracy. In this work, we aim to train models that mitigate group fairness disparity without causing harm to model accuracy.Intuitively, acquiring more data is a natural and promising approach to achieve this goal by reaching a better Pareto frontier of the fairness-accuracy tradeoff. The current data acquisition methods, such as fair active learning approaches, typically require annotating sensitive attributes. However, these sensitive attribute annotations should be protected due to privacy and safety concerns. In this paper, we propose a tractable active data sampling algorithm that does not rely on training group annotations, instead only requiring group annotations on a small validation set. Specifically, the algorithm first scores each new example by its influence on fairness and accuracy evaluated on the validation dataset, and then selects a certain number of examples for training. We theoretically analyze how acquiring more data can improve fairness without causing harm, and validate the possibility of our sampling approach in the context of risk disparity. We also provide the upper bound of generalization error and risk disparity as well as the corresponding connections.Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm. Our code is available at github.com/UCSC-REAL/FairnessWithoutHarm.",NIPS
"Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, facilitating detailed depictions of data and their interconnections across various real-world settings. However, existing TAG datasets predominantly feature textual information only at the nodes, with edges typically represented by mere binary or categorical attributes. This lack of rich textual edge annotations significantly limits the exploration of contextual relationships between entities, hindering deeper insights into graph-structured data. To address this gap, we introduce Textual-Edge Graphs Datasets and Benchmark (TEG-DB), a comprehensive and diverse collection of benchmark textual-edge datasets featuring rich textual descriptions on nodes and edges. The TEG-DB datasets are large-scale and encompass a wide range of domains, from citation networks to social networks. In addition, we conduct extensive benchmark experiments on TEG-DB to assess the extent to which current techniques, including pre-trained language models, graph neural networks, and their combinations, can utilize textual node and edge information. Our goal is to elicit advancements in   textual-edge graph research, specifically in developing methodologies that exploit rich textual node and edge descriptions to enhance graph analysis and provide deeper insights into complex real-world networks. The entire TEG-DB project is publicly accessible as an open-source repository on Github, accessible at https://github.com/Zhuofeng-Li/TEG-Benchmark.",NIPS
"We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered,  hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LASR can be used to discover a new and powerful scaling law for LLMs.",NIPS
"The advent of powerful neural classifiers has increased interest in problems that require both learning and reasoning.These problems are critical for understanding important properties of models, such as trustworthiness, generalization, interpretability, and compliance to safety and structural constraints. However, recent research observed that tasks requiring both learning and reasoning on background knowledge often suffer from reasoning shortcuts (RSs): predictors can solve the downstream reasoning task without associating the correct concepts to the high-dimensional data. To address this issue, we introduce rsbench, a comprehensive benchmark suite designed to systematically evaluate the impact of RSs on models by providing easy access to highly customizable tasks affected by RSs. Furthermore, rsbench implements common metrics for evaluating concept quality and introduces novel formal verification procedures for assessing the presence of RSs in learning tasks. Using rsbench, we highlight that obtaining high quality concepts in both purely neural and neuro-symbolic models is a far-from-solved problem. rsbench is available at: https://unitn-sml.github.io/rsbench.",NIPS
"Continual learning has primarily focused on the issue of catastrophic forgetting and the associated stability-plasticity tradeoffs. However, little attention has been paid to the efficacy of continually learned representations, as representations are learned alongside classifiers throughout the learning process. Our primary contribution is empirically demonstrating that existing online continually trained deep networks produce inferior representations compared to a simple pre-defined random transforms. Our approach embeds raw pixels using a fixed random transform, approximating an RBF-Kernel initialized before any data is seen. We then train a simple linear classifier on top without storing any exemplars, processing one sample at a time in an online continual learning setting. This method, called RanDumb,  significantly outperforms state-of-the-art continually learned representations across all standard online continual learning benchmarks. Our study reveals the significant limitations of representation learning, particularly in low-exemplar and online continual learning scenarios. Extending our investigation to popular exemplar-free scenarios with pretrained models, we find that training only a linear classifier on top of pretrained representations surpasses most continual fine-tuning and prompt-tuning strategies. Overall, our investigation challenges the prevailing assumptions about effective representation learning in the online continual learning.",NIPS
"Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.",NIPS
"We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals.Video demos, data, and code are available online.",NIPS
"We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or userâ€™s responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate end-to-end generative optimization â€“ using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: https://microsoft.github.io/Trace/.",NIPS
"Graph diffusion models have emerged as state-of-the-art techniques in graph generation; yet, integrating domain knowledge into these models remains challenging. Domain knowledge is particularly important in real-world scenarios, where invalid generated graphs hinder deployment in practical applications.Unconstrained and conditioned graph diffusion models fail to guarantee such domain-specific structural properties. We present ConStruct, a novel framework that enables graph diffusion models to incorporate hard constraints on specific properties, such as planarity or acyclicity.Our approach ensures that the sampled graphs remain within the domain of graphs that satisfy the specified property throughout the entire trajectory in both the forward and reverse processes. This is achieved by introducing an edge-absorbing noise model and a new projector operator.ConStruct demonstrates versatility across several structural and edge-deletion invariant constraints and achieves state-of-the-art performance for both synthetic benchmarks and attributed real-world datasets. For example, by incorporating planarity constraints in digital pathology graph datasets, the proposed method outperforms existing baselines, improving data validity by up to 71.1 percentage points.",NIPS
"Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular, due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method on ambiguous classification tasks for uncertainty quantification.",NIPS
"Correspondences in point cloud registration are prone to outliers, significantly reducing registration accuracy and highlighting the need for precise inlier identification. In this paper, we propose a robust inlier identification algorithm for point cloud registration by reformulating the conventional registration problem as an alignment error $\ell_0$-minimization problem. The $\ell_0$-minimization problem is formulated for each local set, where those local sets are built on a compatibility graph of input correspondences. To resolve the $\ell_0$-minimization, we develop a novel two-stage decoupling strategy, which first decouples the alignment error into a rotation fitting error and a translation fitting error. Second, null-space matrices are employed to decouple inlier identification from the estimation of rotation and translation respectively, thereby applying Bayesian theory to $\ell_0$-minimization problems and solving for fitting errors. Correspondences with the smallest errors are identified as inliers to generate a transformation hypothesis for each local set. The best hypothesis is selected to perform registration. We demonstrate that the proposed inlier identification algorithm is robust under high outlier ratios and noise through experiments. Extensive results on the KITTI, 3DMatch, and 3DLoMatch datasets demonstrate that our method achieves state-of-the-art performance compared to both traditional and learning-based methods in various indoor and outdoor scenes.",NIPS
"Poor quality sleep can be characterized by the occurrence of events ranging from body movement to breathing impairment.Utilizing widely-available earbuds equipped with a sleep event detection algorithm, it is possible to offer a convenient and efficient alternative to laborious clinical diagnoses for individuals suffering from sleep disorders. Although various solutions utilizing wearables have been proposed to detect these events, they ignore the fact that individuals often share sleeping spaces with others; roommates or couples, for example (henceforth referred to as wear-aware). To address this issue, we introduce DreamCatcher, the first publicly available dataset for wearer-aware sleep event detection on earables. DreamCatcher encompasses eight distinct sleep events, including synchronous two-channel audio and motion data collected from 12 pairs (24 participants) totaling 210 hours (420 hour.person) with fine-grained label.We further tested multiple benchmark models on three tasks, demonstrating the usability and unique challenge of DreamCatcher.We hope that the proposed DreamCatcher can inspireother researchers to further explore efficient wearer-aware human vocal activity sensing on earables. DreamCatcher was made open-source at site https://anonymous.4open.science/r/open-earsleep-D369.",NIPS
"Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. We break down the problem into two causes: concept ignorance and concept mismapping. To tackle the two challenges, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with the image-to-text concept matching mechanism. Firstly, we introduce a novel image-to-text concept activation module to guide the diffusion model in revisiting ignored concepts. Additionally, an attribute concentration module is proposed to map the text conditions of each entity to its corresponding image area correctly. Extensive experimental evaluations, conducted across three distinct text-to-image alignment benchmarks, demonstrate the superior efficacy of our proposed method, CoMat-SDXL, over the baseline model, SDXL~\cite{podell2023sdxl}. We also show that our method enhances general condition utilization capability and generalizes to the long and complex prompt despite not specifically training on it.",NIPS
"Despite advancements in Text-to-Video (T2V) generation, producing videos with realistic motion remains challenging. Current models often yield static or minimally dynamic outputs, failing to capture complex motions described by text. This issue stems from the internal biases in text encoding which overlooks motions, and inadequate conditioning mechanisms in T2V generation models. To address this, we propose a novel framework called DEcomposed MOtion (DEMO), which enhances motion synthesis in T2V generation by decomposing both text encoding and conditioning into content and motion components. Our method includes a content encoder for static elements and a motion encoder for temporal dynamics, alongside separate content and motion conditioning mechanisms. Crucially, we introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion. Evaluations on benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMO's superior ability to produce videos with enhanced motion dynamics while maintaining high visual quality. Our approach significantly advances T2V generation by integrating comprehensive motion understanding directly from textual descriptions. Project page: https://PR-Ryan.github.io/DEMO-project/",NIPS
"The development of generative language models that can create long and coherent textual outputs via autoregression has lead to a proliferation of uses and a corresponding sweep of analyses as researches work to determine the limitations of this new paradigm. Unlike humans, these 'Large Language Models' (LLMs) are highly sensitive to small changes in their inputs, leading to unwanted inconsistency in their behavior. One problematic inconsistency when LLMs are used to answer multiple-choice questions or analyze multiple inputs is order dependency: the output of an LLM can (and often does) change significantly when sub-sequences are swapped, despite both orderings being semantically identical. In this paper we present , a technique that guarantees the output of an LLM will not have order dependence on a specified set of sub-sequences. We show that this method provably eliminates order dependency, and that it can be applied to any transformer-based LLM to enable text generation that is unaffected by re-orderings. Delving into the implications of our method, we show that, despite our inputs being out of distribution, the impact on expected accuracy is small, where the expectation is over the order of uniformly chosen shuffling of the candidate responses, and usually significantly less in practice. Thus, can be used as a 'dropped-in' method on fully trained models. Finally, we discuss how our method's success suggests that other strong guarantees can be obtained on LLM performance via modifying the input representations.Code is available at github.com/reidmcy/set-based-prompting.",NIPS
"The Area Under the ROC Curve (AUC) is a well-known metric for evaluating instance-level long-tail learning problems. In the past two decades, many AUC optimization methods have been proposed to improve model performance under long-tail distributions. In this paper, we explore AUC optimization methods in the context of pixel-level long-tail semantic segmentation, a much more complicated scenario. This task introduces two major challenges for AUC optimization techniques. On one hand, AUC optimization in a pixel-level task involves complex coupling across loss terms, with structured inner-image and pairwise inter-image dependencies, complicating theoretical analysis. On the other hand, we find that mini-batch estimation of AUC loss in this case requires a larger batch size, resulting in an unaffordable space complexity. To address these issues, we develop a pixel-level AUC loss function and conduct a dependency-graph-based theoretical analysis of the algorithm's generalization ability. Additionally, we design a Tail-Classes Memory Bank (T-Memory Bank) to manage the significant memory demand. Finally, comprehensive experiments across various benchmarks confirm the effectiveness of our proposed AUCSeg method. The code is available at https://github.com/boyuh/AUCSeg.",NIPS
"Nuanced expressiveness, especially through detailed hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations.In this work, we aim to learn expressive human avatars from a monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details.To this end, we introduce EVA, a drivable human model that can recover fine details based on 3D Gaussians and an expressive parametric human model, SMPL-X.Focused on enhancing expressiveness, our work makes three key contributions.First, we highlight the importance of aligning the SMPL-X model with the video frames for effective avatar learning.Recognizing the limitations of current methods for estimating SMPL-X parameters from in-the-wild videos, we introduce a reconstruction module that significantly improves the image-model alignment.Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts.Third, we develop a feedback mechanism that predicts per-pixel confidence to better guide the optimization of 3D Gaussians.Extensive experiments on two benchmarks demonstrate the superiority of our approach both quantitatively and qualitatively, especially on the fine-grained hand and facial details. We make our code available at the project website: https://evahuman.github.io.",NIPS
"First-order optimization (FOO) algorithms are pivotal in numerous computational domains, such as reinforcement learning and deep learning. However, their application to complex tasks often entails significant optimization inefficiency due to their need of many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first general framework that enhances the time efficiency of FOO by leveraging parallel computing to directly mitigate its requirement of many sequential iterations for convergence. To achieve this, OptEx utilizes a kernelized gradient estimation that is based on the history of evaluated gradients to predict the gradients required by the next few sequential iterations in FOO, which helps to break the inherent iterative dependency and hence enables the approximate parallelization of iterations in FOO. We further establish theoretical guarantees for the estimation error of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming that the estimation error diminishes to zero as the history of gradients accumulates and that our SGD-based OptEx enjoys an effective acceleration rate of Î˜(âˆšN ) over standard SGD given parallelism of N, in terms of the sequential iterations required for convergence. Finally, we provide extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training on various datasets, to underscore the substantial efficiency improvements achieved by our OptEx in practice.",NIPS
"Hyperparameter Optimization (HPO) plays a pivotal role in unleashing the potential of iterative machine learning models. This paper addresses a crucial aspect that has largely been overlooked in HPO: the impact of uncertainty in ML model training. The paper introduces the concept of uncertainty-aware HPO and presents a novel approach called the UQ-guided scheme for quantifying uncertainty. This scheme offers a principled and versatile method to empower HPO techniques in handling model uncertainty during their exploration of the candidate space.By constructing a probabilistic model and implementing probability-driven candidate selection and budget allocation, this approach enhances the quality of the resulting model hyperparameters. It achieves a notable performance improvement of over 50\% in terms of accuracy regret and exploration time.",NIPS
"The superior generation capabilities of Denoised Diffusion Probabilistic Models (DDPMs) have been effectively showcased across a multitude of domains. Recently, the application of DDPMs has extended to time series generation tasks, where they have significantly outperformed other deep generative models, often by a substantial margin. However, we have discovered two main challenges with these methods: 1) the inference time is excessively long; 2) there is potential for improvement in the quality of the generated time series. In this paper, we propose a method based on discrete token modeling technique called Similarity-driven Discrete Transformer (SDformer). Specifically, SDformer utilizes a similarity-driven vector quantization method for learning high-quality discrete token representations of time series, followed by a discrete Transformer for data distribution modeling at the token level. Comprehensive experiments show that our method significantly outperforms competing approaches in terms of the generated time series quality while also ensuring a short inference time. Furthermore, without requiring retraining, SDformer can be directly applied to predictive tasks and still achieve commendable results.",NIPS
"Masked prediction has emerged as a promising pretraining paradigm in offline reinforcement learning (RL) due to its versatile masking schemes, enabling flexible inference across various downstream tasks with a unified model. Despite the versatility of masked prediction, it remains unclear how to balance the learning of skills at different levels of complexity. To address this, we propose CurrMask, a curriculum masking pretraining paradigm for sequential decision making. Motivated by how humans learn by organizing knowledge in a curriculum, CurrMask adjusts its masking scheme during pretraining for learning versatile skills.  Through extensive experiments, we show that CurrMask exhibits superior zero-shot performance on skill prompting tasks, goal-conditioned planning tasks, and competitive finetuning performance on offline RL tasks. Additionally, our analysis of training dynamics reveals that CurrMask gradually acquires skills of varying complexity by dynamically adjusting its masking scheme.",NIPS
"The field of risk-constrained reinforcement learning (RCRL) has been developed to effectively reduce the likelihood of worst-case scenarios by explicitly handling risk-measure-based constraints.However, the nonlinearity of risk measures makes it challenging to achieve convergence and optimality.To overcome the difficulties posed by the nonlinearity, we propose a spectral risk measure-constrained RL algorithm, spectral-risk-constrained policy optimization (SRCPO), a bilevel optimization approach that utilizes the duality of spectral risk measures.In the bilevel optimization structure, the outer problem involves optimizing dual variables derived from the risk measures, while the inner problem involves finding an optimal policy given these dual variables.The proposed method, to the best of our knowledge, is the first to guarantee convergence to an optimum in the tabular setting.Furthermore, the proposed method has been evaluated on continuous control tasks and showed the best performance among other RCRL algorithms satisfying the constraints.Our code is available at https://github.com/rllab-snu/Spectral-Risk-Constrained-RL.",NIPS
"As an emerging task that integrates perception and reasoning, topology reasoning in autonomous driving scenes has recently garnered widespread attention.  However, existing work often emphasizes ""perception over reasoning"": they typically boost reasoning performance by enhancing the perception of lanes and directly adopt vanilla MLPs to learn lane topology from lane query. This paradigm overlooks the geometric features intrinsic to the lanes themselves and are prone to being influenced by inherent endpoint shifts in lane detection. To tackle this issue, we propose an interpretable method for lane topology reasoning based on lane geometric distance and lane query similarity, named TopoLogic. This method mitigates the impact of endpoint shifts in geometric space, and introduces explicit similarity calculation in semantic space as a complement. By integrating results from both spaces, our methods provides more comprehensive information for lane topology.  Ultimately, our approach significantly outperforms the existing state-of-the-art methods on the mainstream benchmark OpenLane-V2 (23.9 v.s. 10.9 in TOP$_{ll}$ and 44.1 v.s. 39.8 in OLS on subsetA). Additionally, our proposed geometric distance topology reasoning method can be incorporated into well-trained models without re-training, significantly enhancing the performance of lane topology reasoning. The code is released at https://github.com/Franpin/TopoLogic.",NIPS
"Contrastive Learning (CL) plays a crucial role in molecular representation learning, enabling unsupervised learning from large scale unlabeled molecule datasets. It has inspired various applications in molecular property prediction and drug design.However, existing molecular representation learning methods often introduce potential false positive and false negative pairs through conventional graph augmentations like node masking and subgraph removal. The issue can lead to suboptimal performance when applying standard contrastive learning techniques to molecular datasets. To address the issue of false positive and negative pairs in molecular representation learning, we propose a novel probability-based contrastive learning (CL) framework. Unlike conventional methods, our approach introduces a learnable weight distribution via Bayesian modeling to automatically identify and mitigate false positive and negative pairs. This method is particularly effective because it dynamically adjusts to the data, improving the accuracy of the learned representations. Our model is learned by a stochastic expectation-maximization process, which optimizes the model by iteratively refining the probability estimates of sample weights and updating the model parameters.Experimental results indicate that our method outperforms existing approaches in 13 out of 15 molecular property prediction benchmarks in MoleculeNet dataset and 8 out of 12 benchmarks in the QM9 benchmark, achieving new state-of-the-art results on average.",NIPS
"We present a novel method, Contextual goal-Oriented Data Augmentation (CODA), which uses commonly available unlabeled trajectories and context-goal pairs to solve Contextual Goal-Oriented (CGO) problems. By carefully constructing an action-augmented MDP that is equivalent to the original MDP, CODA creates a fully labeled transition dataset under training contexts without additional approximation error. We conduct a novel theoretical analysis to demonstrate CODA's capability to solve CGO problems in the offline data setup. Empirical results also showcase the effectiveness of CODA, which outperforms other baseline methods across various context-goal relationships of CGO problem. This approach offers a promising direction to solving CGO problems using offline datasets.",NIPS
"This paper makes a step towards modeling the modality discrepancy in the cross-spectral re-identification task. Based on the Lambertain model, we observe that the non-linear modality discrepancy mainly comes from diverse linear transformations acting on the surface of different materials. From this view, we unify all data augmentation strategies for cross-spectral re-identification as mimicking such local linear transformations and categorize them into moderate transformation and radical transformation. By extending the observation, we propose a Random Linear Enhancement (RLE) strategy which includes Moderate Random Linear Enhancement (MRLE)  and Radical Random Linear Enhancement (RRLE)  to push the boundaries of both types of transformation. Moderate Random Linear Enhancement is designed to provide diverse image transformations that satisfy the original linear correlations under constrained conditions, whereas Radical Random Linear Enhancement seeks to generate local linear transformations directly without relying on external information. The experimental results not only demonstrate the superiority and effectiveness of RLE but also confirm its great potential as a general-purpose data augmentation for cross-spectral re-identification.",NIPS
"Dynamic scenes rendering is an intriguing yet challenging problem. Although current methods based on NeRF have achieved satisfactory performance, they still can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS) has garnered researchers' attention due to their outstanding rendering quality and real-time speed. Therefore, a new paradigm has been proposed: defining a canonical 3D gaussians and deforming it to individual frames in deformable fields. However, since the coordinates of canonical 3D gaussians are filled with noise, which can transfer noise into the deformable fields, and there is currently no method that adequately considers the aggregation of 4D information. Therefore, we propose Denoised Deformable Network with Temporal-Spatial Aggregation for Dynamic Scene Rendering (DN-4DGS). Specifically, a Noise Suppression Strategy is introduced to change the distribution of the coordinates of the canonical 3D gaussians and suppress noise. Additionally, a Decoupled Temporal-Spatial Aggregation Module is designed to aggregate information from adjacent points and frames. Extensive experiments on various real-world datasets demonstrate that our method achieves state-of-the-art rendering quality under a real-time level. Code is available at https://github.com/peoplelu/DN-4DGS.",NIPS
"The vertebrate hippocampus is thought to use recurrent connectivity in area CA3 to support episodic memory recall from partial cues. This brain area also contains place cells, whose location-selective firing fields implement maps supporting spatial memory. Here we show that place cells emerge in networks trained to remember temporally continuous sensory episodes. We model CA3 as a recurrent autoencoder that recalls and reconstructs sensory experiences from noisy and partially occluded observations by agents traversing simulated arenas. The agents move in realistic trajectories modeled from rodents and environments are modeled as continuously varying, high-dimensional, sensory experience maps (spatially smoothed Gaussian random fields). Training our autoencoder to accurately pattern-complete and reconstruct sensory experiences with a constraint on total activity causes spatially localized firing fields, i.e., place cells, to emerge in the encoding layer. The emergent place fields reproduce key aspects of hippocampal phenomenology: a) remapping (maintenance of and reversion to distinct learned maps in different environments), implemented via repositioning of experience manifolds in the networkâ€™s hidden layer, b) orthogonality of spatial representations in different arenas, c) robust place field emergence in differently shaped rooms, with single units showing multiple place fields in large or complex spaces, and (d) slow representational drift of place fields.  We argue that these results arise because continuous traversal of space makes sensory experience temporally continuous. We make testable predictions: a) rapidly changing sensory context will disrupt place fields, b) place fields will form even if recurrent connections are blocked, but reversion to previously learned representations upon remapping will be abolished, c) the dimension of temporally smooth experience sets the dimensionality of place fields, including during virtual navigation of abstract spaces.",NIPS
"Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. %In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 15,283 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data.  Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.",NIPS
"Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models exceeding human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on \datasetname, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development. The dataset is released at MathLLMs/MathVision",NIPS
"We study the constant regret guarantees in reinforcement learning (RL). Our objective is to design an algorithm that incurs only finite regret over infinite episodes with high probability. We introduce an algorithm, Cert-LSVI-UCB, for misspecified linear Markov decision processes (MDPs) where both the transition kernel and the reward function can be approximated by some linear function up to misspecification level $\zeta$. At the core of Cert-LSVI-UCB is an innovative certified estimator, which facilitates a fine-grained concentration analysis for multi-phase value-targeted regression, enabling us to establish an instance-dependent regret bound that is constant w.r.t. the number of episodes. Specifically, we demonstrate that for a linear MDP characterized by a minimal suboptimality gap $\Delta$, Cert-LSVI-UCB has a cumulative regret of $\tilde{\mathcal{O}}(d^3H^5/\Delta)$ with high probability, provided that the misspecification level $\zeta$ is below $\tilde{\mathcal{O}}(\Delta / (\sqrt{d}H^2))$. Here $d$ is the dimension of the feature space and $H$ is the horizon. Remarkably, this regret bound is independent of the number of episodes $K$. To the best of our knowledge, Cert-LSVI-UCB is the first algorithm to achieve a constant, instance-dependent, high-probability regret bound in RL with linear function approximation without relying on prior distribution assumptions.",NIPS
"The accurate identification of active sites in proteins is essential for the advancement of life sciences and pharmaceutical development, as these sites are of critical importance for enzyme activity and drug design. Recent advancements in protein language models (PLMs), trained on extensive datasets of amino acid sequences, have significantly improved our understanding of proteins. However, compared to the abundant protein sequence data, functional annotations, especially precise per-residue annotations, are scarce, which limits the performance of PLMs. On the other hand, textual descriptions of proteins, which could be annotated by human experts or a pretrained protein sequence-to-text model, provide meaningful context that could assist in the functional annotations, such as the localization of active sites. This motivates us to construct a $\textbf{ProT}$ein-$\textbf{A}$ttribute text $\textbf{D}$ataset ($\textbf{ProTAD}$), comprising over 570,000 pairs of protein sequences and multi-attribute textual descriptions. Based on this dataset, we propose $\textbf{MMSite}$, a multi-modal framework that improves the performance of PLMs to identify active sites by leveraging biomedical language models (BLMs). In particular, we incorporate manual prompting and design a MACross module to deal with the multi-attribute characteristics of textual descriptions. MMSite is a two-stage (""First Align, Then Fuse"") framework: first aligns the textual modality with the sequential modality through soft-label alignment, and then identifies active sites via multi-modal fusion. Experimental results demonstrate that MMSite achieves state-of-the-art performance compared to existing protein representation learning methods. The dataset and code implementation are available at https://github.com/Gift-OYS/MMSite.",NIPS
"Large language models are probabilistic models, and the process of generating content is essentially sampling from the output distribution of the language model. Existing watermarking techniques inject watermarks into the generated content without altering the output quality. On the other hand, existing acceleration techniques, specifically speculative sampling, leverage a draft model to speed up the sampling process while preserving the output distribution. However, there is no known method to simultaneously accelerate the sampling process and inject watermarks into the generated content. In this paper, we investigate this direction and find that the integration of watermarking and acceleration is non-trivial. We prove a no-go theorem, which states that it is impossible to simultaneously maintain the highest watermark strength and the highest sampling efficiency. Furthermore, we propose two methods that maintain either the sampling efficiency or the watermark strength, but not both. Our work provides a rigorous theoretical foundation for understanding the inherent trade-off between watermark strength and sampling efficiency in accelerating the generation of watermarked tokens for large language models. We also conduct numerical experiments to validate our theoretical findings and demonstrate the effectiveness of the proposed methods.",NIPS
"Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.",NIPS
"Closed-loop neuroscience experimentation, where recorded neural activity is used to modify the experiment on-the-fly, is critical for deducing causal connections and optimizing experimental time. Thus while new optical methods permit on-line recording (via Multi-photon calcium imaging) and stimulation (via holographic stimulation) of large neural populations, a critical barrier in creating closed-loop experiments that can target and modulate single neurons is the real-time inference of neural activity from streaming recordings. In particular, while multi-photon calcium imaging (CI) is crucial in monitoring neural populations, extracting a single neuron's activity from the fluorescence videos often requires batch processing of the video data. Without batch processing, dimmer neurons and events are harder to identify and unrecognized neurons can create false positives when computing the activity of known neurons. We solve these issues by adapting a recently proposed robust time-trace estimator---Sparse Emulation of Unused Dictionary Objects (SEUDO) algorithm---as a basis for a new on-line processing algorithm that simultaneously identifies neurons in the fluorescence video and infers their time traces in a way that is robust to as-yet unidentified neurons. To achieve real-time SEUDO (realSEUDO), we introduce a combination of new algorithmic improvements, a fast C-based implementation, and a new cell finding loop to enable realSEUDO to identify new cells on-the-fly with no ""warm-up"" period. We demonstrate comparable performance to offline algorithms (e.g., CNMF), and improved performance over the current on-line approach (OnACID) at speeds of 120 Hz on average. This speed is faster than the typical 30 Hz framerate, leaving critical computation time for the computation of feedback in a closed-loop setting.",NIPS
"This paper considers the problem of online learning with non-convex loss functions in dynamic environments. Recently, Suggala and Netrapalli [2020] demonstrated that follow the perturbed leader (FTPL) can achieve optimal regret for non-convex losses, but their results are limited to static environments. In this research, we examine dynamic environments and choose \emph{dynamic regret} and \emph{adaptive regret} to measure the performance. First, we propose an algorithm named FTPL-D by restarting FTPL periodically and establish $O(T^\frac{2}{3}(V_T+1)^\frac{1}{3})$ dynamic regret with the prior knowledge of $V_T$, which is the variation of loss functions. In the case that $V_T$ is unknown, we run multiple FTPL-D with different restarting parameters as experts and use a meta-algorithm to track the best one on the fly. To address the challenge of non-convexity, we utilize randomized sampling in the process of tracking experts. Next, we present a novel algorithm called FTPL-A that dynamically maintains a group of FTPL experts and combines them with an advanced meta-algorithm to obtain $O(\sqrt{\tau\log{T}})$ adaptive regret for any interval of length $\tau$. Moreover, we demonstrate that FTPL-A also attains an $\tilde{O}(T^\frac{2}{3}(V_T+1)^\frac{1}{3})$ dynamic regret bound. Finally, we discuss the application to online constrained meta-learning and conduct experiments to verify the effectiveness of our methods.",NIPS
"Large language models (LLMs) have demonstrated remarkable capabilities in solving diverse tasks following human instructions. However, it is challenging to develop a conversational AI assistant for electronic medical health (EHR) data because (1) there is no large-scale instruction-following dataset and (2) existing model architectures are ineffective for handling complex and heterogeneous EHR data.Our paper introduces MIMIC-Instr, a dataset comprising over 400K open-ended instruction-following data based on the MIMIC-IV EHR database. This dataset covers a broad range of topics and can be used to instruction-tune general-purpose LLMs for diverse clinical use cases. Additionally, we propose Llemr, a general framework designed to empower LLMs to process and interpret EHRs with complex data schemas effectively. Llemr exhibits competitive capabilities in answering diverse patient-related based on EHR data.Furthermore, our evaluations on clinical predictive modeling benchmarks show that the fine-tuned Llemr can match the performance of state-of-the-art (SOTA) baselines with curated features.",NIPS
"Surveys have recently gained popularity as a tool to study large language models. By comparing modelsâ€™ survey responses to those of different human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine language models' survey responses on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter â€œAâ€. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or training data. As a result, models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for the survey under consideration, leading to potentially misguided conclusions about model alignment.",NIPS
"Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights $\(\mathbf{W}\)$ and inject learnable matrices $\(\mathbf{\Delta W}\)$. These $\(\mathbf{\Delta W}\)$ matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically exhibit a performance gap compared to full fine-tuning. While recent PEFT methods have narrowed this gap, they do so at the expense of additional learnable parameters. We propose SVFT, a *simple* approach that structures $\(\mathbf{\Delta W}\)$ based on the specific weight matrix $\(\mathbf{W}\)$. SVFT updates $\(\mathbf{W}\)$ as a sparse combination $\(M\)$ of outer products of its singular vectors, training only the coefficients of these combinations. Crucially, we make additional off-diagonal elements in $M$ learnable, enabling a smooth trade-off between trainable parameters and expressivityâ€”an aspect that distinctly sets our approach apart from previous works leveraging singular values. Extensive experiments on language and vision benchmarks show that SVFT recovers up to **96%** of full fine-tuning performance while training only **0.006 to 0.25%** of parameters, outperforming existing methods that achieve only up to **{85\%}** performance with **0.03 to 0.8%** of the trainable parameter budget.",NIPS
"We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling. Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods. We derive GaussianCube by first using a novel densification-constrained Gaussian fitting algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians, and then rearranging these Gaussians into a predefined voxel grid via Optimal Transport. Since GaussianCube is a structured grid representation, it allows us to use standard 3D U-Net as our backbone in diffusion modeling without elaborate designs. More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude. The compactness of GaussianCube greatly eases the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling.",NIPS
"We study the problem of differentially private (DP) mechanisms for representingsets of size $k$ from a large universe.Our first construction creates$(\epsilon,\delta)$-DP representations with error probability of $1/(e^\epsilon + 1)$ using space at most $1.05 k \epsilon \cdot \log(e)$ bits wherethe time to construct a representation is $O(k \log(1/\delta))$ while decoding time is $O(\log(1/\delta))$.We also present a second algorithm for pure $\epsilon$-DP representations with the same error using space at most $k \epsilon \cdot \log(e)$ bits, but requiring large decoding times.Our algorithms match the lower bounds on privacy-utility trade-offs (including constants but ignoring $\delta$ factors) and we also present a new space lower boundmatching our constructions up to small constant factors.To obtain our results, we design a new approach embedding sets into random linear systemsdeviating from most prior approaches that inject noise into non-private solutions.",NIPS
"In this work, we propose Asynchronous Perception Machine (APM), a computationally-efficient architecture for test-time-training (TTT). APM can process patches of an image one at a time in any order asymmetrically, and still encode semantic-awareness in the net. We demonstrate APMâ€™s ability to recognize out-of-distribution images without dataset-specific pre-training, augmentation orany-pretext task. APM offers competitive performance over existing TTT approaches. To perform TTT, APM just distills test sampleâ€™s representation once. APM possesses a unique property: it can learn using just this single representation and starts predicting semantically-aware features. APMâ€™s ability to recover semantic information from a global CLS token validates the insight that CLStokens encode geometric-information of a given scene and can be recovered using appropriate inductive-biases. This offers a novel-insight with consequences for representational-learning. APM demostrates potential applications beyond test-time-training: APM can scale up to a dataset of 2D images and yield semantic-clusterings in a single forward pass. APM also provides first empirical evidence towards validating Hinton at Alâ€™s GLOMâ€™s insight, i.e. if input percept is a field. Therefore, APM helps our community converge towards an implementation which can do both interpolation and perception on a shared-connectionist hardware. Ourcodebase has been made available at https://rajatmodi62.github.io/apmprojectpage/--------It now appears that some of the ideas in GLOM could be made to work.https://www.technologyreview.com/2021/04/16/1021871/geoffrey-hinton-glom-godfather-ai-neural-networks/.-""""""""""""-. .'          './   O      O   \|      O       | \  '------'  /  '.        .'    '-....-'A silent man in deep-contemplation.Silent man emerges only sometimes.And he loves all.",NIPS
"This study presents an Exploratory Retrieval-Augmented Planning (ExRAP) framework, designed to tackle continual instruction following tasks of embodied agents in dynamic, non-stationary environments. The framework enhances Large Language Models' (LLMs) embodied reasoning capabilities by efficiently exploring the physical environment and establishing the environmental context memory, thereby effectively grounding the task planning process in time-varying environment contexts. In ExRAP, given multiple continual instruction following tasks, each instruction is decomposed into queries on the environmental context memory and task executions conditioned on the query results. To efficiently handle these multiple tasks that are performed continuously and simultaneously, we implement an exploration-integrated task planning scheme by incorporating the information-based exploration into the LLM-based planning process. Combined with memory-augmented query evaluation, this integrated scheme not only allows for a better balance between the validity of the environmental context memory and the load of environment exploration, but also improves overall task performance. Furthermore, we devise a temporal consistency refinement scheme for query evaluation to address the inherent decay of knowledge in the memory. Through experiments with VirtualHome, ALFRED, and CARLA, our approach demonstrates robustness against a variety of embodied instruction following scenarios involving different instruction scales and types, and non-stationarity degrees, and it consistently outperforms other state-of-the-art LLM-based task planning approaches in terms of both goal success rate and execution efficiency.",NIPS
"Deep Learning models have taken the front stage in the AI community, yet explainability challenges hinder their widespread adoption. Time series models, in particular, lack attention in this regard. This study tries to reproduce and extend the work of Enguehard (2023b), focusing on time series explainability by incorporating learnable masks and perturbations. Enguehard (2023b) employed two methods to learn these masks and perturbations, the preservation game (yielding SOTA results) and the deletion game (with poor performance). We extend the work by revising the deletion gameâ€™s loss function, testing the robustness of the proposed method on a novel weather dataset, and visualizing the learned masks and perturbations. Despite notable discrepancies in results across many experiments, our findings demonstrate that the proposed method consistently outperforms all baselines and exhibits robust performance across datasets. However, visualizations for the preservation game reveal that the learned perturbations primarily resemble a constant zero signal, questioning the importance of learning perturbations. Nevertheless, our revised deletion game shows promise, recovering meaningful perturbations and, in certain instances, surpassing the performance of the preservation game.",NIPS
"Large language models (LLMs) have demonstrated impressive capabilities across diverse languages. This study explores how LLMs handle multilingualism. Based on observed language ratio shifts among layers and the relationships between network structures and certain capabilities, we hypothesize the LLM's multilingual workflow ($\texttt{MWork}$): LLMs initially understand the query, converting multilingual inputs into English for task-solving. In the intermediate layers, they employ English for thinking and incorporate multilingual knowledge with self-attention and feed-forward structures, respectively. In the final layers, LLMs generate responses aligned with the original language of the query. To verify $\texttt{MWork}$, we introduce Parallel Language-specific Neuron Detection ($\texttt{PLND}$) to identify activated neurons for inputs in different languages without any labeled data. Using $\texttt{PLND}$, we validate $\texttt{MWork}$ through extensive experiments involving the deactivation of language-specific neurons across various layers and structures. Moreover, $\texttt{MWork}$ allows fine-tuning of language-specific neurons with a small dataset, enhancing multilingual abilities in a specific language without compromising others. This approach results in an average improvement of $3.6\%$ for high-resource languages and $2.3\%$ for low-resource languages across all tasks with just $400$ documents.",NIPS
"Dataset Distillation (DD) is designed to generate condensed representations of extensive image datasets, enhancing training efficiency. Despite recent advances, there remains considerable potential for improvement, particularly in addressing the notable redundancy within the color space of distilled images. In this paper, we propose a two-fold optimization strategy to minimize color redundancy at the individual image and overall dataset levels, respectively. At the image level, we employ a palette network, a specialized neural network, to dynamically allocate colors from a reduced color space to each pixel. The palette network identifies essential areas in synthetic images for model training, and consequently assigns more unique colors to them. At the dataset level, we develop a color-guided initialization strategy to minimize redundancy among images. Representative images with the least replicated color patterns are selected based on the information gain. A comprehensive performance study involving various datasets and evaluation scenarios is conducted, demonstrating the superior performance of our proposed color-aware DD compared to existing DD methods.",NIPS
"The attention mechanism within the transformer architecture enables the model to weigh and combine tokens based on their relevance to the query. While self-attention has enjoyed major success, it notably treats all queries $q$ in the same way by applying the mapping $V^\top\text{softmax}(Kq)$, where $V,K$ are the value and key embeddings respectively. In this work, we argue that this uniform treatment hinders the ability to control contextual sparsity and relevance. As a solution, we introduce the Selective Self-Attention (SSA) layer that augments the softmax nonlinearity with a principled temperature scaling strategy. By controlling temperature, SSA adapts the contextual sparsity of the attention map to the query embedding and its position in the context window. Through theory and experiments, we demonstrate that this alleviates attention dilution, aids the optimization process, and enhances the model's ability to control softmax spikiness of individual queries. We also incorporate temperature scaling for value embeddings and show that it boosts the model's ability to suppress irrelevant/noisy tokens. Notably, SSA is a lightweight method which introduces less than 0.5\% new parameters through a weight-sharing strategy and can be fine-tuned on existing LLMs. Extensive empirical evaluations demonstrate that SSA-equipped models achieve a noticeable and consistent accuracy improvement on language modeling benchmarks.",NIPS
"The Fisher information matrix can be used to characterize the local geometry ofthe parameter space of neural networks. It elucidates insightful theories anduseful tools to understand and optimize neural networks. Given its highcomputational cost, practitioners often use random estimators and evaluate onlythe diagonal entries. We examine two popular estimators whose accuracy and samplecomplexity depend on their associated variances. We derive bounds of thevariances and instantiate them in neural networks for regression andclassification. We navigate trade-offs for both estimators based on analyticaland numerical studies. We find that the variance quantities depend on thenon-linearity w.r.t. different parameter groups and should not be neglected whenestimating the Fisher information.",NIPS
"Adapting to dynamic data distributions is a practical yet challenging task. One effective strategy is to use a model ensemble, which leverages the diverse expertise of different models to transfer knowledge to evolving data distributions. However, this approach faces difficulties when the dynamic test distribution is available only in small batches and without access to the original source data. To address the challenge of adapting to dynamic distributions in such practical settings, we propose continual multi-source adaptation to dynamic distributions (CONTRAST), a novel method that optimally combines multiple source models to adapt to the dynamic test data. CONTRAST has two distinguishing features. First, it efficiently computes the optimal combination weights to combine the source models to adapt to the test data distribution continuously as a function of time. Second, it identifies which of the source model parameters to update so that only the model which is most correlated to the target data is adapted, leaving the less correlated ones untouched; this mitigates the issue of ``forgetting"" the source model parameters by focusing only on the source model that exhibits the strongest correlation with the test batch distribution. Through theoretical analysis we show that the proposed method is able to optimally combine the source models and prioritize updates to the model least prone to forgetting. Experimental analysis on diverse datasets demonstrates that the combination of multiple source models does at least as well as the best source (with hindsight knowledge), and performance does not degrade as the test data distribution changes over time (robust to forgetting).",NIPS
"Existing human rendering methods require every part of the human to be fully visible throughout the input video. However, this assumption does not hold in real-life settings where obstructions are common, resulting in only partial visibility of the human. Considering this, we present OccFusion, an approach that utilizes efficient 3D Gaussian splatting supervised by pretrained 2D diffusion models for efficient and high-fidelity human rendering. We propose a pipeline consisting of three stages. In the Initialization stage, complete human masks are generated from partial visibility masks. In the Optimization stage, 3D human Gaussians are optimized with additional supervisions by Score-Distillation Sampling (SDS) to create a complete geometry of the human. Finally, in the Refinement stage, in-context inpainting is designed to further improve rendering quality on the less observed human body parts. We evaluate OccFusion on ZJU-MoCap and challenging OcMotion sequences and found that it achieves state-of-the-art performance in the rendering of occluded humans.",NIPS
"Masked generative models (MGMs) have shown impressive generative ability while providing an order of magnitude efficient sampling steps compared to continuous diffusion models. However, MGMs still underperform in image synthesis compared to recent well-developed continuous diffusion models with similar size in terms of quality and diversity of generated samples. A key factor in the performance of continuous diffusion models stems from the guidance methods, which enhance the sample quality at the expense of diversity. In this paper, we extend these guidance methods to generalized guidance formulation for MGMs and propose a self-guidance sampling method, which leads to better generation quality. The proposed approach leverages an auxiliary task for semantic smoothing in vector-quantized token space, analogous to the Gaussian blur in continuous pixel space. Equipped with the parameter-efficient fine-tuning method and high-temperature sampling, MGMs with the proposed self-guidance achieve a superior quality-diversity trade-off, outperforming existing sampling methods in MGMs with more efficient training and sampling costs. Extensive experiments with the various sampling hyperparameters confirm the effectiveness of the proposed self-guidance.",NIPS
"Tiny machine learning (TinyML) aims to run ML models on small devices and is increasingly favored for its enhanced privacy, reduced latency, and low cost. Recently, the advent of tiny AI accelerators has revolutionized the TinyML field by significantly enhancing hardware processing power. These accelerators, equipped with multiple parallel processors and dedicated per-processor memory instances, offer substantial performance improvements over traditional microcontroller units (MCUs). However, their limited data memory often necessitates downsampling input images, resulting in accuracy degradation. To address this challenge, we propose Data channel EXtension (DEX), a novel approach for efficient CNN execution on tiny AI accelerators. DEX incorporates additional spatial information from original images into input images through patch-wise even sampling and channel-wise stacking, effectively extending data across input channels. By leveraging underutilized processors and data memory for channel extension, DEX facilitates parallel execution without increasing inference latency. Our evaluation with four models and four datasets on tiny AI accelerators demonstrates that this simple idea improves accuracy on average by 3.5%p while keeping the inference latency the same on the AI accelerator. The source code is available at https://github.com/Nokia-Bell-Labs/data-channel-extension.",NIPS
"We consider metrical task systems on general metric spaces with $n$ points, and show that any fully randomized algorithm can be turned into a randomized algorithm that uses only $2\log n$ random bits, and achieves the same competitive ratio up to a factor $2$. This provides the first order-optimal barely random algorithms for metrical task systems, i.e. which use a number of random bits that does not depend on the number of requests addressed to the system. We discuss implications on various aspects of online decision making such as: distributed systems, advice complexity and transaction costs, suggesting broad applicability. We put forward an equivalent view that we call collective metrical task systems where $k$ agents in a metrical task system team up, and suffer the average cost paid by each agent. Our results imply that such team can be $O(\log^2 n)$-competitive as soon as $k\geq n^2$. In comparison, a single agent is always $\Omega(n)$-competitive.",NIPS
"With the widespread adoption of Large Language Models (LLMs), the prevalence of iterative interactions among these models is anticipated to increase. Notably, recent advancements in multi-round on-policy self-improving methods allow LLMs to generate new examples for training subsequent models. At the same time, multi-agent LLM systems, involving automated interactions among agents, are also increasing in prominence. Thus, in both short and long terms, LLMs may actively engage in an evolutionary process. We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades. Our approach involves leveraging Iterated Learning (IL), a Bayesian framework that elucidates how subtle biases are magnified during human cultural evolution, to explain some behaviors of LLMs. This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs. This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions.",NIPS
"Vanilla pixel-level classifiers for semantic segmentation are based on a certain paradigm, involving the inner product of fixed prototypes obtained from the training set and pixel features in the test image.  This approach, however, encounters significant limitations, i.e., feature deviation in the semantic domain and information loss in the spatial domain. The former struggles with large intra-class variance among pixel features from different images, while the latter fails to utilize the structured information of semantic objects effectively.  This leads to blurred mask boundaries as well as a deficiency of fine-grained recognition capability.  In this paper, we propose a novel Semantic and Spatial Adaptive Classifier (SSA-Seg) to address the above challenges.  Specifically, we employ the coarse masks obtained from the fixed prototypes as a guide to adjust the fixed prototype towards the center of the semantic and spatial domains in the test image.  The adapted prototypes in semantic and spatial domains are then simultaneously considered to accomplish classification decisions.  In addition, we propose an online multi-domain distillation learning strategy to improve the adaption process.  Experimental results on three publicly available benchmarks show that the proposed SSA-Seg significantly improves the segmentation performance of the baseline models with only a minimal increase in computational cost.",NIPS
"Graph Transformers excel in long-range dependency modeling, but generally require quadratic memory complexity in the number of nodes in an input graph, and hence have trouble scaling to large graphs. Sparse attention variants such as Exphormer can help, but may require high-degree augmentations to the input graph for good performance, and do not attempt to sparsify an already-dense input graph. As the learned attention mechanisms tend to use few of these edges, however, such high-degree connections may be unnecessary. We show (empirically and with theoretical backing) that attention scores on graphs are usually quite consistent across network widths, and use this observation to propose a two-stage procedure, which we call Spexphormer: first, train a narrow network on the full augmented graph. Next, use only the active connections to train a wider network on a much sparser graph. We establish theoretical conditions when a narrow network's attention scores can match those of a wide network, and show that Spexphormer achieves good performance with drastically reduced memory requirements on various graph datasets.",NIPS
"Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. \name can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment on a wide range of math tasks (including geometry, functions, graph, chess) and complex visual reasoning tasks. Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%). We will release all code and data.",NIPS
"It is widely acknowledged that large language models (LLMs) encode a vast reservoir of knowledge after being trained on mass data. Recent studies disclose knowledge conflicts in LLM generation, wherein outdated or incorrect parametric knowledge (i.e., encoded knowledge) contradicts new knowledge provided in the context. To mitigate such knowledge conflicts, we propose a novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to capitalize on neurons that are crucial in processing contextual cues. Specifically, IRCAN first identifies neurons that significantly contribute to context processing, utilizing a context-aware attribution score derived from integrated gradients. Subsequently, the identified context-aware neurons are strengthened via reweighting. In doing so, we steer LLMs to generate context-sensitive outputs with respect to the new knowledge provided in the context. Extensive experiments conducted across a variety of models and tasks demonstrate that IRCAN not only achieves remarkable improvements in handling knowledge conflicts but also offers a scalable, plug-and-play solution that can be integrated seamlessly with existing models. Our codes are released at https://github.com/danshi777/IRCAN.",NIPS
"We aim to understand the optimal PAC sample complexity in multiclass learning. While finiteness of the Daniely-Shalev-Shwartz (DS) dimension has been shown to characterize the PAC learnability of a concept class [Brukhim, Carmon, Dinur, Moran, and Yehudayoff, 2022], there exist polylog factor gaps in the leading term of the sample complexity. In this paper, we reduce the gap in terms of the dependence on the error parameter to a single log factor and also propose two possible routes towards completely resolving the optimal sample complexity, each based on a key open question we formulate: one concerning list learning with bounded list size, the other concerning a new type of shifting for multiclass concept classes. We prove that a positive answer to either of the two questions would completely resolve the optimal sample complexity up to log factors of the DS dimension.",NIPS
"Optimizing neural networks with loss that contain high-dimensional and high-order differential operators  is expensive to evaluate with back-propagation due to $\mathcal{O}(d^{k})$ scaling of the derivative tensor size and the $\mathcal{O}(2^{k-1}L)$ scaling in the computation graph, where $d$ is the dimension of the domain, $L$ is the number of ops in the forward computation graph, and $k$ is the derivative order. In previous works, the polynomial scaling in $d$ was addressed by amortizing the computation over the optimization process via randomization. Separately, the exponential scaling in $k$ for univariate functions ($d=1$) was addressed with high-order auto-differentiation (AD). In this work, we show how to efficiently perform arbitrary contraction of the derivative tensor of arbitrary order for multivariate functions, by properly constructing the input tangents to univariate high-order AD, which can be used to efficiently randomize any differential operator.  When applied to Physics-Informed Neural Networks (PINNs), our method provides >1000$\times$ speed-up and >30$\times$ memory reduction over randomization with first-order AD, and we can now solve 1-million-dimensional PDEs in 8 minutes on a single NVIDIA A100 GPU. This work opens the possibility of using high-order differential operators in large-scale problems.",NIPS
"Despite the effectiveness of data selection for pretraining and instruction fine-tuninglarge language models (LLMs), improving data efficiency in supervised fine-tuning(SFT) for specialized domains poses significant challenges due to the complexityof fine-tuning data. To bridge this gap, we introduce an effective and scalabledata selection method for SFT, SmallToLarge (S2L), which trains a smallmodel, clusters loss trajectories of the examples, and samples from these clusters toguide data selection for larger models. We prove that during fine-tuning, sampleswithin the same loss trajectory cluster exhibit similar gradients. Then, we showthat S2L subsets have a bounded gradient error w.r.t. the full data, hence guaranteeconvergence to the neighborhood of the optimal solution. We demonstrate throughextensive experiments that S2L significantly improves data efficiency in SFT formathematical problem-solving, reducing the training data requirement to just $11$%of the original MathInstruct dataset to match full dataset performance whileoutperforming state-of-the-art data selection algorithms by an average of $4.7$%across $6$ in- and out-domain evaluation datasets. Remarkably, selecting only 50Kdata for SFT, S2L achieves a $32.7$% accuracy on the challenging MATHbenchmark, improving Phi-2 by $16.6$%. In clinical text summarization on theMIMIC-III dataset, S2L again outperforms training on the full dataset usingonly $50$% of the data. Notably, S2L can perform scalable data selection using areference model $100\times$ smaller than the target model, proportionally reducing thecomputational cost.",NIPS
"We propose a novel method, \textbf{TwinAct}, to tackle the challenge of decoupling actions and actors in order to customize the text-guided diffusion models (TGDMs) for few-shot action image generation. TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (e.g., the actor's appearance) due to the lack of an effective inductive bias with few exemplar images. Our approach introduces a common action space, which is a textual embedding space focused solely on actions, enabling precise customization without actor-related details. Specifically, TwinAct involves three key steps: 1) Building common action space based on a set of representative action phrases; 2) Imitating the customized action within the action space; and 3) Generating highly adaptable customized action images in diverse contexts with action similarity loss. To comprehensively evaluate TwinAct, we construct a novel benchmark, which provides sample images with various forms of actions. Extensive experiments demonstrate TwinAct's superiority in generating accurate, context-independent customized actions while maintaining the identity consistency of different subjects, including animals, humans, and even customized actors.",NIPS
"Can we modify the training data distribution to encourage the underlying optimization method toward finding solutions with superior generalization performance on in-distribution data? In this work, we approach this question for the first time by comparing the inductive bias of gradient descent (GD) with that of sharpness-aware minimization (SAM). By studying a two-layer CNN, we rigorously prove that SAM learns different features more uniformly, particularly in early epochs. That is, SAM is less susceptible to simplicity bias compared to GD. We also show that examples constraining features that are learned early are separable from the rest based on the modelâ€™s output. Based on this observation, we propose a method that (i) clusters examples based on the network output early in training, (ii) identifies a cluster of examples with similar network output, and (iii) upsamples the rest of examples only once to alleviate the simplicity bias. We show empirically that USEFUL effectively improves the generalization performance on the original data distribution when training with various gradient methods, including (S)GD and SAM. Notably, we demonstrate that our method can be combined with SAM variants and existing data augmentation strategies to achieve, to the best of our knowledge, state-of-the-art performance for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10.",NIPS
"Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to equal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial attack methods, leading to acceleration of $1.8\times$ for AutoPrompt, $2.4\times$ for APE and $2.4\times$ for AutoDAN.",NIPS
"Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity. We conduct extensive experiments and find that under the same setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech. We comprehensively analyze why the boundaries learned by REBORN improve the unsupervised ASR performance.",NIPS
"Understanding human emotions is fundamental to enhancing human-computer interaction, especially for embodied agents that mimic human behavior.  Traditional emotion analysis often takes a third-person perspective, limiting the ability of agents to interact naturally and empathetically.  To address this gap, this paper presents $E^3$ for Exploring Embodied Emotion, the first massive first-person view video dataset. $E^3$ contains more than $50$ hours of video, capturing $8$ different emotion types in diverse scenarios and languages. The dataset features videos recorded by individuals in their daily lives, capturing a wide range of real-world emotions conveyed through visual, acoustic, and textual modalities. By leveraging this dataset, we define $4$ core benchmark tasks - emotion recognition, emotion classification, emotion localization, and emotion reasoning - supported by more than $80$k manually crafted annotations, providing a comprehensive resource for training and evaluating emotion analysis models. We further present Emotion-LlaMa, which complements visual modality with acoustic modality to enhance the understanding of emotion in first-person videos. The results of comparison experiments with a large number of baselines demonstrate the superiority of Emotion-LlaMa and set a new benchmark for embodied emotion analysis. We expect that $E^3$ can promote advances in multimodal understanding, robotics, and augmented reality, and provide a solid foundation for the development of more empathetic and context-aware embodied agents.",NIPS
"Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.",NIPS
"Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a modelâ€™s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a userâ€™s preferred response length (recall).Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotatorsâ€”on a set ofâˆ¼16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators.  We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.",NIPS
"Generative AI has revolutionised visual content editing, empowering users to effortlessly modify images and videos. However, not all edits are equal. To perform realistic edits in domains such as natural image or medical imaging, modifications must respect causal relationships inherent to the data generation process. Such image editing falls into the counterfactual image generation regime. Evaluating counterfactual image generation is substantially complex: not only it lacks observable ground truths, but also requires adherence to causal constraints. Although several counterfactual image generation methods and evaluation metrics exist a comprehensive comparison within a unified setting is lacking. We present a comparison framework to thoroughly benchmark counterfactual image generation methods. We evaluate the performance of three conditional image generation model families developed within the Structural Causal Model (SCM) framework. We incorporate several metrics that assess diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We integrate all models that have been used for the task at hand and expand them to novel datasets and causal graphs, demonstrating the superiority of Hierarchical VAEs across most datasets and metrics. Our framework is implemented in a user-friendly Python package that can be extended to incorporate additional SCMs, causal methods, generative models, and datasets for the community to build on.",NIPS
"Bi-level optimizaiton (BO) has become a fundamental mathematical framework for addressing hierarchical machine learning problems.As deep learning models continue to grow in size, the demand for scalable bi-level optimization has become increasingly critical.Traditional gradient-based bi-level optimizaiton algorithms, due to their inherent characteristics, are ill-suited to meet the demands of large-scale applications.In this paper, we introduce **F**orward **G**radient **U**nrolling with **F**orward **G**radient, abbreviated as **$($FG$)^2$U**, which achieves an unbiased stochastic approximation of the meta gradient for bi-level optimizaiton.$($FG$)^2$U circumvents the memory and approximation issues associated with classical bi-level optimizaiton approaches, and delivers significantly more accurate gradient estimates than existing large-scale bi-level optimizaiton approaches.Additionally, $($FG$)^2$U is inherently designed to support parallel computing, enabling it to effectively leverage large-scale distributed computing systems to achieve significant computational efficiency.In practice, $($FG$)^2$U and other methods can be strategically placed at different stages of the training process to achieve a more cost-effective two-phase paradigm.Further, $($FG$)^2$U is easy to implement within popular deep learning frameworks, and can be conveniently adapted to address more challenging zeroth-order bi-level optimizaiton scenarios.We provide a thorough convergence analysis and a comprehensive practical discussion for $($FG$)^2$U, complemented by extensive empirical evaluations, showcasing its superior performance in diverse large-scale bi-level optimizaiton tasks.",NIPS
"This paper pertains to an emerging machine learning paradigm: learning higher- order functions, i.e. functions whose inputs are functions themselves, particularly when these inputs are Neural Networks (NNs). With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure ofNNs. However, are these the sole symmetries present in NN parameterizations? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as scaling symmetries, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose Scale Equivariant Graph MetaNetworks - ScaleGMNs, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing. The source code is publicly available at https://github.com/jkalogero/scalegmn.",NIPS
"Analyzing inter-individual variability of physiological functions is particularly appealing in medical and biological contexts to describe or quantify health conditions. Such analysis can be done by comparing individuals to a reference one with time series as biomedical data.This paper introduces an unsupervised representation learning (URL) algorithm for time series tailored to inter-individual studies. The idea is to represent time series as deformations of a reference time series. The deformations are diffeomorphisms parameterized and learned by our method called TS-LDDMM. Once the deformations and the reference time series are learned, the vector representations of individual time series are given by the parametrization of their corresponding deformation. At the crossroads between URL for time series and shape analysis, the proposed algorithm handles irregularly sampled multivariate time series of variable lengths and provides shape-based representations of temporal data.In this work, we establish a representation theorem for the graph of a time series and derive its consequences on the LDDMM framework. We showcase the advantages of our representation compared to existing methods using synthetic data and real-world examples motivated by biomedical applications.",NIPS
"We study the convergence rate of first-order methods for rectangular matrix factorization, which is a canonical nonconvex optimization problem. Specifically, given a rank-$r$ matrix $\mathbf{A}\in\mathbb{R}^{m\times n}$, we prove that gradient descent (GD) can find a pair of $\epsilon$-optimal solutions $\mathbf{X}_T\in\mathbb{R}^{m\times d}$ and $\mathbf{Y}_T\in\mathbb{R}^{n\times d}$, where $d\geq r$, satisfying $\lVert\mathbf{X}_T\mathbf{Y}_T^\top-\mathbf{A}\rVert_F\leq\epsilon\lVert\mathbf{A}\rVert_F$ in $T=O(\kappa^2\log\frac{1}{\epsilon})$ iterations with high probability, where $\kappa$ denotes the condition number of $\mathbf{A}$. Furthermore, we prove that Nesterov's accelerated gradient (NAG) attains an iteration complexity of $O(\kappa\log\frac{1}{\epsilon})$, which is the best-known bound of first-order methods for rectangular matrix factorization. Different from small balanced random initialization in the existing literature, we adopt an unbalanced initialization, where $\mathbf{X}_0$ is large and $\mathbf{Y}_0$ is $0$. Moreover, our initialization and analysis can be further extended to linear neural networks, where we prove that NAG can also attain an accelerated linear convergence rate. In particular, we only require the width of the network to be greater than or equal to the rank of the output label matrix. In contrast, previous results achieving the same rate require excessive widths that additionally depend on the condition number and the rank of the input data matrix.",NIPS
"Privacy issue is a main concern in developing face recognition techniques. Although synthetic face images can partially mitigate potential legal risks while maintaining effective face recognition (FR) performance, FR models trained by face images synthesized by existing generative approaches frequently suffer from performance degradation problems due to the insufficient discriminative quality of these synthesized samples. In this paper, we systematically investigate what contributes to solid face recognition model training, and reveal that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models. Inspired by this, we propose a novel diffusion-based approach (namely Center-based Semi-hard Synthetic FaceGeneration (CemiFace) which produces facial samples with various levels of similarity to the subject center, thus allowing to generate face datasets containing effective discriminative samples for training face recognition. Experimental results show that with a modest degree of similarity, training on the generated dataset can produce competitive performance compared to previous generation methods. The code will be available at:https://github.com/szlbiubiubiu/CemiFace",NIPS
"Accurately restoring topology is both challenging and crucial in tubular structure extraction tasks, such as blood vessel segmentation and road network extraction. Diverging from traditional approaches based on pixel-level classification, our proposed method, named GraphMorph, focuses on branch-level features of tubular structures to achieve more topologically accurate predictions. GraphMorph comprises two main components: a Graph Decoder and a Morph Module. Utilizing multi-scale features extracted from an image patch by the segmentation network, the Graph Decoder facilitates the learning of branch-level features and generates a graph that accurately represents the tubular structure in this patch. The Morph Module processes two primary inputs: the graph and the centerline probability map, provided by the Graph Decoder and the segmentation network, respectively. Employing a novel SkeletonDijkstra algorithm, the Morph Module produces a centerline mask that aligns with the predicted graph. Furthermore, we observe that employing centerline masks predicted by GraphMorph significantly reduces false positives in the segmentation task, which is achieved by a simple yet effective post-processing strategy. The efficacy of our method in the centerline extraction and segmentation tasks has been substantiated through experimental evaluations across various datasets. Source code will be released soon.",NIPS
"We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates  -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provideour code for evaluation at https://github.com/amackenzie1/highline2024.",NIPS
"We propose a new comprehensive benchmark to revolutionize the current deepfake detection field to the next generation.Predominantly, existing works identify top-notch detection algorithms and models by adhering to the common practice: training detectors on one specific dataset (e.g., FF++) and testing them on other prevalent deepfake datasets.This protocol is often regarded as a ""golden compass"" for navigating SoTA detectors.But can these stand-out ""winners"" be truly applied to tackle the myriad of realistic and diverse deepfakes lurking in the real world?If not, what underlying factors contribute to this gap?In this work, we found the dataset (both train and test) can be the ""primary culprit"" due to the following:(1) forgery diversity: Deepfake techniques are commonly referred to as both face forgery (face-swapping and face-reenactment) and entire image synthesis (AIGC, especially face). Most existing datasets only contain partial types of them, with limited forgery methods implemented (e.g., 2 swapping and 2 reenactment methods in FF++);(2) forgery realism: The dominated training dataset, FF++, contains out-of-date forgery techniques from the past four years. ""Honing skills"" on these forgeries makes it difficult to guarantee effective detection generalization toward nowadays' SoTA deepfakes;(3) evaluation protocol: Most detection works perform evaluations on one type, e.g., face-swapping types only, which hinders the development of universal deepfake detectors.To address this dilemma, we construct a highly diverse and large-scale deepfake detection dataset called DF40,  which comprises 40 distinct deepfake techniques (10 times larger than FF++).We then conduct comprehensive evaluations using 4 standard evaluation protocols and 7 representative detection methods, resulting in over 2,000 evaluations.Through these evaluations, we provide an extensive analysis from various perspectives, leading to 8 new insightful findings contributing to the field.We also open up 3 valuable yet previously underexplored research questions to inspire future works.",NIPS
"In this paper, we study learning-augmented algorithms for the Bahncard problem. The Bahncard problem is a generalization of the ski-rental problem, where a traveler needs to irrevocably and repeatedly decide between a cheap short-term solution and an expensive long-term one with an unknown future. Even though the problem is canonical, only a primal-dual-based learning-augmented algorithm was explicitly designed for it. We develop a new learning-augmented algorithm, named PFSUM, that incorporates both history and short-term future to improve online decision making. We derive the competitive ratio of PFSUM as a function of the prediction error and conduct extensive experiments to show that PFSUM outperforms the primal-dual-based algorithm.",NIPS
"Multivariate time series forecasting plays a crucial role in various fields such as finance, traffic management, energy, and healthcare.  Recent studies have highlighted the advantages of channel independence to resist distribution drift but neglect channel correlations, limiting further enhancements. Several methods utilize mechanisms like attention or mixer to address this by capturing channel correlations, but they either introduce excessive complexity or rely too heavily on the correlation to achieve satisfactory results under distribution drifts, particularly with a large number of channels. Addressing this gap, this paper presents an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS), which incorporates a novel STar Aggregate-Redistribute (STAR) module. Unlike traditional approaches that manage channel interactions through distributed structures, \textit{e.g.}, attention, STAR employs a centralized strategy to improve efficiency and reduce reliance on the quality of each channel. It aggregates all series to form a global core representation, which is then dispatched and fused with individual series representations to facilitate channel interactions effectively. SOFTS achieves superior performance over existing state-of-the-art methods with only linear complexity. The broad applicability of the STAR module across different forecasting models is also demonstrated empirically. We have made our code publicly available at https://github.com/Secilia-Cxy/SOFTS.",NIPS
"This paper seeks to reproduce and extend the results of the paper â€œExplaining Temporal Graph Models Through an Explorer-Navigator Frameworkâ€ by (Xia et al., 2023). The main contribution of the original authors is a novel explainer for temporal graph networks, the Temporal GNN Explainer (T-GNNExplainer), which finds a subset of preceding events that â€œexplainâ€ a prediction made by a temporal graph model. The explorer is tested on two temporal graph models that are trained on two real-world and two synthetic datasets. The explorer is evaluated using a newly proposed metric for explanatory graph models. The authors compare the performance of their explorer to three baseline explainer methods, either adapted from a GNN explainer or developed by the authors. The authors claim that T-GNNExplainer achieves superior performance compared to the baselines when evaluated with their proposed metric. This work reproduces the original experiments by using the code (with minor adjustments), model specifications, and hyperparameters provided by the original authors. To evaluate the robustness of these claims, the method was extended to one new dataset (MOOC). Results show that the T-GNNexplainer performs best on some, but not all metrics as reported in the original findings. We conclude that the main lines of this paper hold up even though all results are less pronounced than claimed. Results show that the T-GNNExplainer does not perform similarly across different T-GNN models, precise dataset specifications are needed to obtain high performance, and there are simpler, less computationally costly explainer methods (like PBONE) that could offer competitive results.",NIPS
"We consider the problem of estimating a structured multivariate density, subject to Markov conditions implied by an undirected graph. In the worst case, without Markovian assumptions, this problem suffers from the curse of dimensionality. Our main result shows how the curse of dimensionality can be avoided or greatly alleviated under the Markov property, and applies to arbitrary graphs. While existing results along these lines focus on sparsity or manifold assumptions, we introduce a new graphical quantity called ``graph resilience'' and show that it dictates the optimal sample complexity. Surprisingly, although one might expect the sample complexity of this problem to scale with local graph parameters such as the degree, this turns out not to be the case. Through explicit examples, we compute uniform deviation bounds and illustrate how the curse of dimensionality in density estimation can thus be circumvented. Notable examples where the rate improves substantially include sequential, hierarchical, and spatial data.",NIPS
"Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length ($\gg4K$) and struggle to effectively utilize information from the middle part of the context. To address these issues, we propose $\textbf{C}$ontinuity-$\textbf{R}$elativity ind$\textbf{E}$xing with g$\textbf{A}$ussian $\textbf{M}$iddle ($\texttt{CREAM}$), which interpolates positional encodings by manipulating position indices. Apart from being simple, $\texttt{CREAM}$ is training-efficient: it only requires fine-tuning at the pre-trained context window (e.g., Llama 2-4K) and can extend LLMs to a much longer target context length (e.g., 256K). To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the ''Lost-in-the-Middle'' problem faced by long-context LLMs. Experimental results show that $\texttt{CREAM}$ successfully extends LLMs to the target length for both Base and Chat versions of $\texttt{Llama2-7B}$ with ``Never Miss A Beat''. Our code is publicly available at https://github.com/bigai-nlco/cream.",NIPS
"Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, directly applying these techniques to video models results in unsatisfied frame quality. This issue arises from the limited frame appearance quality in public video datasets, affecting the performance of both teacher and student video diffusion models. Our study aims to improve video diffusion distillation and meanwhile enabling the student model to improve frame appearance using the abundant high-quality image data. To this end, we propose motion consistency models (MCM), a single-stage video diffusion distillation method that disentangles motion and appearance learning. Specifically, MCM involves a video consistency model that distills motion from the video teacher model, and an image discriminator that boosts frame appearance to match high-quality image data. However, directly combining these components leads to two significant challenges: a conflict in frame learning objectives, where video distillation learns from low-quality video frames while the image discriminator targets high-quality images, and training-inference discrepancies due to the differing quality of video samples used during training and inference. To address these challenges, we introduce disentangled motion distillation and mixed trajectory distillation. The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains. Extensive experiments show that our MCM achieves state-of-the-art video diffusion distillation performance. Additionally, our method can enhance frame quality in video diffusion models, producing frames with high aesthetic value or specific styles.",NIPS
"In this paper, we introduce a novel theoretical framework for multi-task regression, applying random matrix theory to provide precise performance estimations, under high-dimensional, non-Gaussian data distributions. We formulate a multi-task optimization problem as a regularization technique to enable single-task models to leverage multi-task learning information. We derive a closed-form solution for multi-task optimization in the context of linear models. Our analysis provides valuable insights by linking the multi-task learning performance to various model statistics such as raw data covariances, signal-generating hyperplanes, noise levels, as well as the size and number of datasets. We finally propose a consistent estimation of training and testing errors, thereby offering a robust foundation for hyperparameter optimization in multi-task regression scenarios. Experimental validations on both synthetic and real-world datasets in regression and multivariate time series forecasting demonstrate improvements on univariate models, incorporating our method into the training loss and thus leveraging multivariate information.",NIPS
"The use of Text-to-Image (T2I) models is expanding beyond generating generic objects, as they are increasingly adopted by diverse global communities to create visual representations of their unique cultures. Current T2I benchmarks primarily evaluate image-text alignment, aesthetics, and fidelity of generations for complex prompts with generic objects, overlooking the critical dimension of cultural understanding. In this work, we address this gap by defining a framework to evaluate the cultural competence of T2I models and present a scalable approach to collecting cultural artifacts unique to a particular culture from a Knowledge Graph (KG) and Large Language Model (LLM) in loop. We assess the ability of state-of-the-art T2I models to generate culturally faithful and realistic images across eight countries and three cultural domains. Furthermore, we emphasize the importance of T2I models reflecting a culture's diversity and introduce cultural diversity as a novel metric for T2I evaluation, drawing inspiration from the Vendi Score. We introduce T2I-CUBE, a first-of-its-kind benchmark for T2I evaluation. T2I-CUBE includes cultural prompts, metrics, and cultural concept spaces, enabling a comprehensive assessment of T2I models' cultural knowledge and diversity. Our evaluations reveal significant gaps in the cultural knowledge of existing models and provide valuable insights into the cultural diversity of image outputs for under-specified prompts. By introducing a novel approach to evaluating cultural competence in T2I models, T2I-CUBE will be instrumental in fostering the development of models with a good understanding of global culture.",NIPS
"Graphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as \``Insert a new slide.'' However, the derived methods often struggle with complex, visually-intensive software tasks in the real world, such as ``recreating a specific animation effect shown in a video.'' The challenges include visual perception, lengthy procedural planning, and executing multiple actions. Recognizing that humans frequently rely on instructional videos to master complex skills, we introduce \textbf{\our}, a novel multi-modal benchmark designed to evaluate GUI assistants across multiple dimensions of advanced GUI tasks. Sourced from high-quality web instructional videos, \our focuses on advanced tasks involving professional and novel software (\eg Adobe Photoshop or Stable Diffusion WebUI) and complex activities (\eg video editing). Moreover, \our evaluates GUI assistants through a \textit{hierarchical} process, allowing for identification of the specific levels at which they may fail: \textbf{($i$) high-level planning:} reconstruct procedural subtasks from visual conditions without language descriptions; \textbf{($ii$) middle-level planning:} generate sequences of precise action narrations based on visual state (\ie screenshot) and goals; % transitions \textbf{($iii$) atomic action execution:} perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. We evaluate representative large multimodal models on \our, revealing each model's capabilities on these different levels. We observe that the current best model GPT-4o, while proficient at planning from textual queries, still struggles with reversal planning from visual previews and execute certain actions such as dragging. These gaps show the direction for developing stronger models or agent systems for GUI automation from instructional videos.",NIPS
"Detecting text generated by Large Language Models (LLMs) is a pressing need in order to identify and prevent misuse of these powerful models in a wide range of applications, which have highly undesirable consequences such as misinformation and academic dishonesty. Given a piece of subject text, many existing detection methods work by measuring the difficulty of LLM predicting the next token in the text from their prefix. In this paper, we make a critical observation that how well the current tokenâ€™s output logits memorizes the closely preceding input tokens also provides strong evidence. Therefore, we propose a novel bi-directional calculation method that measures the cross-entropy losses between an output logits and the ground-truth token (forward) and between the output logits and the immediately preceding input token (backward). A classifier is trained to make the final prediction based on the statistics of these losses. We evaluate our system, named BISCOPE, on texts generated by five latest commercial LLMs across five heterogeneous datasets, including both natural language and code. BISCOPE demonstrates superior detection accuracy and robustness compared to six existing baseline methods, exceeding the state-of-the-art non-commercial methodsâ€™ detection accuracy by over 0.30 F1 score, achieving over 0.95 detection F1 score on average. It also outperforms the best commercial tool GPTZero that is based on a commercial LLM trained with an enormous volume of data. Code is available at https://github.com/MarkGHX/BiScope.",NIPS
"Foundation models possess strong capabilities in reasoning and memorizing across modalities. To further unleash the power of foundation models, we present FIND, a generalized interface for aligning foundation models' embeddings with unified image and dataset-level understanding spanning modality and granularity. As shown in Fig.1, a lightweight transformer interface without tuning any foundation model weights is enough for segmentation, grounding, and retrieval in an interleaved manner. The proposed interface has the following favorable attributes: (1) Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights.  (2) Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space. (3) Extendable. The proposed interface is adaptive to new tasks, and new models. In light of the interleaved embedding space, we introduce FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleaved segmentation and retrieval. We are the first work aligning foundations models' embeddings for interleave understanding. Meanwhile, our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings.",NIPS
"Nucleic acid-based drugs like aptamers have recently demonstrated great therapeutic potential. However, experimental platforms for aptamer screening are costly, and the scarcity of labeled data presents a challenge for supervised methods to learn protein-aptamer binding. To this end, we develop an unsupervised learning approach based on the predicted pairwise contact map between a protein and a nucleic acid and demonstrate its effectiveness in protein-aptamer binding prediction. Our model is based on FAFormer, a novel equivariant transformer architecture that seamlessly integrates frame averaging (FA) within each transformer block. This integration allows our model to infuse geometric information into node features while preserving the spatial semantics of coordinates, leading to greater expressive power than standard FA models. Our results show that FAFormer outperforms existing equivariant models in contact map prediction across three protein complex datasets, with over 10% relative improvement. Moreover, we curate five real-world protein-aptamer interaction datasets and show that the contact map predicted by FAFormer serves as a strong binding indicator for aptamer screening.",NIPS
"For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K--500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP's improvements can also considerably improve the performance of TabR with default parameters.",NIPS
"Dimension reduction (DR) is an important and widely studied technique in exploratory data analysis. However, traditional DR methods are not applicable to datasets with with a contrastive structure, where data are split into a foreground group of interest (case or treatment group), and a background group (control group). This type of data, common in biomedical studies, necessitates contrastive dimension reduction (CDR) methods to effectively capture information unique to or enriched in the foreground group relative to the background group. Despite the development of various CDR methods, two critical questions remain underexplored: when should these methods be applied, and how can the information unique to the foreground group be quantified? In this work, we address these gaps by proposing a hypothesis test to determine the existence of contrastive information, and introducing a contrastive dimension estimator (CDE) to quantify the unique components in the foreground group. We provide theoretical support for our methods and validate their effectiveness through extensive simulated, semi-simulated, and real experiments involving images, gene expressions, protein expressions, and medical sensors, demonstrating their ability to identify the unique information in the foreground group.",NIPS
"Learning of preference models from human feedback has been central to recent advances in artificial intelligence. Motivated by the cost of obtaining high-quality human annotations, we study efficient human preference elicitation for learning preference models. The key idea in our work is to generalize optimal designs, a methodology for computing optimal information-gathering policies, to questions with multiple answers, represented as lists of items. The policy is a distribution over lists and we elicit preferences from the list proportionally to its probability. To show the generality of our ideas, we study both absolute and ranking feedback models on items in the list. We design efficient algorithms for both and analyze them. Finally, we demonstrate that our algorithms are practical by evaluating them on existing question-answering problems.",NIPS
"We consider the problem of active learning on graphs for node-level tasks, which has crucial applications in many real-world networks where labeling node responses is expensive. In this paper, we propose an offline active learning method that selects nodes to query by explicitly incorporating information from both the network structure and node covariates. Building on graph signal recovery theories and the random spectral sparsification technique, the proposed method adopts a two-stage biased sampling strategy that takes both informativeness and representativeness into consideration for node querying. Informativeness refers to the complexity of graph signals that are learnable from the responses of queried nodes, while representativeness refers to the capacity of queried nodes to control generalization errors given noisy node-level information. We establish a theoretical relationship between generalization error and the number of nodes selected by the proposed method. Our theoretical results demonstrate the trade-off between Informativeness and representativeness in active learning. Extensive numerical experiments show that the proposed method is competitive with existing graph-based active learning methods, especially when node covariates and responses contain noises. Additionally, the proposed method is applicable to both regression and classification tasks on graphs.",NIPS
"Large pre-trained models have been vital in recent advancements in domains like language and vision, making model training for individual downstream tasks more efficient and provide superior performance. However, tackling time-series analysis tasks usually involves designing and training a separate model from scratch leveraging training data and domain expertise specific to the task. We tackle a significant challenge for pre-training a foundational time-series model from multi-domain time-series datasets: extracting semantically useful tokenized inputs to the model across heterogeneous time-series from different domains. We propose Large Pre-trained Time-series Models (LPTM) that introduces a novel method of adaptive segmentation that automatically identifies optimal dataset-specific segmentation strategy during pre-training. This enables LPTM to perform similar to or better than domain-specific state-of-art model when fine-tuned to different downstream time-series analysis tasks and under zero-shot settings. LPTM achieves superior forecasting and time-series classification results taking up to 40% less data and 50% less training time compared to state-of-art baselines.",NIPS
"Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predictions of the trained network, and they scale to large models and datasets. While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of the weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases. As a remedy, we directly place a prior on function space. More precisely, since Lebesgue densities do not exist on infinite-dimensional function spaces, we recast training as finding the so-called weak mode of the posterior measure under a Gaussian process (GP) prior restricted to the space of functions representable by the neural network. Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodicity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize. After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation, leveraging highly scalable methods from matrix-free linear algebra. Our method provides improved results where prior knowledge is abundant (as is the case in many scientific inference tasks). At the same time, it stays competitive for black-box supervised learning problems, where neural networks typically excel.",NIPS
"Graph Neural Networks (GNNs) have emerged as a dominant approach in graph representation learning, yet they often struggle to capture consistent similarity relationships among graphs.  To capture similarity relationships, while graph kernel methods like the Weisfeiler-Lehman subtree (WL-subtree) and Weisfeiler-Lehman optimal assignment (WLOA) perform effectively, they are heavily reliant on predefined kernels and lack sufficient non-linearities. Our work aims to bridge the gap between neural network methods and kernel approaches by enabling GNNs to consistently capture relational structures in their learned representations. Given the analogy between the message-passing process of GNNs and WL algorithms, we thoroughly compare and analyze the properties of WL-subtree and WLOA kernels. We find that the similarities captured by WLOA at different iterations are asymptotically consistent, ensuring that similar graphs remain similar in subsequent iterations, thereby leading to superior performance over the WL-subtree kernel. Inspired by these findings, we conjecture that the consistency in the similarities of graph representations across GNN layers is crucial in capturing relational structures and enhancing graph classification performance. Thus, we propose a loss to enforce the similarity of graph representations to be consistent across different layers. Our empirical analysis verifies our conjecture and shows that our proposed consistency loss can significantly enhance graph classification performance across several GNN backbones on various datasets.",NIPS
"Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional defined over the space of probability distributions. MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks. Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finite-particle setting. Recent works (Chen et al., 2022; Suzuki et al., 2023b) have demonstrated the uniform-in-time propagation of chaos for MFLD, showing that the gap between the particle system and its mean-field limit uniformly shrinks over time as the number of particles increases. In this work, we improve the dependence on logarithmic Sobolev inequality (LSI) constants in their particle approximation errors, which can exponentially deteriorate with the regularization coefficient. Specifically, we establish an LSI-constant-free particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization. As the application, we demonstrate improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution, and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity.",NIPS
"In the medical multi-modal frameworks, the alignment of cross-modality features presents a significant challenge. However, existing works have learned features that are implicitly aligned from the data, without considering the explicit relationships in the medical context. This data-reliance may lead to low generalization of the learned alignment relationships. In this work, we propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of medical visual and textual features. We explore the natural auxiliary role of radiologists' eye-gaze data in aligning medical images and text, and introduce a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. We conduct downstream tasks of image classification and image-text retrieval on four medical datasets, where EGMA achieved state-of-the-art performance and stronger generalization across different datasets. Additionally, we explore the impact of varying amounts of eye-gaze data on model performance, highlighting the feasibility and utility of integrating this auxiliary data into multi-modal alignment framework.",NIPS
"Recent studies on deep ensembles have identified the sharpness of the local minima of individual learners and the diversity of the ensemble members as key factors in improving test-time performance. Building on this, our study investigates the interplay between sharpness and diversity within deep ensembles, illustrating their crucial role in robust generalization to both in-distribution (ID) and out-of-distribution (OOD) data. We discover a trade-off between sharpness and diversity: minimizing the sharpness in the loss landscape tends to diminish the diversity of individual members within the ensemble, adversely affecting the ensemble's improvement. The trade-off is justified through our rigorous theoretical analysis and verified empirically through extensive experiments. To address the issue of reduced diversity, we introduce SharpBalance, a novel training approach that balances sharpness and diversity within ensembles. Theoretically, we show that our training strategy achieves a better sharpness-diversity trade-off. Empirically, we conducted comprehensive evaluations in various data sets (CIFAR-10, CIFAR-100, TinyImageNet) and showed that SharpBalance not only effectively improves the sharpness-diversity trade-off but also significantly improves ensemble performance in ID and OOD scenarios.",NIPS
"Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to 1,024K, InfLLM still effectively captures long-distance dependencies. Our code can be found at https://github.com/thunlp/InfLLM.",NIPS
"Diffusion models have demonstrated their effectiveness in addressing the inherent uncertainty and indeterminacy in monocular 3D human pose estimation (HPE). Despite their strengths, the need for large search spaces and the corresponding demand for substantial training data make these models prone to generating biomechanically unrealistic poses. This challenge is particularly noticeable in occlusion scenarios, where the complexity of inferring 3D structures from 2D images intensifies. In response to these limitations, we introduce the **Di**screte **Di**ffusion **Pose** (**$\text{Di}^2\text{Pose}$**), a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model. Specifically, **$\text{Di}^2\text{Pose}$** employs a two-stage process: it first converts 3D poses into a discrete representation through a pose quantization step, which is subsequently modeled in latent space through a discrete diffusion process. This methodological innovation restrictively confines the search space towards physically viable configurations and enhances the modelâ€™s capability to comprehend how occlusions affect human pose within the latent space. Extensive evaluations conducted on various benchmarks (e.g., Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness.",NIPS
"Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, monologue reasoning, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states.We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I (GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities. Our data, code, and models are available at: https://github.com/ARiSE-Lab/SemCoder.",NIPS
"Multimodal Large Language Models (MLLMs) have gained significant attention due to their impressive capabilities in multimodal understanding. However, existing methods rely heavily on extensive modal-specific pretraining and joint-modal tuning, leading to significant computational burdens when expanding to new modalities. In this paper, we propose \textbf{PathWeave}, a flexible and scalable framework with modal-\textbf{path} s\textbf{w}itching and \textbf{e}xp\textbf{a}nsion abilities that enables MLLMs to continually \textbf{ev}olve on modalities for $\mathbb{X}$-modal reasoning. We leverage the concept of Continual Learning and develop an incremental training strategy atop pre-trained MLLMs, enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining. In detail, a novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration. Additionally, an MoE-based gating module is applied between two types of adapters to further enhance the multimodal interaction. To investigate the proposed method, we establish a challenging benchmark called \textbf{C}ontinual \textbf{L}earning of \textbf{M}odality (MCL), which consists of high-quality QA data from five distinct modalities: image, video, \textcolor{black}{audio, depth} and point cloud. Extensive experiments demonstrate the effectiveness of the proposed AnA framework on learning plasticity and memory stability during continual learning. Furthermore, PathWeave performs comparably to state-of-the-art MLLMs while concurrently reducing parameter training burdens by 98.73\%. Our code locates at \url{https://github.com/JiazuoYu/PathWeave}.",NIPS
"Many real-world graphs are large and have some characteristic subgraph patterns, such as triangles in social networks, cliques in web graphs, and cycles in molecular networks.Detecting such subgraph patterns is important in many applications; therefore, establishing graph neural networks (GNNs) that can detect such patterns and run fast on large graphs is demanding.In this study, we propose a new GNN layer, named \emph{graph homomorphism layer}.It enumerates local subgraph patterns that match the predefined set of patterns $\mathcal{P}^\bullet$, applies non-linear transformations to node features, and aggregates them along with the patterns. By stacking these layers, we obtain a deep GNN model called \emph{deep homomorphism network (DHN)}.The expressive power of the DHN is completely characterised by the set of patterns generated from $\mathcal{P}^\bullet$ by graph-theoretic operations;hence, it serves as a useful theoretical tool to analyse the expressive power of many GNN models.Furthermore, the model runs in the same time complexity as the graph homomorphisms, which is fast in many real-word graphs.Thus, it serves as a practical and lightweight model that solves difficult problems using domain knowledge.",NIPS
"Driving systems often rely on high-definition (HD) maps for precise environmental information, which is crucial for planning and navigation. While current HD map constructors perform well under ideal conditions, their resilience to real-world challenges, e.g., adverse weather and sensor failures, is not well understood, raising safety concerns. This work introduces MapBench, the first comprehensive benchmark designed to evaluate the robustness of HD map construction methods against various sensor corruptions. Our benchmark encompasses a total of 29 types of corruptions that occur from cameras and LiDAR sensors. Extensive evaluations across 31 HD map constructors reveal significant performance degradation of existing methods under adverse weather conditions and sensor failures, underscoring critical safety concerns.     We identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. These insights provide a pathway for developing more reliable HD map construction methods, which are essential for the advancement of autonomous driving technology. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.",NIPS
"Inverse protein folding is a fundamental task in computational protein design, which aims to design protein sequences that fold into the desired backbone structures. While the development of machine learning algorithms for this task has seen significant success, the prevailing approaches, which predominantly employ a discriminative formulation, frequently encounter the error accumulation issue and often fail to capture the extensive variety of plausible sequences. To fill these gaps, we propose Bridge-IF, a generative diffusion bridge model for inverse folding, which is designed to learn the probabilistic dependency between the distributions of backbone structures and protein sequences. Specifically, we harness an expressive structure encoder to propose a discrete, informative prior derived from structures, and establish a Markov bridge to connect this prior with native sequences. During the inference stage, Bridge-IF progressively refines the prior sequence, culminating in a more plausible design. Moreover, we introduce a reparameterization perspective on Markov bridge models, from which we derive a simplified loss function that facilitates more effective training. We also modulate protein language models (PLMs) with structural conditions to precisely approximate the Markov bridge process, thereby significantly enhancing generation performance while maintaining parameter-efficient training. Extensive experiments on well-established benchmarks demonstrate that Bridge-IF predominantly surpasses existing baselines in sequence recovery and excels in the design of plausible proteins with high foldability. The code is available at https://github.com/violet-sto/Bridge-IF.",NIPS
"We establish a new model-agnostic optimization framework for out-of-distribution generalization via multicalibration, a criterion that ensures a predictor is calibrated across a family of overlapping groups. Multicalibration is shown to be associated with robustness of statistical inference under covariate shift. We further establish a link between multicalibration and robustness for prediction tasks both under and beyond covariate shift. We accomplish this by extending multicalibration to incorporate grouping functions that consider covariates and labels jointly. This leads to an equivalence of the extended multicalibration and invariance, an objective for robust learning in existence of concept shift. We show a linear structure of the grouping function class spanned by density ratios, resulting in a unifying framework for robust learning by designing specific grouping functions. We propose MC-Pseudolabel, a post-processing algorithm to achieve both extended multicalibration and out-of-distribution generalization. The algorithm, with lightweight hyperparameters and optimization through a series of supervised regression steps, achieves superior performance on real-world datasets with distribution shift.",NIPS
"Classification with rejection emerges as a learning paradigm which allows models to abstain from making predictions. The predominant approach is to alter the supervised learning pipeline by augmenting typical loss functions, letting model rejection incur a lower loss than an incorrect prediction.Instead, we propose a different distributional perspective, where we seek to find an idealized data distribution which maximizes a pretrained model's performance.This can be formalized via the optimization of a loss's risk with a $ \phi$-divergence regularization term.Through this idealized distribution, a rejection decision can be made by utilizing the density ratio between this distribution and the data distribution.We focus on the setting where our $ \phi $-divergences are specified by the family of $ \alpha $-divergence.Our framework is tested empirically over clean and noisy datasets.",NIPS
"Sequence modeling faces challenges in capturing long-range dependencies across diverse tasks. Recent linear and transformer-based forecasters have shown superior performance in time series forecasting. However, they are constrained by their inherent inability to effectively address long-range dependencies in time series data, primarily due to using fixed-size inputs for prediction. Furthermore, they typically sacrifice essential temporal correlation among consecutive training samples by shuffling them into mini-batches. To overcome these limitations, we introduce a fast and effective Spectral Attention mechanism, which preserves temporal correlations among samples and facilitates the handling of long-range information while maintaining the base model structure. Spectral Attention preserves long-period trends through a low-pass filter and facilitates gradient to flow between samples. Spectral Attention can be seamlessly integrated into most sequence models, allowing models with fixed-sized look-back windows to capture long-range dependencies over thousands of steps. Through extensive experiments on 11 real-world time series datasets using 7 recent forecasting models, we consistently demonstrate the efficacy of our Spectral Attention mechanism, achieving state-of-the-art results.",NIPS
"Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety.Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions.Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing.However, research in building analytics has been hampered by the lack of accessible, available, and comprehensive real-world datasets on multiple building operations.In this paper, we introduce the Building TimeSeries (BTS) dataset.Our dataset covers three buildings over a three-year period, comprising more than ten thousand timeseries data points with hundreds of unique ontologies.Moreover, the metadata is standardized using the Brick schema.To demonstrate the utility of this dataset, we performed benchmarks on two tasks: timeseries ontology classification and zero-shot forecasting.These tasks represent an essential initial step in addressing challenges related to interoperability in building analytics.Access to the dataset and the code used for benchmarking are available here: https://github.com/cruiseresearchgroup/DIEF_BTS",NIPS
"Graph Neural Networks have achieved remarkable accuracy in semi-supervised node classification tasks. However, these results lack reliable uncertainty estimates. Conformal prediction methods provide a theoretical guarantee for node classification tasks, ensuring that the conformal prediction set contains the ground-truth label with a desired probability (e.g., 95\%). In this paper, we empirically show that for each node, aggregating the non-conformity scores of nodes with the same label can improve the efficiency of conformal prediction sets while maintaining valid marginal coverage. This observation motivates us to propose a novel algorithm named $\textit{Similarity-Navigated Adaptive Prediction Sets}$ (SNAPS), which aggregates the non-conformity scores based on feature similarity and structural neighborhood. The key idea behind SNAPS is that nodes with high feature similarity or direct connections tend to have the same label. By incorporating adaptive similar nodes information, SNAPS can generate compact prediction sets and increase the singleton hit ratio (correct prediction sets of size one). Moreover, we theoretically provide a finite-sample coverage guarantee of SNAPS. Extensive experiments demonstrate the superiority of SNAPS, improving the efficiency of prediction sets and singleton hit ratio while maintaining valid coverage.",NIPS
"Probabilistic prediction aims to compute predictive distributions rather than single point predictions. These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers. However, most probabilistic methods assume parametric responses, such as Gaussian or Poisson distributions. When these assumptions fail, such models lead to bad predictions and poorly calibrated uncertainty.  In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data. The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees. The conditional diffusion model makes Treeffuser flexible and non-parametric, while the gradient-boosted trees make it robust and easy to train on CPUs. Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks---including those with multivariate, multimodal, and skewed responses. We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions. We further demonstrate its versatility with an application to inventory allocation under uncertainty using sales data from Walmart. We implement Treeffuser in https://github.com/blei-lab/treeffuser.",NIPS
"Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.",NIPS
"Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited.  In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data.  Discrete Flow Matching offers several key contributions:  (i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ($x$-prediction) and noise-prediction ($\epsilon$-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.",NIPS
"We introduce a new benchmark designed to advance the development of general-purpose, large-scale vision-language models for remote sensing images. While several vision and language datasets in remote sensing have been proposed to pursue this goal, they often have significant limitations. Existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. To address these issues, we present a versatile vision-language benchmark for remote sensing image understanding, termed VERSAL. This benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 124,037 question-answer pairs. It facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. We further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. Our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing.",NIPS
"Owing to advancements in image synthesis techniques, stylization methodologies for large models have garnered remarkable outcomes. However, when it comes to processing facial images, the outcomes frequently fall short of expectations. Facial stylization is predominantly challenged by two significant hurdles. Firstly, obtaining a large dataset of high-quality stylized images is difficult. The scarcity and diversity of artistic styles make it impractical to compile comprehensive datasets for each style. Secondly, while many methods can transfer colors and strokes from style images, these elements alone cannot fully capture a specific style, which encompasses both concrete and abstract visual elements. Additionally, facial stylization often alters the visual features of the face, making it challenging to balance these changes with the need to retain facial information. To address these issues, we propose a novel method called ACFun, which uses only one style image and one facial image for facial stylization. ACFun comprises an Abstract Fusion Module (AFun) and a Concrete Fusion Module (CFun), which separately learn the abstract and concrete features of the style and face. We also design a Face and Style Imagery Alignment Loss to align the style image with the face image in the latent space. Finally, we generate styled facial images from noise directly to complete the facial stylization task. Experiments show that our method outperforms others in facial stylization, producing highly artistic and visually pleasing results.",NIPS
"Originally proposed for handling time series data, Auto-regressive Decision Trees (ARDTs) have not yet been explored for language modeling. This paper delves into both the theoretical and practical applications of ARDTs in this new context. We theoretically demonstrate that ARDTs can compute complex functions, such as simulating automata, Turing machines, and sparse circuits, by leveraging ""chain-of-thought"" computations. Our analysis provides bounds on the size, depth, and computational efficiency of ARDTs, highlighting their surprising computational power. Empirically, we train ARDTs on simple language generation tasks, showing that they can learn to generate  coherent and grammatically correct text on par with a smaller Transformer model. Additionally, we show that ARDTs can be used on top of transformer representations to solve complex reasoning tasks. This research reveals the unique computational abilities of ARDTs, aiming to broaden the architectural diversity in language model development.",NIPS
"Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.",NIPS
"In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs).However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies.In this paper, we present the first *tractable* algorithm  with minimax optimal regret of $\mathrm{O}\left(\sqrt{\mathrm{sp}(h^*) S A T \log(SAT)}\right)$ where $\mathrm{sp}(h^*)$ is the span of the optimal bias function $h^*$, $S\times A$  is the size of the state-action space and $T$ the number of learning steps. Remarkably, our algorithm does not require prior information on $\mathrm{sp}(h^*)$. Our algorithm relies on a novel subroutine, **P**rojected **M**itigated **E**xtended **V**alue **I**teration (`PMEVI`), to compute bias-constrained optimal policies efficiently. This subroutine can be applied to various previous algorithms to obtain improved regret bounds.",NIPS
"Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery.In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings.Codes and demos are publicly available on GitHub at https://github.com/garywei944/ChemFlow.",NIPS
"Given the complexities inherent in visual scenes, such as object occlusion, a comprehensive understanding often requires observation from multiple viewpoints. Existing multi-viewpoint object-centric learning methods typically employ random or sequential viewpoint selection strategies. While applicable across various scenes, these strategies may not always be ideal, as certain scenes could benefit more from specific viewpoints. To address this limitation, we propose a novel active viewpoint selection strategy. This strategy predicts images from unknown viewpoints based on information from observation images for each scene. It then compares the object-centric representations extracted from both viewpoints and selects the unknown viewpoint with the largest disparity, indicating the greatest gain in information, as the next observation viewpoint. Through experiments on various datasets, we demonstrate the effectiveness of our active viewpoint selection strategy, significantly enhancing segmentation and reconstruction performance compared to random viewpoint selection. Moreover, our method can accurately predict images from unknown viewpoints.",NIPS
"The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message passing. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. Cartesian tensors offer a promising alternative, though state-of-the-art methods lack flexibility in message-passing mechanisms, restricting their architectures and expressive power. This work explores higher-rank irreducible Cartesian tensors to address these limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.",NIPS
"While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods.",NIPS
"The performance of 3D object detection in large outdoor point clouds deteriorates significantly in an unseen environment due to the inter-domain gap. To address these challenges, most existing methods for domain adaptation harness self-training schemes and attempt to bridge the gap by focusing on a single factor that causes the inter-domain gap, such as objects' sizes, shapes, and foreground density variation. However, the resulting adaptations suggest that there is still a substantial inter-domain gap left to be minimized. We argue that this is due to two limitations: 1) Biased pseudo-label collection from self-training. 2) Multiple factors jointly contributing to how the object is perceived in the unseen target domain. In this work, we propose a grouping-exploration strategy framework,  Group Explorer Domain Adaptation ($\textbf{GroupEXP-DA}$), to addresses those two issues. Specifically, our grouping divides the available label sets into multiple clusters and ensures all of them have equal learning attention with the group-equivariant spatial feature, avoiding dominant types of objects causing imbalance problems. Moreover, grouping learns to divide objects by considering inherent factors in a data-driven manner, without considering each factor separately as existing works. On top of the group-equivariant spatial feature that selectively detects objects similar to the input group, we additionally introduce an explorative group update strategy that reduces the false negative detection in the target domain, further reducing the inter-domain gap. During inference, only the learned group features are necessary for making the group-equivariant spatial feature, placing our method as a simple add-on that can be applicable to most existing detectors. We show how each module contributes to substantially bridging the inter-domain gaps compared to existing works across large urban outdoor datasets such as NuScenes, Waymo, and KITTI.",NIPS
"Large Language Models (LLMs) have seen an impressive wave of advances, withmodels now excelling in a variety of tasks, such as mathematical reasoning andprogram synthesis. However, their potential to effectively use tools via API callsremains unfulfilled. This is a challenging task even for todayâ€™s state-of-the-artLLMs such as GPT-4 largely due to their unawareness of what APIs are availableand how to use them in a frequently updated tool set. We develop Gorilla, afinetuned LLaMA model that surpasses the performance of GPT-4 on writing APIcalls. Trained with the novel Retriever Aware Training (RAT), when combinedwith a document retriever, Gorilla demonstrates a strong capability to adapt totest-time document changes, allowing flexible user updates or version changes.It also substantially mitigates the issue of hallucination, commonly encounteredwhen prompting LLMs directly. To evaluate the modelâ€™s ability, we introduceAPIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, andTensorHub APIs. The successful integration of the retrieval system with Gorillademonstrates the potential for LLMs to use tools more accurately, keep up withfrequently updated documentation, and consequently increase the reliability andapplicability of their outputs. Gorillaâ€™s code, model, data, and demo are availableat: https://gorilla.cs.berkeley.edu",NIPS
"Event cameras, offering high temporal resolution and high dynamic range, have brought a new perspective to addressing 3D reconstruction challenges in fast-motion and low-light scenarios. Most methods use the Neural Radiance Field (NeRF) for event-based photorealistic 3D reconstruction. However, these NeRF methods suffer from time-consuming training and inference, as well as limited scene-editing capabilities of implicit representations. To address these problems, we propose Event-3DGS, the first event-based reconstruction using 3D Gaussian splatting (3DGS) for synthesizing novel views freely from event streams. Technically, we first propose an event-based 3DGS framework that directly processes event data and reconstructs 3D scenes by simultaneously optimizing scenario and sensor parameters. Then, we present a high-pass filter-based photovoltage estimation module, which effectively reduces noise in event data to improve the robustness of our method in real-world scenarios. Finally, we design an event-based 3D reconstruction loss to optimize the parameters of our method for better reconstruction quality. The results show that our method outperforms state-of-the-art methods in terms of reconstruction quality on both simulated and real-world datasets. We also verify that our method can perform robust 3D reconstruction even in real-world scenarios with extreme noise, fast motion, and low-light conditions. Our code is available in https://github.com/lanpokn/Event-3DGS.",NIPS
"Data serves as the fundamental foundation for advancing deep learning, particularly tabular data presented in a structured format, which is highly conducive to modeling. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Hence, exploring how to effectively use models like LLMs to generate realistic and privacy-preserving synthetic tabular data is emergent. In this paper, we take a step forward to explore LLMs for tabular data synthesis and privacy protection, by introducing a new framework HARMONIC for tabular data generation and evaluation. In our tabular data generation framework, unlike previous small-scale LLM-based methods that rely on continued pre-training, we explore the larger-scale LLMs with fine-tuning to generate tabular data and enhance privacy. Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to discover inter-row relationships. Then, with fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. In our evaluation framework, we develop specific privacy risk metrics for LLM synthetic data generation, as well as performance evaluation metrics for downstream LLM tasks. Our experiments find that this tabular data generation framework achieves equivalent performance to existing methods with better privacy, which also demonstrates our evaluation framework for the effectiveness of synthetic data and privacy risks in LLM scenarios.",NIPS
"Systems with variations of the underlying generating mechanism between different contexts, i.e., different environments or internal states  in which the system operates, are common in the real world, such as soil moisture regimes in Earth science. Besides understanding the shared properties of the system, in practice, the question of context-specific properties, i.e., the change in causal relationships between contexts, arises. For real-world data, contexts are often driven by system variables, e.g., precipitation highly influences soil moisture. Nevertheless, this setup needs to be studied more. To account for such endogenous contexts in causal discovery, our work proposes a constraint-based method that can efficiently discover context-specific causal graphs using an adaptive testing approach. Our approach tests conditional independence on the pooled datasets to infer the dependence between system variables, including the context, to avoid introducing selection bias. To yield context-specific insights, conditional independence is tested on context-specific data. We work out the theoretical framework for this adaptive testing approach and give a detailed discussion of the connection to structural causal models, including sufficiency assumptions, which allow to prove the soundness of our algorithm and to interpret the results causally. A simulation study to evaluate numerical properties shows that our approach  behaves as expected, but also leads to a further understanding of current limitations and viable extensions.",NIPS
"Attribution maps are one of the most established tools to explain the functioning of computer vision models. They assign importance scores to input features, indicating how relevant each feature is for the prediction of a deep neural network. While much research has gone into proposing new attribution methods, their proper evaluation remains a difficult challenge. In this work, we propose a novel evaluation protocol that overcomes two fundamental limitations of the widely used incremental-deletion protocol, i.e., the out-of-domain issue and lacking inter-model comparisons. This allows us to evaluate 23 attribution methods and how eight different design choices of popular vision models affect their attribution quality. We find that intrinsically explainable models outperform standard models and that raw attribution values exhibit a higher attribution quality than what is known from previous work. Further, we show consistent changes in the attribution quality when varying the network design, indicating that some standard design choices promote attribution quality.",NIPS
"Consistent depth estimation across diverse scenes and sensors is a crucial challenge in computer vision, especially when deploying machine learning models in the real world. Traditional methods depend heavily on extensive pixel-wise labeled data, which is costly and labor-intensive to acquire, and frequently have difficulty in scale issues on various depth sensors. In response, we define Universal Depth Completion (UniDC) problem. We also present a baseline architecture, a simple yet effective approach tailored to estimate scene depth across a wide range of sensors and environments using minimal labeled data. Our approach addresses two primary challenges: generalizable knowledge of unseen scene configurations and strong adaptation to arbitrary depth sensors with various specifications. To enhance versatility in the wild, we utilize a foundation model for monocular depth estimation that provides a comprehensive understanding of 3D structures in scenes. Additionally, for fast adaptation to off-the-shelf sensors, we generate a pixel-wise affinity map based on the knowledge from the foundation model. We then adjust depth information from arbitrary sensors to the monocular depth along with the constructed affinity. Furthermore, to boost up both the adaptability and generality, we embed the learned features into hyperbolic space, which builds implicit hierarchical structures of 3D data from fewer examples. Extensive experiments demonstrate the proposed method's superior generalization capabilities for UniDC problem over state-of-the-art depth completion. Source code is publicly available at https://github.com/JinhwiPark/UniDC.",NIPS
We explore the theoretical possibility of learning $d$-dimensional targets with $W$-parameter models by gradient flow (GF) when $W,NIPS
"Universal image representations are critical in enabling real-world fine-grained and instance-level recognition applications, where objects and entities from any domain must be identified at large scale.Despite recent advances, existing methods fail to capture important domain-specific knowledge, while also ignoring differences in data distribution across different domains.This leads to a large performance gap between efficient universal solutions and expensive approaches utilising a collection of specialist models, one for each domain.In this work, we make significant strides towards closing this gap, by introducing a new learning technique, dubbed UDON (Universal Dynamic Online distillatioN).UDON employs multi-teacher distillation, where each teacher is specialized in one domain, to transfer detailed domain-specific knowledge into the student universal embedding.UDON's distillation approach is not only effective, but also very efficient, by sharing most model parameters between the student and all teachers, where all models are jointly trained in an online manner.UDON also comprises a sampling technique which adapts the training process to dynamically allocate batches to domains which are learned slower and require more frequent processing.This boosts significantly the learning of complex domains which are characterised by a large number of classes and long-tail distributions.With comprehensive experiments, we validate each component of UDON, and showcase significant improvements over the state of the art in the recent UnED benchmark.Code: https://github.com/nikosips/UDON.",NIPS
"Diplomacy is one of the most sophisticated activities in human society, involving complex interactions among multiple parties that require skills in social reasoning, negotiation, and long-term strategic planning. Previous AI agents have demonstrated their ability to handle multi-step games and large action spaces in multi-agent tasks. However, diplomacy involves a staggering magnitude of decision spaces, especially considering the negotiation stage required. While recent agents based on large language models (LLMs) have shown potential in various applications, they still struggle with extended planning periods in complex multi-agent settings. Leveraging recent technologies for LLM-based agents, we aim to explore AI's potential to create a human-like agent capable of executing comprehensive multi-agent missions by integrating three fundamental capabilities: 1) strategic planning with memory and reflection; 2) goal-oriented negotiation with social reasoning; and 3) augmenting memory through self-play games for self-evolution without human in the loop.",NIPS
"Inferring the 3D structure underlying a set of multi-view images typically requires solving two co-dependent tasks -- accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D. The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of initial pose estimates. However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D. Moreover, large errors in pose estimation may not be easily corrected and can further degrade the inferred 3D. To allow robust 3D reconstruction and pose estimation in this challenging setup, we propose SparseAGS, a method that adapts this analysis-by-synthesis approach by: a) including novel-view-synthesis-based generative priors in conjunction with photometric objectives to improve the quality of the inferred 3D, and b) explicitly reasoning about outliers and using a discrete search with a continuous optimization-based strategy to correct them. We validate our framework across real-world and synthetic datasets in combination with several off-the-shelf pose estimation systems as initialization. We find that it significantly improves the base systems' pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current multi-view reconstruction baselines.",NIPS
"People who are blind perceive the world differently than those who are sighted. This often translates to different motion characteristics; for instance, when crossing at an intersection, blind individuals may move in ways that could potentially be more dangerous, e.g., exhibit higher veering from the path and employ touch-based exploration around curbs and obstacles that may seem unpredictable. Yet, the ability of 3D motion models to model such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable autonomous systems that reason over diverse human movements in their environments, we will publicly release our novel text-and-motion benchmark.",NIPS
"Computer simulations have long presented the exciting possibility of scientific insight into complex real-world processes. Despite the power of modern computing, however, it remains challenging to systematically perform inference under simulation models. This has led to the rise of simulation-based inference (SBI), a class of machine learning-enabled techniques for approaching inverse problems with stochastic simulators. Many such methods, however, require large numbers of simulation samples and face difficulty scaling to high-dimensional settings, often making inference prohibitive under resource-intensive simulators. To mitigate these drawbacks, we introduce active sequential neural posterior estimation (ASNPE). ASNPE brings an active learning scheme into the inference loop to estimate the utility of simulation parameter candidates to the underlying probabilistic model. The proposed acquisition scheme is easily integrated into existing posterior estimation pipelines, allowing for improved sample efficiency with low computational overhead. We further demonstrate the effectiveness of the proposed method in the travel demand calibration setting, a high-dimensional inverse problem commonly requiring computationally expensive traffic simulators. Our method outperforms well-tuned benchmarks and state-of-the-art posterior estimation methods on a large-scale real-world traffic network, as well as demonstrates a performance advantage over non-active counterparts on a suite of SBI benchmark environments.",NIPS
"Generative modeling over discrete data has recently seen numerous success stories, with applications spanning language modeling, biological sequence design, and graph-structured molecular data. The predominant generative modeling paradigm for discrete data is still autoregressive, with more recent alternatives based on diffusion or flow-matching falling short of their impressive performance in continuous data settings, such as image or video generation. In this work, we introduce Fisher-Flow, a novel flow-matching model for discrete data. Fisher-Flow takes a manifestly geometric perspectiveby considering categorical distributions over discrete data as points residing on a statistical manifold equipped with its natural Riemannian metric: the \emph{Fisher-Rao metric}. As a result, we demonstrate discrete data itself can be continuously reparameterised to points on the positive orthant of the $d$-hypersphere $\mathbb{S}^d_+$, which allows us to define flows that map any source distribution to target in a principled manner by transporting mass along (closed-form) geodesics of $\mathbb{S}^d_+$. Furthermore, the learned flows in Fisher-Flow can be further bootstrapped by leveraging Riemannian optimal transport leading to improved training dynamics. We prove that the gradient flow induced by Fisher-FLow is optimal in reducing the forward KL divergence. We evaluate Fisher-Flow on an array of synthetic and diverse real-world benchmarks, including designing DNA Promoter, and DNA Enhancer sequences. Empirically, we find that Fisher-Flow improves over prior diffusion and flow-matching models on these benchmarks.",NIPS
"Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As batch size, context length, or model size increases, the size of key and value (KV) cache quickly becomes the main contributor to GPU memory usage and the bottleneck of inference latency and throughput. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. Currently, KV cache quantization is performed per-channel or per-token independently. Our analysis shows that distinct channels of a key/value activation embedding are highly interdependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropy, which implies that per-channel independent quantization is sub-optimal. To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4â€“3.5$\times$ relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.",NIPS
"We quantify the efficiency of temporal difference (TD) learning over the direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement learning, with an emphasis on estimation of quantities related to rare events. Policy evaluation is complicated in the rare event setting by the long timescale of the event and by the need for \emph{relative accuracy} in estimates of very small values.  Specifically, we focus on least-squares TD (LSTD) prediction for finite state Markov chains, and show that LSTD can achieve relative accuracy far more efficiently than MC.  We prove a central limit theorem for the LSTD estimator and upper bound the   \emph{relative asymptotic variance}  by simple quantities characterizing the connectivity of states relative to the transition probabilities between them. Using this bound, we show that, even when both the timescale of the rare event and the relative accuracy of the MC estimator are exponentially large in the number of states, LSTD maintains a fixed level of relative accuracy with  a total number of observed transitions of the Markov chain that is only \emph{polynomially} large in the number of states.",NIPS
"Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.",NIPS
"We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach leverages human judgment to distinguish inputs which are algorithmically indistinguishable, or ""look the same"" to predictive algorithms.  We argue that this framing clarifies the problem of human-AI collaboration in prediction tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of ""side information"", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement.  We find empirically that although algorithms often outperform their human counterparts on average, human judgment can improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.",NIPS
"Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (winner'' andloser'' images) for each text prompt.In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data. Codes are available at \url{https://github.com/uclaml/SPIN-Diffusion/}.",NIPS
"The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress towards capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus.",NIPS
"Domain generalisation in computational histopathology is challenging because the images are substantially affected by differences among hospitals due to factors like fixation and staining of tissue and imaging equipment. We hypothesise that focusing on nuclei can improve the out-of-domain (OOD) generalisation in cancer detection. We propose a simple approach to improve OOD generalisation for cancer detection by focusing on nuclear morphology and organisation, as these are domain-invariant features critical in cancer detection. Our approach integrates original images with nuclear segmentation masks during training, encouraging the model to prioritise nuclei and their spatial arrangement. Going beyond mere data augmentation, we introduce a regularisation technique that aligns the representations of masks and original images. We show, using multiple datasets, that our method improves OOD generalisation and also leads to increased robustness to image corruptions and adversarial attacks. The source code is available at https://github.com/undercutspiky/SFL/",NIPS
"Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models (LLMs) while maintaining an identical sampling distribution. However, the conventional approach of training separate draft model to achieve a satisfactory token acceptance rate can be costly and impractical. In this paper, we propose a novel self-speculative decoding framework \emph{Kangaroo} with \emph{double} early exiting strategy, which leverages the shallow sub-network and the \texttt{LM Head} of the well-trained target LLM to construct a self-drafting model. Then, the self-verification stage only requires computing the remaining layers over the \emph{early-exited} hidden states in parallel. To bridge the representation gap between the sub-network and the full model, we train a lightweight and efficient adapter module on top of the sub-network. One significant challenge that comes with the proposed method is that the inference latency of the self-draft model may no longer be negligible compared to the big model. To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional \emph{early exiting} mechanism for both single-sequence and the tree decoding scenarios. Specifically, we dynamically halt the small model's subsequent prediction during the drafting phase once the confidence level for the current step falls below a certain threshold. This approach reduces unnecessary computations and improves overall efficiency. Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to 2.04$\times$, outperforming Medusa-1 with 88.7\% fewer additional parameters. The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.",NIPS
"Deep Neural Networks (DNNs) have revolutionized artificial intelligence, achieving impressive results on diverse data types, including images, videos, and texts. However, DNNs still lag behind Gradient Boosting Decision Trees (GBDT) on tabular data, a format extensively utilized across various domains. This paper introduces DOFEN, which stands for Deep Oblivious Forest ENsemble. DOFEN is a novel DNN architecture inspired by oblivious decision trees and achieves on-off sparse selection of columns. DOFEN surpasses other DNNs on tabular data, achieving state-of-the-art performance on the well-recognized benchmark: Tabular Benchmark, which includes 73 total datasets spanning a wide array of domains. The code of DOFEN is available at: https://github.com/Sinopac-Digital-Technology-Division/DOFEN",NIPS
"Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization. However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension $d_0$ scales at least logarithmically in the number of samples $n$. In this work we remove both of these requirements and instead provide bounds in terms of a measure of distance between data points: notably these bounds hold with high probability even when $d_0$ is held constant versus $n$. We prove our results through a novel application of the hemisphere transform.",NIPS
"Despite existing 3D cloth simulators producing realistic results, they predominantly operate on discrete surface representations (e.g. points and meshes) with a fixed spatial resolution, which often leads to large memory consumption and resolution-dependent simulations. Moreover, back-propagating gradients through the existing solvers is difficult and they hence cannot be easily integrated into modern neural architectures. In response, this paper re-thinks physically plausible cloth simulation: We propose NeuralClothSim, i.e., a new quasistatic cloth simulator using thin shells, in which surface deformation is encoded in neural network weights in form of a neural field. Our memory-efficient solver operates on a new continuous coordinate-based surface representation called neural deformation fields (NDFs); it supervises NDF equilibria with the laws of the non-linear Kirchhoff-Love shell theory with a non-linear anisotropic material model. NDFs are adaptive: They 1) allocate their capacity to the deformation details and 2) allow surface state queries at arbitrary spatial resolutions without re-training. We show how to train NeuralClothSim while imposing hard boundary conditions and demonstrate multiple applications, such as material interpolation and simulation editing. The experimental results highlight the effectiveness of our continuous neural formulation.",NIPS
"The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. Our source code is publicly available at https://github.com/BaiTheBest/SparseLLM.",NIPS
"Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior approaches have thus explored how engineers might select an LLM to use for each sample (i.e. routing). While existing routing methods mostly require training auxiliary models on human-annotated data, our work explores whether it is possible to perform unsupervised routing. We propose Smoothie, a weak supervision-inspired routing approach that requires no labeled data. Given a set of outputs from different LLMs, Smoothie constructs a latent variable graphical model over embedding representations of observable LLM outputs and unknown â€œtrueâ€ outputs. Using this graphical model, we estimate sample-dependent quality scores for each LLM, and route each sample to the LLM with the highest corresponding score. We find that Smoothie's LLM quality-scores correlate with ground-truth model quality (correctly identifying the optimal model on 9/14 tasks), and that Smoothie outperforms baselines for routing by up to 10 points accuracy.",NIPS
"Formal verification (FV) has witnessed growing significance with current emergingÂ program synthesis by the evolving large language models (LLMs). However,Â current formal verification mainly resorts to symbolic verifiers or hand-craft rules,Â resulting in limitations for extensive and flexible verification. On the other hand,Â formal languages for automated theorem proving, such as Isabelle, as another lineÂ of rigorous verification, are maintained with comprehensive rules and theorems. InÂ this paper, we propose FVEL, an interactive Formal Verification EnvironmentÂ with LLMs. Specifically, FVEL transforms a given code to be verified into Isabelle,Â and then conducts verification via neural automated theorem proving with an LLM.Â The joined paradigm leverages the rigorous yet abundant formulated and organizedÂ rules in Isabelle and is also convenient for introducing and adjusting cutting-edgeÂ LLMs. To achieve this goal, we extract a large-scale FVELER. The FVELERÂ dataset includes code dependencies and verification processes that are formulated inÂ Isabelle, containing 758 theories, 29,125 lemmas, and 200,646 proof steps in totalÂ with in-depth dependencies. We benchmark FVELER in the FVEL environment by first fine-tuning LLMs with FVELER and then evaluating them on Code2InvÂ and SV-COMP. The results show that FVEL with FVELER fine-tuned Llama3-8B solves 17.39% (69â†’81) more problems, and Mistral-7B 12% (75â†’84) moreÂ problems in SV-COMP. And the proportion of proof errors is reduced. ProjectÂ page: https://fveler.github.io/.",NIPS
"The path to interpreting a language model often proceeds via analysis of circuits---sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they either rely on inefficient search algorithms or inaccurate approximations. In this paper, we frame circuit discovery as an optimization problem and propose Edge Pruning as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, prunes the edges between components. Our method finds circuits in GPT-2 that use less than half the number of edges than circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient on tasks involving up to 100,000 examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the size of GPT-2.We use this setting for a case study, where we compare the mechanisms behind instruction prompting and in-context learning.We find two circuits with more than 99.96% sparsity that match the performance of the full model. Further analysis reveals that the mechanisms in the two settings overlap substantially. This shows that Edge Pruning is a practical and scalable tool for interpretability, which can shed light on behaviors that only emerge in large models.",NIPS
"A potent class of generative models known as Diffusion Probabilistic Models(DPMs) has become prominent. A forward diffusion process adds gradually noiseto data, while a model learns to gradually denoise. Sampling from pre-trainedDPMs is obtained by solving differential equations (DE) defined by the learntmodel, a process which has shown to be prohibitively slow. Numerous efforts onspeeding-up this process have consisted on crafting powerful ODE solvers.Despite being quick, such solvers do not usually reach the optimal qualityachieved by available slow SDE solvers. Our goal is to propose SDE solvers thatreach optimal quality without requiring several hundreds or thousands of NFEsto achieve that goal. We propose Stochastic Explicit ExponentialDerivative-free Solvers (SEEDS), improving and generalizing ExponentialIntegrator approaches to the stochastic case on several frameworks. After carefully analyzing the formulation of exactsolutions of diffusion SDEs, we craft SEEDS to analytically compute the linearpart of such solutions. Inspired by the Exponential Time-Differencing method,SEEDS use a novel treatment of the stochastic components of solutions,enabling the analytical computation of their variance, and contains high-orderterms allowing to reach optimal quality sampling $\sim3$-$5\times$ faster than previousSDE methods. We validate our approach on several image generation benchmarks,showing that SEEDS outperform or are competitive with previous SDE solvers.Contrary to the latter, SEEDS are derivative and training free, and we fullyprove strong convergence guarantees for them.",NIPS
"We propose a notion of common information that allows one to quantify and separate the information that is shared between two random variables from the information that is unique to each. Our notion of common information is defined by an optimization problem over a family of functions and recovers the G\'acs-K\""orner common information as a special case. Importantly, our notion can be approximated empirically using samples from the underlying data distribution. We then provide a method to partition and quantify the common and unique information using a simple modification of a traditional variational auto-encoder. Empirically, we demonstrate that our formulation allows us to learn semantically meaningful common and unique factors of variation even on high-dimensional data such as images and videos. Moreover, on datasets where ground-truth latent factors are known, we show that we can accurately quantify the common information between the random variables.",NIPS
"Most linear experimental design problems assume homogeneous variance, while the presence of heteroskedastic noise is present in many realistic settings. Let a learner have access to a finite set of measurement vectors $\mathcal{X}\subset \mathbb{R}^d$ that can be probed to receive noisy linear responses of the form $y=x^{\top}\theta^{\ast}+\eta$. Here $\theta^{\ast}\in \mathbb{R}^d$ is an unknown parameter vector, and $\eta$ is independent mean-zero $\sigma_x^2$-sub-Gaussian noise defined by a flexible heteroskedastic variance model, $\sigma_x^2 = x^{\top}\Sigma^{\ast}x$. Assuming that $\Sigma^{\ast}\in \mathbb{R}^{d\times d}$ is an unknown matrix, we propose, analyze and empirically evaluate a novel design for uniformly bounding estimation error of the variance parameters, $\sigma_x^2$. We demonstrate this method on two adaptive experimental design problems under heteroskedastic noise, fixed confidence transductive best-arm identification and level-set identification and prove the first instance-dependent lower bounds in these settings.Lastly, we construct near-optimal algorithms and demonstrate the large improvements in sample complexity gained from accounting for heteroskedastic variance in these designs empirically.",NIPS
"Learning cause and effect relations is arguably one of the central challenges found throughout the data sciences.Formally, determining whether a collection of observational and interventional distributions can be combined to learn a target causal relation is known as the problem of generalized identification (or g-identification) [Lee et al., 2019]. Although g-identification has been well understood and solved in theory, it turns out to be challenging to apply these results in practice, in particular when considering the estimation of the target distribution from finite samples. In this paper, we develop a new, general estimator that exhibits multiply robustness properties for g-identifiable causal functionals. Specifically, we show that any g-identifiable causal effect can be expressed as a function of generalized multi-outcome sequential back-door adjustments that are amenable to estimation. We then construct a corresponding estimator for the g-identification expression that exhibits robustness properties to bias. We analyze the asymptotic convergence properties of the estimator. Finally, we illustrate the use of the proposed estimator in experimental studies. Simulation results corroborate the theory.",NIPS
"We study fair machine learning (ML) under predictive uncertainty to enable reliable and trustworthy decision-making. The seminal work of 'equalized coverage' proposed an uncertainty-aware fairness notion. However, it does not guarantee equal coverage rates across more fine-grained groups (e.g., low-income females) conditioning on the true label and is biased in the assessment of uncertainty. To tackle these limitations, we propose a new uncertainty-aware fairness -- Equal Opportunity of Coverage (EOC) -- that aims to achieve two properties: (1) coverage rates for different groups with similar outcomes are close, and (2) the coverage rate for the entire population remains at a predetermined level. Further, the prediction intervals should be narrow to be informative. We propose Binned Fair Quantile Regression (BFQR), a distribution-free post-processing method to improve EOC with reasonable width for any trained ML models. It first calibrates a hold-out set to bound deviation from EOC, then leverages conformal prediction to maintain EOC on a test set, meanwhile optimizing prediction interval width. Experimental results demonstrate the effectiveness of our method in improving EOC.",NIPS
"We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a constrained MDP into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, respectively, and develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy. Specifically, we first propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual variable via a quadratic-regularized gradient ascent, simultaneously. We prove that the policy primal-dual iterates of RPG-PD converge to a regularized saddle point with a sublinear rate, while the policy iterates converge sublinearly to an optimal constrained policy. We further instantiate RPG-PD in large state or action spaces by including function approximation in policy parametrization, and establish similar sublinear last-iterate policy convergence. Second, we propose an optimistic policy gradient primal-dual (OPG-PD) method that employs the optimistic gradient method to update primal/dual variables, simultaneously. We prove that the policy primal-dual iterates of OPG-PD converge to a saddle point that contains an optimal constrained policy, with a linear rate. To the best of our knowledge, this work appears to be the first non-asymptotic policy last-iterate convergence result for single-time-scale algorithms in constrained MDPs. We further validate the merits and the effectiveness of our methods in computational experiments.",NIPS
"Survival analysis is a crucial semi-supervised task in machine learning with significant real-world applications, especially in healthcare. The most common approach to survival analysis, Coxâ€™s partial likelihood, can be interpreted as a ranking model optimized on a lower bound of the concordance index.  We follow these connections further, with listwise ranking losses that allow for a relaxation of the pairwise independence assumption. Given the inherent transitivity of ranking, we explore differentiable sorting networks as a means to introduce a stronger transitive inductive bias during optimization. Despite their potential, current differentiable sorting methods cannot account for censoring, a crucial aspect of many real-world datasets. We propose a novel method, Diffsurv, to overcome this limitation by extending differentiable sorting methods to handle censored tasks. Diffsurv predicts matrices of possible permutations that accommodate the label uncertainty introduced by censored samples. Our experiments reveal that Diffsurv outperforms established baselines in various simulated and real-world risk prediction scenarios. Furthermore, we demonstrate the algorithmic advantages of Diffsurv by presenting a novel method for top-k risk prediction that surpasses current methods.",NIPS
"Deep neural networks are known to be vulnerable to small adversarial perturbations in test data. To defend against adversarial attacks, probabilistic classifiers have been proposed as an alternative to deterministic ones. However, literature has conflicting findings on the effectiveness of probabilistic classifiers in comparison to deterministic ones. In this paper, we clarify the role of randomization in building adversarially robust classifiers.Given a base hypothesis set of deterministic classifiers, we show the conditions under which a randomized ensemble outperforms the hypothesis set in adversarial risk, extending previous results.Additionally, we show that for any probabilistic binary classifier (including randomized ensembles), there exists a deterministic classifier that outperforms it. Finally, we give an explicit description of the deterministic hypothesis set that contains such a deterministic classifier for many types of commonly used probabilistic classifiers, i.e. randomized ensembles and parametric/input noise injection.",NIPS
"Learning feature interactions can be the key for multivariate predictive modeling. ReLU-activated neural networks create piecewise linear prediction models, and other nonlinear activation functions lead to models with only high-order feature interactions. Recent methods incorporate candidate polynomial terms of fixed orders into deep learning, which is subject to the issue of combinatorial explosion, or learn the orders that are difficult to adapt to different regions of the feature space. We propose a Polyhedron Attention Module (PAM) to create piecewise polynomial models where the input space is split into polyhedrons which define the different pieces and on each piece the hyperplanes that define the polyhedron boundary multiply to form the interactive terms, resulting in interactions of adaptive order to each piece. PAM is interpretable to identify important interactions in predicting a target. Theoretic analysis shows that PAM has stronger expression capability than ReLU-activated networks. Extensive experimental results demonstrate the superior classification performance of PAM on massive datasets of the click-through rate prediction and PAM can learn meaningful interaction effects in a medical problem.",NIPS
"To explain predictions made by complex machine learning models, many feature attribution methods have been developed that assign importance scores to input features. Some recent work challenges the robustness of these methods by showing that they are sensitive to input and model perturbations, while other work addresses this issue by proposing robust attribution methods. However, previous work on attribution robustness has focused primarily on gradient-based feature attributions, whereas the robustness of removal-based attribution methods is not currently well understood. To bridge this gap, we theoretically characterize the robustness properties of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and derive upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical results on synthetic and real-world data validate our theoretical results and demonstrate their practical implications, including the ability to increase attribution robustness by improving the modelâ€™s Lipschitz regularity.",NIPS
"Learning about the three-dimensional world from two-dimensional images is a fundamental problem in computer vision. An ideal neural network architecture for such tasks would leverage the fact that objects can be rotated and translated in three dimensions to make predictions about novel images. However, imposing $SO(3)$-equivariance on two-dimensional inputs is difficult because the group of three-dimensional rotations does not have a natural action on the two-dimensional plane. Specifically, it is possible that an element of $SO(3)$ will rotate an image out of plane. We show that an algorithm that learns a three-dimensional representation of the world from two dimensional images must satisfy certain consistency properties which we formulate as $SO(2)$-equivariance constraints. We use the induced representation of $SO(2)$ on $SO(3)$ to construct and classify architectures that have two-dimensional inputs and which satisfy these consistency constraints. We prove that any architecture which respects said consistency constraints can be realized as an instance of our construction. We show that three previously proposed neural architectures for 3D pose prediction are special cases of our construction. We propose a new algorithm that is a learnable generalization of previously considered methods. We test our architecture on three pose predictions task and achieve SOTA results on both the PASCAL3D+ and SYMSOL pose estimation tasks.",NIPS
"Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the final state. We instantiate all reward-based RL subroutines by concrete provable algorithms, and apply our theory to a large class of models including tabular MDPs and MDPs with generic function approximation. We further provide guarantees when K-wise comparisons are available.",NIPS
"Despite their immense success as a model of macaque visual cortex, deep convolutional neural networks (CNNs) have struggled to predict activity in visual cortex of the mouse, which is thought to be strongly dependent on the animalâ€™s behavioral state. Furthermore, most computational models focus on predicting neural responses to static images presented under head fixation, which are dramatically different from the dynamic, continuous visual stimuli that arise during movement in the real world. Consequently, it is still unknown how natural visual input and different behavioral variables may integrate over time to generate responses in primary visual cortex (V1). To address this, we introduce a multimodal recurrent neural network that integrates gaze-contingent visual input with behavioral and temporal dynamics to explain V1 activity in freely moving mice. We show that the model achieves state-of-the-art predictions of V1 activity during free exploration and demonstrate the importance of each component in an extensive ablation study. Analyzing our model using maximally activating stimuli and saliency maps, we reveal new insights into cortical function, including the prevalence of mixed selectivity for behavioral variables in mouse V1. In summary, our model offers a comprehensive deep-learning framework for exploring the computational principles underlying V1 neurons in freely-moving animals engaged in natural behavior.",NIPS
"To avoid failures on out-of-distribution data, recent works have sought to extract features that have an invariant or stable relationship with the label across domains, discarding ""spurious"" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information that could boost performance if used correctly in the test domain. In this work, we show how this can be done without test-domain labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.",NIPS
"In image recovery problems, one seeks to infer an image from distorted, incomplete, and/or noise-corrupted measurements.Such problems arise in magnetic resonance imaging (MRI), computed tomography, deblurring, super-resolution, inpainting, phase retrieval, image-to-image translation, and other applications. Given a training set of signal/measurement pairs, we seek to do more than just produce one good image estimate. Rather, we aim to rapidly and accurately sample from the posterior distribution. To do this,we propose a regularized conditional Wasserstein GAN that generates dozens of high-quality posterior samples per second. Our regularization comprises an $\ell_1$ penalty and an adaptively weighted standard-deviation reward. Using quantitative evaluation metrics like conditional FrÃ©chet inception distance, we demonstrate that our method produces state-of-the-art posterior samples in both multicoil MRI and large-scale inpainting applications. The code for our model can be found here: https://github.com/matt-bendel/rcGAN.",NIPS
"We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input.Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation.After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation.",NIPS
"While recent large-scale neural codec language models have shown significant improvement in zero-shot TTS by training on thousands of hours of data, they suffer from drawbacks such as a lack of robustness, slow sampling speed similar to previous autoregressive TTS methods, and reliance on pre-trained neural codec representations. Our work proposes P-Flow, a fast and data-efficient zero-shot TTS model that uses speech prompts for speaker adaptation. P-Flow comprises a speech-prompted text encoder for speaker adaptation and a flow matching generative decoder for high-quality and fast speech synthesis. Our speech-prompted text encoder uses speech prompts and text input to generate speaker-conditional text representation. The flow matching generative decoder uses the speaker-conditional output to synthesize high-quality personalized speech significantly faster than in real-time. Unlike the neural codec language models, we specifically train P-Flow on LibriTTS dataset using a continuous mel-representation. Through our training method using continuous speech prompts, P-Flow matches the speaker similarity performance of the large-scale zero-shot TTS models with two orders of magnitude less training data and has more than 20$\times$ faster sampling speed. Our results show that P-Flow has better pronunciation and is preferred in human likeness and speaker similarity to its recent state-of-the-art counterparts, thus defining P-Flow as an attractive and desirable alternative. We provide audio samples on our demo page: [https://research.nvidia.com/labs/adlr/projects/pflow](https://research.nvidia.com/labs/adlr/projects/pflow)",NIPS
"In a backdoor attack, an adversary injects corrupted data into a model's training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.",NIPS
"Goal-conditioned reinforcement learning (RL) is a powerful approach for learning general-purpose skills by reaching diverse goals. However, it has limitations when it comes to task-conditioned policies, where goals are specified by temporally extended instructions written in the Linear Temporal Logic (LTL) formal language. Existing approaches for finding LTL-satisfying policies rely on sampling a large set of LTL instructions during training to adapt to unseen tasks at inference time. However, these approaches do not guarantee generalization to out-of-distribution LTL objectives, which may have increased complexity. In this paper, we propose a novel approach to address this challenge. We show that simple goal-conditioned RL agents can be instructed to follow arbitrary LTL specifications without additional training over the LTL task space. Unlike existing approaches that focus on LTL specifications expressible as regular expressions, our technique is unrestricted and generalizes to $\omega$-regular expressions. Experiment results demonstrate the effectiveness of our approach in adapting goal-conditioned RL agents to satisfy complex temporal logic task specifications zero-shot.",NIPS
"Equivariant networks are specifically designed to ensure consistent behavior with respect to a set of input transformations, leading to higher sample efficiency and more accurate and robust predictions. However, redesigning each component of prevalent deep neural network architectures to achieve chosen equivariance is a difficult problem and can result in a computationally expensive network during both training and inference. A recently proposed alternative towards equivariance that removes the architectural constraints is to use a simple canonicalization network that transforms the input to a canonical form before feeding it to an unconstrained prediction network. We show here that this approach can effectively be used to make a large pretrained network equivariant. However, we observe that the produced canonical orientations can be misaligned with those of the training distribution, hindering performance. Using dataset-dependent priors to inform the canonicalization function, we are able to make large pretrained models equivariant while maintaining their performance. This significantly improves the robustness of these models to deterministic transformations of the data, such as rotations. We believe this equivariant adaptation of large pretrained models can help their domain-specific applications with known symmetry priors.",NIPS
"Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of â€œequi-confidenceâ€ level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence.",NIPS
"Large language models are now tuned to align with the goals of their creators, namely to be ""helpful and harmless."" These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study adversarial alignment, and ask to what extent these models remain aligned when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited.We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.",NIPS
"Geometric representation learning of molecules is challenging yet essential for applications in multiple domains. Despite the impressive breakthroughs made by geometric deep learning in various molecular representation learning tasks, effectively capturing complicated geometric features across spatial dimensions is still underexplored due to the significant difficulties in modeling efficient geometric representations and learning the inherent correlation in 3D structural modeling. These include computational inefficiency, underutilization of vectorial embeddings, and limited generalizability to integrate various geometric properties. To address the raised concerns, we introduce an efficient and effective framework, Scalable Vector Network (SaVeNet), designed to accommodate a range of geometric requirements without depending on costly embeddings. In addition, the proposed framework scales effectively with introduced direction noise. Theoretically, we analyze the desired properties (i.e., invariance and equivariant) and framework efficiency of the SaVeNet. Empirically, we conduct a comprehensive series of experiments to evaluate the efficiency and expressiveness of the proposed model. Our efficiency-focused experiments underscore the model's empirical superiority over existing methods. Experimental results on synthetic and real-world datasets demonstrate the expressiveness of our model, which achieves state-of-the-art performance across various tasks within molecular representation learning.",NIPS
"Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL objective with an auxiliary task that guides part of the architecture to learn a valid probabilistic model as an inductive bias. We demonstrate that our method achieves state-of-the-art regret results against various baselines in experiments on standard hyperparameter optimisation tasks and also outperforms others in the real-world problems of mixed-integer programming tuning, antibody design, and logic synthesis for electronic design automation.",NIPS
"Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention---which is the only component scaling quadratically w.r.t. the sequence length---becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\times$ and $3.3\times$ for sequences of respectively $8k$ and $16k$ tokens.",NIPS
"Deep neural networks have achieved significant success in the last decades, but they are not well-calibrated and often produce unreliable predictions. A large number of literature relies on uncertainty quantification to evaluate the reliability of a learning model, which is particularly important for applications of out-of-distribution (OOD) detection and misclassification detection. We are interested in uncertainty quantification for interdependent node-level classification. We start our analysis based on graph posterior networks (GPNs) that optimize the uncertainty cross-entropy (UCE)-based loss function. We describe the theoretical limitations of the widely-used UCE loss. To alleviate the identified drawbacks, we propose a distance-based regularization that encourages clustered OOD nodes to remain clustered in the latent space. We conduct extensive comparison experiments on eight standard datasets and demonstrate that the proposed regularization outperforms the state-of-the-art in both OOD detection and misclassification detection.",NIPS
"Unconstrained and natural  behavior consists of dynamics that are complex and  unpredictable, especially when trying to predict what will happen  multiple steps into the future. While some success has been found in building representations of animal behavior under constrained or simplified task-based conditions, many of these models cannot be applied to free and naturalistic settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task representation learning model for animal behavior that combines two novel components: (i) an action-prediction objective that aims to predict the  distribution of actions over future timesteps, and (ii) a multi-scale architecture that builds separate latent spaces to accommodate short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and global dynamics in robots in varying environments and terrains, we apply our method to the MABe 2022 Multi-Agent Behavior challenge, where our model ranks first overall on both mice and fly benchmarks. In all of these cases, we show that our model can build representations that capture the many different factors that drive behavior and solve a wide range of downstream tasks.",NIPS
"What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or ""channel"") that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. Unlike humans, the neural network channel is very broad, 2-4 times wider than the human channel. This means that the network channel extends to frequencies higher and lower than those that humans are sensitive to. Thus, noise at those frequencies will impair network performance and spare human performance. Adversarial and augmented-image training are commonly used to increase network robustness and shape bias. Does this training align network and human object recognition channels? Three network channel properties (bandwidth, center frequency, peak noise sensitivity) correlate strongly with shape bias (51% variance explained) and robustness of adversarially-trained networks (66% variance explained). Adversarial training increases robustness but expands the channel bandwidth even further beyond the human bandwidth. Thus, critical band masking reveals that the network channel is more than twice as wide as the human channel, and that adversarial training only makes it worse. Networks with narrower channels might be more robust.",NIPS
"Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences. According to these preferences, it is desirable that a coalition structure (i.e. a partition of agents into coalitions) satisfies some form of stability. The most well-known and natural of such notions is arguably core-stability. Informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition. Unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one. To circumvent these problems, we propose the notion of $\varepsilon$-fractional core-stability, where at most an $\varepsilon$-fraction of all possible coalitions is allowed to core-block. It turns out that such a relaxation may guarantee both existence and polynomial-time computation. Specifically, we design efficient algorithms returning an $\varepsilon$-fractional core-stable partition, with $\varepsilon$ exponentially decreasing in the number of agents, for two fundamental classes of HGs: Simple Fractional and Anonymous. From a probabilistic point of view, being the definition of $\varepsilon$-fractional core equivalent to requiring that uniformly sampled coalitions core-block with probability lower than $\varepsilon$, we further extend the definition to handle more complex sampling distributions. Along this line, when valuations have to be learned from samples in a PAC-learning fashion, we give positive and negative results on which distributions allow the efficient computation of outcomes that are $\varepsilon$-fractional core-stable with arbitrarily high confidence.",NIPS
"Changes in the data distribution at test time can have deleterious effects on the performance of predictive models $p(y|x)$.We consider situations where there are additional meta-data labels (such as group labels), denoted by $z$, that can account for such changes in the distribution.In particular, we assume that the prior distribution $p(y,z)$, which models the dependence between the class label $y$ and the ""nuisance"" factors $z$, may change across domains, either due to a change in the correlation between these terms, or a change in one of their marginals.However, we assume that the generative model for features $p(x|y,z)$ is invariant across domains.We note that this corresponds to an expanded version of the widely used ""label shift"" assumption, where the labels now also include the nuisance factors $z$. Based on this observation,  we propose a test-time label shift correction that adapts to changes in the joint distribution $p(y, z)$ using EM applied to unlabeled samples from the target domain distribution, $p_t(x)$.Importantly, we are able to avoid fitting a generative model $p(x|y,z)$, and merely need to reweight the outputs of a discriminative model $p_s(y,z|x)$ trained on the source distribution.We evaluate our method, which we call ""Test-Time Label-Shift Adaptation"" (TTLSA), on several standard image and text datasets, as well as the CheXpert chest X-ray dataset, and show that it improves performance over methods that target invariance to changes in the distribution, as well as baseline empirical risk minimization methods.Code for reproducing experiments is available at https://github.com/nalzok/test-time-label-shift.",NIPS
"Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available https://github.com/mnoukhov/elastic-reset",NIPS
"Diffusion models have been shown to be capable of generating high-quality images, suggesting that they could contain meaningful internal representations. Unfortunately, the feature maps that encode a diffusion model's internal information are spread not only over layers of the network, but also over diffusion timesteps, making it challenging to extract useful descriptors. We propose Diffusion Hyperfeatures, a framework for consolidating  multi-scale and multi-timestep feature maps into per-pixel feature descriptors that can be used for downstream tasks. These descriptors can be extracted for both synthetic and real images using the generation and inversion processes. We evaluate the utility of our Diffusion Hyperfeatures on the task of semantic keypoint correspondence: our method achieves superior performance on the SPair-71k real image benchmark. We also demonstrate that our method is flexible and transferable: our feature aggregation network trained on the inversion features of real image pairs can be used on the generation features of synthetic image pairs with unseen objects and compositions. Our code is available at https://diffusion-hyperfeatures.github.io.",NIPS
"Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors.This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients.  Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks.  In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem.  Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products.  We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, modalities, attack implementations, and perturbation-based defenses.  With this novel tool, we provide insights into effective gradient perturbation directions, the unfairness of privacy protection, and privacy-preferred model initialization.  Our codes are provided in https://github.com/illidanlab/inversion-influence-function.",NIPS
"Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one example, a query like ``a pink sunflower and a yellow flamingo'' may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. The loss is optimized during inference, without retraining or fine-tuning the model. Human evaluation on three datasets, including one new and challenging set, demonstrate significant improvements of SynGen compared with current state of the art methods. This work highlights how making use of sentence structure during inference can efficiently and substantially improve the faithfulness of text-to-image generation.",NIPS
"Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful.",NIPS
"We developed a new method PROTES for black-box optimization, which is based on the probabilistic sampling from a probability density function given in the low-parametric tensor train format. We tested it on complex multidimensional arrays and discretized multivariable functions taken, among others, from real-world applications, including unconstrained binary optimization and optimal control problems, for which the possible number of elements is up to $2^{1000}$. In numerical experiments, both on analytic model functions and on complex problems, PROTES outperforms popular discrete optimization methods (Particle Swarm Optimization, Covariance Matrix Adaptation, Differential Evolution, and others).",NIPS
"This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique--Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.",NIPS
"How does a single interconnected neural population perform multiple tasks, each with its own dynamical requirements? The relation between task requirements and neural dynamics in Recurrent Neural Networks (RNNs) has been investigated for single tasks. The forces shaping joint dynamics of multiple tasks, however, are largely unexplored. In this work, we first construct a systematic framework to study multiple tasks in RNNs, minimizing interference from input and output correlations with the hidden representation. This allows us to reveal how RNNs tend to share attractors and reuse dynamics, a tendency we define as the ""simplicity bias"".We find that RNNs develop attractors sequentially during training, preferentially reusing existing dynamics and opting for simple solutions when possible. This sequenced emergence and preferential reuse encapsulate the simplicity bias. Through concrete examples, we demonstrate that new attractors primarily emerge due to task demands or architectural constraints, illustrating a balance between simplicity bias and external factors.We examine the geometry of joint representations within a single attractor, by constructing a family of tasks from a set of functions. We show that the steepness of the associated functions controls their alignment within the attractor. This arrangement again highlights the simplicity bias, as points with similar input spacings undergo comparable transformations to reach the shared attractor.Our findings propose compelling applications. The geometry of shared attractors might allow us to infer the nature of unknown tasks. Furthermore, the simplicity bias implies that without specific incentives, modularity in RNNs may not spontaneously emerge, providing insights into the conditions required for network specialization.",NIPS
"By now Bayesian methods are routinely used in practice for solving inverse problems. In inverse problems the parameter or signal of interest is observed only indirectly, as an image of a given map, and the observations are typically further corrupted with noise. Bayes offers a natural way to regularize these problems via the prior distribution and provides a probabilistic solution, quantifying the remaining uncertainty in the problem. However, the computational costs of standard, sampling based Bayesian approaches can be overly large in such complex models. Therefore, in practice variational Bayes is becoming increasingly popular. Nevertheless, the theoretical understanding of these methods is still relatively limited, especially in context of inverse problems.In our analysis we investigate variational Bayesian methods for Gaussian process priors to solve linear inverse problems. We consider both mildly and severely ill-posed inverse problems and work with the popular inducing variable variational Bayes approach proposed by Titsias [Titsias, 2009]. We derive posterior contraction rates for the variational posterior in general settings and show that the minimax estimation rate can be attained by correctly tunned procedures. As specific examples we consider a collection of inverse problems including the heat equation, Volterra operator and Radon transform and inducing variable methods based on population and empirical spectral features.",NIPS
"Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is often necessary to optimize their performance. However, a major obstacle is the limited availability of labeled data. We study the use of pseudolabels, i.e., heuristic labels for unlabeled data, to enhance CLIP via prompt tuning. Conventional pseudolabeling trains a model on labeled data and then generates labels for unlabeled data. VLMs' zero-shot capabilities enable a ``second generation'' of pseudolabeling approaches that do not require task-specific training on labeled data. By using zero-shot pseudolabels as a source of supervision, we observe that learning paradigms such as semi-supervised, transductive zero-shot, and unsupervised learning can all be seen as optimizing the same loss function. This unified view enables the development of versatile training strategies that are applicable across learning paradigms. We investigate them on image classification tasks where CLIP exhibits limitations, by varying prompt modalities, e.g., textual or visual prompts, and learning paradigms. We find that(1) unexplored prompt tuning strategies that iteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5 points in semi-supervised learning, by 28.4 points in transductive zero-shot learning, and by 15.2 points in unsupervised learning, and (2) unlike conventional semi-supervised pseudolabeling, which exacerbates model biases toward classes with higher-quality pseudolabels, prompt tuning leads to a more equitable distribution of per-class accuracy. The code to reproduce the experiments is at https://github.com/BatsResearch/menghini-neurips23-code.",NIPS
"The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, two issues remain unsolved in this line of work. First, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al.), this approach results in slower convergence rates under interpolation. Second, intuitive line-search methods equipped with variance-reduction (VR) fail to converge (Dubois-Taine et al.). So far, no VR methods successfully accelerate these two stepsizes with a convergence guarantee.In this work, we make two contributions:Firstly, we propose two new robust variants of SPS and SLS, called AdaSPS and AdaSLS, which achieve optimal asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation settings, except for the case when we have both strong convexity and non-interpolation. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we propose a novel VR method that can use Polyak stepsizes or line-search to achieve acceleration. When it is equipped with AdaSPS or AdaSLS, the resulting algorithms obtain the optimal ratefor optimizing convex smooth functions. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.",NIPS
"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.In our setting, in addition to random observation noise, the stochastic gradient may be subject to independent \emph{oblivious noise}, which may not have bounded moments and is not necessarily centered. Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is the  bounded variance observation noise and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. The only assumption we make on the oblivious noise $\xi$ is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.In this setting, it is not information-theoretically possible to recover a single solution close to the target when the fraction of inliers $\alpha$ is less than $1/2$. Our main result is an efficient {\em list-decodable} learner that recovers a small list of candidates at least one of which is close to the true solution. On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently smallconstant, the algorithm recovers a single solution.Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, which may be of independent interest.",NIPS
"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",NIPS
"Humans solving algorithmic (or) reasoning problems typically exhibit solution times that grow as a function of problem difficulty. Adaptive recurrent neural networks have been shown to exhibit this property for various language-processing tasks. However, little work has been performed to assess whether such adaptive computation can also enable vision models to extrapolate solutions beyond their training distribution's difficulty level, with prior work focusing on very simple tasks. In this study, we investigate a critical functional role of such adaptive processing using recurrent neural networks: to dynamically scale computational resources conditional on input requirements that allow for zero-shot generalization to novel difficulty levels not seen during training using two challenging visual reasoning tasks: PathFinder and Mazes. We combine convolutional recurrent neural networks (ConvRNNs) with a learnable halting mechanism based on Graves (2016). We explore various implementations of such adaptive ConvRNNs (AdRNNs) ranging from tying weights across layers to more sophisticated biologically inspired recurrent networks that possess lateral connections and gating. We show that 1) AdRNNs learn to dynamically halt processing early (or late) to solve easier (or harder) problems, 2) these RNNs zero-shot generalize to more difficult problem settings not shown during training by dynamically increasing the number of recurrent iterations at test time. Our study provides modeling evidence supporting the hypothesis that recurrent processing enables the functional advantage of adaptively allocating compute resources conditional on input requirements and hence allowing generalization to harder difficulty levels of a visual reasoning problem without training.",NIPS
"Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin.",NIPS
"Many applications seek to recover low-rank approximations of noisy tensor data. We consider several practical and effective matricization strategies which construct specific matrices from such tensors and then apply spectral methods; the strategies include tensor unfolding, partial tracing, power iteration, and recursive unfolding. We settle the behaviors of unfolding and partial tracing, identifying sharp thresholds in signal-to-noise ratio above which the signal is partially recovered. In particular, we extend previous results to a much larger class of tensor shapes where axis lengths may be different. For power iteration and recursive unfolding, we prove that under conditions where previous algorithms partially recovery the signal, these methods achieve (asymptotically) exact recovery. Our analysis deploys random matrix theory to obtain sharp thresholds which elude perturbation and concentration bounds. Specifically, we rely upon recent disproportionate random matrix results, which describe sequences of matrices with diverging aspect ratio.",NIPS
"Feedforward generalizable models for implicit shape reconstruction from unoriented point cloud present multiple advantages, including high performance and inference speed. However, they still suffer from generalization issues, ranging from underfitting the input point cloud, to misrepresenting samples outside of the training data distribution, or with toplogies unseen at training.  We propose here an efficient mechanism to remedy some of these limitations at test time. We combine the inter-shape data prior of the network with an intra-shape regularization prior of a NystrÃ¶m Kernel Ridge Regression, that we further adapt by fitting its hyperprameters to the current shape. The resulting shape function defined in a shape specific Reproducing Kernel Hilbert Space benefits from desirable stability and efficiency properties and grants a shape adaptive expressiveness-robustness trade-off. We demonstrate the improvement obtained through our method  with respect to baselines and the state-of-the-art using synthetic and real data.",NIPS
"Classical analysis of convex and non-convex optimization methods often requires the Lipschitz continuity of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm  bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to  stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with bounded variance in the stochastic setting.",NIPS
"Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose \emph{Reflexion}, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. We release all code, demos, and datasets at \url{https://github.com/noahshinn024/reflexion}.",NIPS
"We prove that, for finite-arm bandits with linear function approximation, the global convergence of policy gradient (PG) methods depends on inter-related properties between the policy update and the representation. textcolor{blue}{First}, we establish a few key observations that frame the study: \textbf{(i)} Global convergence can be achieved under linear function approximation without policy or reward realizability, both for the standard Softmax PG and natural policy gradient (NPG). \textbf{(ii)} Approximation error is not a key quantity for characterizing global convergence in either algorithm. \textbf{(iii)} The conditions on the representation that imply global convergence are different between these two algorithms. Overall, these observations call into question approximation error as an appropriate quantity for characterizing the global convergence of PG methods under linear function approximation. \textcolor{blue}{Second}, motivated by these observations, we establish new general results: \textbf{(i)} NPG with linear function approximation achieves global convergence \emph{if and only if} the projection of the reward  onto the representable space preserves the optimal action's rank, a quantity that is not strongly related to approximation error. \textbf{(ii)} The global convergence of Softmax PG occurs if the representation satisfies a non-domination condition and can preserve the ranking of rewards, which goes well beyond policy or reward realizability. We provide experimental results to support these theoretical findings.",NIPS
"Segmentation of curvilinear structures such as vasculature and road networks is challenging due to relatively weak signals and complex geometry/topology. To facilitate and accelerate large scale annotation, one has to adopt semi-automatic approaches such as proofreading by experts. In this work, we focus on uncertainty estimation for such tasks, so that highly uncertain, and thus error-prone structures can be identified for human annotators to verify. Unlike most existing works, which provide pixel-wise uncertainty maps, we stipulate it is crucial to estimate uncertainty in the units of topological structures, e.g., small pieces of connections and branches. To achieve this, we leverage tools from topological data analysis, specifically discrete Morse theory (DMT), to first capture the structures, and then reason about their uncertainties. To model the uncertainty, we (1) propose a joint prediction model that estimates the uncertainty of a structure while taking the neighboring structures into consideration (inter-structural uncertainty); (2) propose a novel Probabilistic DMT to model the inherent uncertainty within each structure (intra-structural uncertainty) by sampling its representations via a perturb-and-walk scheme. On various 2D and 3D datasets, our method produces better structure-wise uncertainty maps compared to existing works. Code available at: https://github.com/Saumya-Gupta-26/struct-uncertainty",NIPS
"Spectral clustering is a popular and effective algorithm designed to find $k$ clusters in a graph $G$.In the classical spectral clustering algorithm, the vertices of $G$ are embedded into $\mathbb{R}^k$ using $k$ eigenvectors of the graph Laplacian matrix.However, computing this embedding is computationally expensive and dominates the running time of the algorithm.In this paper, we present a simple spectral clustering algorithm based on a vertex embedding with $O(\log(k))$ vectors computed by the power method.The vertex embedding is computed in nearly-linear time with respect to the size of the graph, andthe algorithm provably recovers the ground truth clusters under natural assumptions on the input graph.We evaluate the new algorithm on several synthetic and real-world datasets, finding that it is significantly faster than alternative clustering algorithms,while producing results with approximately the same clustering accuracy.",NIPS
"The ability for the brain to discriminate among visual stimuli is constrained by their retinal representations. Previous studies of visual discriminability have been limited to either low-dimensional artificial stimuli or pure theoretical considerations without a realistic encoding model. Here we propose a novel framework for understanding stimulus discriminability achieved by retinal representations of naturalistic stimuli with the method of information geometry. To model the joint probability distribution of neural responses conditioned on the stimulus, we created a stochastic encoding model of a population of salamander retinal ganglion cells based on a three-layer convolutional neural network model. This model not only accurately captured the mean response to natural scenes but also a variety of second-order statistics. With the model and the proposed theory, we computed the Fisher information metric over stimuli to study the most discriminable stimulus directions. We found that the most discriminable stimulus varied substantially across stimuli, allowing an examination of the relationship between the most discriminable stimulus and the current stimulus. By examining responses generated by the most discriminable stimuli we further found that the most discriminative response mode is often aligned with the most stochastic mode. This finding carries the important implication that under natural scenes, retinal noise correlations are information-limiting rather than increasing information transmission as has been previously speculated. We additionally observed that sensitivity saturates less in the population than for single cells and that as a function of firing rate, Fisher information varies less than sensitivity. We conclude that under natural scenes, population coding benefits from complementary coding and helps to equalize the information carried by different firing rates, which may facilitate decoding of the stimulus under principles of information maximization.",NIPS
"Point clouds are versatile representations of 3D objects and have found widespread application in science and engineering. Many successful deep-learning models have been proposed that use them as input. The domain of chemical and materials modeling is especially challenging because exact compliance with physical constraints is highly desirable for a model to be usable in practice. These constraints include smoothness and invariance with respect to translations, rotations, and permutations of identical atoms. If these requirements are not rigorously fulfilled, atomistic simulations might lead to absurd outcomes even if the model has excellent accuracy. Consequently, dedicated architectures, which achieve invariance by restricting their design space, have been developed. General-purpose point-cloud models are more varied but often disregard rotational symmetry. We propose a general symmetrization method that adds rotational equivariance to any given model while preserving all the other requirements.Our approach simplifies the development of better atomic-scale machine-learning schemes by relaxing the constraints on the design space and making it possible to incorporate ideas that proved effective in other domains.We demonstrate this idea by introducing the Point Edge Transformer (PET) architecture, which is not intrinsically equivariant but achieves state-of-the-art performance on several benchmark datasets of molecules and solids. A-posteriori application of our general protocol makes PET exactly equivariant, with minimal changes to its accuracy.",NIPS
"In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory of Adam, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradient complexity of $\mathcal{O}(\epsilon^{-3})$.",NIPS
"We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the unobserved solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn a latent SDE in $\mathbb{R}^n$ from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves inside a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In the context of learning problems, SDEs on the $n$-dimensional unit sphere are arguably the most relevant incarnation of this setup. For variational inference, the sphere not only facilitates using a uniform prior on the initial state of the SDE, but we also obtain a particularly simple and intuitive expression for the KL divergence between the approximate posterior and prior process in the evidence lower bound. We provide empirical evidence that a latent SDE of the proposed type can be learned efficiently by means of an existing one-step geometric Euler-Maruyama scheme. Despite restricting ourselves to a less diverse class of SDEs, we achieve competitive or even state-of-the-art performance on a collection of time series interpolation and classification benchmarks.",NIPS
"The promising zero-shot generalization of vision-language models such as CLIP has led to their adoption using prompt learning for numerous downstream tasks. Previous works have shown test-time prompt tuning using entropy minimization to adapt text prompts for unseen domains. While effective, this overlooks the key cause for performance degradation to unseen domains -- distribution shift. In this work, we explicitly handle this problem by aligning the out-of-distribution (OOD) test sample statistics to those of the source data using prompt tuning. We use a single test sample to adapt multi-modal prompts at test time by minimizing the feature distribution shift to bridge the gap in the test domain. Evaluating against the domain generalization benchmark, our method improves zero-shot top-1 accuracy beyond existing prompt-learning techniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset generalization with unseen categories across 10 datasets, our method improves consistently across all datasets compared to the existing state-of-the-art. Our source code and models are available at https://jameelhassan.github.io/promptalign",NIPS
"In this paper, we consider the optimization problem Submodular Cover (SCP), which is to find a minimum cardinality subset of a finite universe $U$ such that the value of a submodular function $f$ is above an input threshold $\tau$. In particular, we consider several variants of SCP including the general case, the case where $f$ is additionally assumed to be monotone, and finally the case where $f$ is a regularized monotone submodular function. Our most significant contributions are that: (i) We propose a scalable algorithm for monotone SCP that achieves nearly the same approximation guarantees as the standard greedy algorithm in significantly faster time; (ii) We are the first to develop an algorithm for general SCP that achieves a solution arbitrarily close to being feasible; and finally (iii) we are the first to develop algorithms for regularized SCP. Our algorithms are then demonstrated to be effective in an extensive experimental section on data summarization and graph cut, two applications of SCP.",NIPS
"Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.",NIPS
"Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average  2.7% performance improvement over the image, natural language, and graph domains.",NIPS
"We give the first tester-learner for halfspaces that succeeds universally over a wide class of structured distributions. Our universal tester-learner runs in fully polynomial time and has the following guarantee: the learner achieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution that the tester accepts, and moreover, the tester accepts whenever the marginal is any distribution that satisfies a Poincare inequality. In contrast to prior work on testable learning, our tester is not tailored to any single target distribution but rather succeeds for an entire target class of distributions. The class of Poincare distributions includes all strongly log-concave distributions, and, assuming the Kannan--Lovasz--Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner achieves error $\mathrm{opt} + \epsilon$ while accepting all log-concave distributions unconditionally (without assuming KLS).Our tests rely on checking hypercontractivity of the unknown distribution using a sum-of-squares (SOS) program, and crucially make use of the fact that Poincare distributions are certifiably hypercontractive in the SOS framework.",NIPS
"In this paper, we propose an online convex optimization approach with two different levels of adaptivity. On a higher level, our approach is agnostic to the unknown types and curvatures of the online functions, while at a lower level, it can exploit the unknown niceness of the environments and attain problem-dependent guarantees. Specifically, we obtain $\mathcal{O}(\log V_T)$, $\mathcal{O}(d \log V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and the $\hat{\mathcal{O}}(\cdot)$-notation omits $\log V_T$ factors. Our result not only safeguards the worst-case guarantees but also directly implies the small-loss bounds in analysis. Moreover, when applied to adversarial/stochastic convex optimization and game theory problems, our result enhances the existing universal guarantees. Our approach is based on a multi-layer online ensemble framework incorporating novel ingredients, including a carefully designed optimism for unifying diverse function types and cascaded corrections for algorithmic stability. Notably, despite its multi-layer structure, our algorithm necessitates only one gradient query per round, making it favorable when the gradient evaluation is time-consuming. This is facilitated by a novel regret decomposition equipped with carefully designed surrogate losses.",NIPS
"A growing line of work shows how learned predictions can be used to break through worst-case barriers to improve the running time of an algorithm. However, incorporating predictions into data structures with strong theoretical guarantees remains underdeveloped.  This paper takes a step in this direction by showing that predictions can be leveraged in the fundamental online list labeling problem. In the problem, $n$ items arrive over time and must be stored in sorted order in an array of size $\Theta(n)$.  The array slot of an element is its label and the goal is to maintain sorted order while minimizing the total number of elements moved (i.e., relabeled). We design a new list labeling data structure and bound its performance in two models.  In the worst-case learning-augmented model, we give guarantees in terms of the error in the predictions.  Our data structure provides strong guarantees: it is optimal for any prediction error and guarantees the best-known worst-case bound even when the predictions are entirely erroneous. We also consider a stochastic error model and bound the performance in terms of the expectation and variance of the error. Finally, the theoretical results are demonstrated empirically.  In particular, we show that our data structure has strong performance on real temporal data sets where predictions are constructed from elements that arrived in the past, as is typically done in a practical use case.",NIPS
"Energy-based learning algorithms have recently gained a surge of interest due to their compatibility with analog (post-digital) hardware. Existing algorithms include contrastive learning (CL), equilibrium propagation (EP) and coupled learning (CpL), all consisting in contrasting two states, and differing in the type of perturbation used to obtain the second state from the first one. However, these algorithms have never been explicitly compared on equal footing with same models and datasets, making it difficult to assess their scalability and decide which one to select in practice. In this work, we carry out a comparison of seven learning algorithms, namely CL and different variants of EP and CpL depending on the signs of the perturbations. Specifically, using these learning algorithms, we train deep convolutional Hopfield networks (DCHNs) on five vision tasks (MNIST, F-MNIST, SVHN, CIFAR-10 and CIFAR-100). We find that, while all algorithms yield comparable performance on MNIST, important differences in performance arise as the difficulty of the task increases. Our key findings reveal that negative perturbations are better than positive ones, and highlight the centered variant of EP (which uses two perturbations of opposite sign) as the best-performing algorithm. We also endorse these findings with theoretical arguments. Additionally, we establish new SOTA results with DCHNs on all five datasets, both in performance and speed. In particular, our DCHN simulations are 13.5 times faster with respect to Laborieux et al. (2021), which we achieve thanks to the use of a novel energy minimisation algorithm based on asynchronous updates, combined with reduced precision (16 bits).",NIPS
"Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to~sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination that allows an $\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.",NIPS
"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",NIPS
"Ordinal classification (OC), i.e., labeling instances along classes with a natural ordering, is common in multiple  applications such as size or budget based recommendations and disease severity labeling.  Often in practical scenarios, it is desirable to obtain a small set of likely classes with a guaranteed high chance of including the true class. Recent works on conformal prediction (CP) address this problem for the classification setting with non-ordered labels but the resulting prediction sets (PS) are often non-contiguous and unsuitable for ordinal classification. In this work, we propose a framework to adapt existing CP methods to generate contiguous sets with guaranteed coverage and minimal cardinality. Our framework employs a novel non-parametric approach for modeling unimodal distributions. Empirical results on both synthetic and real-world datasets demonstrate our method outperforms SOTA baselines by 4% on Accuracy@K and 8% on PS size.",NIPS
"The design automation of analog circuits poses significant challenges in terms of the large design space, complex interdependencies between circuit specifications, and resource-intensive simulations. To address these challenges, this paper presents an innovative framework called the Graph of Circuits Explorer (GCX). Leveraging graph structure learning along with graph neural networks, GCX enables the creation of a surrogate model that facilitates efficient exploration of the optimal design space within a semi-supervised learning framework which reduces the need for large labelled datasets. The proposed approach comprises three key stages. First, we learn the geometric representation of circuits and enrich it with technology information to create a comprehensive feature vector. Subsequently, integrating feature-based graph learning with few-shot and zero-shot learning  enhances the generalizability in predictions for unseen circuits. Finally, we introduce two algorithms namely, EASCO and ASTROG which upon integration with GCX optimize the available samples to yield the optimal circuit configuration meeting the designer's criteria. The effectiveness of the proposed approach is demonstrated through simulated performance evaluation of various circuits, using derived parameters in 180nm CMOS technology. Furthermore, the generalizability of the approach is extended to higher-order topologies and different technology nodes such as 65nm and 45nm CMOS process nodes.",NIPS
"Conversational Recommender Systems (CRS) actively elicit user preferences to generate adaptive recommendations. Mainstream reinforcement learning-based CRS solutions heavily rely on handcrafted reward functions, which may not be aligned with user intent in CRS tasks. Therefore, the design of task-specific rewards is critical to facilitate CRS policy learning, which remains largely under-explored in the literature. In this work, we propose a novel approach to address this challenge by learning intrinsic rewards from interactions with users. Specifically, we formulate intrinsic reward learning as a multi-objective bi-level optimization problem. The inner level optimizes the CRS policy augmented by the learned intrinsic rewards, while the outer level drives the intrinsic rewards to optimize two CRS-specific objectives: maximizing the success rate and minimizing the number of turns to reach a successful recommendation}in conversations. To evaluate the effectiveness of our approach, we conduct extensive experiments on three public CRS benchmarks. The results show that our algorithm significantly improves CRS performance by exploiting informative learned intrinsic rewards.",NIPS
"Online learning holds the promise of enabling efficient long-term credit assignment in recurrent neural networks. However, current algorithms fall short of offline backpropagation by either not being scalable or failing to learn long-range dependencies. Here we present a high-performance online learning algorithm that merely doubles the memory and computational requirements of a single inference pass. We achieve this by leveraging independent recurrent modules in multi-layer networks, an architectural motif that has recently been shown to be particularly powerful. Experiments on synthetic memory problems and on the challenging long-range arena benchmark suite reveal that our algorithm performs competitively, establishing a new standard for what can be achieved through online learning. This ability to learn long-range dependencies offers a new perspective on learning in the brain and opens a promising avenue in neuromorphic computing.",NIPS
"Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. We evaluate ExPT on few-shot experimental design in challenging domains and demonstrate its superior generality and performance compared to existing methods. The source code is available at https://github.com/tung-nd/ExPT.git.",NIPS
"Most existing works on federated bandits take it for granted that all clients are altruistic about sharing their data with the server for the collective good whenever needed. Despite their compelling theoretical guarantee on performance and communication efficiency, this assumption is overly idealistic and oftentimes violated in practice, especially when the algorithm is operated over self-interested clients, who are reluctant to share data without explicit benefits. Negligence of such self-interested behaviors can significantly affect the learning efficiency and even the practical operability of federated bandit learning. In light of this, we aim to spark new insights into this under-explored research area by formally introducing an incentivized communication problem for federated bandits, where the server shall motivate clients to share data by providing incentives. Without loss of generality, we instantiate this bandit problem with the contextual linear setting and propose the first incentivized communication protocol, namely, Inc-FedUCB, that achieves near-optimal regret with provable communication and incentive cost guarantees. Extensive empirical experiments on both synthetic and real-world datasets further validate the effectiveness of the proposed method across various environments.",NIPS
"Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.",NIPS
"Instruction tuning is an effective technique to align large language models (LLMs) with human intent. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs.",NIPS
"While performing favourably on the independent and identically distributed (i.i.d.) instances, most of the existing neural methods for vehicle routing problems (VRPs) struggle to generalize in the presence of a distribution shift. To tackle this issue, we propose an ensemble-based deep reinforcement learning method for VRPs, which learns a group of diverse sub-policies to cope with various instance distributions. In particular, to prevent convergence of the parameters to the same one, we enforce diversity across sub-policies by leveraging Bootstrap with random initialization. Moreover, we also explicitly pursue inequality between sub-policies by exploiting regularization terms during training to further enhance diversity. Experimental results show that our method is able to outperform the state-of-the-art neural baselines on randomly generated instances of various distributions, and also generalizes favourably on the benchmark instances from TSPLib and CVRPLib, which confirmed the effectiveness of the whole method and the respective designs.",NIPS
"Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Fr\'echet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall.  More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique $f$-divergence from a family we call the \mbox{\em PR-divergences}. Conversely, any $f$-divergence can be written as a linear combination of PR-divergences and  corresponds to a weighted precision-recall trade-off. Through comprehensive evaluations, we show that our approach improves the performance of existing state-of-the-art models like BigGAN in terms of either precision or recall when tested on datasets such as ImageNet.",NIPS
"People are adept at learning a wide variety of structured patterns from small amounts of data, presenting a conundrum from the standpoint of the bias-variance tradeoff: what kinds of representations and algorithms support the joint flexibility and data-paucity of human learning? One possibility is that people ""learn by programming"": inducing probabilistic models to fit observed data. Here, we experimentally test human learning in the domain of structured 2-dimensional patterns, using a task in which participants repeatedly predicted where a dot would move based on its previous trajectory. We evaluate human performance against standard parametric and non-parametric time-series models, as well as two Bayesian program synthesis models whose hypotheses vary in their degree of structure: a compositional Gaussian Process model and a structured ""Language of Thought"" (LoT) model. We find that signatures of human pattern learning are best explained by the LoT model, supporting the idea that the flexibility and data-efficiency of human structure learning can be understood as probabilistic inference over an expressive space of programs.",NIPS
"Most of existing neural methods for multi-objective combinatorial optimization (MOCO) problems solely rely on decomposition, which often leads to repetitive solutions for the respective subproblems, thus a limited Pareto set. Beyond decomposition, we propose a novel neural heuristic with diversity enhancement (NHDE) to produce more Pareto solutions from two perspectives. On the one hand, to hinder duplicated solutions for different subproblems, we propose an indicator-enhanced deep reinforcement learning method to guide the model, and design a heterogeneous graph attention mechanism to capture the relations between the instance graph and the Pareto front graph. On the other hand, to excavate more solutions in the neighborhood of each subproblem, we present a multiple Pareto optima strategy to sample and preserve desirable solutions. Experimental results on classic MOCO problems show that our NHDE is able to generate a Pareto front with higher diversity, thereby achieving superior overall performance. Moreover, our NHDE is generic and can be applied to different neural methods for MOCO.",NIPS
"Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of a reference model. Our experiments show that QUAM excels in capturing epistemic uncertainty for deep learning models and outperforms previous methods on challenging tasks in the vision domain.",NIPS
"Vision transformers (ViTs) are top-performing models on many computer vision benchmarks and can accurately predict human behavior on object recognition tasks. However, researchers question the value of using ViTs as models of biological learning because ViTs are thought to be more â€œdata hungryâ€ than brains, with ViTs requiring more training data than brains to reach similar levels of performance. To test this assumption, we directly compared the learning abilities of ViTs and animals, by performing parallel controlled-rearing experiments on ViTs and newborn chicks. We first raised chicks in impoverished visual environments containing a single object, then simulated the training data available in those environments by building virtual animal chambers in a video game engine. We recorded the first-person images acquired by agents moving through the virtual chambers and used those images to train self-supervised ViTs that leverage time as a teaching signal, akin to biological visual systems. When ViTs were trained â€œthrough the eyesâ€ of newborn chicks, the ViTs solved the same view-invariant object recognition tasks as the chicks. Thus, ViTs were not more data hungry than newborn chicks: both learned view-invariant object representations in impoverished visual environments. The flexible and generic attention-based learning mechanism in ViTsâ€”combined with the embodied data streams available to newborn animalsâ€”appears sufficient to drive the development of animal-like object recognition.",NIPS
"Distributed training of Deep Learning models has been critical to many recent successes in the field. Current standard methods primarily rely on synchronous centralized algorithms which induce major communication bottlenecks and synchronization locks at scale. Decentralized asynchronous algorithms are emerging as a potential alternative but their practical applicability still lags. In order to mitigate the increase in communication cost that naturally comes with scaling the number of workers, we introduce a principled asynchronous, randomized, gossip-based optimization algorithm which works thanks to a continuous local momentum named $\textbf{A}^2\textbf{CiD}^2$. Our method allows each worker to continuously process mini-batches without stopping, and run a peer-to-peer averaging routine in parallel, reducing idle time. In addition to inducing a significant communication acceleration at no cost other than adding a local momentum variable, minimal adaptation is required to incorporate $\textbf{A}^2\textbf{CiD}^2$ to standard asynchronous approaches. Our theoretical analysis proves accelerated rates compared to previous asynchronous decentralized baselines and we empirically show that using our $\textbf{A}^2\textbf{CiD}^2$ momentum significantly decrease communication costs in poorly connected networks. In particular, we show consistent improvement on the ImageNet dataset using up to 64 asynchronous workers (A100 GPUs) and various communication network topologies.",NIPS
"We study the problem of communication-efficient distributed vector mean estimation, which is a commonly used subroutine in distributed optimization and Federated Learning (FL). Rand-$k$ sparsification is a commonly used technique to reduce communication cost, where each client sends $k < d$ of its coordinates to the server. However, Rand-$k$ is agnostic to any correlations, that might exist between clients in practical scenarios. The recently proposed Rand-$k$-Spatial estimator leverages the cross-client correlation information at the server to improve Rand-$k$'s performance. Yet, the performance of Rand-$k$-Spatial is suboptimal, and improving mean estimation is key to a faster convergence in distributed optimization. We propose the Rand-Proj-Spatial estimator with a more flexible encoding-decoding procedure, which generalizes the encoding of Rand-$k$ by projecting the client vectors to a random $k$-dimensional subspace. We utilize Subsampled Randomized Hadamard Transform (SRHT) as the projection matrix, and show that Rand-Proj-Spatial with SRHT outperforms Rand-$k$-Spatial, using the correlation information more efficiently. Furthermore, we propose an approach to incorporate varying degrees of correlation, and suggest a practical variant of Rand-Proj-Spatial when the correlation information is not available to the server. Finally, experiments on real-world distributed optimization tasks showcase the superior performance of Rand-Proj-Spatial compared to Rand-$k$-Spatial and other more sophisticated sparsification techniques.",NIPS
"This paper deals with offline (or batch) Reinforcement Learning (RL) in episodic Regular Decision Processes (RDPs). RDPs are the subclass of Non-Markov Decision Processes where the dependency on the history of past events can be captured by a finite-state automaton. We consider a setting where the automaton that underlies the RDP is unknown, and a learner strives to learn a near-optimal policy using pre-collected data, in the form of non-Markov sequences of observations, without further exploration. We present RegORL, an algorithm that suitably combines automata learning techniques and state-of-the-art algorithms for offline RL in MDPs. RegORL has a modular design allowing one to use any off-the-shelf offline RL algorithm in MDPs. We report a non-asymptotic high-probability sample complexity bound for RegORL to yield an $\varepsilon$-optimal policy, which makes appear a notion of concentrability relevant for RDPs. Furthermore, we present a sample complexity lower bound for offline RL in RDPs. To our best knowledge, this is the first work presenting a provably efficient algorithm for offline learning in RDPs.",NIPS
"Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature attributions for any black box model. Shapley interaction indices extend the SV to define any-order feature interactions. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our approach reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP with a greatly simplified calculation. We illustrate the computational efficiency and effectiveness by explaining language, image classification and high-dimensional synthetic models.",NIPS
"Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find \textsc{MTDiff} outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, \textsc{MTDiff} generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks.",NIPS
"Kernel design is a pivotal but challenging aspect of time series analysis, especially in the context of small datasets. In recent years, Reservoir Computing (RC) has emerged as a powerful tool to compare time series based on the underlying dynamics of the generating process rather than the observed data. However, the performance of RC highly depends on the hyperparameter setting, which is hard to interpret and costly to optimize because of the recurrent nature of RC. Here, we present a new kernel for time series based on the recently established equivalence between reservoir dynamics and Nonlinear Vector AutoRegressive (NVAR) processes. The kernel is non-recurrent and depends on a small set of meaningful hyperparameters, for which we suggest an effective heuristic. We demonstrate excellent performance on a wide range of real-world classification tasks, both in terms of accuracy and speed. This further advances the understanding of RC representation learning models and extends the typical use of the NVAR framework to kernel design and representation of real-world time series data.",NIPS
"Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints. In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number. We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions. Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains. We observe improved stability during training and a higher chance of converging to a homeomorphic encoder.",NIPS
"As large-scale training regimes have gained popularity, the use of pretrained models for downstream tasks has become common practice in machine learning. While pretraining has been shown to enhance the performance of models in practice, the transfer of robustness properties from pretraining to downstream tasks remains poorly understood. In this study, we demonstrate that the robustness of a linear predictor on downstream tasks can be constrained by the robustness of its underlying representation, regardless of the protocol used for pretraining. We prove (i) a bound on the loss that holds independent of any downstream task, as well as (ii) a criterion for robust classification in particular. We validate our theoretical results in practical applications, show how our results can be used for calibrating expectations of downstream robustness, and when our results are useful for optimal transfer learning. Taken together, our results offer an initial step towards characterizing the requirements of the representation function for reliable post-adaptation performance.",NIPS
"Extreme multi-label classification (XMLC) is the task of selecting a small subset of relevant labels from a very large set of possible labels. As such, it is characterized by long-tail labels, i.e., most labels have very few positive instances. With standard performance measures such as precision@k, a classifier can ignore tail labels and still report good performance. However, it is often argued that correct predictions in the tail are more ""interesting"" or ""rewarding,"" but the community has not yet settled on a metric capturing this intuitive concept. The existing propensity-scored metrics fall short on this goal by confounding the problems of long-tail and missing labels. In this paper, we analyze generalized metrics budgeted ""at k"" as an alternative solution. To tackle the challenging problem of optimizing these metrics, we formulate it in the expected test utility (ETU) framework, which aims to optimize the expected performance on a given test set. We derive optimal prediction rules and construct their computationally efficient approximations with provable regret guarantees and being robust against model misspecification. Our algorithm, based on block coordinate descent, scales effortlessly to XMLC problems and obtains promising results in terms of long-tail performance.",NIPS
"We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query (PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.",NIPS
"Algorithmic stability is an important notion that has proven powerful for deriving generalization bounds for practical algorithms. The last decade has witnessed an increasing number of stability bounds for different algorithms applied on different classes of loss functions. While these bounds have illuminated various properties of optimization algorithms, the analysis of each case typically required a different proof technique with significantly different mathematical tools. In this study, we make a novel connection between learning theory and applied probability and introduce a unified guideline for proving Wasserstein stability bounds for stochastic optimization algorithms. We illustrate our approach on stochastic gradient descent (SGD) and we obtain time-uniform  stability bounds (i.e., the bound does not increase with the number of iterations) for strongly convex losses and non-convex losses with additive noise, where we recover similar results to the prior art or extend them to more general cases by using a single proof technique. Our approach is flexible and can be generalizable to other popular optimizers, as it mainly requires developing Lyapunov functions, which are often readily available in the literature. It also illustrates that ergodicity is an important component for obtaining time-uniform bounds --  which might not be achieved for convex or non-convex losses unless additional noise is injected to the iterates. Finally, we slightly stretch our analysis technique and prove time-uniform bounds for SGD under convex and non-convex losses (without additional additive noise), which, to our knowledge, is novel.",NIPS
"Recent works on over-parameterized neural networks have shown that  the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step towards understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as \emph{deep matrix factorization}. We show that with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of all layer matrices), which in turn leads to better generalization.",NIPS
"Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs.  Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations.  After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal.  We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed.",NIPS
"We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes, in the fixed-dimensional asymptotic regime, i.e., the dimension of the feature data is fixed while the number of nodes is large. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical convolution in the regime of high graph signal. Furthermore, we prove a corresponding non-asymptotic result.",NIPS
"Sketching algorithms have recently proven to be a powerful approach both for designing low-space streaming algorithms as well as fast polynomial time approximation schemes (PTAS). In this work, we develop new techniques to extend the applicability of sketching-based approaches to the sparse dictionary learning and the Euclidean $k$-means clustering problems. In particular, we initiate the study of the challenging setting where the dictionary/clustering assignment for each of the $n$ input points must be output, which has surprisingly received little attention in prior work. On the fast algorithms front, we obtain a new approach for designing PTAS's for the $k$-means clustering problem, which generalizes to the first PTAS for the sparse dictionary learning problem. On the streaming algorithms front, we obtain new upper bounds and lower bounds for dictionary learning and $k$-means clustering. In particular, given a design matrix $\mathbf A\in\mathbb R^{n\times d}$ in a turnstile stream, we show an $\tilde O(nr/\epsilon^2 + dk/\epsilon)$ space upper bound for $r$-sparse dictionary learning of size $k$, an $\tilde O(n/\epsilon^2 + dk/\epsilon)$ space upper bound for $k$-means clustering, as well as an $\tilde O(n)$ space upper bound for $k$-means clustering on random order row insertion streams with a natural ""bounded sensitivity"" assumption. On the lower bounds side, we obtain a general $\tilde\Omega(n/\epsilon + dk/\epsilon)$ lower bound for $k$-means clustering, as well as an $\tilde\Omega(n/\epsilon^2)$ lower bound for algorithms which can estimate the cost of a single fixed set of candidate centers.",NIPS
"We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model & task scaling. We conduct extensive empirical studies and reveal the following key insights:    1) performing gradient descent updates by alternating on diverse modalities, loss functions, and tasks, with varying input resolutions, efficiently improves the model.    2) sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigating the conflicts between modalities. IMP achieves competitive performance on a wide range of downstream tasks including video classification, image classification, image-text, and video-text retrieval. Most notably, we train a sparse IMP-MoE-L focusing on video tasks that achieves new state-of-the-art in zero-shot video classification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% on Kinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%, respectively, while using only 15% of their total training computational cost.",NIPS
"Learning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so.",NIPS
"The striking ability of unsupervised word translation has been demonstrated recently with the help of low-dimensional word vectors / pretraining, which is used by all successful methods and assumed to be necessary. We test and challenge this assumption by developing a method that can also make use of high dimensional signal. Freed from the limits of low dimensions, we show that relying on low-dimensional vectors and their incidental properties miss out on better denoising methods and signals in high dimensions, thus stunting the potential of the data. Our results show that unsupervised translation can be achieved more easily and robustly than previously thought -- less than 80MB and minutes of CPU time is required to achieve over 50\% accuracy for English to Finnish, Hungarian, and Chinese translations when trained in the same domain; even under domain mismatch, the method still works fully unsupervised on English NewsCrawl to Chinese Wikipedia and English Europarl to Spanish Wikipedia, among others. These results challenge prevailing assumptions on the necessity and superiority of low-dimensional vectors and show that the higher dimension signal can be used rather than thrown away.",NIPS
"Closed-form differential equations, including partial differential equations and higher-order ordinary differential equations, are one of the most important tools used by scientists to model and better understand natural phenomena. Discovering these equations directly from data is challenging because it requires modeling relationships between various derivatives that are not observed in the data (equation-data mismatch) and it involves searching across a huge space of possible equations. Current approaches make strong assumptions about the form of the equation and thus fail to discover many well-known phenomena. Moreover, many of them resolve the equation-data mismatch by estimating the derivatives, which makes them inadequate for noisy and infrequent observations. To this end, we propose D-CIPHER, which is robust to measurement artifacts and can uncover a new and very general class of differential equations. We further design a novel optimization procedure, CoLLie, to help D-CIPHER search through this class efficiently. Finally, we demonstrate empirically that it can discover many well-known equations that are beyond the capabilities of current methods.",NIPS
"The strength of modern generative models lies in their ability to be controlled through prompts. Hard prompts comprise interpretable words and tokens, and are typically hand-crafted by humans.  Soft prompts, on the other hand, consist of continuous feature vectors.  These can be discovered using powerful optimization methods, but they cannot be easily edited, re-used across models, or plugged into a text-based interface. We describe an easy-to-use approach to automatically optimize hard text prompts through efficient gradient-based optimization. Our approach can be readily applied to text-to-image and text-only applications alike. This method allows API users to easily generate, discover, and mix and match image concepts without prior knowledge of how to prompt the model. Furthermore, using our method, we can bypass token-level content filters imposed by Midjourney by optimizing through the open-sourced text encoder.",NIPS
"Diffusion-based purification defenses leverage diffusion models to remove crafted perturbations of adversarial examples and achieve state-of-the-art robustness. Recent studies show that even advanced attacks cannot break such defenses effectively, since the purification process induces an extremely deep computational graph which poses the potential problem of gradient obfuscation, high memory cost, and unbounded randomness. In this paper, we propose a unified framework DiffAttack to perform effective and efficient attacks against diffusion-based purification defenses, including both DDPM and score-based approaches. In particular, we propose a deviated-reconstruction loss at intermediate diffusion steps to induce inaccurate density gradient estimation to tackle the problem of vanishing/exploding gradients. We also provide a segment-wise forwarding-backwarding algorithm, which leads to memory-efficient gradient backpropagation. We validate the attack effectiveness of DiffAttack compared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that DiffAttack decreases the robust accuracy of models compared with SOTA attacks by over 20\% on CIFAR-10 under $\ell_\infty$ attack $(\epsilon=8/255)$, and over 10\% on ImageNet under $\ell_\infty$ attack $(\epsilon=4/255)$. We conduct a series of ablations studies, and we find 1) DiffAttack with the deviated-reconstruction loss added over uniformly sampled time steps is more effective than that added over only initial/final steps, and 2) diffusion-based purification with a moderate diffusion length is more robust under DiffAttack.",NIPS
"Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth.  It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance. Code is available at the Github repository \url{https://github.com/natrask/BracketGraphs}.",NIPS
"Neural Radiance Field (NeRF) has enabled novel view synthesis with high fidelity given images and camera poses. Subsequent works even succeeded in eliminating the necessity of pose priors by jointly optimizing NeRF and camera pose. However, these works are limited to relatively simple settings such as photometrically consistent and occluder-free image collections or a sequence of images from a video. So they have difficulty handling unconstrained images with varying illumination and transient occluders. In this paper, we propose UP-NeRF (Unconstrained Pose-prior-free Neural Radiance Fields) to optimize NeRF with unconstrained image collections without camera pose prior. We tackle these challenges with surrogate tasks that optimize color-insensitive feature fields and a separate module for transient occluders to block their influence on pose estimation. In addition, we introduce a candidate head to enable more robust pose estimation and transient-aware depth supervision to minimize the effect of incorrect prior. Our experiments verify the superior performance of our method compared to the baselines including BARF and its variants in a challenging internet photo collection, Phototourism dataset. The code of UP-NeRF is available at https://github.com/mlvlab/UP-NeRF.",NIPS
"Persistent homology (PH) provides  topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case---where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest---and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes---a recent family of MPH descriptors---as signed Radon measures leads to natural extensions of vectorization strategies from one parameter to multiple parameters. The resulting feature vectors are easy to define and to compute, and provably stable. While, as a proof of concept, we focus on simple choices of signed barcodes and vectorizations, we already see notable performance improvements when comparing our feature vectors to state-of-the-art topology-based methods on various types of data.",NIPS
"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks-- subtle,  perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25% higher accuracy on perturbed data.",NIPS
"The Kohn-Sham equations underlie many important applications such as the discovery of new catalysts. Recent machine learning work on catalyst modeling has focused on prediction of the energy, but has so far not yet demonstrated significant out-of-distribution generalization. Here we investigate another approach based on the pointwise learning of the Kohn-Sham charge-density. On a new dataset of bulk catalysts with charge densities, we show density models can generalize to new structures with combinations of elements not seen at train time, a form of combinatorial generalization. We show that over 80% of binary and ternary test cases achieve faster convergence than standard baselines in Density Functional Theory, amounting to an average reduction of 13% in the number of iterations required to reach convergence, which may be of independent interest. Our results suggest that density learning is a viable alternative, trading greater inference costs for a step towards combinatorial generalization, a key property for applications.",NIPS
"Solving the quantum many-body SchrÃ¶dinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose ""Wasserstein Quantum Monte Carlo"" (WQMC), which uses the gradient flow induced by the Wasserstein metric, rather than the Fisher--Rao metric, and corresponds to transporting the probability mass, rather than teleporting it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems.",NIPS
"Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.",NIPS
"How can one publish a dataset with sensitive attributes in a way that both preserves privacy and enables joins with other datasets on those same sensitive attributes? This problem arises in many contexts, e.g., a hospital and an airline may want to jointly determine whether people who take long-haul flights are more likely to catch respiratory infections. If they join their data by a common keyed user identifier such as email address, they can determine the answer, though it breaks privacy.  This paper shows how the hospital can generate a private sketch and how the airline can privately join with the hospital's sketch by email address. The proposed solution satisfies pure differential privacy and gives approximate answers to linear queries and optimization problems over those joins. Whereas prior work such as secure function evaluation requires sender/receiver interaction, a distinguishing characteristic of the proposed approach is that it is non-interactive. Consequently, the sketch can be published to a repository for any organization to join with, facilitating data discovery. The accuracy of the method is demonstrated through both theoretical analysis and extensive empirical evidence.",NIPS
"Prior theoretical and empirical works have established that semi-supervised learning algorithms can leverage the unlabeled data to improve over the labeled sample complexity of supervised learning (SL) algorithms. However, existing theoretical work focuses on regimes where the unlabeled data is sufficient to learn a good decision boundary using unsupervised learning (UL) alone. This begs the question: Can SSL algorithms simultaneously improve upon both UL and SL? To this end, we derive a tight lower bound for 2-Gaussian mixture models that explicitly depends on the labeled and the unlabeled dataset size as well as the signal-to-noise ratio of the mixture distribution. Surprisingly, our result implies that no SSL algorithm improves upon the minimax-optimal statistical error rates of SL or UL algorithms for these distributions. Nevertheless, in our real-world experiments, SSL algorithms can often outperform UL and SL algorithms. In summary, our work suggests that while it is possible to prove the performance gains of SSL algorithms, this would require careful tracking of constants in the theoretical analysis.",NIPS
"Bridging logical reasoning and deep learning is crucial for advanced AI systems. In this work, we present a new framework that addresses this goal by generating interpretable and verifiable logical rules through differentiable learning, without relying on pre-specified logical structures. Our approach builds upon SATNet, a differentiable MaxSAT solver that learns the underlying rules from input-output examples. Despite its efficacy, the learned weights in SATNet are not straightforwardly interpretable, failing to produce human-readable rules. To address this, we propose a novel specification method called ``maximum equality'', which enables the interchangeability between the learned weights of SATNet and a set of propositional logical rules in weighted MaxSAT form. With the decoded weighted MaxSAT formula, we further introduce several effective verification techniques to validate it against the ground truth rules. Experiments on stream transformations and Sudoku problems show that our decoded rules are highly reliable: using exact solvers on them could achieve 100% accuracy, whereas the original SATNet fails to give correct solutions in many cases. Furthermore, we formally verify that our decoded logical rules are functionally equivalent to the ground truth ones.",NIPS
"This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in three cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining four cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.",NIPS
"This paper presents a novel method to enhance the reliability of image classification models during deployment in the face of transient hardware errors. By utilizing enriched text embeddings derived from GPT-3 with question prompts per class and CLIP pretrained text encoder, we investigate their impact as an initialization for the classification layer. Our approach achieves a remarkable $5.5\times$ average increase in hardware reliability (and up to $14\times$) across various architectures in the most critical layer, with minimal accuracy drop ($0.3\%$ on average) compared to baseline PyTorch models. Furthermore, our method seamlessly integrates with any image classification backbone, showcases results across various network architectures, decreases parameter and FLOPs overhead, and follows a consistent training recipe. This research offers a practical and efficient solution to bolster the robustness of image classification models against hardware failures, with potential implications for future studies in this domain. Our code and models are released at https://github.com/TalalWasim/TextGuidedResilience.",NIPS
"No-regret learners seek to minimize the difference between the loss they cumulated through the actions they played, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformations considered by the learner determines a natural notion of rationality. As the set of transformations each learner considers grows, the strategies played by the learners recover more complex game-theoretic equilibria, including correlated equilibria in normal-form games and extensive-form correlated equilibria in extensive-form games. At the extreme, a no-swap-regret agent is one that minimizes regret against the set of all functions from the set of strategies to itself. While it is known that the no-swap-regret condition can be attained efficiently in nonsequential (normal-form) games, understanding what is the strongest notion of rationality that can be attained efficiently in the worst case in sequential (extensive-form) games is a longstanding open problem. In this paper we provide a positive result, by showing that it is possible, in any sequential game, to retain polynomial-time (in the game tree size) iterations while achieving sublinear regret with respect to all linear transformations of the mixed strategy space, a notion called no-linear-swap regret. This notion of hindsight rationality is as strong as no-swap-regret in nonsequential games, and stronger than no-trigger-regret in sequential gamesâ€”thereby proving the existence of a subset of extensive-form correlated equilibria robust to linear deviations, which we call linear-deviation correlated equilibria, that can be approached efficiently.",NIPS
"Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the existing PbRL methods. Notably, on high-dimensional control tasks, our algorithm surpasses offline RL methods that learn with ground-truth reward information. Finally, we show that our algorithm can be successfully applied to fine-tune large language models.",NIPS
"Given a point set $P\subseteq M$ from a metric space $(M,d)$ and numbers $k, z \in N$, the *metric $k$-center problem with $z$ outliers* is to find a set $C^\ast\subseteq P$ of $k$ points such that the maximum distance of all but at most $z$ outlier points of $P$ to their nearest center in ${C}^\ast$ is minimized. We consider this problem in the fully dynamic model, i.e., under insertions and deletions of points, for the case that the metric space has a bounded doubling dimension $dim$. We utilize a hierarchical data structure to maintain the points and their neighborhoods, which enables us to efficiently find the clusters. In particular, our data structure can be queried at any time to generate a $(3+\varepsilon)$-approximate solution for input values of $k$ and $z$ in worst-case query time $\varepsilon^{-O(dim)}k \log{n} \log\log{\Delta}$, where $\Delta$ is the ratio between the maximum and minimum distance between two points in $P$. Moreover, it allows insertion/deletion of a point in worst-case update time $\varepsilon^{-O(dim)}\log{n}\log{\Delta}$. Our result achieves a significantly faster query time with respect to $k$ and $z$ than the current state-of-the-art by Pellizzoni, Pietracaprina, and Pucci, which uses $\varepsilon^{-O(dim)}(k+z)^2\log{\Delta}$ query time to obtain a $(3+\varepsilon)$-approximation.",NIPS
"In this paper, we propose a novel and powerful method to harness Bayesian optimization for variational quantum eigensolvers (VQEs) - a hybrid quantum-classical protocol used to approximate the ground state of a quantum Hamiltonian. Specifically, we derive a VQE-kernel which incorporates important prior information about quantum circuits: the kernel feature map of the VQE-kernel exactly matches the known functional form of the VQE's objective function and thereby significantly reduces the posterior uncertainty.Moreover, we propose a novel acquisition function for Bayesian optimization called \emph{Expected Maximum Improvement over Confident Regions} (EMICoRe) which can actively exploit the inductive bias of the VQE-kernel by treating regions with low predictive uncertainty as indirectly ""observed"". As a result, observations at as few as three points in the search domain are sufficient to determine the complete objective function along an entire one-dimensional subspace of the optimization landscape. Our numerical experiments demonstrate that our approach improves over state-of-the-art baselines.",NIPS
"The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby facilitating more effective learning of features. Experiments on both synthetic and real data corroborate our theory.",NIPS
"Attention mechanisms play a crucial role in cognitive systems by allowing them to flexibly allocate cognitive resources. Transformers, in particular, have become a dominant architecture in machine learning, with attention as their central innovation. However, the underlying intuition and formalism of attention in Transformers is based on ideas of keys and queries in database management systems. In this work, we pursue a structural inference perspective, building upon, and bringing together, previous theoretical descriptions of attention such as; Gaussian Mixture Models, alignment mechanisms and Hopfield Networks. Specifically, we demonstrate that attention can be viewed as inference over an implicitly defined set of possible adjacency structures in a graphical model, revealing the generality of such a mechanism. This perspective unifies different attentional architectures in machine learning and suggests potential modifications and generalizations of attention. Here we investigate two and demonstrate their behaviour on explanatory toy problems: (a) extending the value function to incorporate more nodes of a graphical model yielding a mechanism with a bias toward attending  multiple tokens; (b) introducing a geometric prior (with conjugate hyper-prior) over the adjacency structures producing a mechanism which dynamically scales the context window depending on input. Moreover, by describing a link between structural inference and precision-regulation in Predictive Coding Networks, we discuss how this framework can bridge the gap between attentional mechanisms in machine learning and Bayesian conceptions of attention in Neuroscience. We hope by providing a new lens on attention architectures our work can guide the development of new and improved attentional mechanisms.",NIPS
"Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer ReLU neural network trained by a  curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the specific regime of the theoretical results.",NIPS
"A unifying theme in the design of intelligent agents is to efficiently optimize a policy based on what prior knowledge of the problem is available and what actions can be taken to learn more about it. Bandits are a canonical instance of this task that has been intensely studied in the literature. Most methods, however, typically rely solely on an agent's experimentation in a single environment (or multiple closely related environments). In this paper, we relax this assumption and consider the design of bandit algorithms from a combination of batch data and qualitative assumptions about the relatedness across different environments, represented in the form of causal models. In particular, we show that it is possible to exploit invariances across environments, wherever they may occur in the underlying causal model, to consistently improve learning. The resulting bandit algorithm has a sub-linear regret bound with an explicit dependency on a term that captures how informative related environments are for the task at hand; and may have substantially lower regret than experimentation-only bandit instances.",NIPS
"We investigate theory and algorithms for pool-based active learning for multiclass classification using multinomial logistic regression.  Using finite sample analysis, we prove that the Fisher Information Ratio (FIR)  lower and upper bounds  the excess risk. Based on our theoretical analysis, we propose an active learning algorithm that  employs regret minimization to minimize the FIR. To verify our derived excess risk bounds, we conduct experiments on synthetic datasets. Furthermore, we compare FIRAL with five other methods and found that our scheme  outperforms them: it consistently produces the smallest classification error in the multiclass logistic regression setting, as demonstrated through experiments on MNIST, CIFAR-10, and 50-class ImageNet.",NIPS
"We investigate the problem of machine learning-based (ML) predictive inference on individual treatment effects (ITEs). Previous work has focused primarily on developing ML-based â€œmeta-learnersâ€ that can provide point estimates of the conditional average treatment effect (CATE)â€”these are model-agnostic approaches for combining intermediate nuisance estimates to produce estimates of CATE. In this paper, we develop conformal meta-learners, a general framework for issuing predictive intervals for ITEs by applying the standard conformal prediction (CP) procedure on top of CATE meta-learners. We focus on a broad class of meta-learners based on two-stage pseudo-outcome regression and develop a stochastic ordering framework to study their validity. We show that inference with conformal meta-learners is marginally valid if their (pseudo-outcome) conformity scores stochastically dominate â€œoracleâ€ conformity scores evaluated on the unobserved ITEs. Additionally, we prove that commonly used CATE meta-learners, such as the doubly-robust learner, satisfy a model- and distribution-free stochastic (or convex) dominance condition, making their conformal inferences valid for practically-relevant levels of target coverage. Whereas existing procedures conduct inference on nuisance parameters (i.e., potential outcomes) via weighted CP, conformal meta-learners enable direct inference on the target parameter (ITE). Numerical experiments show that conformal meta-learners provide valid intervals with competitive efficiency while retaining the favorable point estimation properties of CATE meta-learners.",NIPS
"We study an abstract framework for interactive learning called interactive estimation in which the goal is to estimate a target from its ``similarity'' to points queried by the learner.We introduce a combinatorial measure called Dissimilarity dimension which largely captures learnability in our model.We present a simple, general, and broadly-applicable algorithm, for which we obtain both regret and PAC generalization bounds that are polynomial in the new dimension. We show that our framework subsumes and thereby unifies two classic learning models:statistical-query learning and structured bandits. We also delineate how the Dissimilarity dimension is related to well-known parameters for both frameworks, in some cases yielding significantly improved analyses.",NIPS
"In-context learningâ€“â€“the ability to configure a model's behavior with different promptsâ€“â€“has revolutionized the field of natural language processing, alleviating the need for task-specific models and paving the way for generalist models capable of assisting with any query. Computer vision, in contrast, has largely stayed in the former regime: specialized decoders and finetuning protocols are generally required to perform dense tasks such as semantic segmentation and depth estimation. In this work we explore a simple mechanism for in-context learning of such scene understanding tasks: nearest neighbor retrieval from a prompt of annotated features. We propose a new pretraining protocolâ€“â€“leveraging attention within and across imagesâ€“â€“which yields representations particularly useful in this regime. The resulting Hummingbird model, suitably prompted, performs various scene understanding tasks without modification while approaching the performance of specialists that have been finetuned for each task. Moreover, Hummingbird can be configured to perform new tasks much more efficiently than finetuned models, raising the possibility of scene understanding in the interactive assistant regime.",NIPS
"Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n< k$ neurons can thus be seen as fitting an under-parameterized ""student"" network with $n$ neurons to a ""teacher"" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that ""copy-average"" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student neuron averages the remaining $k-n+1$ teacher neurons. For the student network with $n=1$ neuron, we provide additionally a closed-form solution of the non-trivial critical point(s) for commonly used activation functions through solving an equivalent constrained optimization problem. Empirically, we find for the erf activation function that gradient flow converges either to the optimal copy-average critical point or to another point where each student neuron approximately copies a different teacher neuron. Finally, we find similar results for the ReLU activation function, suggesting that the optimal solution of underparameterized networks has a universal structure.",NIPS
"Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizers are evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.",NIPS
"Algorithms for the minimum-cost bipartite matching can be used to estimate Wasserstein distance between two distributions.Given two sets $A$ and $B$ of $n$ points in a $2$-dimensional Euclidean space, one can use a fast implementation of the Hungarian method to compute a minimum-cost bipartite matching of $A$ and $B$ in $\tilde{O}(n^2)$ time. Let $\Delta$ be the spread, i.e., the ratio of the distance of the farthest to the closest pair of points in $A\cup B$. In this paper, we present a new algorithm to compute a minimum-cost bipartite matching of $A$ and $B$ with a similar worst-case execution time of $\tilde{O}(n^2 \log \Delta)$. However, when $A$ and $B$ are drawn independently and identically from a fixed distribution that is not known to the algorithm, the execution time of our algorithm is, in expectation, $\tilde{O}(n^{7/4}\log \Delta)$.To the best of our knowledge, our algorithm is the first one to achieve a sub-quadratic execution time even for stochastic point sets with real-valued coordinates.Our algorithm extends to any dimension $d$, where it runs in $\tilde{O}(n^{2-\frac{1}{2d}}\Phi(n))$ time for stochastic point sets $A$ and $B$; here $\Phi(n)$ is the query/update time of a dynamic weighted nearest neighbor data structure. Our algorithm can be seen as a careful adaptation of the Hungarian method in the geometric divide-and-conquer framework.",NIPS
"We consider the problem of minimizing a continuous function given given access to a natural quantum generalization of a stochastic gradient oracle. We provide two new methods for the special case of minimizing a Lipschitz convex function. Each method obtains a dimension versus accuracy trade-off which is provably unachievable classically and we prove that one method is asymptotically optimal in low-dimensional settings. Additionally, we provide quantum algorithms for computing a critical point of a smooth non-convex function at rates not known to be achievable classically. To obtain these results we build upon the quantum multivariate mean estimation result of Cornelissen et al. and provide a general quantum variance reduction technique of independent interest.",NIPS
"To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. In this paper, we propose a differentiable regularizer that is a lower bound on the distance of the data points to the classification boundary. The proposed regularizer requires knowledge of the model's Lipschitz constant along certain directions. To this end, we develop a scalable method for calculating guaranteed differentiable upper bounds on the Lipschitz constant of neural networks accurately and efficiently.  The relative accuracy of the bounds prevents excessive regularization and allows for more direct manipulation of the decision boundary. Furthermore, our Lipschitz bounding algorithm exploits the monotonicity and Lipschitz continuity of the activation layers, and the resulting bounds can be used to design new layers with controllable bounds on their Lipschitz constant. Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm obtains competitively improved results compared to the state-of-the-art.",NIPS
"There are two categories of methods in Federated Learning (FL) for joint training across multiple clients: i) parallel FL (PFL), where clients train models in a parallel manner; and ii) sequential FL (SFL), where clients train models in a sequential manner. In contrast to that of PFL, the convergence theory of SFL on heterogeneous data is still lacking. In this paper, we establish the convergence guarantees of SFL for strongly/general/non-convex objectives on heterogeneous data. The convergence guarantees of SFL are better than that of PFL on heterogeneous data with both full and partial client participation. Experimental results validate the counterintuitive analysis result that SFL outperforms PFL on extremely heterogeneous data in cross-device settings.",NIPS
"We consider alternating gradient descent (AGD) with fixed step size applied to the asymmetric matrix factorization objective.  We show that, for a rank-$r$ matrix $A \in \mathbb{R}^{m \times n}$,  $T = C ( \frac{\sigma_1(A)}{\sigma_r(A)} )^2 \log(1/\epsilon)$  iterations of alternating gradient descent suffice to reach an $\epsilon$-optimal factorization   $\| A - X_{T} Y_{T}' \|^2 \leq \epsilon \| A \|^2$   with high probability  starting from an atypical random initialization. The  factors have rank $d \geq r$ so that $X_{T}\in \mathbb{R}^{m \times d}$ and $Y_{T} \in\mathbb{R}^{n \times d}$, and mild overparameterization suffices for the constant  $C$ in the iteration complexity $T$ to be an absolute constant.   Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves the convergence rate of gradient descent in practice. Our proof is conceptually simple: a uniform Polyak-Lojasiewicz (PL) inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization.  Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems.",NIPS
"Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control are foundational and extensively researched problems in optimal control. We investigate LQR and LQG problems with semi-adversarial perturbations and time-varying adversarial bandit loss functions. The best-known sublinear regret algorithm~\cite{gradu2020non} has a $T^{\frac{3}{4}}$ time horizon dependence, and its authors posed an open question about whether a tight rate of $\sqrt{T}$ could be achieved. We answer in the affirmative, giving an algorithm for bandit LQR and LQG which attains optimal regret, up to logarithmic factors. A central component of our method is a new scheme for bandit convex optimization with memory, which is of independent interest.",NIPS
"Causal identification is at the core of the causal inference literature, where complete algorithms have been proposed to identify causal queries of interest. The validity of these algorithms hinges on the restrictive assumption of having access to a correctly specified causal structure. In this work, we study the setting where a probabilistic model of the causal structure is available. Specifically, the edges in a causal graph exist with uncertainties which may, for example, represent degree of belief from domain experts. Alternatively, the uncertainty about an edge may reflect the confidence of a particular statistical test. The question that naturally arises in this setting is: Given such a probabilistic graph and a specific causal effect of interest, what is the subgraph which has the highest plausibility and for which the causal effect is identifiable? We show that answering this question reduces to solving an NP-hard combinatorial optimization problem which we call the edge ID problem. We propose efficient algorithms to approximate this problem and evaluate them against both real-world networks and randomly generated graphs.",NIPS
"Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.",NIPS
"Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to extend the in-domain model to the distinctive target domains where the data distributions differ. Most prior works focus on capturing the inter-domain transferability but largely overlook rich intra-domain structures, which empirically results in even worse discriminability. In this work, we introduce a novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The core of our method is briefly condensed as follows: (i)-by casting the DA problem to graph primitives, SPA composes a coarse graph alignment mechanism with a novel spectral regularizer towards aligning the domain graphs in eigenspaces; (ii)-we further develop a fine-grained message propagation module --- upon a novel neighbor-aware self-training mechanism --- in order for enhanced discriminability in the target domain. On standardized benchmarks, the extensive experiments of SPA demonstrate that its performance has surpassed the existing cutting-edge DA methods. Coupled with dense model analysis, we conclude that our approach indeed possesses superior efficacy, robustness, discriminability, and transferability. Code and data are available at: https://github.com/CrownX/SPA.",NIPS
"Federated learning (FL) has emerged as a powerful scheme to facilitate the collaborative learning of models amongst a set of agents holding their own private data.  Although the agents benefit from the global model trained on shared data, by participating in federated learning, they may also incur costs (related to privacy and communication) due to data sharing. In this paper, we model a collaborative FL framework, where every agent attempts to achieve an optimal trade-off between her learning payoff and data sharing cost. We show the existence of Nash equilibrium (NE) under mild assumptions on agents' payoff and costs. Furthermore, we show that agents can discover the NE via best response dynamics. However, some of the NE may be bad in terms of overall welfare for the agents, implying little incentive for some fraction of the agents to participate in the learning. To remedy this, we design a budget-balanced mechanism involving payments to the agents, that ensures that any $p$-mean welfare function of the agents' utilities is maximized at NE. In addition, we introduce a FL protocol FedBR-BG that incorporates our budget-balanced mechanism, utilizing best response dynamics. Our empirical validation on MNIST and CIFAR-10 substantiates our theoretical analysis. We show that FedBR-BG outperforms the basic best-response-based protocol without additional incentivization, the standard federated learning protocol FedAvg, as well as a recent baseline MWFed in terms of achieving superior $p$-mean welfare.",NIPS
"The success of contrastive learning is well known to be dependent on data augmentation.Although the degree of data augmentations has been well controlled by utilizing pre-defined techniques in some domains like vision, time-series data augmentation is less explored and remains a challenging problem due to the complexity of the data generation mechanism, such as the intricate mechanism involved in the cardiovascular system.Moreover, there is no widely recognized and general time-series augmentation method that can be applied across different tasks.In this paper, we propose a novel data augmentation method for time-series tasks that aims to connect intra-class samples together, and thereby find order in the latent space.Our method builds upon the well-known data augmentation technique of mixup by incorporating a novel approach that accounts for the non-stationary nature of time-series data.Also, by controlling the degree of chaos created by data augmentation, our method leads to improved feature representations and performance on downstream tasks.We evaluate our proposed method on three time-series tasks, including heart rate estimation, human activity recognition, and cardiovascular disease detection. Extensive experiments against the state-of-the-art methods show that the proposed method outperforms prior works on optimal data generation and known data augmentation techniques in three tasks, reflecting the effectiveness of the presented method. The source code is available at double-blind policy.",NIPS
"Deep neural network (DNN) inference based on secure 2-party computation (2PC) can offer cryptographically-secure privacy protection but suffers from orders of magnitude latency overhead due to enormous communication. Previous works heavily rely on a proxy metric of ReLU counts to approximate the communication overhead and focus on reducing the ReLUs to improve the communication efficiency. However, we observe these works achieve limited communication reduction for state-of-the-art (SOTA) 2PC protocols due to the ignorance of other linear and non-linear operations, which now contribute to the majority of communication. In this work, we present CoPriv, a framework that jointly optimizes the 2PC inference protocol and the DNN architecture. CoPriv features a new 2PC protocol for convolution based on Winograd transformation and develops DNN-aware optimization to significantly reduce the inference communication. CoPriv further develops a 2PC-aware network optimization algorithm that is compatible with the proposed protocol and simultaneously reduces the communication for all the linear and non-linear operations. We compare CoPriv with the SOTA 2PC protocol, CrypTFlow2, and demonstrate 2.1Ã— communication reduction for both ResNet-18 and ResNet-32 on CIFAR-100. We also compare CoPriv with SOTA network optimization methods, including SNL, MetaPruning, etc. CoPriv achieves 9.98Ã— and 3.88Ã— online and total communication reduction with a higher accuracy compare to SNL, respectively. CoPriv also achieves 3.87Ã— online communication reduction with more than 3% higher accuracy compared to MetaPruning.",NIPS
"As black-box machine learning models become more complex and are applied in high-stakes settings, the need for providing explanations for their predictions becomes crucial. Although Local Interpretable Model-agnostic Explanations (LIME) \cite{ribeiro2016should} is a widely adopted method for understanding model behavior, it suffers from instability with respect to random seeds \cite{zafar2019dlime, shankaranarayana2019alime, bansal2020sam} and exhibits low local fidelity (i.e., how the explanation explains model's local behaviors) \cite{rahnama2019study, laugel2018defining}. Our study demonstrates that this instability is caused by small sample weights, resulting in the dominance of regularization and slow convergence. Additionally, LIME's sampling approach is non-local and biased towards the reference, leading to diminished local fidelity and instability to references. To address these challenges, we propose \textsc{Glime}, an enhanced framework that extends LIME and unifies several previous methods. Within the \textsc{Glime} framework, we derive an equivalent formulation of LIME that achieves significantly faster convergence and improved stability. By employing a local and unbiased sampling distribution, \textsc{Glime} generates explanations with higher local fidelity compared to LIME, while being independent of the reference choice. Moreover, \textsc{Glime} offers users the flexibility to choose sampling distribution based on their specific scenarios.",NIPS
"Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling (SatLM) approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it out of the description more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness of the answer with respect to the parsed specification and avoid planning errors in the solving process. We evaluate SATLM on 8 different datasets and show that it consistently outperforms program-aided LMs in the imperative paradigm. In particular, SATLM outperforms program-aided LMs by 23% on a challenging subset of the GSM arithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and BoardgameQA, surpassing previous models that are trained on the respective training sets.",NIPS
"Collaborative machine learning (ML) is widely used to enable institutions to learn better models from distributed data. While collaborative approaches to learning intuitively protect user data, they remain vulnerable to either the server, the clients, or both, deviating from the protocol. Indeed, because the protocol is asymmetric, a malicious server can abuse its power to reconstruct client data points. Conversely, malicious clients can corrupt learning with malicious updates. Thus, both clients and servers require a guarantee when the other cannot be trusted to fully cooperate. In this work, we propose a peer-to-peer (P2P) learning scheme that is secure against malicious servers and robust to malicious clients. Our core contribution is a generic framework that transforms any (compatible) algorithm for robust aggregation of model updates to the setting where servers and clients can act maliciously. Finally, we demonstrate the computational efficiency of our approach even with 1-million parameter models trained by 100s of peers on standard datasets.",NIPS
"When interacting with people, AI agents do not just influence the state of the world -- they also influence the actions people take in response to the agent, and even their underlying intentions and strategies. Accounting for and leveraging this influence has mostly been studied in settings where it is sufficient to assume that human behavior is near-optimal: competitive games, or general-sum settings like autonomous driving alongside human drivers. Instead, we focus on influence in settings where there is a need to capture human suboptimality. For instance, imagine a collaborative task in which, due either to cognitive biases or lack of information, people do not perform very well -- how could an agent influence them towards more optimal behavior? Assuming near-optimal human behavior will not work here, and so the agent needs to learn from real human data. But experimenting online with humans is potentially unsafe, and creating a high-fidelity simulator of the environment is often impractical. Hence, we  focus on learning from an offline dataset of human-human interactions. Our observation is that offline reinforcement learning (RL) can learn to effectively influence suboptimal humans by extending and combining elements of observed human-human behavior. We demonstrate that offline RL can solve two challenges with effective influence. First, we show that by learning from a dataset of suboptimal human-human interaction on a variety of tasks -- none of which contains examples of successful influence -- an agent can learn influence strategies to steer humans towards better performance even on new tasks. Second, we show that by also modeling and conditioning on human behavior, offline RL can learn to affect not just the human's actions but also their underlying strategy, and adapt to changes in their strategy.",NIPS
"This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 updates, called MKOR, that improves the training time and convergence properties of deep neural networks (DNNs). Second-order techniques, while enjoying higher convergence rates vs first-order counterparts, have cubic complexity with respect to either the model size and/or the training batch size. Hence they exhibit poor scalability and performance in transformer models, e.g. large language models (LLMs), because the batch sizes in these models  scale by the attention mechanism sequence length, leading to large model size and batch sizes. MKOR's complexity is quadratic with respect to the model size, alleviating the computation bottlenecks in  second-order methods. Because of their high computation complexity, state-of-the-art implementations of second-order methods can only afford to update the second order information infrequently, and thus do not fully exploit the promise of better convergence from these updates. By reducing the communication complexity of the second-order updates as well as achieving a linear communication complexity, MKOR increases the frequency of second order updates. We also propose a hybrid version of MKOR (called MKOR-H) that mid-training falls backs to a first order optimizer if the second order updates  no longer accelerate convergence.  Our experiments show that MKOR outperforms state -of-the-art first order methods, e.g. the LAMB optimizer, and best implementations of second-order methods, i.e. KAISA/KFAC, up to 2.57x and 1.85x respectively on BERT-Large-Uncased on 64 GPUs.",NIPS
"We propose an end-to-end trainable, cross-category method for reconstructing multiple man-made articulated objects from a single RGBD image, focusing on part-level shape reconstruction and pose and kinematics estimation. We depart from previous works that rely on learning instance-level latent space, focusing on man-made articulated objects with predefined part counts. Instead, we propose a novel alternative approach that employs part-level representation, representing instances as combinations of detected parts. While our detect-then-group approach effectively handles instances with diverse part structures and various part counts, it faces issues of false positives, varying part sizes and scales, and an increasing model size due to end-to-end training. To address these challenges, we propose 1) test-time kinematics-aware part fusion to improve detection performance while suppressing false positives, 2) anisotropic scale normalization for part shape learning to accommodate various part sizes and scales, and 3) a balancing strategy for cross-refinement between feature space and output space to improve part detection while maintaining model size. Evaluation on both synthetic and real data demonstrates that our method successfully reconstructs variously structured multiple instances that previous works cannot handle, and outperforms prior works in shape reconstruction and kinematics estimation.",NIPS
"Anti-spoofing detection has become a necessity for face recognition systems due to the security threat posed by spoofing attacks. Despite great success in traditional attacks, most deep-learning-based methods perform poorly in 3D masks, which can highly simulate real faces in appearance and structure, suffering generalizability insufficiency while focusing only on the spatial domain with single frame input. This has been mitigated by the recent introduction of a biomedical technology called rPPG (remote photoplethysmography). However, rPPG-based methods are sensitive to noisy interference and require at least one second (> 25 frames) of observation time, which induces high computational overhead. To address these challenges, we propose a novel 3D mask detection framework, called FASTEN (Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the network for focusing more on fine-grained details in large movements, which can eliminate redundant spatio-temporal feature interference and quickly capture splicing traces of 3D masks in fewer frames. Our proposed network contains three key modules: 1) a facial optical flow network to obtain non-RGB inter-frame flow information; 2) flow attention to assign different significance to each frame; 3) spatio-temporal aggregation to aggregate high-level spatial features and temporal transition features. Through extensive experiments, FASTEN only requires five frames of input and outperforms eight competitors for both intra-dataset and cross-dataset evaluations in terms of multiple detection metrics. Moreover, FASTEN has been deployed in real-world mobile devices for practical 3D mask detection.",NIPS
"We study a federated linear bandits model, where $M$ clients communicate with a central server to solve a linear contextual bandits problem with finite adversarial action sets that may be different across clients. To address the unique challenges of **adversarial finite** action sets, we propose the FedSupLinUCB algorithm, which extends the principles of SupLinUCB and OFUL algorithms in linear contextual bandits. We prove that FedSupLinUCB achieves a total regret of $\tilde{O}(\sqrt{d T})$, where $T$ is the total number of arm pulls from all clients, and $d$ is the ambient dimension of the linear model. This matches the minimax lower bound and thus is order-optimal (up to polylog terms). We study both asynchronous and synchronous cases and show that the communication cost can be controlled as $O(d M^2 \log(d)\log(T))$ and $O(\sqrt{d^3 M^3} \log(d))$, respectively. The FedSupLinUCB design is further extended to two scenarios: (1) variance-adaptive, where a total regret of $\tilde{O} (\sqrt{d \sum \nolimits_{t=1}^{T} \sigma_t^2})$ can be achieved with $\sigma_t^2$ being the noise variance of round $t$; and (2) adversarial corruption, where a total regret of $\tilde{O}(\sqrt{dT} + d C_p)$ can be achieved with $C_p$ being the total corruption budget. Experiment results corroborate the theoretical analysis and demonstrate the effectiveness of \alg on both synthetic and real-world datasets.",NIPS
"Modern recommender systems perform large-scale retrieval by embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In this paper, we propose a novel generative retrieval approach, where the retrieval model autoregressively decodes the identifiers of the target candidates. To that end, we create semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based sequence-to-sequence model is trained to predict the Semantic ID of the next item that the user will interact with. We show that recommender systems trained with the proposed paradigm significantly outperform the current SOTA models on various datasets. In addition, we show that incorporating Semantic IDs into the sequence-to-sequence model enhances its ability to generalize, as evidenced by the improved retrieval performance observed for items with no prior interaction history.",NIPS
"Models that rely solely on pairwise relationships often fail to capture the complete statistical structure of the complex multivariate data found in diverse domains, such as socio-economic, ecological, or biomedical systems. Non-trivial dependencies between groups of more than two variables can play a significant role in the analysis and modelling of such systems, yet extracting such high-order interactions from data remains challenging. Here, we introduce a hierarchy of $d$-order ($d \geq 2$) interaction measures, increasingly inclusive of possible factorisations of the joint probability distribution, and define non-parametric, kernel-based tests to establish systematically the statistical significance of $d$-order interactions. We also establish mathematical links with lattice theory, which elucidate the derivation of the interaction measures and their composite permutation tests; clarify the connection of simplicial complexes with kernel matrix centring; and provide a means to enhance computational efficiency. We illustrate our results numerically with validations on synthetic data, and through an application to neuroimaging data.",NIPS
"Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study shows that with most fine-tuning approaches, the performance on pre-training tasks deteriorates significantly. Therefore, we propose a novel method, Learning-to-Modulate (L2M), that avoids the degradation of learned skills by modulating the information flow of the frozen pre-trained model via a learnable modulation pool. Our method achieves state-of-the-art performance on the Continual-World benchmark, while retaining performance on the pre-training tasks. Finally, to aid future research in this area, we release a dataset encompassing 50 Meta-World and 16 DMControl tasks.",NIPS
"The efficient coding hypothesis proposes that the response properties of sensory systems are adapted to the statistics of their inputs such that they capture maximal information about the environment, subject to biological constraints. While elegant, information theoretic properties are notoriously difficult to measure in practical settings or to employ as objective functions in optimization. This difficulty has necessitated that computational models designed to test the hypothesis employ several different information metrics ranging from approximations and lower bounds to proxy measures like reconstruction error. Recent theoretical advances have characterized a novel and ecologically relevant efficiency metric, the ``manifold capacity,â€ which is the number of object categories that may be represented in a linearly separable fashion. However, calculating manifold capacity is a computationally intensive iterative procedure that until now has precluded its use as an objective. Here we outline the simplifying assumptions that allow manifold capacity to be optimized directly, yielding Maximum Manifold Capacity Representations (MMCR). The resulting method is closely related to and inspired by advances in the field of self supervised learning (SSL), and we demonstrate that MMCRs are competitive with state of the art results on standard SSL benchmarks. Empirical analyses reveal differences between MMCRs and representations learned by other SSL frameworks, and suggest a mechanism by which manifold compression gives rise to class separability.  Finally we evaluate a set of SSL methods on a suite of neural predicitivity benchmarks, and find MMCRs are higly competitive as models of the ventral stream.",NIPS
"Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We demonstrate through experiments that our framework recovers variable importance rankings for complex simulation setups where other methods fail. Further, we show that our framework accurately estimates the true importance of a variable for the underlying data distribution. We provide theoretical guarantees on the consistency and finite sample error rates for our estimator. Finally, we demonstrate its utility with a real-world case study exploring which genes are important for predicting HIV load in persons with HIV, highlighting an important gene that has not previously been studied in connection with HIV.",NIPS
"Learning curve extrapolation aims to predict model performance in later epochs of training, based on the performance in earlier epochs.In this work, we argue that, while the inherent uncertainty in the extrapolation of learning curves warrants a Bayesian approach, existing methods are (i) overly restrictive, and/or (ii) computationally expensive. We describe the first application of prior-data fitted neural networks (PFNs) in this context. A PFN is a transformer, pre-trained on data generated from a prior, to perform approximate Bayesian inference in a single forward pass. We propose LC-PFN, a PFN trained to extrapolate 10 million artificial right-censored learning curves generated from a parametric prior proposed in prior art using MCMC. We demonstrate that LC-PFN can approximate the posterior predictive distribution more accurately than MCMC, while being over 10 000 times faster. We also show that the same LC-PFN achieves competitive performance extrapolating a total of 20 000 real learning curves from four learning curve benchmarks (LCBench, NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets with varying input modalities (tabular, image, text, and protein data). Finally, we investigate its potential in the context of model selection and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead.",NIPS
"We introduce a new approach for computing optimal equilibria via learning in games. It applies to extensive-form settings with any number of players, including mechanism design, information design, and solution concepts such as correlated, communication, and certification equilibria. We observe that optimal equilibria are minimax equilibrium strategies of a player in an extensive-form zero-sum game. This reformulation allows to apply techniques for learning in zero-sum games, yielding the first learning dynamics that converge to optimal equilibria, not only in empirical averages, but also in iterates. We demonstrate the practical scalability and flexibility of our approach by attaining state-of-the-art performance in benchmark tabular games, and by computing an optimal mechanism for a sequential auction design problem using deep reinforcement learning.",NIPS
"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.",NIPS
"DNN pruning is a popular way to reduce the size of a model, improve the inferencelatency, and minimize the power consumption on DNN accelerators. However,existing approaches might be too complex, expensive or ineffective to apply toa variety of vision/language tasks, DNN architectures and to honor structuredpruning constraints. In this paper, we propose an efficient yet effective train-timepruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamicfunction of weights during training to generate soft pruning masks for the weightsin a parameter-free manner for a given pruning target. While differentiable, thesimplicity and efficiency of PDP make it universal enough to deliver state-of-the-artrandom/structured/channel pruning results on various vision and natural languagetasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1kaccuracy at 86.6% sparsity, which is 1.7% higher accuracy than those from thestate-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on Multi-GenreNatural Language Inference with 90% sparsity for BERT, while the next best fromthe existing techniques shows 81.5% accuracy. In addition, PDP can be applied tostructured pruning, such as N:M pruning and channel pruning. For 1:4 structuredpruning of ResNet18, PDP improved the top-1 ImageNet1k accuracy by over 3.6%over the state-of-the-art. For channel pruning of ResNet50, PDP reduced the top-1ImageNet1k accuracy by 0.6% from the state-of-the-art.",NIPS
"We present new insights and a novel paradigm for learning implicit neural representations (INR) of shapes. In particular, we shed light on the popular eikonal loss used for imposing a signed distance function constraint in INR. We show analytically that as the representation power of the network increases, the optimization approaches a partial differential equation (PDE) in the continuum limit that is unstable. We show that this instability can manifest in existing network optimization, leading to irregularities in the reconstructed surface and/or convergence to sub-optimal local minima, and thus fails to capture fine geometric and topological structure. We show analytically how other terms added to the loss, currently used in the literature for other purposes, can actually eliminate these instabilities. However, such terms can over-regularize the surface, preventing the representation of fine shape detail. Based on a similar PDE theory for the continuum limit, we introduce a new regularization term that still counteracts the eikonal instability but without over-regularizing. Furthermore, since stability is now guaranteed in the continuum limit, this stabilization also allows for considering new network structures that are able to represent finer shape detail. We introduce such a structure based on quadratic layers. Experiments on multiple benchmark data sets show that our new regularization and network are able to capture more precise shape details and more accurate topology than existing state-of-the-art.",NIPS
"Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as an extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although better than previous approaches in terms of memory usage, BT-RvNN can be still exorbitantly expensive. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10-16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d}$ into a token contextualizer of the form  $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models. Our code is available at the link: https://github.com/JRC1995/BeamRecursionFamily.",NIPS
"This paper presents new projection-free algorithms for Online Convex Optimization (OCO) over a convex domain $\mathcal{K} \subset \mathbb{R}^d$. Classical OCO algorithms (such as Online Gradient Descent) typically need to perform Euclidean projections onto the convex set $\mathcal{K}$ to ensure feasibility of their iterates. Alternative algorithms, such as those based on the Frank-Wolfe method, swap potentially-expensive Euclidean projections onto $\mathcal{K}$ for linear optimization over $\mathcal{K}$. However, such algorithms have a sub-optimal regret in OCO compared to projection-based algorithms. In this paper, we look at a third type of algorithms that output approximate Newton iterates using a self-concordant barrier for the set of interest. The use of a self-concordant barrier automatically ensures feasibility without the need of projections. However, the computation of the Newton iterates requires a matrix inverse, which can still be expensive. As our main contribution, we show how the stability of the Newton iterates can be leveraged to only compute the inverse Hessian a vanishing fractions of the rounds, leading to a new efficient projection-free OCO algorithm with a state-of-the-art regret bound.",NIPS
"The adaptive leaky integrate-and-fire (ALIF) model is fundamental within computational neuroscience and has been instrumental in studying our brains $\textit{in silico}$. Due to the sequential nature of simulating these neural models, a commonly faced issue is the speed-accuracy trade-off: either accurately simulate a neuron using a small discretisation time-step (DT), which is slow, or more quickly simulate a neuron using a larger DT and incur a loss in simulation accuracy. Here we provide a solution to this dilemma, by algorithmically reinterpreting the ALIF model, reducing the sequential simulation complexity and permitting a more efficient parallelisation on GPUs. We computationally validate our implementation to obtain over a $50\times$ training speedup using small DTs on synthetic benchmarks. We also obtained a comparable performance to the standard ALIF implementation on different supervised classification tasks - yet in a fraction of the training time. Lastly, we showcase how our model makes it possible to quickly and accurately fit real electrophysiological recordings of cortical neurons, where very fine sub-millisecond DTs are crucial for capturing exact spike timing.",NIPS
"Dataset Distillation is the task of synthesizing small datasets from large ones while still retaining comparable predictive accuracy to the original uncompressed dataset. Despite significant empirical progress in recent years, there is little understanding of the theoretical limitations/guarantees of dataset distillation, specifically, what excess risk is achieved by distillation compared to the original dataset, and how large are distilled datasets? In this work, we take a theoretical view on kernel ridge regression (KRR) based methods of dataset distillation such as Kernel Inducing Points. By transforming ridge regression in random Fourier features (RFF) space, we provide the first proof of the existence of small (size) distilled datasets and their corresponding excess risk for shift-invariant kernels. We prove that a small set of instances exists in the original input space such that its solution in the RFF space coincides with the solution of the original data. We further show that a KRR solution can be generated using this distilled set of instances which gives an approximation towards the KRR solution optimized on the full input data. The size of this set is linear in the dimension of the RFF space of the input set or alternatively near linear in the number of effective degrees of freedom, which is a function of the kernel, number of data points, and the regularization parameter $\lambda$. The error bound of this distilled set is also a function of $\lambda$.  We verify our bounds analytically and empirically.",NIPS
"Delayed feedback is a critical problem in dynamic recommender systems. In practice, the feedback result often depends on the frequency of recommendation. Most existing online learning literature fails to consider optimization of the recommendation frequency, and regards the reward from each successfully recommended message to be equal. In this paper, we consider a novel cascading bandits setting, where individual messages from a selected list are sent to a user periodically. Whenever a user does not like a message, she may abandon the system with a probability positively correlated with the recommendation frequency.  A learning agent needs to learn both the underlying message attraction probabilities and users' abandonment probabilities through the randomly delayed feedback. We first show a dynamic programming solution to finding the optimal message sequence in deterministic scenarios, in which the reward is allowed to vary with different messages. Then we propose a polynomial time UCB-based offline learning algorithm, and discuss its performance by characterizing its regret bound. For the online setting, we propose a learning algorithm which allows adaptive content for a given user. Numerical experiment on AmEx dataset confirms the effectiveness of our algorithms.",NIPS
"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.",NIPS
"Agents with the ability to comprehend and reason about the dynamics of objects would be expected to exhibit improved robustness and generalization in novel scenarios. However, achieving this capability necessitates not only an effective scene representation but also an understanding of the mechanisms governing interactions among object subsets. Recent studies have made significant progress in representing scenes using object slots. In this work, we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models object dynamics by leveraging communication among slots along with a modular architecture capable of dynamically selecting reusable mechanisms for predicting the future states of each object slot. Crucially, RSM leverages the Central Contextual Information (CCI), enabling selected mechanisms to access the remaining slots through a bottleneck, effectively allowing for modeling of higher order and complex interactions that might require a sparse subset of objects. Experimental results demonstrate the superior performance of RSM compared to state-of-the-art methods across various future prediction and related downstream tasks, including Visual Question Answering and action planning. Furthermore, we showcase RSMâ€™s Out-of-Distribution generalization ability to handle scenes in intricate scenarios.",NIPS
"This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.",NIPS
"Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.",NIPS
"We show that many definitions of stability found in the learning theory literature are equivalent to one another. We distinguish between two families of definitions of stability: distribution-dependent and distribution-independent Bayesian stability. Within each family, we establish equivalences between various definitions, encompassing approximate differential privacy, pure differential privacy, replicability, global stability, perfect generalization, TV stability, mutual information stability, KL-divergence stability, and RÃ©nyi-divergence stability. Along the way, we prove boosting results that enable the amplification of the stability of a learning rule. This work is a step towards a more systematic taxonomy of stability notions in learning theory,  which can promote clarity and an improved understanding of an array of stability concepts that have emerged in recent years.",NIPS
"Informational parsimony provides a useful inductive bias for learning representations that achieve better generalization by being robust to noise and spurious correlations. We propose information gating as a way to learn parsimonious representations that identify the minimal information required for a task. When gating information, we can learn to reveal as little information as possible so that a task remains solvable, or hide as little information as possible so that a task becomes unsolvable. We gate information using a differentiable parameterization of the signal-to-noise ratio, which can be applied to arbitrary values in a network, e.g., erasing pixels at the input layer or activations in some intermediate layer. When gating at the input layer, our models learn which visual cues matter for a given task. When gating intermediate layers, our models learn which activations are needed for subsequent stages of computation. We call our approach InfoGating. We apply InfoGating to various objectives such as multi-step forward and inverse dynamics models, Q-learning, and behavior cloning, highlighting how InfoGating can naturally help in discarding information not relevant for control. Results show that learning to identify and use minimal information can improve generalization in downstream tasks. Policies based on InfoGating are considerably more robust to irrelevant visual features, leading to improved pretraining and finetuning of RL models.",NIPS
"We show that Deep Neural Networks introduce two heteroscedastic Gumbel noise sources into Q-Learning.  To account for these noise sources, we propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control.  In discrete control, we derive a closed-form expression for the loss function of our algorithm.  In continuous control, this loss function is intractable and we therefore derive an approximation with a hyperparameter whose value regulates pessimism in Q-Learning.  We present a default value for our pessimism hyperparameter that enables DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D and show that tuning this hyperparameter may further improve sample efficiency.",NIPS
"We study the connection between gradient-based meta-learning and convex optimisation. We observe that gradient descent with momentum is a special case of meta-gradients, and building on recent results in optimisation, we prove convergence rates for meta learning in the single task setting. While a meta-learned update rule can yield faster convergence up to constant factor, it is not sufficient for acceleration. Instead, some form of optimism is required. We show that optimism in meta-learning can be captured through the recently proposed Bootstrapped Meta-Gradient (Flennerhag et. al., 2022) method, providing deeper insight into its underlying mechanics.",NIPS
"According to World Health Organization, there is an estimated 2.2 billion people with a near or distance vision impairment worldwide. Difficulty in self-navigation is one of the greatest challenges to independence for the blind and low vision (BLV) people. Through consultations with several BLV service providers, we realized that negotiating surface discontinuities is one of the very prominent challenges when navigating an outdoor environment within the urban. Surface discontinuities are commonly formed by rises and drop-offs along a pathway. They could be a threat to balancing during a walk and perceiving such a threat is highly challenging to the BLVs. In this paper, we introduce SurDis, a novel dataset of depth maps and stereo images that exemplifies the issue of surface discontinuity in the urban areas of Klang Valley, Malaysia. We seek to address the limitation of existing datasets of such nature in these areas. Current mobility tools for the BLVs predominantly focus on furniture, indoor built environments, traffic signs, vehicles, humans and various types of objects' detection above the surface of a pathway. We emphasize a specific purpose for SurDis â€“ to support the development of assistive wearable technology for the BLVs to negotiate surface discontinuity. We consulted BLV volunteers on the specifications of surface condition that could become hazardous for navigation using 3D printed replicas of actual scaled-down scenes, and identified locations that are frequented by the BLVs as our target data collection fields. With feedback from these volunteers, we developed a lightweight, small and unobtrusive prototype equipped with a tiny stereo camera and an embedded system on a single board computer to capture the samples from 10 different locations. We describe instrument development, data collection, preprocessing, annotation, and experiments conducted. The dataset contains: (1) more than 17000 depth maps generated from 200 sets of stereo image sequences, (2) annotations of surface discontinuity in the depth maps, and (3) bitmap stereo image pairs corresponding to the depth maps in (1).",NIPS
"Video-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, sub-activity, and atomic action level.  We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on few-shot activity parsing, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language.",NIPS
"Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents---object manipulation by following human guidance, e.g., â€œmove the red mug next to the box while keeping it upright.â€ To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.",NIPS
"Visual correspondence of 2D animation is the core of many applications and deserves careful study. Existing correspondence datasets for 2D cartoon suffer from simple frame composition and monotonic movements, making them  insufficient to simulate real animations. In this work, we present a new 2D animation visual correspondence dataset, AnimeRun, by converting open source 3D movies to full scenes in 2D style, including simultaneous moving background and interactions of multiple subjects. Statistics show that our proposed dataset not only resembles real anime more in image composition, but also possesses richer and more complex motion patterns compared to existing datasets. With this dataset, we establish a comprehensive benchmark by evaluating several existing optical flow and segment matching methods, and analyze shortcomings of these methods on animation data. Data are available at https://lisiyao21.github.io/projects/AnimeRun.",NIPS
"In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present \textbf{Pythae}, a versatile \textit{open-source} Python library providing both a \textit{unified implementation} and a dedicated framework allowing \textit{straightforward}, \emph{reproducible} and \textit{reliable} use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpolation. The open-source library can be found at \url{https://github.com/clementchadebec/benchmark_VAE}.",NIPS
"Contemporary vision benchmarks predominantly consider tasks on which humans can achieve near-perfect performance. However, humans are frequently presented with visual data that they cannot classify with 100% certainty, and models trained on standard vision benchmarks achieve low performance when evaluated on this data. To address this issue, we introduce a procedure for creating datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a collection of noisy images extracted from videos. All images are annotated with ground truth values and a test set is annotated with human uncertainty judgments. We use this dataset to characterize human uncertainty in vision tasks and evaluate existing visual event classification models. Experimental results suggest that existing vision models are not sufficiently equipped to provide meaningful outputs for ambiguous images and that datasets of this nature can be used to assess and improve such models through model training and direct evaluation of model calibration. These findings motivate large-scale ambiguous dataset creation and further research focusing on noisy visual data.",NIPS
"Despite the prevalence of recent success in learning from static graphs, learning from time-evolving graphs remains an open challenge. In this work, we design new, more stringent evaluation procedures for link prediction specific to dynamic graphs, which reflect real-world considerations, to better compare the strengths and weaknesses of methods. First, we create two visualization techniques to understand the reoccurring patterns of edges over time and show that many edges reoccur at later time steps. Based on this observation, we propose a pure memorization-based baseline called EdgeBank. EdgeBank achieves surprisingly strong performance across multiple settings which highlights that the negative edges used in the current evaluation are easy. To sample more challenging negative edges, we introduce two novel negative sampling strategies that improve robustness and better match real-world applications. Lastly, we introduce six new dynamic graph datasets from a diverse set of domains missing from current benchmarks, providing new challenges and opportunities for future research. Our code repository is accessible at https://github.com/fpour/DGB.git.",NIPS
"Knowledge tracing (KT) is the task of using students' historical learning interaction data to model their knowledge mastery over time so as to make predictions on their future interaction performance. Recently, remarkable progress has been made of using various deep learning techniques to solve the KT problem. However, the success behind deep learning based knowledge tracing (DLKT) approaches is still left somewhat unknown and proper measurement and analysis of these DLKT approaches remain a challenge. First, data preprocessing procedures in existing works are often private and custom, which limits experimental standardization. Furthermore, existing DLKT studies often differ in terms of the evaluation protocol and are far away real-world educational contexts. To address these problems, we introduce a comprehensive python based benchmark platform, \textsc{pyKT}, to guarantee valid comparisons across DLKT methods via thorough evaluations. The \textsc{pyKT} library consists of a standardized set of integrated data preprocessing procedures on 7 popular datasets across different domains, and 10 frequently compared DLKT model implementations for transparent experiments. Results from our fine-grained and rigorous empirical KT studies yield a set of observations and suggestions for effective DLKT, e.g., wrong evaluation setting may cause label leakage that generally leads to performance inflation; and the improvement of many DLKT approaches is minimal compared to the very first DLKT model proposed by Piech et al. \cite{piech2015deep}. We have open sourced \textsc{pyKT} and our experimental results at \url{https://pykt.org/}. We welcome contributions from other research groups and practitioners.",NIPS
"Understanding human tasks through video observations is an essential capability of intelligent agents. The challenges of such capability lie in the difficulty of generating a detailed understanding of situated actions, their effects on object states (\ie, state changes), and their causal dependencies. These challenges are further aggravated by the natural parallelism from multi-tasking and partial observations in multi-agent collaboration. Most prior works leverage action localization or future prediction as an \textit{indirect} metric for evaluating such task understanding from videos. To make a \textit{direct} evaluation, we introduce the EgoTaskQA benchmark that provides a single home for the crucial dimensions of task understanding through question answering on real-world egocentric videos. We meticulously design questions that target the understanding of (1) action dependencies and effects, (2) intents and goals, and (3) agents' beliefs about others. These questions are divided into four types, including descriptive (what status?), predictive (what will?), explanatory (what caused?), and counterfactual (what if?) to provide diagnostic analyses on \textit{spatial, temporal, and causal} understandings of goal-oriented tasks. We evaluate state-of-the-art video reasoning models on our benchmark and show their significant gaps between humans in understanding complex goal-oriented egocentric videos. We hope this effort would drive the vision community to move onward with goal-oriented video understanding and reasoning.",NIPS
"Extensive literature on backdoor poison attacks has studied attacks and defenses for backdoors using  â€œdigital trigger patterns.â€ In contrast, â€œphysical backdoorsâ€ use physical objects as triggers, have only recently been identified, and are qualitatively different enough to resist most defenses targeting digital trigger backdoors. Research on physical backdoors is limited by access to large datasets containing real images of physical objects co-located with misclassification targets. Building these datasets is time- and labor-intensive.This work seeks to address the challenge of accessibility for research on physical backdoor attacks. We hypothesize that there may be naturally occurring physically co-located objects already present in popular datasets such as ImageNet. Once identified, a careful relabeling of these data can transform them into training samples for physical backdoor attacks. We propose a method to scalably identify these subsets of potential triggers in existing datasets, along with the specific classes they can poison. We call these naturally occurring trigger-class subsets natural backdoor datasets. Our techniques successfully identify natural backdoors in widely-available datasets, and produce models behaviorally equivalent to those trained on manually curated datasets. We release our code to allow the research community to create their own datasets for research on physical backdoor attacks.",NIPS
"Large language models produce human-like text that drive a growing number of applications.  However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful.  Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward.  To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks.  We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks.  Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.",NIPS
"Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique usersâ€™ data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithmsâ€™ generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.",NIPS
"Unlike RGB cameras that use visible light bands (384âˆ¼769 THz) and Lidars that use infrared bands (361âˆ¼331 THz), Radars use relatively longer wavelength radio bands (77âˆ¼81 GHz), resulting in robust measurements in adverse weathers. Unfortunately, existing Radar datasets only contain a relatively small number of samples compared to the existing camera and Lidar datasets. This may hinder the development of sophisticated data-driven deep learning techniques for Radar-based perception. Moreover, most of the existing Radar datasets only provide 3D Radar tensor (3DRT) data that contain power measurements along the Doppler, range, and azimuth dimensions. As there is no elevation information, it is challenging to estimate the 3D bounding box of an object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide 4DRT-based object detection baseline neural networks (baseline NNs) and show that the height information is crucial for 3D object detection. And by comparing the baseline NN with a similarly-structured Lidar-based neural network, we demonstrate that 4D Radar is a more robust sensor for adverse weather conditions. All codes are available at https://github.com/kaist-avelab/k-radar.",NIPS
"The promise of Mobile Health (mHealth) is the ability to use wearable sensors to monitor participant physiology at high frequencies during daily life to enable temporally-precise health interventions. However, a major challenge is frequent missing data. Despite a rich imputation literature, existing techniques are ineffective for the pulsative signals which comprise many mHealth applications, and a lack of available datasets has stymied progress. We address this gap with PulseImpute, the first large-scale pulsative signal imputation challenge which includes realistic mHealth missingness models, an extensive set of baselines, and clinically-relevant downstream tasks. Our baseline models include a novel transformer-based architecture designed to exploit the structure of pulsative signals. We hope that PulseImpute will enable the ML community to tackle this important and challenging task.",NIPS
"The ability to associate touch with sight is essential for tasks that require physically interacting with objects in the world. We propose a dataset with paired visual and tactile data called Touch and Go, in which human data collectors probe objects in natural environments using tactile sensors, while simultaneously recording egocentric video. In contrast to previous efforts, which have largely been confined to lab settings or simulated environments, our dataset spans a large number of â€œin the wildâ€ objects and scenes. We successfully apply our dataset to a variety of multimodal learning tasks: 1) self-supervised visuo-tactile feature learning, 2) tactile-driven image stylization, i.e., making the visual appearance of an object more consistent with a given tactile signal, and 3) predicting future frames of a tactile signal from visuo-tactile inputs.",NIPS
"Action recognition has improved dramatically with massive-scale video datasets. Yet, these datasets are accompanied with issues related to curation cost, privacy, ethics, bias, and copyright. Compared to that, only minor efforts have been devoted toward exploring the potential of synthetic video data. In this work, as a stepping stone towards addressing these shortcomings, we study the transferability of video representations learned solely from synthetically-generated video clips, instead of real data. We propose SynAPT, a novel benchmark for action recognition based on a combination of existing synthetic datasets, in which a model is pre-trained on synthetic videos rendered by various graphics simulators, and then transferred to a set of downstream action recognition datasets, containing different categories than the synthetic data. We provide an extensive baseline analysis on SynAPT revealing that the simulation-to-real gap is minor for datasets with low object and scene bias, where models pre-trained with synthetic data even outperform their real data counterparts. We posit that the gap between real and synthetic action representations can be attributed to contextual bias and static objects related to the action, instead of the temporal dynamics of the action itself. The SynAPT benchmark is available at https://github.com/mintjohnkim/SynAPT.",NIPS
"We introduce \textit{Nocturne}, a new 2D driving simulator for investigating multi-agent coordination under partial observability. The focus of Nocturne is to enable research into inference and theory of mind in real-world multi-agent settings without the computational overhead of computer vision and feature extraction from images. Agents in this simulator only observe an obstructed view of the scene, mimicking human visual sensing constraints. Unlike existing benchmarks that are bottlenecked by rendering human-like observations directly using a camera input, Nocturne uses efficient intersection methods to compute a vectorized set of visible features in a C++ back-end, allowing the simulator to run at $2000+$ steps-per-second. Using open-source trajectory and map data, we construct a simulator to load and replay arbitrary trajectories and scenes from real-world driving data. Using this environment, we benchmark reinforcement-learning and imitation-learning agents and demonstrate that the agents are quite far from human-level coordination ability and deviate significantly from the expert trajectories.",NIPS
"In recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for emotional response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables predicting a continuous spectrum of wellbeing. In experiments, we show how video models that are primarily trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.",NIPS
"As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.",NIPS
"Large-scale graph training is a notoriously challenging problem for graph neural networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to find the rationale for designing scalable GNNs. To this end, we first systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent benchmark for them by a greedy hyperparameter searching. In addition, regarding efficiency, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, throughput, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training manner, named EnGCN, to address the existing issues. Remarkably, our proposed method has achieved new state-of-the-art (SOTA) performance on large-scale datasets. Our code is available at https://github.com/VITA-Group/LargeScaleGCN_Benchmarking.",NIPS
"Traditional biological and pharmaceutical manufacturing plants are controlled by human workers or pre-defined thresholds. Modernized factories have advanced process control algorithms such as model predictive control (MPC). However, there is little exploration of applying deep reinforcement learning to control manufacturing plants. One of the reasons is the lack of high fidelity simulations and standard APIs for benchmarking. To bridge this gap, we develop an easy-to-use library that includes five high-fidelity simulation environments: BeerFMTEnv, ReactorEnv, AtropineEnv, PenSimEnv and mAbEnv, which cover a wide range of manufacturing processes. We build these environments on published dynamics models. Furthermore, we benchmark online and offline, model-based and model-free reinforcement learning algorithms for comparisons of follow-up research.",NIPS
"Deep classifiers are known to rely on spurious features, leading to reduced generalization. The severity of this problem varies significantly by class. We identify $15$ classes in ImageNet with very strong spurious cues, and collect segmentation masks for these challenging objects to form \emph{Hard ImageNet}. Leveraging noise, saliency, and ablation based metrics, we demonstrate that models rely on spurious features in Hard ImageNet far more than in RIVAL10, an ImageNet analog to CIFAR10. We observe Hard ImageNet objects are less centered and occupy much less space in their images than RIVAL10 objects, leading to greater spurious feature reliance. Further, we use robust neural features to automatically rank our images based on the degree of spurious cues present. Comparing images with high and low rankings within a class reveals the exact spurious features models rely upon, and shows reduced performance when spurious features are absent. With Hard ImageNet's image rankings, object segmentations, and our extensive evaluation suite, the community can begin to address the problem of learning to detect challenging objects \emph{for the right reasons}, despite the presence of strong spurious cues.",NIPS
"The use of cameras and computational algorithms for noninvasive, low-cost and scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs is very attractive. However, diverse data representing a range of environments, body motions, illumination conditions and physiological states is laborious, time consuming and expensive to obtain. Synthetic data have proven a valuable tool in several areas of machine learning, yet are not widely available for camera measurement of physiological states. Synthetic data offer ""perfect"" labels (e.g., without noise and with precise synchronization), labels that may not be possible to obtain otherwise (e.g., precise pixel level segmentation maps) and provide a high degree of control over variation and diversity in the dataset.  We present SCAMPS, a dataset of synthetics containing 2,800 videos (1.68M frames) with aligned cardiac and respiratory signals and facial action intensities. The RGB frames are provided alongside segmentation maps and precise descriptive statistics about the underlying waveforms, including inter-beat interval, heart rate variability, and pulse arrival time. Finally, we present baseline results training on these synthetic data and testing on real-world datasets to illustrate generalizability.",NIPS
"Out-of-distribution (OOD) detection is vital to safety-critical machine learning applications and has thus been extensively studied, with a plethora of methods developed in the literature. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often results in unfair comparisons and inconclusive results. From the problem setting perspective, OOD detection is closely related to neighboring fields including anomaly detection (AD), open set recognition (OSR), and model uncertainty, since methods developed for one domain are often applicable to each other. To help the community to improve the evaluation and advance, we build a unified, well-structured codebase called OpenOOD, which implements over 30 methods developed in relevant fields and provides a comprehensive benchmark under the recently proposed generalized OOD detection framework. With a comprehensive comparison of these methods, we are gratified that the field has progressed significantly over the past few years, where both preprocessing methods and the orthogonal post-hoc methods show strong potential.",NIPS
"The unbiased learning to rank (ULTR) problem has been greatly advanced by recent deep learning techniques and well-designed debias algorithms. However, promising results on the existing benchmark datasets may not be extended to the practical scenario due to some limitations of existing datasets. First, their semantic feature extractions are outdated while state-of-the-art large-scale pre-trained language models like BERT cannot be utilized due to the lack of original text. Second, display features are incomplete; thus in-depth study on ULTR is impossible such as the displayed abstract for analyzing the click necessary bias. Third, synthetic user feedback has been adopted by most existing datasets and real-world user feedback is greatly missing. To overcome these disadvantages, we introduce the Baidu-ULTR dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008 expert annotated queries(397,572 query document pairs). Baidu-ULTR is the first billion-level dataset for ULTR. Particularly, it offers: (1)the original semantic features and pre-trained language models of different sizes; (2)sufficient display information such as position, displayed height, and displayed abstract, enabling the comprehensive study of multiple displayed biases; and (3)rich user feedback on search result pages (SERPs) like dwelling time, allowing for user engagement optimization and promoting the exploration of multi-task learning in ULTR. Furthermore, we present the design principle of Baidu-ULTR and the performance of representative ULTR algorithms on Baidu-ULTR. The Baidu-ULTR dataset and corresponding baseline implementations are available at https://github.com/ChuXiaokai/baiduultrdataset. The dataset homepage is available at https://searchscience.baidu.com/dataset.html.",NIPS
"Finance is a particularly challenging playground for deep reinforcement learning. However, establishing high-quality market environments and benchmarks for financial reinforcement learning is challenging due to three major factors, namely, low signal-to-noise ratio of financial data, survivorship bias of historical data, and backtesting overfitting. In this paper, we present an openly accessible FinRL-Meta library that has been actively maintained by the AI4Finance community. First, following a DataOps paradigm, we will provide hundreds of market environments through an automatic data curation pipeline that processes dynamic datasets from real-world markets into gym-style market environments. Second, we reproduce popular papers as stepping stones for users to design new trading strategies. We also deploy the library on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, FinRL-Meta provides tens of Jupyter/Python demos organized into a curriculum and a documentation website to serve the rapidly growing community. FinRL-Meta is available at: \url{https://github.com/AI4Finance-Foundation/FinRL-Meta}",NIPS
"Existing benchmark datasets for recommender systems (RS)  either are created  at a small scale or involve very limited forms of user feedback. RS models evaluated on such datasets often lack practical values for large-scale real-world applications. In this paper, we describe Tenrec, a novel and publicly available data collection for RS that records various user feedback from four different recommendation scenarios. To be specific, Tenrec has the following five characteristics: (1) it is large-scale, containing around 5 million users and 140 million interactions; (2) it has not only positive user feedback, but also true  negative feedback (vs. one-class recommendation); (3) it contains overlapped users and items across four different scenarios; (4) it contains various types of  user positive feedback, in forms of clicking, liking, sharing, and following, etc; (5) it contains additional features beyond the user IDs and item IDs. We verify Tenrec on ten diverse  recommendation  tasks by running several classical baseline models per task. Tenrec has the potential to become a  useful benchmark dataset for a majority of popular recommendation tasks.  Our source codes and datasets will be included  in supplementary materials.",NIPS
"Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.",NIPS
"Joint visual and language modeling on large-scale datasets has recently shown good progress in multi-modal tasks when compared to single modal learning. However, robustness of these  approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of video-language models against various real-world perturbations. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbations. The study reveals some interesting initial findings from the studied models: 1) models are more robust when text is perturbed versus when video is perturbed, 2) models that are pre-trained are more robust than those trained from scratch, 3) models attend more to scene and objects rather than motion and action. We hope this study will serve as a benchmark and guide future research in robust video-language learning. The benchmark introduced in this study along with the code and datasets is available at https://bit.ly/3CNOly4.",NIPS
"Commercial adoption of automatic music composition requires the capability of generating diverse and high-quality music suitable for the desired context (e.g., music for romantic movies, action games, restaurants, etc.). In this paper, we introduce combinatorial music generation, a new task to create varying background music based on given conditions. Combinatorial music generation creates short samples of music with rich musical metadata, and combines them to produce a complete music. In addition, we introduce ComMU, the first symbolic music dataset consisting of short music samples and their corresponding 12 musical metadata for combinatorial music generation. Notable properties of ComMU are that (1) dataset is manually constructed by professional composers with an objective guideline that induces regularity, and (2) it has 12 musical metadata that embraces composers' intentions. Our results show that we can generate diverse high-quality music only with metadata, and that our unique metadata such as track-role and extended chord quality improves the capacity of the automatic composition. We highly recommend watching our video before reading the paper (https://pozalabs.github.io/ComMU/).",NIPS
"Detecting which nodes in graphs are outliers is a relatively new machine learning task with numerous applications. Despite the proliferation of algorithms developed in recent years for this task, there has been no standard comprehensive setting for performance evaluation. Consequently, it has been difficult to understand which methods work well and when under a broad range of settings. To bridge this gap, we presentâ€”to the best of our knowledgeâ€”the first comprehensive benchmark for unsupervised outlier node detection on static attributed graphs called BOND, with the following highlights. (1) We benchmark the outlier detection performance of 14 methods ranging from classical matrix factorization to the latest graph neural networks. (2) Using nine real datasets, our benchmark assesses how the different detection methods respond to two major types of synthetic outliers and separately to â€œorganicâ€ (real non-synthetic) outliers. (3) Using an existing random graph generation technique, we produce a family of synthetically generated datasets of different graph sizes that enable us to compare the running time and memory usage of the different outlier detection algorithms. Based on our experimental results, we discuss the pros and cons of existing graph outlier detection algorithms, and we highlight opportunities for future research. Importantly, our code is freely available and meant to be easily extendable: https://github.com/pygod-team/pygod/tree/main/benchmark",NIPS
"Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now.  In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark,TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of the video. We validate our pipeline on synthetic data and propose a simple end-to-end point tracking model, TAP-Net, showing that it outperforms all prior methods on our benchmark when trained on synthetic data.",NIPS
"Universal self-supervised (SSL) algorithms hold enormous promise for making machine learning accessible to high-impact domains such as protein biology, manufacturing, and genomics. We present DABS 2.0: a set of improved datasets and algorithms for advancing research on universal SSL. We extend the recently-introduced DABS benchmark with the addition of five real-world science and engineering domains: protein biology, bacterial genomics, multispectral satellite imagery, semiconductor wafers, and particle physics, bringing the total number of domains in the benchmark to twelve. We also propose a new universal SSL algorithm, Capri, and a generalized version of masked autoencoding, and apply both on all twelve domains---the most wide-ranging exploration of SSL yet. We find that multiple algorithms show gains across domains, outperforming previous baselines. In addition, we demonstrate the usefulness of DABS for scientific study of SSL by investigating the optimal corruption rate for each algorithm, showing that the best setting varies based on the domain. Code will be released at http://github.com/alextamkin/dabs}{http://github.com/alextamkin/dabs",NIPS
"The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of $\textit{language}$: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the $\textit{Language-complete ARC}$: a collection of natural language descriptions by a group of human participants  who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.",NIPS
"Estimating personalized effects of treatments is a complex, yet pervasive problem. To tackle it, recent developments in the machine learning (ML) literature on heterogeneous treatment effect estimation gave rise to many sophisticated, but opaque, tools: due to their flexibility, modularity and ability to learn constrained representations, neural networks in particular have become central to this literature. Unfortunately, the assets of such black boxes come at a cost: models typically involve countless nontrivial operations, making it difficult to understand what they have learned. Yet, understanding these models can be crucial -- in a medical context, for example, discovered knowledge on treatment effect heterogeneity could inform treatment prescription in clinical practice. In this work, we therefore use post-hoc feature importance methods to identify features that influence the model's predictions. This allows us to evaluate treatment effect estimators along a new and important dimension that has been overlooked in previous work: We construct a benchmarking environment to empirically investigate the ability of personalized treatment effect models to identify predictive covariates -- covariates that determine differential responses to treatment. Our benchmarking environment then enables us to provide new insight into the strengths and weaknesses of different types of treatment effects models as we modulate different challenges specific to treatment effect estimation -- e.g. the ratio of prognostic to predictive information, the possible nonlinearity of potential outcomes and the presence and type of confounding.",NIPS
"For the deployment of artificial intelligence (AI) in high risk settings, such as healthcare, methods that provide interpretability/explainability or allow fine-grained error analysis are critical. Many recent methods for interpretability/explainability and fine-grained error analysis use concepts, which are meta-labels which are semantically meaningful to humans.  However, there are only a few datasets that include concept-level meta-labels and most of these meta-labels are relevant for natural images that do not require domain expertise. Previous densely annotated datasets in medicine focused on meta-labels that are relevant to a single disease such as osteoarthritis or melanoma. In dermatology, skin disease is described using an established clinical lexicon that allow clinicians to describe physical exam findings to one another. To provide the first medical dataset densely annotated by domain experts to provide annotations useful across multiple disease processes, we developed SkinCon: a skin disease dataset densely annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick 17k skin disease dataset densely annotated with 48 clinical concepts, 22 of which have at least 50 images representing the concept. The concepts used were chosen by two dermatologists considering the clinical descriptor terms used to describe skin lesions. Examples include ""plaque"", ""scale"", and ""erosion"". These same concepts were also used to label 656 skin disease images from the Diverse Dermatology Images dataset, providing an additional external dataset with diverse skin tone representations. We review the potential applications for the SkinCon dataset, such as probing models, concept-based explanations, concept bottlenecks, error analysis, and slice discovery. Furthermore, we use SkinCon to demonstrate two of these use cases: debugging mistakes of an existing dermatology AI model with concepts and developing interpretable models with post-hoc concept bottleneck models.",NIPS
"Proteomics is the interdisciplinary field focusing on the large-scale study of proteins. Proteins essentially organize and execute all functions within organisms. Today, the bottom-up analysis approach is the most commonly used workflow, where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). MS-based proteomics has transformed various fields in life sciences, such as drug discovery and biomarker identification. Today, proteomics is entering a phase where it is helpful for clinical decision-making. Computational methods are vital in turning large amounts of acquired raw MS data into information and, ultimately, knowledge. Deep learning has proved its success in multiple domains as a robust framework for supervised and unsupervised machine learning problems. In proteomics, scientists are increasingly leveraging the potential of deep learning to predict the properties of peptides based on their sequence to improve their confident identification. However, a reference dataset is missing, covering several proteomics tasks, enabling performance comparison, and evaluating reproducibility and generalization. Here, we present a large labeled proteomics dataset spanning several tasks in the domain to address this challenge. We focus on two common applications: peptide retention time and MS/MS spectrum prediction. We review existing methods and task formulations from a machine learning perspective and recommend suitable evaluation metrics and visualizations. With an accessible dataset, we aim to lower the entry barrier and enable faster development in machine learning for proteomics.",NIPS
"Integer sequences are of central importance to the modeling of concepts admitting complete finitary descriptions. We introduce a novel view on the learning of such concepts and lay down a set of benchmarking tasks aimed at conceptual understanding by machine learning models. These tasks indirectly assess model ability to abstract, and challenge them to reason both interpolatively and extrapolatively from the knowledge gained by observing representative examples. To further aid research in knowledge representation and reasoning, we present FACT, the Finitary Abstraction Comprehension Toolkit. The toolkit surrounds a large dataset of integer sequences comprising both organic and synthetic entries, a library for data pre-processing and generation, a set of model performance evaluation tools, and a collection of baseline model implementations, enabling the making of the future advancements with ease.",NIPS
"Evaluating new techniques on realistic datasets plays a crucial role in the development of ML research and its broader adoption by practitioners. In recent years, there has been a significant increase of publicly available unstructured data resources for computer vision and NLP tasks. However, tabular data â€” which is prevalent in many high-stakes domains â€” has been lagging behind. To bridge this gap, we present Bank Account Fraud (BAF), the first publicly available 1 privacy-preserving, large-scale, realistic suite of tabular datasets. The suite was generated by applying state-of-the-art tabular data generation techniques on an anonymized,real-world bank account opening fraud detection dataset. This setting carries a set of challenges that are commonplace in real-world applications, including temporal dynamics and significant class imbalance. Additionally, to allow practitioners to stress test both performance and fairness of ML methods, each dataset variant of BAF contains specific types of data bias. With this resource, we aim to provide the research community with a more realistic, complete, and robust test bed to evaluate novel and existing methods.",NIPS
"The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale radiance fields datasets  for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the  classification and segmentation models that directly take this radiance fields format as input and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in ""https://postech-cvlab.github.io/PeRFception/"".",NIPS
"Large, diachronic datasets of political discourse are hard to come across, especially for resource-lean languages such as Greek. In this paper, we introduce a curated dataset of the Greek Parliament Proceedings that extends chronologically from 1989 up to 2020. It consists of more than 1 million speeches with extensive meta-data, extracted from 5,355 parliamentary sitting record files. We explain how it was constructed and the challenges that had to be overcome. The dataset can be used for both computational linguistics and political analysis---ideally, combining the two. We present such an application, showing (i) how the dataset can be used to study the change of word usage through time, (ii) between significant historical events and political parties, (iii) by evaluating and employing algorithms for detecting semantic shifts.",NIPS
"There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.We make three contributions.- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.All datasets, models, and code has been made open-source via the OpenHands toolkit.",NIPS
"In order to diagnostically analyze and improve the capability of pretrained language models (PLMs) in text generation, we propose TGEA 2.0, to date the largest dataset built on machine-authored texts by PLMs with fine-grained semantic annotations on a wide variety of pathological generation errors. We collect 170K nominal, phrasal and sentential prompts from 6M natural sentences in 3 domains. These prompts are fed into 4 generative PLMs with their best decoding strategy to generate paragraphs. 195,629 sentences are extracted from these generated paragraphs for manual annotation, where 36K erroneous sentences are detected, 42K erroneous spans are located and categorized into an error type defined in a two-level error taxonomy. We define a \textbf{Mi}nimal \textbf{S}et of \textbf{E}rror-related \textbf{W}ords (MiSEW) for each erroneous span, which not only provides error-associated words but also rationalizes the reasoning behind the error. Quality control with a pre-annotation and feedback loop is performed before and during the entire annotation process. With the diagnostically annotated dataset, we propose 5 diagnosis benchmark tasks (i.e., erroneous text detection, MiSEW extraction, erroneous span location and correction together with error type classification) and 2 pathology mitigation benchmark tasks (pairwise comparison and word prediction). Experiment results on these benchmark tasks demonstrate that TGEA 2.0 is a challenging dataset that could facilitate further research on automatic diagnosis and pathology mitigation over machine texts. The dataset will be publicly available at https://github.com/tjunlp-lab/TGEA/.",NIPS
"Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility.  Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular-based codebase (currently including implementations of 8 state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a standardized protocol of complete backdoor learning. We also provide comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of evaluations in total. We present abundant analysis from different perspectives about these 8,000 evaluations, studying the effects of different factors in backdoor learning.  All codes and evaluations of BackdoorBench are publicly available at https://backdoorbench.github.io.",NIPS
"In recent years there has been significant progress in the field of 3D learning on classification, detection and segmentation problems. The vast majority of the existing studies focus on canonical closed-set conditions, neglecting the intrinsic open nature of the real-world. This limits the abilities of robots and autonomous systems involved in safety-critical applications that require managing novel and unknown signals. In this context exploiting 3D data can be a valuable asset since it provides rich information about the geometry of perceived objects and scenes. With this paper we provide the first broad study on 3D Open Set learning. We introduce 3DOS: a novel testbed for semantic novelty detection that considers several settings with increasing difficulties in terms of semantic (category) shift, and covers both in-domain (synthetic-to-synthetic, real-to-real) and cross-domain (synthetic-to-real) scenarios. Moreover, we investigate the related 2D Open Set literature to understand if and how its recent improvements are effective on 3D data. Our extensive benchmark positions several algorithms in the same coherent picture, revealing their strengths and limitations. The results of our analysis may serve as a reliable foothold for future tailored 3D Open Set methods.",NIPS
"Artificial lights commonly leave strong lens flare artifacts on images captured at night. Nighttime flare not only affects the visual quality but also degrades the performance of vision algorithms. Existing flare removal methods mainly focus on removing daytime flares and fail in nighttime. Nighttime flare removal is challenging because of the unique luminance and spectrum of artificial lights and the diverse patterns and image degradation of the flares captured at night. The scarcity of nighttime flare removal datasets limits the research on this crucial task. In this paper, we introduce, Flare7K, the first nighttime flare removal dataset, which is generated based on the observation and statistics of real-world nighttime lens flares. It offers 5,000 scattering and 2,000 reflective flare images, consisting of 25 types of scattering flares and 10 types of reflective flares. The 7,000 flare patterns can be randomly added to flare-free images, forming the flare-corrupted and flare-free image pairs. With the paired data, we can train deep models to restore flare-corrupted images taken in the real world effectively. Apart from abundant flare patterns, we also provide rich annotations, including the labeling of light source, glare with shimmer, reflective flare, and streak, which are commonly absent from existing datasets. Hence, our dataset can facilitate new work in nighttime flare removal and more fine-grained analysis of flare patterns. Extensive experiments show that our dataset adds diversity to existing flare datasets and pushes the frontier of nighttime flare removal.",NIPS
"Dataset Condensation is a newly emerging technique aiming at learning a tiny dataset that captures the rich information encoded in the original dataset. As the size of datasets contemporary machine learning models rely on becomes increasingly large, condensation methods become a prominent direction for accelerating network training and reducing data storage. Despite numerous methods have been proposed in this rapidly growing field, evaluating and comparing different condensation methods is non-trivial and still remains an open issue. The quality of condensed dataset are often shadowed by many critical contributing factors to the end performance, such as data augmentation and model architectures. The lack of a systematic way to evaluate and compare condensation methods not only hinders our understanding of existing techniques, but also discourages practical usage of the synthesized datasets. This work provides the first large-scale standardized benchmark on Dataset Condensation. It consists of a suite of evaluations to comprehensively reflect the generability and effectiveness of condensation methods through the lens of their generated dataset. Leveraging this benchmark, we conduct a large-scale study of current condensation methods, and report many insightful findings that open up new possibilities for future development. The benchmark library, including evaluators, baseline methods, and generated datasets, is open-sourced to facilitate future research and application.",NIPS
"Satellite imagery is increasingly available, high resolution, and temporally detailed.  Changes in spatio-temporal datasets such as satellite images are particularly interesting as they reveal the many events and forces that shape our world.  However, finding such interesting and meaningful change events from the vast data is challenging.  In this paper, we present new datasets for such change events that include semantically meaningful events like road construction.  Instead of manually annotating the very large corpus of satellite images, we introduce a novel unsupervised approach that takes a large spatio-temporal dataset from satellite images and finds interesting change events.  To evaluate the meaningfulness on these datasets we create 2 benchmarks namely CaiRoad and CalFire which capture the events of road construction and forest fires.  These new benchmarks can be used to evaluate semantic retrieval/classification performance.  We explore these benchmarks qualitatively and quantitatively by using several methods and show that these new datasets are indeed challenging for many existing methods.",NIPS
"Current state-of-the-art vision-and-language models are evaluated on tasks either individually or in a multi-task setting, overlooking the challenges of continually learning (CL) tasks as they arrive. Existing CL benchmarks have facilitated research on task adaptation and mitigating ""catastrophic forgetting"", but are limited to vision-only and language-only tasks. We present CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL setting, and to systematically evaluate how upstream continual learning can rapidly generalize to new multimodal and unimodal tasks. CLiMB includes implementations of several CL algorithms and a modified Vision-Language Transformer (ViLT) model that can be deployed on both multimodal and unimodal tasks. We find that common CL methods can help mitigate forgetting during multimodal task learning, but do not enable cross-task knowledge transfer. We envision that CLiMB will facilitate research on a new class of CL algorithms for this challenging multimodal setting.",NIPS
"Computational inference of aesthetics is an ill-defined task due to its subjective nature. Many datasets have been proposed to tackle the problem by providing pairs of images and aesthetic scores based on human ratings. However, humans are better at expressing their opinion, taste, and emotions by means of language rather than summarizing them in a single number. In fact, photo critiques provide much richer information as they reveal how and why users rate the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo Critique Dataset (RPCD), which contains tuples of image and photo critiques. RPCD consists of 74K images and 220K comments and is collected from a Reddit community used by hobbyists and professional photographers to improve their photography skills by leveraging constructive community feedback. The proposed dataset differs from previous aesthetics datasets mainly in three aspects, namely (i) the large scale of the dataset and the extension of the comments criticizing different aspects of the image, (ii) it contains mostly UltraHD images, and (iii) it can easily be extended to new data as it is collected through an automatic pipeline. To the best of our knowledge, in this work, we propose the first attempt to estimate the aesthetic quality of visual stimuli from the critiques. To this end, we exploit the polarity of the sentiment of criticism as an indicator of aesthetic judgment. We demonstrate how sentiment polarity correlates positively with the aesthetic judgment available for two aesthetic assessment benchmarks. Finally, we experiment with several models by using the sentiment scores as a target for ranking images. Dataset and baselines are available https://github.com/mediatechnologycenter/aestheval.",NIPS
"Human-designed visual manuals are crucial components in shape assembly activities. They provide step-by-step guidance on how we should move and connect different parts in a convenient and physically-realizable way. While there has been an ongoing effort in building agents that perform assembly tasks, the information in human-design manuals has been largely overlooked. We identify that this is due to 1) a lack of realistic 3D assembly objects that have paired manuals and 2) the difficulty of extracting structured information from purely image-based manuals. Motivated by this observation, we present IKEA-Manual, a dataset consisting of 102 IKEA objects paired with assembly manuals. We provide fine-grained annotations on the IKEA objects and assembly manuals, including decomposed assembly parts, assembly plans, manual segmentation, and 2D-3D correspondence between 3D parts and visual manuals. We illustrate the broad application of our dataset on four tasks related to shape assembly: assembly plan generation, part segmentation, pose estimationand 3D part assembly.",NIPS
"Federated Learning (FL) is a novel approach enabling several clients holding sensitive data to collaboratively train machine learning models, without centralizing data. The cross-silo FL setting corresponds to the case of few ($2$--$50$) reliable clients, each holding medium to large datasets, and is typically found in applications such as healthcare, finance, or industry. While previous works have proposed representative datasets for cross-device FL, few realistic healthcare cross-silo FL datasets exist, thereby slowing algorithmic research in this critical application. In this work, we propose a novel cross-silo dataset suite focused on healthcare, FLamby (Federated Learning AMple Benchmark of Your cross-silo strategies), to bridge the gap between theory and practice of cross-silo FL.FLamby encompasses 7 healthcare datasets with natural splits, covering multiple tasks, modalities, and data volumes, each accompanied with baseline training code. As an illustration, we additionally benchmark standard FL algorithms on all datasets.Our flexible and modular suite allows researchers to easily download datasets, reproduce results and re-use the different components for their research. FLamby is available at~\url{www.github.com/owkin/flamby}.",NIPS
"We describe a customizable benchmark for hierarchical and ontological multi-label classification, a task where labels are equipped with a graph structure and data items can be assigned multiple labels.  We find that current benchmarks do not adequately represent the problem space, casting doubt on the generalizability of current results. We consider three dimensions of the problem space: context (availability of rich features on the data and labels), distribution of labels over data, and graph structure. For context, the lack of complex features on the labels (and in some cases, the data) artificially prevent the use of modern representation learning techniques as an appropriate baseline.  For distribution, we find the long tail of labels over data constitute a few-shot learning problem that artificially confounds the results: for most common benchmarks, over 40% of the labels have fewer than 5 data points in the training set.  For structure, we find that the correlation between performance and the height of the tree can explain some of the variation in performance, informing practical utility. In this paper, we demonstrate how the lack of diversity in benchmarks can confound performance analysis, then present a declarative query system called Ontologue for generating custom benchmarks with specific properties, then use this system to design 4 new benchmarks extracted from DBPedia that better represent the problem space. We evaluate state-of-the-art algorithms on both existing and new benchmarks and show that the performance conclusions can vary significantly depending on the dimensions we consider.  We intend the system and derived benchmarks to improve the analysis of generalizability for these problems.",NIPS
"Labeling articulated objects in unconstrained settings has a wide variety of applications including entertainment, neuroscience, psychology, ethology, and many fields of medicine. Large offline labeled datasets do not exist for all but the most common articulated object categories (e.g., humans). Hand labeling these landmarks within a video sequence is a laborious task. Learned landmark detectors can help, but can be error-prone when trained from only a few examples. Multi-camera systems that train fine-grained detectors have shown significant promise in detecting such errors, allowing for self-supervised solutions that only need a small percentage of the video sequence to be hand-labeled. The approach, however, is based on calibrated cameras and rigid geometry, making it expensive, difficult to manage, and impractical in real-world scenarios. In this paper, we address these bottlenecks by combining a non-rigid 3D neural prior with deep flow to obtain high-fidelity landmark estimates from videos with only two or three uncalibrated, handheld cameras. With just a few annotations (representing $1-2\%$ of the frames), we are able to produce 2D results comparable to state-of-the-art fully supervised methods, along with 3D reconstructions that are impossible with other existing approaches. Our Multi-view Bootstrapping in the Wild (MBW) approach demonstrates impressive results on standard human datasets, as well as tigers, cheetahs, fish, colobus monkeys, chimpanzees, and flamingos from videos captured casually in a zoo. We release the codebase for MBW as well as this challenging zoo dataset consisting of image frames of tail-end distribution categories with their corresponding 2D and 3D labels generated from minimal human intervention.",NIPS
"It is well-known in the video understanding community that human action recognition models suffer from background bias, i.e., over-relying on scene cues in making their predictions. However, it is difficult to quantify this effect using existing evaluation frameworks. We introduce the Human-centric Analysis Toolkit (HAT), which enables evaluation of learned background bias without the need for new manual video annotation. It does so by automatically generating synthetically manipulated videos and leveraging the recent advances in image segmentation and video inpainting. Using HAT we perform an extensive analysis of 74 action recognition models trained on the Kinetics dataset. We confirm that all these models focus more on the scene background than on the human motion; further, we demonstrate that certain model design decisions (such as training with fewer frames per video or using dense as opposed to uniform temporal sampling) appear to worsen the background bias. We open-source HAT to enable the community to design more robust and generalizable human action recognition models.",NIPS
"Molecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or self-designed tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization---the number of molecules evaluated by the oracle---is rarely discussed, despite being an essential consideration for realistic discovery applications.To fill this gap, we have created an open-source benchmark for practical molecular optimization, PMO, to facilitate the transparent and reproducible evaluation of algorithmic advances in molecular optimization. This paper thoroughly investigates the performance of 25 molecular design algorithms on 23 single-objective (scalar) optimization tasks with a particular focus on sample efficiency. Our results show that most ``state-of-the-art'' methods fail to outperform their predecessors under a limited oracle budget allowing 10K queries and that no existing algorithm can efficiently solve certain molecular optimization problems in this setting. We analyze the influence of the optimization algorithm choices, molecular assembly strategies, and oracle landscapes on the optimization performance to inform future algorithm development and benchmarking. PMO provides a standardized experimental setup to comprehensively evaluate and compare new molecule optimization methods with existing ones. All code can be found at https://github.com/wenhao-gao/mol_opt.",NIPS
"Zero-cost proxies (ZC proxies) are a recent architecture performance prediction technique aiming to significantly speed up algorithms for neural architecture search (NAS). Recent work has shown that these techniques show great promise, but certain aspects, such as evaluating and exploiting their complementary strengths, are under-studied. In this work, we create NAS-Bench-Suite: we evaluate 13 ZC proxies across 28 tasks, creating by far the largest dataset (and unified codebase) for ZC proxies, enabling orders-of-magnitude faster experiments on ZC proxies, while avoiding confounding factors stemming from different implementations. To demonstrate the usefulness of NAS-Bench-Suite, we run a large-scale analysis of ZC proxies, including a bias analysis, and the first information-theoretic analysis which concludes that ZC proxies capture substantial complementary information. Motivated by these findings, we present a procedure to improve the performance of ZC proxies by reducing biases such as cell size, and we also show that incorporating all 13 ZC proxies into the surrogate models used by NAS algorithms can improve their predictive performance by up to 42%. Our code and datasets are available at https://github.com/automl/naslib/tree/zerocost.",NIPS
"We are now witnessing significant progress of deep learning methods in a variety of tasks (or datasets) of proteins. However, there is a lack of a standard benchmark to evaluate the performance of different methods, which hinders the progress of deep learning in this field. In this paper, we propose such a benchmark called PEER, a comprehensive and multi-task benchmark for Protein sEquence undERstanding. PEER provides a set of diverse protein understanding tasks including protein function prediction, protein localization prediction, protein structure prediction, protein-protein interaction prediction, and protein-ligand interaction prediction. We evaluate different types of sequence-based methods for each task including traditional feature engineering approaches, different sequence encoding methods as well as large-scale pre-trained protein language models. In addition, we also investigate the performance of these methods under the multi-task learning setting. Experimental results show that large-scale pre-trained protein language models achieve the best performance for most individual tasks, and jointly training multiple tasks further boosts the performance. The datasets and source codes of this benchmark will be open-sourced soon.",NIPS
"Flying at speed through complex environments is a challenging task that has been performed successfully by insects since the Carboniferous, but which remains a challenge for robotic and autonomous systems. Insects navigate the world using optical flow sensed by their compound eyes, which they process using a deep neural network weighing just a few milligrams. Deploying an insect-inspired network architecture in computer vision could therefore enable more efficient and effective ways of estimating structure and self-motion using optical flow. Training a bio-informed deep network to implement these tasks requires biologically relevant training, test, and validation data. To this end, we introduce FlyView, a novel bio-informed truth dataset for visual navigation. This simulated dataset is rendered using open source 3D scenes in which the observer's position is known at every frame, and is accompanied by truth data on depth, self-motion, and motion flow. This dataset comprising 42,475 frames has several key features that are missing from existing optical flow datasets, including: (i) panoramic cameras with a monocular and binocular field of view matched to that of a fly's compound eyes; (ii) dynamically meaningful self-motion modelled on motion primitives, or the 3D trajectories of drones and flies; and (iii) complex natural and indoor environments including reflective surfaces.",NIPS
"Machine learning on blockchain graphs is an emerging field with many applications such as ransomware payment tracking, price manipulation analysis, and money laundering detection. However, analyzing blockchain data requires domain expertise and computational resources, which pose a significant barrier and hinder advancement in this field. We introduce Chartalist, the first comprehensive platform to methodically access and use machine learning across a large selection of blockchains to address this challenge. Chartalist contains ML-ready datasets from unspent transaction output (UTXO) (e.g., Bitcoin) and account-based blockchains (e.g., Ethereum). We envision that Chartalist can facilitate data modeling, analysis, and representation of blockchain data and attract a wider community of scientists to analyze blockchains. Chartalist is an open-science initiative at https://github.com/cakcora/Chartalist.",NIPS
"The recent advances of deep learning have dramatically changed how machine learning, especially in the domain of natural language processing, can be applied to legal domain. However, this shift to the data-driven approaches calls for larger and more diverse datasets, which are nevertheless still small in number, especially in non-English languages. Here we present the first large-scale benchmark of Korean legal AI datasets, LBOX OPEN, that consists of one legal corpus, two classification tasks, two legal judgement prediction (LJP) tasks, and one summarization task. The legal corpus consists of 147k Korean precedents (259M tokens), of which 63k are sentenced in last 4 years and 96k are from the first and the second level courts in which factual issues are reviewed. The two classification tasks are case names (11.3k) and statutes (2.8k) prediction from the factual description of individual cases. The LJP tasks consist of (1) 10.5k criminal examples where the model is asked to predict fine amount, imprisonment with labor, and imprisonment without labor ranges for the given facts, and (2) 4.7k civil examples where the inputs are facts and claim for relief and outputs are the degrees of claim acceptance. The summarization task consists of the Supreme Court precedents and the corresponding summaries (20k). We also release realistic variants of the datasets by extending the domain (1) to infrequent case categories in case name (31k examples) and statute (17.7k) classification tasks, and (2) to long input sequences in the summarization task (51k). Finally, we release LCUBE, the first Korean legal language model trained on the legal corpus from this study. Given the uniqueness of the Law of South Korea and the diversity of the legal tasks covered in this work, we believe that LBOX OPEN contributes to the multilinguality of global legal research. LBOX OPEN and LCUBE will be publicly available.",NIPS
"Animal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. The lack of APT datasets hinders the development and evaluation of video-based animal pose estimation and tracking methods, limiting the applications in real world, e.g., understanding animal behavior in wildlife conservation. To fill this gap, we make the first step and propose APT-36K, i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips collected and filtered from 30 animal species with 15 frames for each video, resulting in 36,000 frames in total. After manual annotation and careful double-check, high-quality keypoint and tracking annotations are provided for all the animal instances. Based on APT-36K, we benchmark several representative models on the following three tracks: (1) supervised animal pose estimation on a single frame under intra- and inter-domain transfer learning settings, (2) inter-species domain generalization test for unseen animals, and (3) animal pose estimation with animal tracking. Based on the experimental results, we gain some empirical insights and show that APT-36K provides a useful animal pose estimation and tracking benchmark, offering new challenges and opportunities for future research. The code and dataset will be made publicly available at https://github.com/pandorgan/APT-36K.",NIPS
"This paper describes the OCCGEN toolkit, which allows extracting multilingual parallel data balanced in gender within occupations. OCCGEN can extract datasets that reflect gender diversity (beyond binary) more fairly in society to be further used to explicitly mitigate occupational gender stereotypes. We propose two use cases that extract evaluation datasets for machine translation in four high-resourcelanguages from different linguistic families and in a low-resource African language. Our analysis of these use cases shows that translation outputs in high-resource languages tend to worsen in feminine subsets (compared to masculine). This can be explained because less attention is paid to the source sentence. Then, more attention is given to the target prefix overgeneralizing to the most frequent masculine forms.",NIPS
"Progress in reinforcement learning (RL) research is often driven by the design of new, challenging environments---a costly undertaking requiring skills orthogonal to that of a typical machine learning researcher. The complexity of environment development has only increased with the rise of procedural-content generation (PCG) as the prevailing paradigm for producing varied environments capable of testing the robustness and generalization of RL agents. Moreover, existing environments often require complex build processes, making reproducing results difficult. To address these issues, we introduce GriddlyJS, a web-based Integrated Development Environment (IDE) based on the Griddly engine. GriddlyJS allows researchers to easily design and debug arbitrary, complex PCG grid-world environments, as well as visualize, evaluate, and record the performance of trained agent models. By connecting the RL workflow to the advanced functionality enabled by modern web standards, GriddlyJS allows publishing interactive agent-environment demos that reproduce experimental results directly to the web. To demonstrate the versatility of GriddlyJS, we use it to quickly develop a complex compositional puzzle-solving environment alongside arbitrary human-designed environment configurations and their solutions for use in a automatic curriculum learning and offline RL context. The GriddlyJS IDE is open source and freely available at https://griddly.ai.",NIPS
"In the last years, neural networks (NN) have evolved from laboratory environments to the state-of-the-art for many real-world problems. It was shown that NN models (i.e., their weights and biases) evolve on unique trajectories in weight space during training. Following, a population of such neural network models (referred to as model zoo) would form structures in weight space. We think that the geometry, curvature and smoothness of these structures contain information about the state of training and can reveal latent properties of individual models. With such model zoos, one could investigate novel approaches for (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn rich representations of such populations, or (iv) exploit the model zoos for generative modelling of NN weights and biases. Unfortunately, the lack of standardized model zoos and available benchmarks significantly increases the friction for further research about populations of NNs. With this work, we publish a novel dataset of model zoos containing systematically generated and diverse populations of NN models for further research. In total the proposed model zoo dataset is based on eight image datasets, consists of 27 model zoos trained with varying hyperparameter combinations and includes 50â€™360 unique NN models as well as their sparsified twins, resulting in over 3â€™844â€™360 collected model states. Additionally, to the model zoo data we provide an in-depth analysis of the zoos and provide benchmarks for multiple downstream tasks. The dataset can be found at www.modelzoos.cc.",NIPS
"We introduce Meta-Album, an image classification meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. It includes 40 open datasets, each having at least 20 classes with 40 examples per class, with verified licences. They stem from diverse domains, such as ecology (fauna and flora), manufacturing (textures, vehicles), human actions, and optical character recognition, featuring various image scales (microscopic, human scales, remote sensing). All datasets are preprocessed, annotated, and formatted uniformly, and come in 3 versions (Micro $\subset$ Mini $\subset$ Extended) to match usersâ€™ computational resources. We showcase the utility of the first 30 datasets on few-shot learning problems. The other 10 will be released shortly after. Meta-Album is already more diverse and larger (in number of datasets) than similar efforts, and we are committed to keep enlarging it via a series of competitions. As competitions terminate, their test data are released, thus creating a rolling benchmark, available through OpenML.org. Our website https://meta-album.github.io/ contains the source code of challenge winning methods, baseline methods, data loaders, and instructions for contributing either new datasets or algorithms to our expandable meta-dataset.",NIPS
"Graph Neural Networks (GNNs) have achieved great success on a node classification task. Despite the broad interest in developing and evaluating GNNs, they have been assessed with limited benchmark datasets. As a result, the existing evaluation of GNNs lacks fine-grained analysis from various characteristics of graphs. Motivated by this, we conduct extensive experiments with a synthetic graph generator that can generate graphs having controlled characteristics for fine-grained analysis. Our empirical studies clarify the strengths and weaknesses of GNNs from four major characteristics of real-world graphs with class labels of nodes, i.e., 1) class size distributions (balanced vs. imbalanced), 2) edge connection proportions between classes (homophilic vs. heterophilic), 3) attribute values (biased vs. random), and 4) graph sizes (small vs. large). In addition, to foster future research on GNNs, we publicly release our codebase that allows users to evaluate various GNNs with various graphs. We hope this work offers interesting insights for future research.",NIPS
"Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, the Hanabi challenge, and Google Research Football, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods are a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at https://github.com/marlbenchmark/on-policy.",NIPS
"We introduce Breaking Bad, a large-scale dataset of fractured objects. Our dataset consists of over one million fractured objects simulated from ten thousand base models. The fracture simulation is powered by a recent physically based algorithm that efficiently generates a variety of fracture modes of an object. Existing shape assembly datasets decompose objects according to semantically meaningful parts, effectively modeling the construction process. In contrast, Breaking Bad models the destruction process of how a geometric object naturally breaks into fragments. Our dataset serves as a benchmark that enables the study of fractured object reassembly and presents new challenges for geometric shape understanding. We analyze our dataset with several geometry measurements and benchmark three state-of-the-art shape assembly deep learning methods under various settings. Extensive experimental results demonstrate the difficulty of our dataset, calling on future research in model designs specifically for the geometric shape assembly task. We host our dataset at https://breaking-bad-dataset.github.io/.",NIPS
"Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.",NIPS
"Graph neural architecture search (GraphNAS) has recently aroused considerable attention in both academia and industry. However, two key challenges seriously hinder the further research of GraphNAS. First, since there is no consensus for the experimental setting, the empirical results in different research papers are often not comparable and even not reproducible, leading to unfair comparisons. Secondly, GraphNAS often needs extensive computations, which makes it highly inefficient and inaccessible to researchers without access to large-scale computation. To solve these challenges, we propose NAS-Bench-Graph, a tailored benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we construct a unified, expressive yet compact search space, covering 26,206 unique graph neural network (GNN) architectures and propose a principled evaluation protocol. To avoid unnecessary repetitive training, we have trained and evaluated all of these architectures on nine representative graph datasets, recording detailed metrics including train, validation, and test performance in each epoch, the latency, the number of parameters, etc. Based on our proposed benchmark, the performance of GNN architectures can be directly obtained by a look-up table without any further computation, which enables fair, fully reproducible, and efficient comparisons.  To demonstrate its usage, we make in-depth analyses of our proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS. We also showcase how the benchmark can be easily compatible with GraphNAS open libraries such as AutoGL and NNI. To the best of our knowledge, our work is the first benchmark for graph neural architecture search.",NIPS
"While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game of vision-and-language associations (e.g., between werewolves and a full moon), used as a dynamic evaluation benchmark. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player tries to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (>90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, allowing future data collection that can be used to develop models with better association abilities.",NIPS
"Training and evaluating language models increasingly requires the construction of meta-datasets -- diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a variety of novel instruction tuning tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce BigBio a community library of 126+ biomedical NLP datasets, currently covering 13 task categories and 10+ languages. BigBio facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. BigBio is an ongoing community effort and is available at https://github.com/bigscience-workshop/biomedical",NIPS
"We introduce the Multi-Agent Tracking Environment (MATE), a novel multi-agent environment simulates the target coverage control problems in the real world. MATE hosts an asymmetric cooperative-competitive game consisting of two groups of learning agents--""cameras"" and ""targets""--with opposing interests. Specifically, ""cameras"", a group of directional sensors, are mandated to actively control the directional perception area to maximize the coverage rate of targets. On the other side, ""targets"" are mobile agents that aim to transport cargo between multiple randomly assigned warehouses while minimizing the exposure to the camera sensor networks. To showcase the practicality of MATE, we benchmark the multi-agent reinforcement learning (MARL) algorithms from different aspects, including cooperation, communication, scalability, robustness, and asymmetric self-play. We start by reporting results for cooperative tasks using MARL algorithms (MAPPO, IPPO, QMIX, MADDPG) and the results after augmenting with multi-agent communication protocols (TarMAC, I2C). We then evaluate the effectiveness of the popular self-play techniques (PSRO, fictitious self-play) in an asymmetric zero-sum competitive game. This process of co-evolution between cameras and targets helps to realize a less exploitable camera network. We also observe the emergence of different roles of the target agents while incorporating I2C into target-target communication. MATE is written purely in Python and integrated with OpenAI Gym API to enhance user-friendliness. Our project is released at https://github.com/UnrealTracking/mate.",NIPS
"Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of the causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting great challenges set forth by our benchmark.",NIPS
"We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting significant room for future work on physical and social human-robot communications and interactions.",NIPS
"Accurate intraoperative diagnosis is essential for providing safe and effective care during brain tumor surgery. Our standard-of-care diagnostic methods are time, resource, and labor intensive, which restricts access to optimal surgical treatments. To address these limitations, we propose an alternative workflow that combines stimulated Raman histology (SRH), a rapid optical imaging method, with deep learning-based automated interpretation of SRH images for intraoperative brain tumor diagnosis and real-time surgical decision support. Here, we present OpenSRH, the first public dataset of clinical SRH images from 300+ brain tumors patients and 1300+ unique whole slide optical images. OpenSRH contains data from the most common brain tumors diagnoses, full pathologic annotations, whole slide tumor segmentations, raw and processed optical imaging data for end-to-end model development and validation. We provide a framework for patch-based whole slide SRH classification and inference using weak (i.e. patient-level) diagnostic labels. Finally, we benchmark two computer vision tasks: multi-class histologic brain tumor classification and patch-based contrastive representation learning. We hope OpenSRH will facilitate the clinical translation of rapid optical imaging and real-time ML-based surgical decision support in order to improve the access, safety, and efficacy of cancer surgery in the era of precision medicine.",NIPS
"One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take context into account. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a ~256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.",NIPS
"Neural architecture search (NAS) has been successfully used to design numerous high-performance neural networks. However, NAS is typically compute-intensive, so most existing approaches restrict the search to decide the operations and topological structure of a single block only, then the same block is stacked repeatedly to form an end-to-end model. Although such an approach reduces the size of search space, recent studies show that a macro search space, which allows blocks in a model to be different, can lead to better performance. To provide a systematic study of the performance of NAS algorithms on a macro search space, we release Blox â€“ a benchmark that consists of 91k unique models trained on the CIFAR-100 dataset. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. We perform extensive experiments to compare existing algorithms that are well studied on cell-based search spaces, with the emerging blockwise approaches that aim to make NAS scalable to much larger macro search spaces. The Blox benchmark and code are available at https://github.com/SamsungLabs/blox.",NIPS
"Optical Chemical Structure Recognition (OCSR) deals with the translation from chemical images to molecular structures, this being the main way chemical compounds are depicted in scientific documents. Traditionally, rule-based methods have followed a framework based on the detection of chemical entities, such as atoms and bonds, followed by a compound structure reconstruction step. Recently, neural architectures analog to image captioning have been explored to solve this task, yet they still show to be data inefficient, using millions of examples just to show performances comparable with traditional methods. Looking to motivate and benchmark new approaches based on atomic-level entities detection and graph reconstruction, we present CEDe, a unique collection of chemical entity bounding boxes manually curated by experts for scientific literature datasets. These annotations combine to more than 700,000 chemical entity bounding boxes with the necessary information for structure reconstruction. Also, a large synthetic dataset containing one million molecular images and annotations is released in order to explore transfer-learning techniques that could help these architectures perform better under low-data regimes. Benchmarks show that detection-reconstruction based models can achieve performances on par with or better than image captioning-like models, even with 100x fewer training examples.",NIPS
"The lack of publicly available high-quality and accurately labeled datasets has long been a major bottleneck for singing voice synthesis (SVS). To tackle this problem, we present M4Singer, a free-to-use Multi-style, Multi-singer Mandarin singing collection with elaborately annotated Musical scores as well as its benchmarks. Specifically, 1) we construct and release a large high-quality Chinese singing voice corpus, which is recorded by 20 professional singers, covering 700 Chinese pop songs as well as all the four SATB types (i.e.,  soprano, alto, tenor, and bass); 2) we take extensive efforts to manually compose the musical scores for each recorded song, which are necessary to the study of the prosody modeling for SVS. 3) To facilitate the use and demonstrate the quality of M4Singer, we conduct four different benchmark experiments: score-based SVS, controllable singing voice (CSV), singing voice conversion (SVC) and automatic music transcription (AMT).",NIPS
"Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.",NIPS
"Although online handwriting verification has made great progress recently, the verification performances are still far behind the real usage owing to the small scale of the datasets as well as the limited biometric mediums. Therefore, this paper proposes a new handwriting verification benchmark dataset named Multimodal Signature and Digit String (MSDS), which consists of two subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings), contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to the best of our knowledge, is the largest publicly available Chinese signature dataset for handwriting verification, at least eight times larger than existing online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit Strings, i.e, the actual phone numbers of users, which have not been explored yet. Extensive experiments with different baselines are respectively conducted for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of state-of-the-art methods on MSDS-TDS are generally better than those on MSDS-ChS, which indicates that the handwritten Token Digit String could be a more effective biometric than handwritten Chinese signature. This is a promising discovery that could inspire us to explore new biometric traits. The MSDS dataset is available at https://github.com/HCIILAB/MSDS.",NIPS
"3D human pose and shape estimation (a.k.a. ``human mesh recovery'') has achieved substantial progress. Researchers mainly focus on the development of novel algorithms, while less attention has been paid to other critical factors involved. This could lead to less optimal baselines, hindering the fair and faithful evaluations of newly designed methodologies. To address this problem, this work presents the \textit{first} comprehensive benchmarking study from three under-explored perspectives beyond algorithms. \emph{1) Datasets.} An analysis on 31 datasets reveals the distinct impacts of data samples: datasets featuring critical attributes (\emph{i.e.} diverse poses, shapes, camera characteristics, backbone features) are more effective. Strategical selection and combination of high-quality datasets can yield a significant boost to the model performance. \emph{2) Backbones.} Experiments with 10 backbones, ranging from CNNs to transformers, show the knowledge learnt from a proximity task is readily transferable to human mesh recovery. \emph{3) Training strategies.} Proper augmentation techniques and loss designs are crucial. With the above findings, we achieve a PA-MPJPE of 47.3 (mm) on the 3DPW test set with a relatively simple model. More importantly, we provide strong baselines for fair comparisons of algorithms, and recommendations for building effective training configurations in the future. Codebase is available at \url{https://github.com/smplbody/hmr-benchmarks}.",NIPS
"Visual search is an essential part of almost any everyday human interaction with the visual environment. Nowadays, several algorithms are able to predict gaze positions during simple observation, but few models attempt to simulate human behavior during visual search in natural scenes. Furthermore, these models vary widely in their design and exhibit differences in the datasets and metrics with which they were evaluated. Thus, there is a need for a reference point, on which each model can be tested and from where potential improvements can be derived. In this study, we select publicly available state-of-the-art visual search models and datasets in natural scenes, and provide a common framework for their evaluation. To this end, we apply a unified format and criteria, bridging the gaps between them, and we estimate the modelsâ€™ efficiency and similarity with humans using a specific set of metrics. This integration has allowed us to enhance the Ideal Bayesian Searcher by combining it with a neural network-based visual search model, which enables it to generalize to other datasets. The present work sheds light on the limitations of current models and how integrating different approaches with a unified criteria can lead to better algorithms. Moreover, it moves forward on bringing forth a solution for the urgent need for benchmarking data and metrics to support the development of more general human visual search computational models. All of the code used here, including metrics, plots, and visual search models, alongside the preprocessed datasets, are available at $\url{https://github.com/FerminT/VisualSearchBenchmark}$.",NIPS
"The ability to estimate 3D human body pose and movement, also known as human pose estimation (HPE), enables many applications for home-based health monitoring, such as remote rehabilitation training. Several possible solutions have emerged using sensors ranging from RGB cameras, depth sensors, millimeter-Wave (mmWave) radars, and wearable inertial sensors. Despite previous efforts on datasets and benchmarks for HPE, few dataset exploits multiple modalities and focuses on home-based health monitoring. To bridge the gap, we present mRI, a multi-modal 3D human pose estimation dataset with mmWave, RGB-D, and Inertial Sensors. Our dataset consists of over 160k synchronized frames from 20 subjects performing rehabilitation exercises and supports the benchmarks of HPE and action detection. We perform extensive experiments using our dataset and delineate the strength of each modality. We hope that the release of mRI can catalyze the research in pose estimation, multi-modal learning, and action understanding, and more importantly facilitate the applications of home-based health monitoring.",NIPS
"This paper introduces ActionSense, a multimodal dataset and recording framework with an emphasis on wearable sensing in a kitchen environment.  It provides rich, synchronized data streams along with ground truth data to facilitate learning pipelines that could extract insights about how humans interact with the physical world during activities of daily living, and help lead to more capable and collaborative robot assistants.  The wearable sensing suite captures motion, force, and attention information; it includes eye tracking with a first-person camera, forearm muscle activity sensors, a body-tracking system using 17 inertial sensors, finger-tracking gloves, and custom tactile sensors on the hands that use a matrix of conductive threads.  This is coupled with activity labels and with externally-captured data from multiple RGB cameras, a depth camera, and microphones.  The specific tasks recorded in ActionSense are designed to highlight lower-level physical skills and higher-level scene reasoning or action planning.  They include simple object manipulations (e.g., stacking plates), dexterous actions (e.g., peeling or cutting vegetables), and complex action sequences (e.g., setting a table or loading a dishwasher).  The resulting dataset and underlying experiment framework are available at https://action-sense.csail.mit.edu. Preliminary networks and analyses explore modality subsets and cross-modal correlations.  ActionSense aims to support applications including learning from demonstrations, dexterous robot control, cross-modal predictions, and fine-grained action segmentation. It could also help inform the next generation of smart textiles that may one day unobtrusively send rich data streams to in-home collaborative or autonomous robot assistants.",NIPS
"It is crucial that image datasets for computer vision are representative and contain accurate demographic information to ensure their robustness and fairness, especially for smaller subpopulations. To address this issue, we present Dollar Street - a supervised dataset that contains 38,479 images of everyday household items from homes around the world. This dataset was manually curated and fully labeled, including tags for objects (e.g. â€œtoilet,â€ â€œtoothbrush,â€ â€œstoveâ€) and demographic data such as region, country and home monthly income. This dataset includes images from homes with no internet access and incomes as low as \$26.99 per month, visually capturing valuable socioeconomic diversity of traditionally under-represented populations. All images and data are licensed under CC-BY, permitting their use in academic and commercial work. Moreover, we show that this dataset can improve the performance of classification tasks for images of household items from lower income homes, addressing a critical need for datasets that combat bias.",NIPS
"Foundational Models (FMs) have demonstrated unprecedented capabilities including zero-shot learning, high fidelity data synthesis, and out of domain generalization. However, the parameter capacity of FMs is still limited, leading to poor out-of-the-box performance of FMs on many expert tasks (e.g. retrieval of car manuals technical illustrations from language queries), data for which is either unseen or belonging to a long-tail part of the data distribution of the huge datasets used for FM pre-training. This underlines the necessity to explicitly evaluate and finetune FMs on such expert tasks, arguably ones that appear the most in practical real-world applications. In this paper, we propose a first of its kind FETA benchmark built around the task of teaching FMs to understand technical documentation, via learning to match their graphical illustrations to corresponding language descriptions. Our FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. FETA is equipped with a procedure for completely automatic annotation extraction (code would be released upon acceptance), allowing easy extension of FETA to more documentation types and application domains in the future. Our automatic annotation leads to an automated performance metric shown to be consistent with metrics computed on human-curated annotations (also released). We provide multiple baselines and analysis of popular FMs on FETA leading to several interesting findings that we believe would be very valuable to the FM community, paving the way towards real-world application of FMs for many practical expert tasks currently being `overlooked' by standard benchmarks focusing on common objects.",NIPS
"Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning (ML), leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006+, a traffic dataset for network intrusion detection. This type of data meets the premise of shifting the input distribution: it covers a large time span (10 years), with naturally occurring changes over time (e.g. users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testing splits. We validate the performance degradation over time with diverse models, ranging from classical approaches to deep learning. Finally, we show that by acknowledging the distribution shift problem and properly addressing it, the performance can be improved compared to the classical training which assumes independent and identically distributed data (on average, by up to 3% for our approach). Dataset and code are available at https://github.com/bit-ml/AnoShift/.",NIPS
"The ability of a document classifier to handle inputs that are drawn from a distribution different from the training distribution is crucial for robust deployment and generalizability. The RVL-CDIP corpus is the de facto standard benchmark for document classification, yet to our knowledge all studies that use this corpus do not include evaluation on out-of-distribution documents. In this paper, we curate and release a new out-of-distribution benchmark for evaluating out-of-distribution performance for document classifiers. Our new out-of-distribution benchmark consists of two types of documents: those that are not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and those that are one of the 16 in-domain categories yet are drawn from a distribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N). While prior work on document classification for in-domain RVL-CDIP documents reports high accuracy scores, we find that these models exhibit accuracy drops of between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and further struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain RVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new resource for analyzing out-of-distribution performance on document classifiers.",NIPS
"Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the  WorldStratified dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate 10,000 sq km of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel. We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution. License-wise, the high-resolution Airbus imagery is CC-BY-NC, while the labels, Sentinel2 imagery, and trained weights are under CC-BY, and the source code under BSD, to allow for the widest use and dissemination. The dataset is available at \url{https://zenodo.org/record/6810792} and the software package at \url{https://github.com/worldstrat/worldstrat}.",NIPS
"Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. Clinical practitioners use all available data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between all relevant data over a treatment period. Existing datasets are limited in that they neither provide data nor consider the explicit relationship modeling between the data modalities. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitation. This is the first OCT and near-IR fundus dataset that includes clinical labels, biomarker labels, disease labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 near-IR fundus images each with at least 49 OCT scans, and 16 biomarkers, along with 4 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. We benchmark the utility of OLIVES dataset for ophthalmic data as well as provide benchmarks and concrete research directions for core and emerging machine learning paradigms within medical image analysis.",NIPS
"A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, there remains a major gap between humans and AI systems in terms of the sample efficiency with which they learn new visual reasoning tasks. Humans' remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality -- allowing them to efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluidic intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and generating image datasets corresponding to these rules at scale. Our proposed benchmark includes measures of sample efficiency, generalization, compositionality, and transfer across task rules. We systematically evaluate modern neural architectures and find that convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are much less data efficient than humans, even after learning informative visual representations using self-supervision. Overall, we hope our challenge will spur interest in developing neural architectures that can learn to harness compositionality for more efficient learning.",NIPS
"Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that do not show up in conventional monitoring systems---known as ``dark vessels''---is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of dark vessels day or night, under all-weather conditions. SAR images, however, require a domain-specific treatment and are not widely accessible to the ML community. Maritime objects (vessels and offshore infrastructure) are relatively small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We also provide an overview of the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data  (\href{https://iuu.xview.us/}{https://iuu.xview.us/}) and code (\href{https://github.com/DIUx-xView}{https://github.com/DIUx-xView}) to support ongoing development and evaluation of ML approaches for this important application.",NIPS
"To improve the sustainability and resilience of modern food systems, designing improved crop management strategies is crucial. The increasing abundance of data on agricultural systems suggests that future strategies could benefit from adapting to environmental conditions, but how to design these adaptive policies poses a new frontier. A natural technique for learning policies in these kinds of sequential decision-making problems is reinforcement learning (RL). To obtain the large number of samples required to learn effective RL policies, existing work has used mechanistic crop growth models (CGMs) as simulators. These solutions focus on single-year, single-crop simulations for learning strategies for a single agricultural management practice. However, to learn sustainable long-term policies we must be able to train in multi-year environments, with multiple crops, and consider a wider array of management techniques. We introduce CYCLESGYM, an RL environment based on the multi-year, multi-crop CGM Cycles. CYCLESGYM allows for long-term planning in agroecosystems, provides modular state space and reward constructors and weather generators, and allows for complex actions. For RL researchers, this is a novel benchmark to investigate issues arising in real-world applications. For agronomists, we demonstrate the potential of RL as a powerful optimization tool for agricultural systems management in multi-year case studies on nitrogen (N) fertilization and crop planning scenarios.",NIPS
"Echocardiography is one of the most commonly used diagnostic imaging modalities in cardiology. Application of deep learning models to echocardiograms can enable automated identification of cardiac structures, estimation of cardiac function, and prediction of clinical outcomes. However, a major hindrance to realizing the full potential of deep learning is the lack of large-scale, fully curated and annotated data sets required for supervised training. High-quality pre-trained representations that can transfer useful visual features of echocardiograms to downstream tasks can help adapt deep learning models to new setups using fewer examples. In this paper, we design a suite of benchmarks that can be used to pre-train and evaluate echocardiographic representations with respect to various clinically-relevant tasks using publicly accessible data sets. In addition, we develop a unified evaluation protocol---which we call the echocardiographic task adaptation benchmark (ETAB)---that measures how well a visual representation of echocardiograms generalizes to common downstream tasks of interest. We use our benchmarking framework to evaluate state-of-the-art vision modeling pipelines. We envision that our standardized, publicly accessible benchmarks would encourage future research and expedite progress in applying deep learning to high-impact problems in cardiovascular medicine.",NIPS
"We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databasesâ€”MIMIC-III and eICUâ€”and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the prediction confidence. We believe our dataset, EHRSQL, could serve as a practical benchmark to develop and assess QA models on structured EHR data and take one step further towards bridging the gap between text-to-SQL research and its real-life deployment in healthcare. EHRSQL is available at https://github.com/glee4810/EHRSQL.",NIPS
"Out-of-distribution (OOD) learning deals with scenarios in which training and test data follow different distributions. Although general OOD problems have been intensively studied in machine learning, graph OOD is only an emerging area of research. Currently, there lacks a systematic benchmark tailored to graph OOD method evaluation. In this work, we aim at developing an OOD benchmark, known as GOOD, for graphs specifically. We explicitly make distinctions between covariate and concept shifts and design data splits that accurately reflect different shifts. We consider both graph and node prediction tasks as there are key differences in designing shifts. Overall, GOOD contains 11 datasets with 17 domain selections. When combined with covariate, concept, and no shifts, we obtain 51 different splits. We provide performance results on 10 commonly used baseline methods with 10 random runs. This results in 510 dataset-model combinations in total. Our results show significant performance gaps between in-distribution and OOD settings. Our results also shed light on different performance trends between covariate and concept shifts by different methods. Our GOOD benchmark is a growing project and expects to expand in both quantity and variety of resources as the area develops. The GOOD benchmark can be accessed via https://github.com/divelab/GOOD/.",NIPS
"High-quality data is necessary for modern machine learning. However, the acquisition of such data is difficult due to noisy and ambiguous annotations of humans. The aggregation of such annotations to determine the label of an image leads to a lower data quality. We propose a data-centric image classification benchmark with nine real-world datasets and multiple annotations per image to allow researchers to investigate and quantify the impact of such data quality issues. With the benchmark we can study the impact of annotation costs and (semi-)supervised methods on the data quality for image classification by applying a novel methodology to a range of different algorithms and diverse datasets. Our benchmark uses a two-phase approach via a data label improvement method in the first phase and a fixed evaluation model in the second phase. Thereby, we give a measure for the relation between the input labeling effort and the performance of (semi-)supervised algorithms to enable a deeper insight into how labels should be created for effective model training. Across thousands of experiments, we show that one annotation is not enough and that the inclusion of multiple annotations allows for a better approximation of the real underlying class distribution. We identify that hard labels can not capture the ambiguity of the data and this might lead to the common issue of overconfident models. Based on the presented datasets, benchmarked methods, and analysis, we create multiple research opportunities for the future directed at the improvement of label noise estimation approaches, data annotation schemes, realistic (semi-)supervised learning, or more reliable image collection.",NIPS
"There are multiple scales of abstraction from which we can describe the same image, depending on whether we are focusing on fine-grained details or a more global attribute of the image. In brain mapping, learning to automatically parse images to build representations of both small-scale features (e.g., the presence of cells or blood vessels) and global properties of an image (e.g., which brain region the image comes from) is a crucial and open challenge. However, most existing datasets and benchmarks for neuroanatomy consider only a single downstream task at a time. To bridge this gap, we introduce a new dataset, annotations, and multiple downstream tasks that provide diverse ways to readout information about brain structure and architecture from the same image. Our multi-task neuroimaging benchmark (MTNeuro) is built on volumetric, micrometer-resolution X-ray microtomography images spanning a large thalamocortical section of mouse brain, encompassing multiple cortical and subcortical regions. We generated a number of different prediction challenges and evaluated several supervised and self-supervised models for brain-region prediction and pixel-level semantic segmentation of microstructures. Our experiments not only highlight the rich heterogeneity of this dataset, but also provide insights into how self-supervised approaches can be used to learn representations that capture multiple attributes of a single image and perform well on a variety of downstream tasks. Datasets, code, and pre-trained baseline models are provided at: https://mtneuro.github.io/.",NIPS
"Humans naturally use verbal utterances and nonverbal gestures to refer to various objects (known as $\textit{referring expressions}$) in different interactional scenarios. As collecting real human interaction datasets are costly and laborious, synthetic datasets are often used to train models to unambiguously detect relationships among objects. However, existing synthetic data generation tools that provide referring expressions generally neglect nonverbal gestures. Additionally, while a few small-scale datasets contain multimodal cues (verbal and nonverbal), these datasets only capture the nonverbal gestures from an exo-centric perspective (observer). As models can use complementary information from multimodal cues to recognize referring expressions, generating multimodal data from multiple views can help to develop robust models. To address these critical issues, in this paper, we present a novel embodied simulator, CAESAR, to generate multimodal referring expressions containing both verbal utterances and nonverbal cues captured from multiple views. Using our simulator, we have generated two large-scale embodied referring expression datasets, which we have released publicly. We have conducted experimental analyses on embodied spatial relation grounding using various state-of-the-art baseline models. Our experimental results suggest that visual perspective affects the models' performance; and that nonverbal cues improve spatial relation grounding accuracy. Finally, we will release the simulator publicly to allow researchers to generate new embodied interaction datasets.",NIPS
"The past few years have seen the development of many benchmarks for Neural Architecture Search (NAS), fueling rapid progress in NAS research. However, recent work, which shows that good hyperparameter settings can be more important than using the best architecture, calls for a shift in focus towards Joint Architecture and Hyperparameter Search (JAHS). Therefore, we present JAHS-Bench-201, the first collection of surrogate benchmarks for JAHS, built to also facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To the best of our knowledge, JAHS-Bench-201 is based on the most extensive dataset of neural network performance data in the public domain. It is composed of approximately 161 million data points and 20 performance metrics for three deep learning tasks, while featuring a 14-dimensional search and fidelity space that extends the popular NAS-Bench-201 space. With JAHS-Bench-201, we hope to democratize research on JAHS and lower the barrier to entry of an extremely compute intensive field, e.g., by reducing the compute time to run a JAHS algorithm from 5 days to only a few seconds.",NIPS
"Among United Nations' 17 Sustainable Development Goals (SDGs), we highlight SDG 8 on Decent Work and Economic Growth.  Specifically, we consider how to achieve subgoal 8.8, ""protect labour rights and promote safe working environments for all workers [...]"", in light of poor health, safety and environment (HSE) conditions being a widespread problem at workplaces. In EU alone, it is estimated that more than 4000 deaths occur each year due to poor working conditions. To handle the problem and achieve SDG 8, governmental agencies conduct labour inspections and it is therefore essential that these are carried out efficiently. Current research suggests that machine learning (ML) can be used to improve labour inspections, for instance by selecting organisations for inspections more effectively. However, the research in this area is very limited, in part due to a lack of publicly available data. Consequently, we introduce a new dataset called the Labour Inspection Checklists Dataset (LICD), which we have made publicly available. LICD consists of 63634 instances where each instance is an inspection conducted by the Norwegian Labour Inspection Authority. LICD has 577 features and labels. The dataset provides several ML research opportunities; we discuss two demonstration experiments. One experiment deals with the problem of selecting a relevant checklist for inspecting a given target organisation. The other experiment concerns the problem of predicting HSE violations, given a specific checklist and a target organisation. Our experimental results, while promising, suggest that achieving good ML classification performance is difficult for both problems. This motivates future research to improve ML performance, inspire other data analysis efforts, and ultimately achieve SDG 8.",NIPS
"Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.",NIPS
"Twitter bot detection has become an increasingly important task to combat misinformation, facilitate social media moderation, and preserve the integrity of the online discourse. State-of-the-art bot detection methods generally leverage the graph structure of the Twitter network, and they exhibit promising performance when confronting novel Twitter bots that traditional methods fail to detect. However, very few of the existing Twitter bot detection datasets are graph-based, and even these few graph-based datasets suffer from limited dataset scale, incomplete graph structure, as well as low annotation quality. In fact, the lack of a large-scale graph-based Twitter bot detection benchmark that addresses these issues has seriously hindered the development and evaluation of novel graph-based bot detection approaches. In this paper, we propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark that presents the largest dataset to date, provides diversified entities and relations on the Twitter network, and has considerably better annotation quality than existing datasets. In addition, we re-implement 35 representative Twitter bot detection baselines and evaluate them on 9 datasets, including TwiBot-22, to promote a fair comparison of model performance and a holistic understanding of research progress. To facilitate further research, we consolidate all implemented codes and datasets into the TwiBot-22 evaluation framework, where researchers could consistently evaluate new models and datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation framework are publicly available at \url{https://twibot22.github.io/}.",NIPS
"Despite impressive successes, deep reinforcement learning (RL) systems still fall short of human performance on generalization to new tasks and environments that differ from their training. As a benchmark tailored for studying RL generalization, we introduce Avalon, a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. This setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks. Avalon includes a highly efficient simulator, a library of baselines, and a benchmark with scoring metrics evaluated against hundreds of hours of human performance, all of which are open-source and publicly available. We find that standard RL baselines make progress on most tasks but are still far from human performance, suggesting Avalon is challenging enough to advance the quest for generalizable RL.",NIPS
"Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: $\texttt{PascalVOC-SP}$, $\texttt{COCO-SP}$, $\texttt{PCQM-Contact}$, $\texttt{Peptides-func}$ and $\texttt{Peptides-struct}$ that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP GNNs and Graph Transformer architectures that are intended to capture LRI.",NIPS
"Euclidean geometry is among the earliest forms of mathematical thinking. While the geometric primitives underlying its constructions, such as perfect lines and circles, do not often occur in the natural world, humans rarely struggle to perceive and reason with them. Will computer vision models trained on natural images show the same sensitivity to Euclidean geometry? Here we explore these questions by studying few-shot generalization in the universe of Euclidean geometry constructions. We introduce Geoclidean, a domain-specific language for Euclidean geometry, and use it to generate two datasets of geometric concept learning tasks for benchmarking generalization judgements of humans and machines. We find that humans are indeed sensitive to Euclidean geometry and generalize strongly from a few visual examples of a geometric concept. In contrast, low-level and high-level visual features from standard computer vision models pretrained on natural images do not support correct generalization. Thus Geoclidean represents a novel few-shot generalization benchmark for geometric concept learning, where the performance of humans and of AI models diverge. The Geoclidean framework and dataset are publicly available for download.",NIPS
"Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the evaluated domain adaptation methods are high compared to those of fully supervised baselines. This affirms the need for benchmarks such as CARLANE to further strengthen research in Unsupervised Domain Adaptation for lane detection. CARLANE, all evaluated models and the corresponding implementations are publicly available at https://carlanebenchmark.github.io.",NIPS
"There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the system's overall throughput. In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop and a modest workstation, to a high-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool achieves one million frames per second for the environment execution on Atari environments and three million frames per second on MuJoCo environments. When running EnvPool on a laptop, the speed is 2.8x that of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has great potential to become the de facto RL environment execution engine. Example runs show that it only takes five minutes to train agents to play Atari Pong and MuJoCo Ant on a laptop.  EnvPool is open-sourced at https://github.com/sail-sg/envpool.",NIPS
"Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning -- including BYOL, SwAV and MAE -- are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling -- a mechanism eschewed by these newer algorithms -- appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms may have to straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms.",NIPS
"Augmented Reality or AR filters on selfies have become very popular on social media platforms for a variety of applications, including marketing, entertainment and aesthetics. Given the wide adoption of AR face filters and the importance of faces in our social structures and relations, there is increased interest by the scientific community to analyze the impact of such filters from a psychological, artistic and sociological perspective. However, there are few quantitative analyses in this area mainly due to a lack of publicly available datasets of facial images with applied AR filters. The proprietary, close nature of most social media platforms does not allow users, scientists and practitioners to access the code and the details of the available AR face filters. Scraping faces from these platforms to collect data is ethically unacceptable and should, therefore, be avoided in research. In this paper, we present OpenFilter, a flexible framework to apply AR filters available in social media platforms on existing large collections of human faces. Moreover, we share FairBeauty and B-LFW, two beautified versions of the publicly available FairFace and LFW datasets and we outline insights derived from the analysis of these beautified datasets.",NIPS
"While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($\sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and neural networks. This leads to a series of challenges which should guide researchers aiming to build tabular-specific neural network: 1) be robust to uninformative features, 2) preserve the orientation of the data, and 3) be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20\,000 compute hours hyperparameter search for each learner.",NIPS
"With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC, https://clearinghouse.net), which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence ""extreme"" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further summarization research and to facilitate the development of applications to assist in the CRLC's mission at https://multilexsum.github.io.",NIPS
"Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_\text{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et al. More information can be referred to https://wukong-dataset.github.io/wukong-dataset/.",NIPS
"Facial analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Many existing algorithmic audits examine the performance of these systems on later stage elements of facial analysis systems like facial recognition and age, emotion, or perceived gender prediction; however, a core component to these systems has been vastly understudied from a fairness perspective: face detection, sometimes called face localization. Since face detection is a pre-requisite step in facial analysis systems, the bias we observe in face detection will flow downstream to the other components like facial recognition and emotion prediction. Additionally, no prior work has focused on the robustness of these systems under various perturbations and corruptions, which leaves open the question of how various people are impacted by these phenomena. We present the first of its kind detailed benchmark of face detection systems, specifically examining the robustness to noise of commercial and academic models. We use both standard and recently released academic facial datasets to quantitatively analyze trends in face detection robustness. Across all the datasets and systems, we generally find that photos of individuals who are masculine presenting, older, of darker skin type, or have dim lighting are more susceptible to errors than their counterparts in other identities.",NIPS
"Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22.grand-challenge.org.",NIPS
"Vision-Language Pre-training (VLP) has been shown to be an efficient method to improve the performance of models on different vision-and-language downstream tasks. Substantial studies have shown that neural networks may be able to learn some general rules about language and visual concepts from a large-scale weakly labeled image-text dataset. However, most of the public cross-modal datasets that contain more than 100M image-text pairs are in English; there is a lack of available large-scale and high-quality Chinese VLP datasets. In this work, we propose a new framework for automatic dataset acquisition and cleaning with which we construct a new large-scale and high-quality cross-modal dataset named as TaiSu, containing 166 million images and 219 million Chinese captions. Compared with the recently released Wukong dataset, our dataset is achieved with much stricter restrictions on the semantic correlation of image-text pairs. We also propose to combine texts collected from the web with texts generated by a pre-trained image-captioning model. To the best of our knowledge, TaiSu is currently the largest publicly accessible Chinese cross-modal dataset. Furthermore, we test our dataset on several vision-language downstream tasks. TaiSu outperforms BriVL by a large margin on the zero-shot image-text retrieval task and zero-shot image classification task. TaiSu also shows better performance than Wukong on the image-retrieval task without using image augmentation for training. Results demonstrate that TaiSu can serve as a promising VLP dataset, both for understanding and generative tasks. More information can be referred to https://github.com/ksOAn6g5/TaiSu.",NIPS
"Distribution shifts occur when the test distribution differs from the training distribution, and can considerably degrade performance of machine learning models deployed in the real world. While recent works have studied robustness to distribution shifts, distribution shifts arising from the passage of time have the additional structure of timestamp metadata. Real-world examples of such shifts are underexplored, and it is unclear whether existing models can leverage trends in past distribution shifts to reliably extrapolate into the future. To address this gap, we curate Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including drug discovery, patient prognosis, and news classification. On these datasets, we systematically benchmark 13 approaches with various inductive biases. We evaluate methods in domain-generalization, continual learning, self-supervised learning, and ensemble learning, which leverage timestamps to extract the common structure of the distribution shifts. We extend several domain-generalization methods to the temporal distribution shift setting by treating windows of time as different domains. Finally, we propose two evaluation strategies to evaluate model performance under temporal distribution shifts---evaluation with a fixed time split (Eval-Fix) and evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple evaluation protocol for the broader machine learning community, while Eval-Stream serves as a complementary benchmark for continual learning approaches. Our experiments demonstrate that existing methods are limited in tackling temporal distribution shift: across all settings, we observe an average performance drop of 20% from in-distribution to out-of-distribution data.",NIPS
"Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and repre- sentative of a wide range of problems. We introduce PDEBENCH, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBENCH comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems con- tribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more real- istic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of ini- tial and boundary conditions and PDE parameters; (3) more extensible source codes with user-friendly APIs for data generation and baseline results with popular machine learning models (FNO, U-Net, PINN, Gradient-Based Inverse Method). PDEBENCH allows researchers to extend the benchmark freely for their own pur- poses using a standardized API and to compare the performance of new models to existing baseline methods. We also propose new evaluation metrics with the aim to provide a more holistic understanding of learning methods in the context of Scientific ML. With those metrics we identify tasks which are challenging for recent ML methods and propose these tasks as future challenges for the community. The code is available at https://github.com/pdebench/PDEBench.",NIPS
"Physical simulations are at the core of many critical industrial systems. However, today's physical simulators  have some limitations such as computation time, dealing with missing or uncertain data, or even  non-convergence for some feasible cases. Recently, the use of data-driven approaches to learn complex physical simulations has been considered as a promising approach to address those issues. However, this comes often at the cost of some accuracy which may hinder the industrial use. To drive this new research topic towards a better real-world applicability, we propose a new benchmark suite ""Learning Industrial Physical Simulations""(LIPS) to meet the need of developing efficient, industrial application-oriented, augmented simulators. To define how to assess such benchmark performance, we propose a set of four generic categories of criteria. The proposed benchmark suite is a modular and configurable framework that can deal with different physical problems. To demonstrate this ability, we propose in this paper to investigate two distinct use-cases with different physical simulations, namely: the power grid and the pneumatic. For each use case, several benchmarks are described and assessed with existing models. None of the models perform well under all expected criteria, inviting the community to develop  new industry-applicable solutions and possibly showcase their performance publicly upon online LIPS instance on Codabench.",NIPS
"There are already some text-based visual question answering (TextVQA) benchmarks for developing machine's ability to answer questions based on texts in images in recent years. However, models developed on these benchmarks cannot work effectively in many real-life scenarios (e.g. traffic monitoring, shopping ads and e-learning videos) where temporal reasoning ability is required. To this end, we propose a new task named Video Text Visual Question Answering (ViteVQA in short) that aims at answering questions by reasoning texts and visual information spatiotemporally in a given video. In particular, on the one hand, we build the first ViteVQA benchmark dataset named M4-ViteVQA --- the abbreviation of Multi-category Multi-frame Multi-resolution Multi-modal benchmark for ViteVQA, which contains 7,620 video clips of 9 categories (i.e., shopping, traveling, driving, vlog, sport, advertisement, movie, game and talking) and 3 kinds of resolutions (i.e., 720p, 1080p and 1176x664), and 25,123 question-answer pairs. On the other hand, we develop a baseline method named T5-ViteVQA for the ViteVQA task. T5-ViteVQA consists of five transformers. It first extracts optical character recognition (OCR) tokens, question features, and video representations via two OCR transformers, one language transformer and one video-language transformer, respectively. Then, a multimodal fusion transformer and an answer generation module are applied to fuse multimodal information and generate the final prediction. Extensive experiments on M4-ViteVQA demonstrate the superiority of T5-ViteVQA to the existing approaches of TextVQA and VQA tasks. The ViteVQA benchmark is available in https://github.com/bytedance/VTVQA.",NIPS
"We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and  benchmark its performance against real-world audio measurements. In addition, we demonstrate two downstream tasks---embodied navigation and far-field automatic speech recognition---and highlight sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.",NIPS
"Offline reinforcement learning (RL) aims at learning effective policies from historical data without extra environment interactions. During our experience of applying offline RL, we noticed that previous offline RL benchmarks commonly involve significant reality gaps, which we have identified include rich and overly exploratory datasets, degraded baseline, and missing policy validation. In many real-world situations, to ensure system safety, running an overly exploratory policy to collect various data is prohibited, thus only a narrow data distribution is available. The resulting policy is regarded as effective if it is better than the working behavior policy; the policy model can be deployed only if it has been well validated, rather than accomplished the training. In this paper, we present a Near real-world offline RL benchmark, named NeoRL, to reflect these properties. NeoRL datasets are collected with a more conservative strategy. Moreover, NeoRL contains the offline training and offline validation pipeline before the online test, corresponding to real-world situations. We then evaluate recent state-of-the-art offline RL algorithms in NeoRL. The empirical results demonstrate that some offline RL algorithms are less competitive to the behavior cloning and the deterministic behavior policy, implying that they could be less effective in real-world tasks than in the previous benchmarks. We also disclose that current offline policy evaluation methods could hardly select the best policy. We hope this work will shed some light on future research and deploying RL in real-world systems.",NIPS
"Most existing neural architecture search (NAS) benchmarks and algorithms prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet. This makes the performance of NAS approaches in more diverse areas poorly understood. In this paper, we present NAS-Bench-360, a benchmark suite to evaluate methods on domains beyond those traditionally studied in architecture search, and use it to address the following question: do state-of-the-art NAS methods perform well on diverse tasks? To construct the benchmark, we curate ten tasks spanning a diverse array of application domains, dataset sizes, problem dimensionalities, and learning objectives. Each task is carefully chosen to interoperate with modern CNN-based search methods while possibly being far-afield from its original development domain. To speed up and reduce the cost of NAS research, for two of the tasks we release the precomputed performance of 15,625 architectures comprising a standard CNN search space. Experimentally, we show the need for more robust NAS evaluation of the kind NAS-Bench-360 enables by showing that several modern NAS procedures perform inconsistently across the ten tasks, with many catastrophically poor results. We also demonstrate how NAS-Bench-360 and its associated precomputed results will enable future scientific discoveries by testing whether several recent hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360 is hosted at https://nb360.ml.cmu.edu.",NIPS
"Full waveform inversion (FWI) is widely used in geophysics to reconstruct high-resolution velocity maps from seismic data. The recent success of data-driven FWI methods results in a rapidly increasing demand for open datasets to serve the geophysics community. We present OpenFWI, a collection of large-scale multi-structural benchmark datasets, to facilitate diversified, rigorous, and reproducible research on FWI. In particular, OpenFWI consists of $12$ datasets ($2.1$TB in total) synthesized from multiple sources. It encompasses diverse domains in geophysics (interface, fault, CO$_2$ reservoir, etc.), covers different geological subsurface structures (flat, curve, etc.), and contain various amounts of data samples (2K - 67K). It also includes a dataset for 3D FWI. Moreover, we use OpenFWI to perform benchmarking over four deep learning methods, covering both supervised and unsupervised learning regimes. Along with the benchmarks, we implement additional experiments, including physics-driven methods, complexity analysis, generalization study, uncertainty quantification, and so on, to sharpen our understanding of datasets and methods. The studies either provide valuable insights into the datasets and the performance, or uncover their current limitations. We hope OpenFWI supports prospective research on FWI and inspires future open-source efforts on AI for science. All datasets and related information can be accessed through our website at https://openfwi-lanl.github.io/",NIPS
"The COVID-19 pandemic continues to bring up various topics discussed or debated on social media. In order to explore the impact of pandemics on people's lives, it is crucial to understand the public's concerns and attitudes towards pandemic-related entities (e.g., drugs, vaccines) on social media. However, models trained on existing named entity recognition (NER) or targeted sentiment analysis (TSA) datasets have limited ability to understand COVID-19-related social media texts because these datasets are not designed or annotated from a medical perspective. In this paper, we release METS-CoV, a dataset containing medical entities and targeted sentiments from COVID-19 related tweets. METS-CoV contains 10,000 tweets with 7 types of entities, including 4 medical entity types (Disease, Drug, Symptom, and Vaccine) and 3 general entity types (Person, Location, and Organization). To further investigate tweet users' attitudes toward specific entities, 4 types of entities (Person, Organization, Drug, and Vaccine) are selected and annotated with user sentiments, resulting in a targeted sentiment dataset with 9,101 entities (in 5,278 tweets). To the best of our knowledge, METS-CoV is the first dataset to collect medical entities and corresponding sentiments of COVID-19 related tweets. We benchmark the performance of classical machine learning models and state-of-the-art deep learning models on NER and TSA tasks with extensive experiments. Results show that this dataset has vast room for improvement for both NER and TSA tasks. With rich annotations and comprehensive benchmark results, we believe METS-CoV is a fundamental resource for building better medical social media understanding tools and facilitating computational social science research, especially on epidemiological topics. Our data, annotation guidelines, benchmark models, and source code are publicly available (\url{https://github.com/YLab-Open/METS-CoV}) to ensure reproducibility.",NIPS
"Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that 2M background nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes.",NIPS
"Recording the dynamics of unscripted human interactions in the wild is challenging due to the delicate trade-offs between several factors: participant privacy, ecological validity, data fidelity, and logistical overheads. To address these, following a 'datasets for the community by the community' ethos, we propose the Conference Living Lab (ConfLab): a new concept for multimodal multisensor data collection of in-the-wild free-standing social conversations. For the first instantiation of ConfLab described here, we organized a real-life professional networking event at a major international conference. Involving 48 conference attendees, the dataset captures a diverse mix of status, acquaintance, and networking motivations. Our capture setup improves upon the data fidelity of prior in-the-wild datasets while retaining privacy sensitivity: 8 videos (1920x1080, 60 fps) from a non-invasive overhead view, and custom wearable sensors with onboard recording of body motion (full 9-axis IMU), privacy-preserving low-frequency audio (1250 Hz), and Bluetooth-based proximity. Additionally, we developed custom solutions for distributed hardware synchronization at acquisition, and time-efficient continuous annotation of body keypoints and actions at high sampling rates. Our benchmarks showcase some of the open research tasks related to in-the-wild privacy-preserving social data analysis: keypoints detection from overhead camera views, skeleton-based no-audio speaker detection, and F-formation detection.",NIPS
"Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoptions in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performances. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and an integral part of analytics, it is critical to systematically study and compare different APIs with each other and to characterize how individual APIs change over time. However, this practically important topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition, and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the APIâ€™s output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML  as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIsâ€™ performance changes substantially over timeâ€”several APIsâ€™ accuracies dropped on specific benchmark datasets. Even when the APIâ€™s aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIsâ€™ performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS.",NIPS
"In our continuously evolving world, entities change over time and new, previously non-existing or unknown, entities appear. We study how this evolutionary scenario impacts the performance on a well established entity linking (EL) task. For that study, we introduce TempEL, an entity linking dataset that consists of time-stratified English Wikipedia snapshots from 2013 to 2022, from which we collect both anchor mentions of entities, and these target entitiesâ€™ descriptions. By capturing such temporal aspects, our newly introduced TempEL resource contrasts with currently existing entity linking datasets, which are composed of fixed mentions linked to a single static version of a target Knowledge Base (e.g., Wikipedia 2010 for CoNLL-AIDA). Indeed, for each of our collected temporal snapshots, TempEL contains links to entities that are continual, i.e., occur in all of the years, as well as completely new entities that appear for the first time at some point. Thus, we enable to quantify the performance of current state-of-the-art EL models for: (i) entities that are subject to changes over time in their Knowledge Base descriptions as well as their mentionsâ€™ contexts, and (ii) newly created entities that were previously non-existing (e.g., at the time the EL model was trained). Our experimental results show that in terms of temporal performance degradation, (i) continual entities suffer a decrease of up to 3.1% EL accuracy, while (ii) for new entities this accuracy drop is up to 17.9%. This highlights the challenge of the introduced TempEL dataset and opens new research prospects in the area of time-evolving entity disambiguation.",NIPS
"Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate strong transferability to a variety of datasets/tasks. However, it remains challenging to evaluate the transferablity of these foundation models due to the lack of easy-to-use toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation of Language-augmented Visual Task-level Transfer), the first benchmark to compare and evaluate pre-trained language-augmented visual models. Several highlights include: (i) Datasets. As downstream evaluation suites, it consists of 20 image classification datasets and 35 object detection datasets, each of which is augmented with external knowledge. (ii) Toolkit. An automatic hyper-parameter tuning toolkit is developed to ensure the fairness in model adaption. To leverage the full power of language-augmented visual models, novel language-aware initialization methods are proposed to significantly improve the adaption performance. (iii) Metrics. A variety of evaluation metrics are used, including sample-efficiency (zero-shot and few-shot) and parameter-efficiency (linear probing and full model fine-tuning). We will publicly release ELEVATER.",NIPS
"Criminal justice is an increasingly important application domain for machine learning and algorithmic fairness, as predictive tools are becoming widely used in police, courts, and prison systems worldwide. A few relevant benchmarks have received significant attention, e.g., the COMPAS dataset, often without proper consideration of the domain context. To raise awareness of publicly available criminal justice datasets and encourage their responsible use, we conduct a survey, consider contexts, highlight potential uses, and identify gaps and limitations. We provide datasheets for 15 datasets and upload them to a public repository. We compare the datasets across several dimensions, including size, coverage of the population, and potential use, highlighting concerns. We hope that this work can provide a useful starting point for researchers looking for appropriate datasets related to criminal justice, and that the repository will continue to grow as a community effort.",NIPS
"We present Myriad, a testbed written in JAX which enables machine learning researchers to benchmark imitation learning and reinforcement learning algorithms against trajectory optimization-based methods in real-world environments. Myriad contains 17 optimal control problems presented in continuous time which span medicine, ecology, epidemiology, and engineering. As such, Myriad strives to serve as a stepping stone towards application of modern machine learning techniques for impactful real-world tasks. The repository also provides machine learning practitioners access to trajectory optimization techniques, not only for standalone use, but also for integration within a typical automatic differentiation workflow. Indeed, the combination of classical control theory and deep learning in a fully GPU-compatible package unlocks potential for new algorithms to arise. We present one such novel approach for use in dynamics learning and control tasks. Trained in a fully end-to-end fashion, our model leverages an implicit planning module over neural ordinary differential equations, enabling simultaneous learning and planning with unknown environment dynamics. All environments, optimizers and tools are available in the software package at \url{https://github.com/nikihowe/myriad}.",NIPS
"Named Entity Recognition and Disambiguation (NERD) systems are foundational for information retrieval, question answering, event detection, and other natural language processing (NLP) applications. We introduce TweetNERD, a dataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on Tweets. This is the largest and most temporally diverse open sourced dataset benchmark for NERD on Tweets and can be used to facilitate research in this area. We describe evaluation setup with TweetNERD for three NERD tasks: Named Entity Recognition (NER), Entity Linking with True Spans (EL), and End to End Entity Linking (End2End); and provide performance of existing publicly available methods on specific TweetNERD splits. TweetNERD is available at: https://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0 International (CC BY 4.0) license. Check out more details at https://github.com/twitter-research/TweetNERD.",NIPS
"Weak supervision (WS) is a powerful method to build labeled datasets for training supervised models in the face of little-to-no labeled data. It replaces hand-labeling data with aggregating multiple noisy-but-cheap label estimates expressed by labeling functions (LFs). While it has been used successfully in many domains, weak supervision's application scope is limited by the difficulty of constructing labeling functions for domains with complex or high-dimensional features. To address this, a handful of methods have proposed automating the LF design process using a small set of ground truth labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating automated WS (AutoWS) techniques in challenging WS settings---a set of diverse application domains on which it has been previously difficult or impossible to apply traditional WS techniques. While AutoWS is a promising direction toward expanding the application-scope of WS, the emergence of powerful methods such as zero-shot foundation models reveal the need to understand how AutoWS techniques compare or cooperate with modern zero-shot or few-shot learners. This informs the central question of AutoWS-Bench-101: given an initial set of 100 labels for each task, we ask whether a practitioner should use an AutoWS method to generate additional labels or use some simpler baseline, such as zero-shot predictions from a foundation model or supervised learning. We observe that it is necessary for AutoWS methods to incorporate signal from foundation models if they are to outperform simple few-shot baselines, and AutoWS-Bench-101 promotes future research in this direction. We conclude with a thorough ablation study of AutoWS methods.",NIPS
"As shown by recent studies, machine intelligence-enabled systems are vulnerable to test cases resulting from either adversarial manipulation or natural distribution shifts. This has raised great concerns about deploying machine learning algorithms for real-world applications, especially in safety-critical domains such as autonomous driving (AD). On the other hand, traditional AD testing on naturalistic scenarios requires hundreds of millions of driving miles due to the high dimensionality and rareness of the safety-critical scenarios in the real world. As a result, several approaches for autonomous driving evaluation have been explored, which are usually, however, based on different simulation platforms, types of safety-critical scenarios, scenario generation algorithms, and driving route variations. Thus, despite a large amount of effort in autonomous driving testing, it is still challenging to compare and understand the effectiveness and efficiency of different testing scenario generation algorithms and testing mechanisms under similar conditions. In this paper, we aim to provide the first unified platform SafeBench to integrate different types of safety-critical testing scenarios, scenario generation algorithms, and other variations such as driving routes and environments. In particular, we consider 8 safety-critical testing scenarios following National Highway Traffic Safety Administration (NHTSA) and develop 4 scenario generation algorithms considering 10 variations for each scenario. Meanwhile, we implement 4 deep reinforcement learning-based AD algorithms with 4 types of input (e.g., birdâ€™s-eye view, camera) to perform fair comparisons on SafeBench. We find our generated testing scenarios are indeed more challenging and observe the trade-off between the performance of AD agents under benign and safety-critical testing scenarios. We believe our unified platform SafeBench for large-scale and effective autonomous driving testing will motivate the development of new testing scenario generation and safe AD algorithms. SafeBench is available at https://safebench.github.io.",NIPS
"The availability of compute and data to train larger and larger language models increases the demand for robust methods of benchmarking the true progress of LM training. Recent years witnessed significant progress in standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or KILT have become a de facto standard tools to compare large language models. Following the trend to replicate GLUE for other languages, the KLEJ benchmark\ (klej is the word for glue in Polish) has been released for Polish. In this paper, we evaluate the progress in benchmarking for low-resourced languages. We note that only a handful of languages have such comprehensive benchmarks. We also note the gap in the number of tasks being evaluated by benchmarks for resource-rich English/Chinese and the rest of the world.In this paper, we introduce LEPISZCZE (lepiszcze is the Polish word for glew, the Middle English predecessor of glue), a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark.We design LEPISZCZE with flexibility in mind. Including new models, datasets, and tasks is as simple as possible while still offering data versioning and model tracking. In the first run of the benchmark, we test 13 experiments (task and dataset pairs) based on the five most recent LMs for Polish. We use five datasets from the Polish benchmark and add eight novel datasets. As the paper's main contribution, apart from LEPISZCZE, we provide insights and experiences learned while creating the benchmark for Polish as the blueprint to design similar benchmarks for other low-resourced languages.",NIPS
"Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predefined triggers. As various attack and defense models have been proposed, it is of great significance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires specific evaluation protocols; (2) The evaluation metrics only consider whether the attacks could flip the models' predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and fine-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations.",NIPS
"Wasserstein Generative Adversarial Networks (WGANs) are the popular generative models built on the theory of Optimal Transport (OT) and the Kantorovich duality. Despite the success of WGANs, it is still unclear how well the underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance, W1) and the OT gradient needed to update the generator. In this paper, we address these questions. We construct 1-Lipschitz functions and use them to build ray monotone transport plans. This strategy yields pairs of continuous benchmark distributions with the analytically known OT plan, OT cost and OT gradient in high-dimensional spaces such as spaces of images. We thoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral normalization, entropic regularization, etc.) using these benchmark pairs. Even though these solvers perform well in WGANs, none of them faithfully compute W1 in high dimensions. Nevertheless, many provide a meaningful approximation of the OT gradient. These observations suggest that these solvers should not be treated as good estimators of W1 but to some extent they indeed can be used in variational problems requiring the minimization of W1.",NIPS
"Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of digital twins. Among different hand morphable models, MANO has been widely used in vision and graphics community. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic hand data. In this paper, we extend MANO with Diverse Accessories and Rich Textures, namely DART. DART is composed of 50 daily 3D accessories which varies in appearance and shape, and 325 hand-crafted 2D texture maps covers different kinds of blemishes or make-ups. Unity GUI is also provided to generate synthetic hand data with user-defined settings, e.g., pose, camera, background, lighting, textures, and accessories. Finally, we release DARTset, which contains large-scale (800K), high-fidelity synthetic hand images, paired with perfect-aligned 3D labels. Experiments demonstrate its superiority in diversity. As a complement to existing hand datasets, DARTset boosts the generalization in both hand pose estimation and mesh recovery tasks. Raw ingredients (textures, accessories), Unity GUI, source code and DARTset are publicly available at dart2022.github.io.",NIPS
"In stereo vision, self-similar or bland regions can make it difficult to match patches between two images. Active stereo-based methods mitigate this problem by projecting a pseudo-random pattern on the scene so that each patch of an image pair can be identified without ambiguity. However, the projected pattern significantly alters the appearance of the image. If this pattern acts as a form of adversarial noise, it could negatively impact the performance of deep learning-based methods, which are now the de-facto standard for dense stereo vision. In this paper, we propose the Active-Passive SimStereo dataset and a corresponding benchmark to evaluate the performance gap between passive and active stereo images for stereo matching algorithms. Using the proposed benchmark and an additional ablation study, we show that the feature extraction and matching modules of a selection of twenty selected deep learning-based stereo matching methods generalize to active stereo without a problem. However, the disparity refinement modules of three of the twenty architectures (ACVNet, CascadeStereo, and StereoNet) are negatively affected by the active stereo patterns due to their reliance on the appearance of the input images.",NIPS
"Continual learning on graph data, which aims to accommodate new tasks over newly emerged graph data while maintaining the model performance over existing tasks, is attracting increasing attention from the community. Unlike continual learning on Euclidean data ($\textit{e.g.}$, images, texts, etc.) that has established benchmarks and unified experimental settings, benchmark tasks are rare for Continual Graph Learning (CGL). Moreover, due to the variety of graph data and its complex topological structures, existing works adopt different protocols to configure datasets and experimental settings. This creates a great obstacle to compare different techniques and thus hinders the development of CGL. To this end, we systematically study the task configurations in different application scenarios and develop a comprehensive Continual Graph Learning Benchmark (CGLB) curated from different public datasets. Specifically, CGLB contains both node-level and graph-level continual graph learning tasks under task-incremental (currently widely adopted) and class-incremental (more practical, challenging, yet underexplored) settings, as well as a toolkit for training, evaluating, and visualizing different CGL methods. Within CGLB, we also systematically explain the difference among these task configurations by comparing them to classical continual learning settings. Finally, we comprehensively compare state-of-the-art baselines on CGLB to investigate their effectiveness. Given CGLB and the developed toolkit, the barrier to exploring CGL has been greatly lowered and researchers can focus more on the model development without worrying about tedious work on pre-processing of datasets or encountering unseen pitfalls. The benchmark and the toolkit are available through https://github.com/QueuQ/CGLB.",NIPS
"Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 57 benchmark datasets, named ADBench. Our extensive experiments (98,436 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can easily conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.",NIPS
"Keyphrases  are an important tool for efficiently dealing with the ever-increasing amount of information present on the internet. While there are many recent papers on English keyphrase generation, keyphrase generation for other languages remains vastly understudied, mostly due to the absence of datasets. To address this, we present a novel dataset called Papyrus, composed of 16427 pairs of abstracts and keyphrases. We release four versions of this dataset, corresponding to different subtasks. Papyrus-e considers only English keyphrases, Papyrus-f considers French keyphrases, Papyrus-m considers keyphrase generation in any language (mostly French and English), and Papyrus-a considers keyphrase generation in several languages. We train a state-of-the-art model on all four tasks and show that they lead to better results for non-English languages, with an average improvement of 14.2\% on keyphrase extraction and 2.0\% on generation. We also show an improvement of 0.4\% on extraction and 0.7\% on generation over English state-of-the-art results by concatenating Papyrus-e with the Kp20K training set.",NIPS
"Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",NIPS
"There has been a rapidly growing interest in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems in the machine learning research literature, aiming to assist doctors in telemedicine services. These systems are designed to interact with patients, collect evidence about their symptoms and relevant antecedents, and possibly make predictions about the underlying diseases. Doctors would review the interactions, including the evidence and the predictions, collect if necessary additional information from patients, before deciding on next steps. Despite recent progress in this area, an important piece of doctors' interactions with patients is missing in the design of these systems, namely the differential diagnosis. Its absence is largely due to the lack of datasets that include such information for models to train on. In this work, we present a large-scale synthetic dataset of roughly 1.3 million patients that includes a differential diagnosis, along with the ground truth pathology, symptoms and antecedents for each patient. Unlike existing datasets which only contain binary symptoms and antecedents, this dataset also contains categorical and multi-choice symptoms and antecedents useful for efficient data collection. Moreover, some symptoms are organized in a hierarchy, making it possible to design systems able to interact with patients in a logical way. As a proof-of-concept, we extend two existing AD and ASD systems to incorporate the differential diagnosis, and provide empirical evidence that using differentials as training signals is essential for the efficiency of such systems or for helping doctors better understand the reasoning of those systems.",NIPS
"Video-quality measurement is a critical task in video processing. Nowadays, many implementations of new encoding standards - such as AV1, VVC, and LCEVC - use deep-learning-based decoding algorithms with perceptual metrics that serve as optimization objectives. But investigations of the performance of modern video- and image-quality metrics commonly employ videos compressed using older standards, such as AVC. In this paper, we present a new benchmark for video-quality metrics that evaluates video compression. It is based on a new dataset consisting of about 2,500 streams encoded using different standards, including AVC, HEVC, AV1, VP9, and VVC.  Subjective scores were collected using crowdsourced pairwise comparisons. The list of evaluated metrics includes recent ones based on machine learning and neural networks. The results demonstrate that new no-reference metrics exhibit high correlation with subjective quality and approach the capability of top full-reference metrics.",NIPS
"Achieving human-level dexterity is an important open problem in robotics. However, tasks of dexterous hand manipulation even at the baby level are challenging to solve through reinforcement learning (RL). The difficulty lies in the high degrees of freedom and the required cooperation among heterogeneous agents (e.g., joints of fingers). In this study, we propose the Bimanual Dexterous Hands Benchmark (Bi-DexHands), a simulator that involves two dexterous hands with tens of bimanual manipulation tasks and thousands of target objects. Tasks in Bi-DexHands are first designed to match human-level motor skills according to literature in cognitive science, and then are built in Issac Gym; this enables highly efficient RL trainings, reaching 30,000+ FPS by only one single NVIDIA RTX 3090. We provide a comprehensive benchmark for popular RL algorithms under different settings; this includes multi-agent RL, offline RL, multi-task RL, and meta RL. Our results show that PPO type on-policy algorithms can learn to solve simple manipulation tasks that are equivalent up to 48-month human baby (e.g., catching a flying object, opening a bottle), while multi-agent RL can further help to learn manipulations that require skilled bimanual cooperation (e.g., lifting a pot, stacking blocks). Despite the success on each individual task, when it comes to mastering multiple manipulation skills, existing RL algorithms fail to work in most of the multi-task and the few-shot learning tasks, which calls for more future development from the RL community. Our project is open-sourced at https://github.com/PKU-MARL/DexterousHands.",NIPS
"Combining information from multiple views is essential for discriminating similar objects. However, existing datasets for multi-view object classification have several limitations, such as synthetic and coarse-grained objects, no validation split for hyperparameter tuning, and a lack of view-level information quantity annotations for analyzing multi-view-based methods. To address this issue, this study proposes a new dataset, MVP-N, which contains 44 retail products, 16k real captured views with human-perceived information quantity annotations, and 9k multi-view sets. The fine-grained categorization of objects naturally generates multi-view label noise owing to the inter-class view similarity, allowing the study of learning from noisy labels in the multi-view case. Moreover, this study benchmarks four multi-view-based feature aggregation methods and twelve soft label methods on MVP-N. Experimental results show that MVP-N will be a valuable resource for facilitating the development of real-world multi-view object classification methods. The dataset and code are publicly available at https://github.com/SMNUResearch/MVP-N.",NIPS
"Personalized Federated Learning (pFL), which utilizes and deploys distinct local models, has gained increasing attention in recent years due to its success in handling the statistical heterogeneity of FL clients. However, standardized evaluation and systematical analysis of diverse pFL methods remain a challenge. Firstly, the highly varied datasets, FL simulation settings and pFL implementations prevent easy and fair comparisons of pFL methods. Secondly, the current pFL literature diverges in the adopted evaluation and ablation protocols. Finally, the effectiveness and robustness of pFL methods are under-explored in various practical scenarios, such as the generalization to new clients and the participation of resource-limited clients. To tackle these challenges, we propose the first comprehensive pFL benchmark, pFL-Bench, for facilitating rapid, reproducible, standardized and thorough pFL evaluation. The proposed benchmark contains more than 10 dataset variants in various application domains with a unified data partition and realistic heterogeneous settings; a modularized and easy-to-extend pFL codebase with more than 20 competitive pFL method implementations; and systematic evaluations under containerized environments in terms of generalization, fairness, system overhead, and convergence. We highlight the benefits and potential of state-of-the-art pFL methods and hope pFL-Bench enables further pFL research and broad applications that would otherwise be difficult owing to the absence of a dedicated benchmark. The code is released at https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench.",NIPS
"Recent breakthroughs in the development of agents to solve challenging sequential decision making problems such as Go, StarCraft, or DOTA, have relied on both simulated environments and large-scale datasets.  However, progress on this research has been hindered by the scarcity of open-sourced datasets and the prohibitive computational cost to work with them.  Here we present the NetHack Learning Dataset (NLD), a large and highly-scalable dataset of trajectories from the popular game of NetHack, which is both extremely challenging for current methods and very fast to run. NLD consists of three parts: 10 billion state transitions from 1.5 million human trajectories collected on the NAO public NetHack server from 2009 to 2020; 3 billion state-action-score transitions from 100,000 trajectories collected from the symbolic bot winner of the NetHack Challenge 2021; and, accompanying code for users to record, load and stream any collection of such trajectories in a highly compressed form.  We evaluate a wide range of existing algorithms for learning from demonstrations, showing that significant research advances are needed to fully leverage large-scale datasets for challenging sequential decision making tasks.",NIPS
"While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to readily compare several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. While the first release of OpenXAI supports only tabular datasets, the explanation methods and metrics that we consider are general enough to be applicable to other data modalities. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/. OpenXAI will be regularly updated to incorporate text and image datasets, other new metrics and explanation methods, and welcomes inputs from the community.",NIPS
"This paper introduces Honor of Kings Arena, a reinforcement learning (RL) environment based on the Honor of Kings, one of the worldâ€™s most popular games at present. Compared to other environments studied in most previous work, ours presents new generalization challenges for competitive reinforcement learning. It is a multi-agent problem with one agent competing against its opponent; and it requires the generalization ability as it has diverse targets to control and diverse opponents to compete with. We describe the observation, action, and reward specifications for the Honor of Kings domain and provide an open-source Python-based interface for communicating with the game engine. We provide twenty target heroes with a variety of tasks in Honor of Kings Arena and present initial baseline results for RL-based methods with feasible computing resources.  Finally, we showcase the generalization challenges imposed by Honor of Kings Arena and possible remedies to the challenges. All of the software, including the environment-class, are publicly available.",NIPS
"Post-processing ensemble prediction systems can improve the reliability of weather forecasting, especially for extreme event prediction. In recent years, different machine learning models have been developed to improve the quality of weather post-processing. However, these models require a comprehensive dataset of weather simulations to produce high-accuracy results, which comes at a high computational cost to generate. This paper introduces the ENS-10 dataset, consisting of ten ensemble members spanning 20 years (1998--2017). The ensemble members are generated by perturbing numerical weather simulations to capture the chaotic behavior of the Earth. To represent the three-dimensional state of the atmosphere, ENS-10 provides the most relevant atmospheric variables at 11 distinct pressure levels and the surface at \ang{0.5} resolution for forecast lead times T=0, 24, and 48 hours (two data points per week). We propose the ENS-10 prediction correction task for improving the forecast quality at a 48-hour lead time through ensemble post-processing. We provide a set of baselines and compare their skill at correcting the predictions of three important atmospheric variables. Moreover, we measure the baselines' skill at improving predictions of extreme weather events using our dataset. The ENS-10 dataset is available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.",NIPS
"Surrogate models are necessary to optimize meaningful quantities in physical dynamics as their recursive numerical resolutions are often prohibitively expensive. It is mainly the case for fluid dynamics and the resolution of Navierâ€“Stokes equations. However, despite the fast-growing field of data-driven models for physical systems, reference datasets representing real-world phenomena are lacking. In this work, we develop \textsc{AirfRANS}, a dataset for studying the two-dimensional incompressible steady-state Reynolds-Averaged Navierâ€“Stokes equations over airfoils at a subsonic regime and for different angles of attacks. We also introduce metrics on the stress forces at the surface of geometries and visualization of boundary layers to assess the capabilities of models to accurately predict the meaningful information of the problem. Finally, we propose deep learning baselines on four machine learning tasks to study \textsc{AirfRANS} under different constraints for generalization considerations: big and scarce data regime, Reynolds number, and angle of attack extrapolation.",NIPS
"We introduce VISOR, a new dataset of pixel annotations and a benchmark suite for segmenting hands and active objects in egocentric video. VISOR annotates videos from EPIC-KITCHENS, which comes with a new set of challenges not encountered in current video segmentation datasets. Specifically, we need to ensure both short- and long-term consistency of pixel-level annotations as objects undergo transformative interactions, e.g. an onion is peeled, diced and cooked - where we aim to obtain accurate pixel-level annotations of the peel, onion pieces, chopping board, knife, pan, as well as the acting hands. VISOR introduces an annotation pipeline, AI-powered in parts, for scalability and quality. In total, we publicly release 272K manual semantic masks of 257 object classes, 9.9M interpolated dense masks, 67K hand-object relations, covering 36 hours of 179 untrimmed videos. Along with the annotations, we introduce three challenges in video object segmentation, interaction understanding and long-term reasoning.For data, code and leaderboards: http://epic-kitchens.github.io/VISOR",NIPS
"Social media platforms were conceived to act as online town squares' where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them intomosh pits' where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users.  However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded.To facilitate and encourage research in this important direction, we contribute for the first time MACD - a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49\% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform - ShareChat. We also release AbuseXLMR, an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id's to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as a dataset benchmark for future research.",NIPS
"Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dmcontrol physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dmcontrol and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt.Videos of the results and links to the code and dataset are available at https://microsoft.github.io/MoCapAct.",NIPS
"Large multimodal models demonstrate remarkable generalist ability to perform
diverse multimodal tasks in a zero-shot manner. Large-scale web-based
image-text pairs contribute fundamentally to this success, but suffer from
excessive noise. Recent studies use alternative captions synthesized by
captioning models and have achieved notable benchmark performance. However, our
experiments reveal significant Scalability Deficiency and World Knowledge Loss
issues in models trained with synthetic captions, which have been largely
obscured by their initial benchmark success. Upon closer examination, we
identify the root cause as the overly-simplified language structure and lack of
knowledge details in existing synthetic captions. To provide higher-quality and
more scalable multimodal pretraining data, we propose CapsFusion, an advanced
framework that leverages large language models to consolidate and refine
information from both web-based image-text pairs and synthetic captions.
Extensive experiments show that CapsFusion captions exhibit remarkable
all-round superiority over existing captions in terms of model performance
(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample
efficiency (requiring 11-16 times less computation than baselines), world
knowledge depth, and scalability. These effectiveness, efficiency and
scalability advantages position CapsFusion as a promising candidate for future
scaling of LMM training.",CVPR
"Deep learning methods can not only detect false data injection attacks (FDIA)
but also locate attacks of FDIA. Although adversarial false data injection
attacks (AFDIA) based on deep learning vulnerabilities have been studied in the
field of single-label FDIA detection, the adversarial attack and defense
against multi-label FDIA locational detection are still not involved. To bridge
this gap, this paper first explores the multi-label adversarial example attacks
against multi-label FDIA locational detectors and proposes a general
multi-label adversarial attack framework, namely muLti-labEl adverSarial falSe
data injectiON attack (LESSON). The proposed LESSON attack framework includes
three key designs, namely Perturbing State Variables, Tailored Loss Function
Design, and Change of Variables, which can help find suitable multi-label
adversarial perturbations within the physical constraints to circumvent both
Bad Data Detection (BDD) and Neural Attack Location (NAL). Four typical LESSON
attacks based on the proposed framework and two dimensions of attack objectives
are examined, and the experimental results demonstrate the effectiveness of the
proposed attack framework, posing serious and pressing security concerns in
smart grids.",CVPR
"False negatives (FN) in 3D object detection, {\em e.g.}, missing predictions
of pedestrians, vehicles, or other obstacles, can lead to potentially dangerous
situations in autonomous driving. While being fatal, this issue is understudied
in many current 3D detection methods. In this work, we propose Hard Instance
Probing (HIP), a general pipeline that identifies \textit{FN} in a multi-stage
manner and guides the models to focus on excavating difficult instances. For 3D
object detection, we instantiate this method as FocalFormer3D, a simple yet
effective detector that excels at excavating difficult objects and improving
prediction recall. FocalFormer3D features a multi-stage query generation to
discover hard objects and a box-level transformer decoder to efficiently
distinguish objects from massive object candidates. Experimental results on the
nuScenes and Waymo datasets validate the superior performance of FocalFormer3D.
The advantage leads to strong performance on both detection and tracking, in
both LiDAR and multi-modal settings. Notably, FocalFormer3D achieves a 70.5 mAP
and 73.9 NDS on nuScenes detection benchmark, while the nuScenes tracking
benchmark shows 72.1 AMOTA, both ranking 1st place on the nuScenes LiDAR
leaderboard. Our code is available at
\url{https://github.com/NVlabs/FocalFormer3D}.",CVPR
"While deep learning has led to huge progress in complex image classification
tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call
into question how reliably these classifiers work in the wild. Furthermore, for
safety-critical tasks the black-box nature of their decisions is problematic,
and explanations or at least methods which make decisions plausible are needed
urgently. In this paper, we address these problems by generating images that
optimize a classifier-derived objective using a framework for guided image
generation. We analyze the decisions of image classifiers by visual
counterfactual explanations (VCEs), detection of systematic mistakes by
analyzing images where classifiers maximally disagree, and visualization of
neurons and spurious features. In this way, we validate existing observations,
e.g. the shape bias of adversarially robust models, as well as novel failure
modes, e.g. systematic errors of zero-shot CLIP classifiers. Moreover, our VCEs
outperform previous work while being more versatile.",CVPR
"Reward finetuning has emerged as a promising approach to aligning foundation
models with downstream objectives. Remarkable success has been achieved in the
language domain by using reinforcement learning (RL) to maximize rewards that
reflect human preference. However, in the vision domain, existing RL-based
reward finetuning methods are limited by their instability in large-scale
training, rendering them incapable of generalizing to complex, unseen prompts.
In this paper, we propose Proximal Reward Difference Prediction (PRDP),
enabling stable black-box reward finetuning for diffusion models for the first
time on large-scale prompt datasets with over 100K prompts. Our key innovation
is the Reward Difference Prediction (RDP) objective that has the same optimal
solution as the RL objective while enjoying better training stability.
Specifically, the RDP objective is a supervised regression objective that tasks
the diffusion model with predicting the reward difference of generated image
pairs from their denoising trajectories. We theoretically prove that the
diffusion model that obtains perfect reward difference prediction is exactly
the maximizer of the RL objective. We further develop an online algorithm with
proximal updates to stably optimize the RDP objective. In experiments, we
demonstrate that PRDP can match the reward maximization ability of
well-established RL-based methods in small-scale training. Furthermore, through
large-scale training on text prompts from the Human Preference Dataset v2 and
the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a
diverse set of complex, unseen prompts whereas RL-based methods completely
fail.",CVPR
"Introducing interpretability and reasoning into Multiple Instance Learning
(MIL) methods for Whole Slide Image (WSI) analysis is challenging, given the
complexity of gigapixel slides. Traditionally, MIL interpretability is limited
to identifying salient regions deemed pertinent for downstream tasks, offering
little insight to the end-user (pathologist) regarding the rationale behind
these selections. To address this, we propose Self-Interpretable MIL (SI-MIL),
a method intrinsically designed for interpretability from the very outset.
SI-MIL employs a deep MIL framework to guide an interpretable branch grounded
on handcrafted pathological features, facilitating linear predictions. Beyond
identifying salient regions, SI-MIL uniquely provides feature-level
interpretations rooted in pathological insights for WSIs. Notably, SI-MIL, with
its linear prediction constraints, challenges the prevalent myth of an
inevitable trade-off between model interpretability and performance,
demonstrating competitive results compared to state-of-the-art methods on
WSI-level prediction tasks across three cancer types. In addition, we
thoroughly benchmark the local and global-interpretability of SI-MIL in terms
of statistical analysis, a domain expert study, and desiderata of
interpretability, namely, user-friendliness and faithfulness.",CVPR
"Recent advances in decentralized deep learning algorithms have demonstrated
cutting-edge performance on various tasks with large pre-trained models.
However, a pivotal prerequisite for achieving this level of competitiveness is
the significant communication and computation overheads when updating these
models, which prohibits the applications of them to real-world scenarios. To
address this issue, drawing inspiration from advanced model merging techniques
without requiring additional training, we introduce the Decentralized Iterative
Merging-And-Training (DIMAT) paradigm--a novel decentralized deep learning
framework. Within DIMAT, each agent is trained on their local data and
periodically merged with their neighboring agents using advanced model merging
techniques like activation matching until convergence is achieved. DIMAT
provably converges with the best available rate for nonconvex functions with
various first-order methods, while yielding tighter error bounds compared to
the popular existing approaches. We conduct a comprehensive empirical analysis
to validate DIMAT's superiority over baselines across diverse computer vision
tasks sourced from multiple datasets. Empirical results validate our
theoretical claims by showing that DIMAT attains faster and higher initial gain
in accuracy with independent and identically distributed (IID) and non-IID
data, incurring lower communication overhead. This DIMAT paradigm presents a
new opportunity for the future decentralized learning, enhancing its
adaptability to real-world with sparse and light-weight communication and
computation.",CVPR
"The objective of Entity Alignment (EA) is to identify equivalent entity pairs
from multiple Knowledge Graphs (KGs) and create a more comprehensive and
unified KG. The majority of EA methods have primarily focused on the structural
modality of KGs, lacking exploration of multi-modal information. A few
multi-modal EA methods have made good attempts in this field. Still, they have
two shortcomings: (1) inconsistent and inefficient modality modeling that
designs complex and distinct models for each modality; (2) ineffective modality
fusion due to the heterogeneous nature of modalities in EA. To tackle these
challenges, we propose PathFusion, consisting of two main components: (1) MSP,
a unified modeling approach that simplifies the alignment process by
constructing paths connecting entities and modality nodes to represent multiple
modalities; (2) IRF, an iterative fusion method that effectively combines
information from different modalities using the path as an information carrier.
Experimental results on real-world datasets demonstrate the superiority of
PathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement
on Hits@1, and 0.194-0.245 absolute improvement on MRR.",CVPR
"We present a method to reconstruct indoor and outdoor static scene geometry
and appearance from an omnidirectional video moving in a small circular sweep.
This setting is challenging because of the small baseline and large depth
ranges, making it difficult to find ray crossings. To better constrain the
optimization, we estimate geometry as a signed distance field within a
spherical binoctree data structure and use a complementary efficient tree
traversal strategy based on a breadth-first search for sampling. Unlike regular
grids or trees, the shape of this structure well-matches the camera setting,
creating a better memory-quality trade-off. From an initial depth estimate, the
binoctree is adaptively subdivided throughout the optimization; previous
methods use a fixed depth that leaves the scene undersampled. In comparison
with three neural optimization methods and two non-neural methods, ours shows
decreased geometry error on average, especially in a detailed scene, while
significantly reducing the required number of voxels to represent such details.",CVPR
"Largely due to their implicit nature, neural fields lack a direct mechanism
for filtering, as Fourier analysis from discrete signal processing is not
directly applicable to these representations. Effective filtering of neural
fields is critical to enable level-of-detail processing in downstream
applications, and support operations that involve sampling the field on regular
grids (e.g. marching cubes). Existing methods that attempt to decompose neural
fields in the frequency domain either resort to heuristics or require extensive
modifications to the neural field architecture. We show that via a simple
modification, one can obtain neural fields that are low-pass filtered, and in
turn show how this can be exploited to obtain a frequency decomposition of the
entire signal. We demonstrate the validity of our technique by investigating
level-of-detail reconstruction, and showing how coarser representations can be
computed effectively.",CVPR
"Recently, some large kernel convnets strike back with appealing performance
and efficiency. However, given the square complexity of convolution, scaling up
kernels can bring about an enormous amount of parameters and the proliferated
parameters can induce severe optimization problem. Due to these issues, current
CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e.,
51x5 + 5x51) and start to saturate as the kernel size continues growing. In
this paper, we delve into addressing these vital issues and explore whether we
can continue scaling up kernels for more performance gains. Inspired by human
vision, we propose a human-like peripheral convolution that efficiently reduces
over 90% parameter count of dense grid convolution through parameter sharing,
and manage to scale up kernel size to extremely large. Our peripheral
convolution behaves highly similar to human, reducing the complexity of
convolution from O(K^2) to O(logK) without backfiring performance. Built on
this, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK
outperforms modern vision Transformers and ConvNet architectures like Swin,
ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet
classification, semantic segmentation on ADE20K and object detection on MS
COCO. For the first time, we successfully scale up the kernel size of CNNs to
an unprecedented 101x101 and demonstrate consistent improvements.",CVPR
"Diagnosis in histopathology requires a global whole slide images (WSIs)
analysis, requiring pathologists to compound evidence from different WSI
patches. The gigapixel scale of WSIs poses a challenge for histopathology
multi-modal models. Training multi-model models for histopathology requires
instruction tuning datasets, which currently contain information for individual
image patches, without a spatial grounding of the concepts within each patch
and without a wider view of the WSI. Therefore, they lack sufficient diagnostic
capacity for histopathology. To bridge this gap, we introduce Quilt-Instruct, a
large-scale dataset of 107,131 histopathology-specific instruction
question/answer pairs, grounded within diagnostically relevant image patches
that make up the WSI. Our dataset is collected by leveraging educational
histopathology videos from YouTube, which provides spatial localization of
narrations by automatically extracting the narrators' cursor positions.
Quilt-Instruct supports contextual reasoning by extracting diagnosis and
supporting facts from the entire WSI. Using Quilt-Instruct, we train
Quilt-LLaVA, which can reason beyond the given single image patch, enabling
diagnostic reasoning across patches. To evaluate Quilt-LLaVA, we propose a
comprehensive evaluation dataset created from 985 images and 1283
human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using
public histopathology datasets, where Quilt-LLaVA significantly outperforms
SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set
VQA. Our code, data, and model are publicly accessible at
quilt-llava.github.io.",CVPR
"Estimating the relative camera pose from $n \geq 5$ correspondences between
two calibrated views is a fundamental task in computer vision. This process
typically involves two stages: 1) estimating the essential matrix between the
views, and 2) disambiguating among the four candidate relative poses that
satisfy the epipolar geometry. In this paper, we demonstrate a novel approach
that, for the first time, bypasses the second stage. Specifically, we show that
it is possible to directly estimate the correct relative camera pose from
correspondences without needing a post-processing step to enforce the
cheirality constraint on the correspondences. Building on recent advances in
certifiable non-minimal optimization, we frame the relative pose estimation as
a Quadratically Constrained Quadratic Program (QCQP). By applying the
appropriate constraints, we ensure the estimation of a camera pose that
corresponds to a valid 3D geometry and that is globally optimal when certified.
We validate our method through exhaustive synthetic and real-world experiments,
confirming the efficacy, efficiency and accuracy of the proposed approach. Code
is available at https://github.com/javrtg/C2P.",CVPR
"Recovering degraded low-resolution text images is challenging, especially for
Chinese text images with complex strokes and severe degradation in real-world
scenarios. Ensuring both text fidelity and style realness is crucial for
high-quality text image super-resolution. Recently, diffusion models have
achieved great success in natural image synthesis and restoration due to their
powerful data distribution modeling abilities and data generation capabilities.
In this work, we propose an Image Diffusion Model (IDM) to restore text images
with realistic styles. For diffusion models, they are not only suitable for
modeling realistic image distribution but also appropriate for learning text
distribution. Since text prior is important to guarantee the correctness of the
restored text structure according to existing arts, we also propose a Text
Diffusion Model (TDM) for text recognition which can guide IDM to generate text
images with correct structures. We further propose a Mixture of Multi-modality
module (MoM) to make these two diffusion models cooperate with each other in
all the diffusion steps. Extensive experiments on synthetic and real-world
datasets demonstrate that our Diffusion-based Blind Text Image Super-Resolution
(DiffTSR) can restore text images with more accurate text structures as well as
more realistic appearances simultaneously.",CVPR
"Foundation models such as ChatGPT have made significant strides in robotic
tasks due to their universal representation of real-world domains. In this
paper, we leverage foundation models to tackle grasp detection, a persistent
challenge in robotics with broad industrial applications. Despite numerous
grasp datasets, their object diversity remains limited compared to real-world
figures. Fortunately, foundation models possess an extensive repository of
real-world knowledge, including objects we encounter in our daily lives. As a
consequence, a promising solution to the limited representation in previous
grasp datasets is to harness the universal knowledge embedded in these
foundation models. We present Grasp-Anything, a new large-scale grasp dataset
synthesized from foundation models to implement this solution. Grasp-Anything
excels in diversity and magnitude, boasting 1M samples with text descriptions
and more than 3M objects, surpassing prior datasets. Empirically, we show that
Grasp-Anything successfully facilitates zero-shot grasp detection on
vision-based tasks and real-world robotic experiments. Our dataset and code are
available at https://grasp-anything-2023.github.io.",CVPR
"Is vision good enough for language? Recent advancements in multimodal models
primarily stem from the powerful reasoning abilities of large language models
(LLMs). However, the visual component typically depends only on the
instance-level contrastive language-image pre-training (CLIP). Our research
reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still
exhibit systematic shortcomings. To understand the roots of these errors, we
explore the gap between the visual embedding space of CLIP and vision-only
self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP
perceives as similar despite their clear visual differences. With these pairs,
we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes
areas where state-of-the-art systems, including GPT-4V, struggle with
straightforward questions across nine basic visual patterns, often providing
incorrect answers and hallucinated explanations. We further evaluate various
CLIP-based vision-and-language models and found a notable correlation between
visual patterns that challenge CLIP models and those problematic for multimodal
LLMs. As an initial effort to address these issues, we propose a Mixture of
Features (MoF) approach, demonstrating that integrating vision self-supervised
learning features with MLLMs can significantly enhance their visual grounding
capabilities. Together, our research suggests visual representation learning
remains an open challenge, and accurate visual grounding is crucial for future
successful multimodal systems.",CVPR
"Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.",CVPR
"To address prevalent issues in medical imaging, such as data acquisition
challenges and label availability, transfer learning from natural to medical
image domains serves as a viable strategy to produce reliable segmentation
results. However, several existing barriers between domains need to be broken
down, including addressing contrast discrepancies, managing anatomical
variability, and adapting 2D pretrained models for 3D segmentation tasks. In
this paper, we propose ProMISe,a prompt-driven 3D medical image segmentation
model using only a single point prompt to leverage knowledge from a pretrained
2D image foundation model. In particular, we use the pretrained vision
transformer from the Segment Anything Model (SAM) and integrate lightweight
adapters to extract depth-related (3D) spatial context without updating the
pretrained weights. For robust results, a hybrid network with complementary
encoders is designed, and a boundary-aware loss is proposed to achieve precise
boundaries. We evaluate our model on two public datasets for colon and pancreas
tumor segmentations, respectively. Compared to the state-of-the-art
segmentation methods with and without prompt engineering, our proposed method
achieves superior performance. The code is publicly available at
https://github.com/MedICL-VU/ProMISe.",CVPR
"Customized generation using diffusion models has made impressive progress in
image generation, but remains unsatisfactory in the challenging video
generation task, as it requires the controllability of both subjects and
motions. To that end, we present DreamVideo, a novel approach to generating
personalized videos from a few static images of the desired subject and a few
videos of target motion. DreamVideo decouples this task into two stages,
subject learning and motion learning, by leveraging a pre-trained video
diffusion model. The subject learning aims to accurately capture the fine
appearance of the subject from provided images, which is achieved by combining
textual inversion and fine-tuning of our carefully designed identity adapter.
In motion learning, we architect a motion adapter and fine-tune it on the given
videos to effectively model the target motion pattern. Combining these two
lightweight and efficient adapters allows for flexible customization of any
subject with any motion. Extensive experimental results demonstrate the
superior performance of our DreamVideo over the state-of-the-art methods for
customized video generation. Our project page is at
https://dreamvideo-t2v.github.io.",CVPR
"3D visual grounding is a challenging task that often requires direct and
dense supervision, notably the semantic label for each object in the scene. In
this paper, we instead study the naturally supervised setting that learns from
only 3D scene and QA pairs, where prior works underperform. We propose the
Language-Regularized Concept Learner (LARC), which uses constraints from
language as regularization to significantly improve the accuracy of
neuro-symbolic concept learners in the naturally supervised setting. Our
approach is based on two core insights: the first is that language constraints
(e.g., a word's relation to another) can serve as effective regularization for
structured representations in neuro-symbolic models; the second is that we can
query large language models to distill such constraints from language
properties. We show that LARC improves performance of prior works in naturally
supervised 3D visual grounding, and demonstrates a wide range of 3D visual
reasoning capabilities-from zero-shot composition, to data efficiency and
transferability. Our method represents a promising step towards regularizing
structured visual reasoning frameworks with language-based priors, for learning
in settings without dense supervision.",CVPR
"Surgical decisions are informed by aligning rapid portable 2D intraoperative
images (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,
CT). 2D/3D image registration often fails in practice: conventional
optimization methods are prohibitively slow and susceptible to local minima,
while neural networks trained on small datasets fail on new patients or require
impractical landmark supervision. We present DiffPose, a self-supervised
approach that leverages patient-specific simulation and differentiable
physics-based rendering to achieve accurate 2D/3D registration without relying
on manually labeled data. Preoperatively, a CNN is trained to regress the pose
of a randomly oriented synthetic X-ray rendered from the preoperative CT. The
CNN then initializes rapid intraoperative test-time optimization that uses the
differentiable X-ray renderer to refine the solution. Our work further proposes
several geometrically principled methods for sampling camera poses from
$\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving
registration in the tangent space $\mathfrak{se}(3)$ with geodesic and
multiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy
across surgical datasets at intraoperative speeds, improving upon existing
unsupervised methods by an order of magnitude and even outperforming supervised
baselines. Our code is available at https://github.com/eigenvivek/DiffPose.",CVPR
"Medical generative models, acknowledged for their high-quality sample
generation ability, have accelerated the fast growth of medical applications.
However, recent works concentrate on separate medical generation models for
distinct medical tasks and are restricted to inadequate medical multi-modal
knowledge, constraining medical comprehensive diagnosis. In this paper, we
propose MedM2G, a Medical Multi-Modal Generative framework, with the key
innovation to align, extract, and generate medical multi-modal within a unified
model. Extending beyond single or two medical modalities, we efficiently align
medical multi-modal through the central alignment approach in the unified
space. Significantly, our framework extracts valuable clinical knowledge by
preserving the medical visual invariant of each imaging modal, thereby
enhancing specific medical information for multi-modal generation. By
conditioning the adaptive cross-guided parameters into the multi-flow diffusion
framework, our model promotes flexible interactions among medical multi-modal
for generation. MedM2G is the first medical generative model that unifies
medical generation tasks of text-to-image, image-to-text, and unified
generation of medical modalities (CT, MRI, X-ray). It performs 5 medical
generation tasks across 10 datasets, consistently outperforming various
state-of-the-art works.",CVPR
"Generative Adversarial Networks (GANs) have been widely used to recover vivid
textures in image super-resolution (SR) tasks. In particular, one discriminator
is utilized to enable the SR network to learn the distribution of real-world
high-quality images in an adversarial training manner. However, the
distribution learning is overly coarse-grained, which is susceptible to virtual
textures and causes counter-intuitive generation results. To mitigate this, we
propose the simple and effective Semantic-aware Discriminator (denoted as SeD),
which encourages the SR network to learn the fine-grained distributions by
introducing the semantics of images as a condition. Concretely, we aim to
excavate the semantics of images from a well-trained semantic extractor. Under
different semantics, the discriminator is able to distinguish the real-fake
images individually and adaptively, which guides the SR network to learn the
more fine-grained semantic-aware textures. To obtain accurate and abundant
semantics, we take full advantage of recently popular pretrained vision models
(PVMs) with extensive datasets, and then incorporate its semantic features into
the discriminator through a well-designed spatial cross-attention module. In
this way, our proposed semantic-aware discriminator empowered the SR network to
produce more photo-realistic and pleasing images. Extensive experiments on two
typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our
proposed methods.",CVPR
"Recent studies have shown promising performance in open-vocabulary object
detection (OVD) by utilizing pseudo labels (PLs) from pretrained vision and
language models (VLMs). However, teacher-student self-training, a powerful and
widely used paradigm to leverage PLs, is rarely explored for OVD. This work
identifies two challenges of using self-training in OVD: noisy PLs from VLMs
and frequent distribution changes of PLs. To address these challenges, we
propose SAS-Det that tames self-training for OVD from two key perspectives.
First, we present a split-and-fusion (SAF) head that splits a standard
detection into an open-branch and a closed-branch. This design can reduce noisy
supervision from pseudo boxes. Moreover, the two branches learn complementary
knowledge from different training data, significantly enhancing performance
when fused together. Second, in our view, unlike in closed-set tasks, the PL
distributions in OVD are solely determined by the teacher model. We introduce a
periodic update strategy to decrease the number of updates to the teacher,
thereby decreasing the frequency of changes in PL distributions, which
stabilizes the training process. Extensive experiments demonstrate SAS-Det is
both efficient and effective. SAS-Det outperforms recent models of the same
scale by a clear margin and achieves 37.4 AP50 and 29.1 APr on novel categories
of the COCO and LVIS benchmarks, respectively. Code is available at
\url{https://github.com/xiaofeng94/SAS-Det}.",CVPR
"In recent years, image editing has advanced remarkably. With increased human
control, it is now possible to edit an image in a plethora of ways; from
specifying in text what we want to change, to straight up dragging the contents
of the image in an interactive point-based manner. However, most of the focus
has remained on editing single images at a time. Whether and how we can
simultaneously edit large batches of images has remained understudied. With the
goal of minimizing human supervision in the editing process, this paper
presents a novel method for interactive batch image editing using StyleGAN as
the medium. Given an edit specified by users in an example image (e.g., make
the face frontal), our method can automatically transfer that edit to other
test images, so that regardless of their initial state (pose), they all arrive
at the same final state (e.g., all facing front). Extensive experiments
demonstrate that edits performed using our method have similar visual quality
to existing single-image-editing methods, while having more visual consistency
and saving significant time and human effort.",CVPR
"Recent advancements in the text-to-3D task leverage finetuned text-to-image
diffusion models to generate multi-view images, followed by NeRF
reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still
suffer from multi-view inconsistency and the resulting NeRF artifacts. Although
training longer with SFT improves consistency, it also causes distribution
shift, which reduces diversity and realistic details. We argue that the SFT of
multi-view diffusion models resembles the instruction finetuning stage of the
LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.
Essentially, RLFT methods optimize models beyond their SFT data distribution by
using their own outputs, effectively mitigating distribution shift. To this
end, we introduce Carve3D, a RLFT method coupled with the Multi-view
Reconstruction Consistency (MRC) metric, to improve the consistency of
multi-view diffusion models. To compute MRC on a set of multi-view images, we
compare them with their corresponding renderings of the reconstructed NeRF at
the same viewpoints. We validate the robustness of MRC with extensive
experiments conducted under controlled inconsistency levels. We enhance the
base RLFT algorithm to stabilize the training process, reduce distribution
shift, and identify scaling laws. Through qualitative and quantitative
experiments, along with a user study, we demonstrate Carve3D's improved
multi-view consistency, the resulting superior NeRF reconstruction quality, and
minimal distribution shift compared to longer SFT. Project webpage:
https://desaixie.github.io/carve-3d.",CVPR
"We introduce a novel superpoint-based transformer architecture for efficient
semantic segmentation of large-scale 3D scenes. Our method incorporates a fast
algorithm to partition point clouds into a hierarchical superpoint structure,
which makes our preprocessing 7 times faster than existing superpoint-based
approaches. Additionally, we leverage a self-attention mechanism to capture the
relationships between superpoints at multiple scales, leading to
state-of-the-art performance on three challenging benchmark datasets: S3DIS
(76.0% mIoU 6-fold validation), KITTI-360 (63.5% on Val), and DALES (79.6%).
With only 212k parameters, our approach is up to 200 times more compact than
other state-of-the-art models while maintaining similar performance.
Furthermore, our model can be trained on a single GPU in 3 hours for a fold of
the S3DIS dataset, which is 7x to 70x fewer GPU-hours than the best-performing
methods. Our code and models are accessible at
github.com/drprojects/superpoint_transformer.",CVPR
"Current 3D stylization methods often assume static scenes, which violates the
dynamic nature of our real world. To address this limitation, we present
S-DyRF, a reference-based spatio-temporal stylization method for dynamic neural
radiance fields. However, stylizing dynamic 3D scenes is inherently challenging
due to the limited availability of stylized reference images along the temporal
axis. Our key insight lies in introducing additional temporal cues besides the
provided reference. To this end, we generate temporal pseudo-references from
the given stylized reference. These pseudo-references facilitate the
propagation of style information from the reference to the entire dynamic 3D
scene. For coarse style transfer, we enforce novel views and times to mimic the
style details present in pseudo-references at the feature level. To preserve
high-frequency details, we create a collection of stylized temporal pseudo-rays
from temporal pseudo-references. These pseudo-rays serve as detailed and
explicit stylization guidance for achieving fine style transfer. Experiments on
both synthetic and real-world datasets demonstrate that our method yields
plausible stylized results of space-time view synthesis on dynamic 3D scenes.",CVPR
"Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian
splat representation has been introduced for novel view synthesis from sparse
image sets. Making such representations suitable for applications like network
streaming and rendering on low-power devices requires significantly reduced
memory consumption as well as improved rendering efficiency. We propose a
compressed 3D Gaussian splat representation that utilizes sensitivity-aware
vector clustering with quantization-aware training to compress directional
colors and Gaussian parameters. The learned codebooks have low bitrates and
achieve a compression rate of up to $31\times$ on real-world scenes with only
minimal degradation of visual quality. We demonstrate that the compressed splat
representation can be efficiently rendered with hardware rasterization on
lightweight GPUs at up to $4\times$ higher framerates than reported via an
optimized GPU compute pipeline. Extensive experiments across multiple datasets
demonstrate the robustness and rendering speed of the proposed approach.",CVPR
"Unlike color photography images, which are consistently encoded into RGB
channels, biological images encompass various modalities, where the type of
microscopy and the meaning of each channel varies with each experiment.
Importantly, the number of channels can range from one to a dozen and their
correlation is often comparatively much lower than RGB, as each of them brings
specific information content. This aspect is largely overlooked by methods
designed out of the bioimage field, and current solutions mostly focus on
intra-channel spatial attention, often ignoring the relationship between
channels, yet crucial in most biological applications. Importantly, the
variable channel type and count prevent the projection of several experiments
to a unified representation for large scale pre-training. In this study, we
propose ChAda-ViT, a novel Channel Adaptive Vision Transformer architecture
employing an Inter-Channel Attention mechanism on images with an arbitrary
number, order and type of channels. We also introduce IDRCell100k, a bioimage
dataset with a rich set of 79 experiments covering 7 microscope modalities,
with a multitude of channel types, and counts varying from 1 to 10 per
experiment. Our architecture, trained in a self-supervised manner, outperforms
existing approaches in several biologically relevant downstream tasks.
Additionally, it can be used to bridge the gap for the first time between
assays with different microscopes, channel numbers or types by embedding
various image and experimental modalities into a unified biological image
representation. The latter should facilitate interdisciplinary studies and pave
the way for better adoption of deep learning in biological image-based
analyses. Code and Data available at https://github.com/nicoboou/chadavit.",CVPR
"The recent progress in language-based open-vocabulary object detection can be
largely attributed to finding better ways of leveraging large-scale data with
free-form text annotations. Training such models with a discriminative
objective function has proven successful, but requires good positive and
negative samples. However, the free-form nature and the open vocabulary of
object descriptions make the space of negatives extremely large. Prior works
randomly sample negatives or use rule-based techniques to build them. In
contrast, we propose to leverage the vast knowledge built into modern
generative models to automatically build negatives that are more relevant to
the original data. Specifically, we use large-language-models to generate
negative text descriptions, and text-to-image diffusion models to also generate
corresponding negative images. Our experimental analysis confirms the relevance
of the generated negative data, and its use in language-based detectors
improves performance on two complex benchmarks. Code is available at
\url{https://github.com/xiaofeng94/Gen-Enhanced-Negs}.",CVPR
"The Zero-Shot Learning (ZSL) task pertains to the identification of entities
or relations in texts that were not seen during training. ZSL has emerged as a
critical research area due to the scarcity of labeled data in specific domains,
and its applications have grown significantly in recent years. With the advent
of large pretrained language models, several novel methods have been proposed,
resulting in substantial improvements in ZSL performance. There is a growing
demand, both in the research community and industry, for a comprehensive ZSL
framework that facilitates the development and accessibility of the latest
methods and pretrained models.In this study, we propose a novel ZSL framework
called Zshot that aims to address the aforementioned challenges. Our primary
objective is to provide a platform that allows researchers to compare different
state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we
have designed our framework to support the industry with readily available APIs
for production under the standard SpaCy NLP pipeline. Our API is extendible and
evaluable, moreover, we include numerous enhancements such as boosting the
accuracy with pipeline ensembling and visualization utilities available as a
SpaCy extension.",CVPR
"Panoramic videos have the advantage of providing an immersive and interactive
viewing experience. Nevertheless, their spherical nature gives rise to various
and uncertain user viewing behaviors, which poses significant challenges for
panoramic video quality assessment (PVQA). In this work, we propose an
end-to-end optimized, blind PVQA method with explicit modeling of user viewing
patterns through visual scanpaths. Our method consists of two modules: a
scanpath generator and a quality assessor. The scanpath generator is initially
trained to predict future scanpaths by minimizing their expected code length
and then jointly optimized with the quality assessor for quality prediction.
Our blind PVQA method enables direct quality assessment of panoramic images by
treating them as videos composed of identical frames. Experiments on three
public panoramic image and video quality datasets, encompassing both synthetic
and authentic distortions, validate the superiority of our blind PVQA model
over existing methods.",CVPR
"Molecule discovery serves as a cornerstone in numerous scientific domains,
fueling the development of new materials and innovative drug designs. Recent
developments of in-silico molecule discovery have highlighted the promising
results of cross-modal techniques, which bridge molecular structures with their
descriptive annotations. However, these cross-modal methods frequently
encounter the issue of data scarcity, hampering their performance and
application. In this paper, we address the low-resource challenge by utilizing
artificially-real data generated by Large Language Models (LLMs). We first
introduce a retrieval-based prompting strategy to construct high-quality pseudo
data, then explore the optimal method to effectively leverage this pseudo data.
Experiments show that using pseudo data for domain adaptation outperforms all
existing methods, while also requiring a smaller model scale, reduced data size
and lower training cost, highlighting its efficiency. Furthermore, our method
shows a sustained improvement as the volume of pseudo data increases, revealing
the great potential of pseudo data in advancing low-resource cross-modal
molecule discovery. Our code and data are available at
https://github.com/SCIR-HI/ArtificiallyR2R.",CVPR
"Robot manipulation relies on accurately predicting contact points and
end-effector directions to ensure successful operation. However, learning-based
robot manipulation, trained on a limited category within a simulator, often
struggles to achieve generalizability, especially when confronted with
extensive categories. Therefore, we introduce an innovative approach for robot
manipulation that leverages the robust reasoning capabilities of Multimodal
Large Language Models (MLLMs) to enhance the stability and generalization of
manipulation. By fine-tuning the injected adapters, we preserve the inherent
common sense and reasoning ability of the MLLMs while equipping them with the
ability for manipulation. The fundamental insight lies in the introduced
fine-tuning paradigm, encompassing object category understanding, affordance
prior reasoning, and object-centric pose prediction to stimulate the reasoning
ability of MLLM in manipulation. During inference, our approach utilizes an RGB
image and text prompt to predict the end effector's pose in chain of thoughts.
After the initial contact is established, an active impedance adaptation policy
is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover,
in real world, we design a test-time adaptation (TTA) strategy for manipulation
to enable the model better adapt to the current real-world scene configuration.
Experiments in simulator and real-world show the promising performance of
ManipLLM. More details and demonstrations can be found at
https://sites.google.com/view/manipllm.",CVPR
"Continual learning empowers models to adapt autonomously to the ever-changing
environment or data streams without forgetting old knowledge. Prompt-based
approaches are built on frozen pre-trained models to learn the task-specific
prompts and classifiers efficiently. Existing prompt-based methods are
inconsistent between training and testing, limiting their effectiveness. Two
types of inconsistency are revealed. Test predictions are made from all
classifiers while training only focuses on the current task classifier without
holistic alignment, leading to Classifier inconsistency. Prompt inconsistency
indicates that the prompt selected during testing may not correspond to the one
associated with this task during training. In this paper, we propose a novel
prompt-based method, Consistent Prompting (CPrompt), for more aligned training
and testing. Specifically, all existing classifiers are exposed to prompt
training, resulting in classifier consistency learning. In addition, prompt
consistency learning is proposed to enhance prediction robustness and boost
prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based
counterparts and achieves state-of-the-art performance on multiple continual
learning benchmarks. Detailed analysis shows that improvements come from more
consistent training and testing.",CVPR
"Diffusion models have recently received increasing research attention for
their remarkable transfer abilities in semantic segmentation tasks. However,
generating fine-grained segmentation masks with diffusion models often requires
additional training on annotated datasets, leaving it unclear to what extent
pre-trained diffusion models alone understand the semantic relations of their
generated images. To address this question, we leverage the semantic knowledge
extracted from Stable Diffusion (SD) and aim to develop an image segmentor
capable of generating fine-grained segmentation maps without any additional
training. The primary difficulty stems from the fact that semantically
meaningful feature maps typically exist only in the spatially lower-dimensional
layers, which poses a challenge in directly extracting pixel-level semantic
relations from these feature maps. To overcome this issue, our framework
identifies semantic correspondences between image pixels and spatial locations
of low-dimensional feature maps by exploiting SD's generation process and
utilizes them for constructing image-resolution segmentation maps. In extensive
experiments, the produced segmentation maps are demonstrated to be well
delineated and capture detailed parts of the images, indicating the existence
of highly accurate pixel-level semantic knowledge in diffusion models.",CVPR
"Portable 360$^\circ$ cameras are becoming a cheap and efficient tool to
establish large visual databases. By capturing omnidirectional views of a
scene, these cameras could expedite building environment models that are
essential for visual localization. However, such an advantage is often
overlooked due to the lack of valuable datasets. This paper introduces a new
benchmark dataset, 360Loc, composed of 360$^\circ$ images with ground truth
poses for visual localization. We present a practical implementation of
360$^\circ$ mapping combining 360$^\circ$ images with lidar data to generate
the ground truth 6DoF poses. 360Loc is the first dataset and benchmark that
explores the challenge of cross-device visual positioning, involving
360$^\circ$ reference frames, and query frames from pinhole, ultra-wide FoV
fisheye, and 360$^\circ$ cameras. We propose a virtual camera approach to
generate lower-FoV query frames from 360$^\circ$ images, which ensures a fair
comparison of performance among different query types in visual localization
tasks. We also extend this virtual camera approach to feature matching-based
and pose regression-based methods to alleviate the performance loss caused by
the cross-device domain gap, and evaluate its effectiveness against
state-of-the-art baselines. We demonstrate that omnidirectional visual
localization is more robust in challenging large-scale scenes with symmetries
and repetitive structures. These results provide new insights into 360-camera
mapping and omnidirectional visual localization with cross-device queries.",CVPR
"Text-to-Image (T2I) generation methods based on diffusion model have garnered
significant attention in the last few years. Although these image synthesis
methods produce visually appealing results, they frequently exhibit spelling
errors when rendering text within the generated images. Such errors manifest as
missing, incorrect or extraneous characters, thereby severely constraining the
performance of text image generation based on diffusion models. To address the
aforementioned issue, this paper proposes a novel approach for text image
generation, utilizing a pre-trained diffusion model (i.e., Stable Diffusion
[27]). Our approach involves the design and training of a light-weight
character-level text encoder, which replaces the original CLIP encoder and
provides more robust text embeddings as conditional guidance. Then, we
fine-tune the diffusion model using a large-scale dataset, incorporating local
attention control under the supervision of character-level segmentation maps.
Finally, by employing an inference stage refinement process, we achieve a
notably high sequence accuracy when synthesizing text in arbitrarily given
images. Both qualitative and quantitative results demonstrate the superiority
of our method to the state of the art. Furthermore, we showcase several
potential applications of the proposed UDiffText, including text-centric image
synthesis, scene text editing, etc. Code and model will be available at
https://github.com/ZYM-PKU/UDiffText .",CVPR
"Our brain can effortlessly recognize objects even when partially hidden from
view. Seeing the visible of the hidden is called amodal completion; however,
this task remains a challenge for generative AI despite rapid progress. We
propose to sidestep many of the difficulties of existing approaches, which
typically involve a two-step process of predicting amodal masks and then
generating pixels. Our method involves thinking outside the box, literally! We
go outside the object bounding box to use its context to guide a pre-trained
diffusion inpainting model, and then progressively grow the occluded object and
trim the extra background. We overcome two technical challenges: 1) how to be
free of unwanted co-occurrence bias, which tends to regenerate similar
occluders, and 2) how to judge if an amodal completion has succeeded. Our
amodal completion method exhibits improved photorealistic completion results
compared to existing approaches in numerous successful completion cases. And
the best part? It doesn't require any special training or fine-tuning of
models.",CVPR
"Creating high-dynamic videos such as motion-rich actions and sophisticated
visual effects poses a significant challenge in the field of artificial
intelligence. Unfortunately, current state-of-the-art video generation methods,
primarily focusing on text-to-video generation, tend to produce video clips
with minimal motions despite maintaining high fidelity. We argue that relying
solely on text instructions is insufficient and suboptimal for video
generation. In this paper, we introduce PixelDance, a novel approach based on
diffusion models that incorporates image instructions for both the first and
last frames in conjunction with text instructions for video generation.
Comprehensive experimental results demonstrate that PixelDance trained with
public data exhibits significantly better proficiency in synthesizing videos
with complex scenes and intricate motions, setting a new standard for video
generation.",CVPR
"Human beings possess the capability to multiply a melange of multisensory
cues while actively exploring and interacting with the 3D world. Current
multi-modal large language models, however, passively absorb sensory data as
inputs, lacking the capacity to actively interact with the objects in the 3D
environment and dynamically collect their multisensory information. To usher in
the study of this area, we propose MultiPLY, a multisensory embodied large
language model that could incorporate multisensory interactive data, including
visual, audio, tactile, and thermal information into large language models,
thereby establishing the correlation among words, actions, and percepts. To
this end, we first collect Multisensory Universe, a large-scale multisensory
interaction dataset comprising 500k data by deploying an LLM-powered embodied
agent to engage with the 3D environment. To perform instruction tuning with
pre-trained LLM on such generated data, we first encode the 3D scene as
abstracted object-centric representations and then introduce action tokens
denoting that the embodied agent takes certain actions within the environment,
as well as state tokens that represent the multisensory state observations of
the agent at each time step. In the inference time, MultiPLY could generate
action tokens, instructing the agent to take the action in the environment and
obtain the next multisensory state observation. The observation is then
appended back to the LLM via state tokens to generate subsequent text or action
tokens. We demonstrate that MultiPLY outperforms baselines by a large margin
through a diverse set of embodied tasks involving object retrieval, tool use,
multisensory captioning, and task decomposition.",CVPR
"The newly proposed Generalized Referring Expression Segmentation (GRES)
amplifies the formulation of classic RES by involving multiple/non-target
scenarios. Recent approaches focus on optimizing the last modality-fused
feature which is directly utilized for segmentation and object-existence
identification. However, the attempt to integrate all-grained information into
a single joint representation is impractical in GRES due to the increased
complexity of the spatial relationships among instances and deceptive text
descriptions. Furthermore, the subsequent binary target justification across
all referent scenarios fails to specify their inherent differences, leading to
ambiguity in object understanding. To address the weakness, we propose a
$\textbf{H}$ierarchical Semantic $\textbf{D}$ecoding with $\textbf{C}$ounting
Assistance framework (HDC). It hierarchically transfers complementary modality
information across granularities, and then aggregates each well-aligned
semantic correspondence for multi-level decoding. Moreover, with complete
semantic context modeling, we endow HDC with explicit counting capability to
facilitate comprehensive object perception in multiple/single/non-target
settings. Experimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO
benchmarks demonstrate the effectiveness and rationality of HDC which
outperforms the state-of-the-art GRES methods by a remarkable margin. Code will
be available $\href{https://github.com/RobertLuo1/HDC}{here}$.",CVPR
"There has been a growing interest in the task of generating sound for silent
videos, primarily because of its practicality in streamlining video
post-production. However, existing methods for video-sound generation attempt
to directly create sound from visual representations, which can be challenging
due to the difficulty of aligning visual representations with audio
representations. In this paper, we present SonicVisionLM, a novel framework
aimed at generating a wide range of sound effects by leveraging vision-language
models(VLMs). Instead of generating audio directly from video, we use the
capabilities of powerful VLMs. When provided with a silent video, our approach
first identifies events within the video using a VLM to suggest possible sounds
that match the video content. This shift in approach transforms the challenging
task of aligning image and audio into more well-studied sub-problems of
aligning image-to-text and text-to-audio through the popular diffusion models.
To improve the quality of audio recommendations with LLMs, we have collected an
extensive dataset that maps text descriptions to specific sound effects and
developed a time-controlled audio adapter. Our approach surpasses current
state-of-the-art methods for converting video to audio, enhancing
synchronization with the visuals, and improving alignment between audio and
video components. Project page:
https://yusiissy.github.io/SonicVisionLM.github.io/",CVPR
"3D human pose data collected in controlled laboratory settings present
challenges for pose estimators that generalize across diverse scenarios. To
address this, domain generalization is employed. Current methodologies in
domain generalization for 3D human pose estimation typically utilize
adversarial training to generate synthetic poses for training. Nonetheless,
these approaches exhibit several limitations. First, the lack of prior
information about the target domain complicates the application of suitable
augmentation through a single pose augmentor, affecting generalization on
target domains. Moreover, adversarial training's discriminator tends to enforce
similarity between source and synthesized poses, impeding the exploration of
out-of-source distributions. Furthermore, the pose estimator's optimization is
not exposed to domain shifts, limiting its overall generalization ability.
  To address these limitations, we propose a novel framework featuring two pose
augmentors: the weak and the strong augmentors. Our framework employs
differential strategies for generation and discrimination processes,
facilitating the preservation of knowledge related to source poses and the
exploration of out-of-source distributions without prior information about
target poses. Besides, we leverage meta-optimization to simulate domain shifts
in the optimization process of the pose estimator, thereby improving its
generalization ability. Our proposed approach significantly outperforms
existing methods, as demonstrated through comprehensive experiments on various
benchmark datasets.Our code will be released at
\url{https://github.com/davidpengucf/DAF-DG}.",CVPR
"Optical flow is an indispensable building block for various important
computer vision tasks, including motion estimation, object tracking, and
disparity measurement. In this work, we propose TransFlow, a pure transformer
architecture for optical flow estimation. Compared to dominant CNN-based
methods, TransFlow demonstrates three advantages. First, it provides more
accurate correlation and trustworthy matching in flow estimation by utilizing
spatial self-attention and cross-attention mechanisms between adjacent frames
to effectively capture global dependencies; Second, it recovers more
compromised information (e.g., occlusion and motion blur) in flow estimation
through long-range temporal association in dynamic scenes; Third, it enables a
concise self-learning paradigm and effectively eliminate the complex and
laborious multi-stage pre-training procedures. We achieve the state-of-the-art
results on the Sintel, KITTI-15, as well as several downstream tasks, including
video object detection, interpolation and stabilization. For its efficacy, we
hope TransFlow could serve as a flexible baseline for optical flow estimation.",CVPR
"In the realm of video object segmentation (VOS), the challenge of operating
under low-light conditions persists, resulting in notably degraded image
quality and compromised accuracy when comparing query and memory frames for
similarity computation. Event cameras, characterized by their high dynamic
range and ability to capture motion information of objects, offer promise in
enhancing object visibility and aiding VOS methods under such low-light
conditions. This paper introduces a pioneering framework tailored for low-light
VOS, leveraging event camera data to elevate segmentation accuracy. Our
approach hinges on two pivotal components: the Adaptive Cross-Modal Fusion
(ACMF) module, aimed at extracting pertinent features while fusing image and
event modalities to mitigate noise interference, and the Event-Guided Memory
Matching (EGMM) module, designed to rectify the issue of inaccurate matching
prevalent in low-light settings. Additionally, we present the creation of a
synthetic LLE-DAVIS dataset and the curation of a real-world LLE-VOS dataset,
encompassing frames and events. Experimental evaluations corroborate the
efficacy of our method across both datasets, affirming its effectiveness in
low-light scenarios.",CVPR
"This paper explores the possibility of extending the capability of
pre-trained neural image compressors (e.g., adapting to new data or target
bitrates) without breaking backward compatibility, the ability to decode
bitstreams encoded by the original model. We refer to this problem as continual
learning of image compression. Our initial findings show that baseline
solutions, such as end-to-end fine-tuning, do not preserve the desired backward
compatibility. To tackle this, we propose a knowledge replay training strategy
that effectively addresses this issue. We also design a new model architecture
that enables more effective continual learning than existing baselines.
Experiments are conducted for two scenarios: data-incremental learning and
rate-incremental learning. The main conclusion of this paper is that neural
image compressors can be fine-tuned to achieve better performance (compared to
their pre-trained version) on new data and rates without compromising backward
compatibility. Our code is available at
https://gitlab.com/viper-purdue/continual-compression",CVPR
"In this paper, we present a method to reconstruct the world and multiple
dynamic humans in 3D from a monocular video input. As a key idea, we represent
both the world and multiple humans via the recently emerging 3D Gaussian
Splatting (3D-GS) representation, enabling to conveniently and efficiently
compose and render them together. In particular, we address the scenarios with
severely limited and sparse observations in 3D human reconstruction, a common
challenge encountered in the real world. To tackle this challenge, we introduce
a novel approach to optimize the 3D-GS representation in a canonical space by
fusing the sparse cues in the common space, where we leverage a pre-trained 2D
diffusion model to synthesize unseen views while keeping the consistency with
the observed 2D appearances. We demonstrate our method can reconstruct
high-quality animatable 3D humans in various challenging examples, in the
presence of occlusion, image crops, few-shot, and extremely sparse
observations. After reconstruction, our method is capable of not only rendering
the scene in any novel views at arbitrary time instances, but also editing the
3D scene by removing individual humans or applying different motions for each
human. Through various experiments, we demonstrate the quality and efficiency
of our methods over alternative existing approaches.",CVPR
"We propose EMAGE, a framework to generate full-body human gestures from audio
and masked gestures, encompassing facial, local body, hands, and global
movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new
mesh-level holistic co-speech dataset. BEAT2 combines a MoShed SMPL-X body with
FLAME head parameters and further refines the modeling of head, neck, and
finger movements, offering a community-standardized, high-quality 3D motion
captured dataset. EMAGE leverages masked body gesture priors during training to
boost inference performance. It involves a Masked Audio Gesture Transformer,
facilitating joint training on audio-to-gesture generation and masked gesture
reconstruction to effectively encode audio and body gesture hints. Encoded body
hints from masked gestures are then separately employed to generate facial and
body movements. Moreover, EMAGE adaptively merges speech features from the
audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance
the results' fidelity and diversity. Experiments demonstrate that EMAGE
generates holistic gestures with state-of-the-art performance and is flexible
in accepting predefined spatial-temporal gesture inputs, generating complete,
audio-synchronized results. Our code and dataset are available
https://pantomatrix.github.io/EMAGE/",CVPR
"Existing research based on deep learning has extensively explored the problem
of daytime image dehazing. However, few studies have considered the
characteristics of nighttime hazy scenes. There are two distinctions between
nighttime and daytime haze. First, there may be multiple active colored light
sources with lower illumination intensity in nighttime scenes, which may cause
haze, glow and noise with localized, coupled and frequency inconsistent
characteristics. Second, due to the domain discrepancy between simulated and
real-world data, unrealistic brightness may occur when applying a dehazing
model trained on simulated data to real-world data. To address the above two
issues, we propose a semi-supervised model for real-world nighttime dehazing.
First, the spatial attention and frequency spectrum filtering are implemented
as a spatial-frequency domain information interaction module to handle the
first issue. Second, a pseudo-label-based retraining strategy and a local
window-based brightness loss for semi-supervised training process is designed
to suppress haze and glow while achieving realistic brightness. Experiments on
public benchmarks validate the effectiveness of the proposed method and its
superiority over state-of-the-art methods. The source code and Supplementary
Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD.",CVPR
"Inspired by the success of Large Language Models in dealing with new tasks
via In-Context Learning (ICL) in NLP, researchers have also developed Large
Vision-Language Models (LVLMs) with ICL capabilities. However, when
implementing ICL using these LVLMs, researchers usually resort to the simplest
way like random sampling to configure the in-context sequence, thus leading to
sub-optimal results. To enhance the ICL performance, in this study, we use
Visual Question Answering (VQA) as case study to explore diverse in-context
configurations to find the powerful ones. Additionally, through observing the
changes of the LVLM outputs by altering the in-context sequence, we gain
insights into the inner properties of LVLMs, improving our understanding of
them. Specifically, to explore in-context configurations, we design diverse
retrieval methods and employ different strategies to manipulate the retrieved
demonstrations. Through exhaustive experiments on three VQA datasets: VQAv2,
VizWiz, and OK-VQA, we uncover three important inner properties of the applied
LVLM and demonstrate which strategies can consistently improve the ICL VQA
performance. Our code is provided in:
https://github.com/GaryJiajia/OFv2_ICL_VQA.",CVPR
"There has been significant attention to the research on dense video
captioning, which aims to automatically localize and caption all events within
untrimmed video. Several studies introduce methods by designing dense video
captioning as a multitasking problem of event localization and event captioning
to consider inter-task relations. However, addressing both tasks using only
visual input is challenging due to the lack of semantic content. In this study,
we address this by proposing a novel framework inspired by the cognitive
information processing of humans. Our model utilizes external memory to
incorporate prior knowledge. The memory retrieval method is proposed with
cross-modal video-to-text matching. To effectively incorporate retrieved text
features, the versatile encoder and the decoder with visual and textual
cross-attention modules are designed. Comparative experiments have been
conducted to show the effectiveness of the proposed method on ActivityNet
Captions and YouCook2 datasets. Experimental results show promising performance
of our model without extensive pretraining from a large video dataset.",CVPR
"We are witnessing significant breakthroughs in the technology for generating
3D objects from text. Existing approaches either leverage large text-to-image
models to optimize a 3D representation or train 3D generators on object-centric
datasets. Generating entire scenes, however, remains very challenging as a
scene contains multiple 3D objects, diverse and scattered. In this work, we
introduce SceneWiz3D, a novel approach to synthesize high-fidelity 3D scenes
from text. We marry the locality of objects with globality of scenes by
introducing a hybrid 3D representation: explicit for objects and implicit for
scenes. Remarkably, an object, being represented explicitly, can be either
generated from text using conventional text-to-3D approaches, or provided by
users. To configure the layout of the scene and automatically place objects, we
apply the Particle Swarm Optimization technique during the optimization
process. Furthermore, it is difficult for certain parts of the scene (e.g.,
corners, occlusion) to receive multi-view supervision, leading to inferior
geometry. We incorporate an RGBD panorama diffusion model to mitigate it,
resulting in high-quality geometry. Extensive evaluation supports that our
approach achieves superior quality over previous approaches, enabling the
generation of detailed and view-consistent 3D scenes.",CVPR
"Integration of Large Language Models (LLMs) into visual domain tasks,
resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in
vision-language tasks, particularly for visual question answering (VQA).
However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial
reasoning and localization awareness. Despite generating highly descriptive and
elaborate textual answers, these models fail at simple tasks like
distinguishing a left vs right location. In this work, we explore how
image-space coordinate based instruction fine-tuning objectives could inject
spatial awareness into V-LLMs. We discover optimal coordinate representations,
data-efficient instruction fine-tuning objectives, and pseudo-data generation
strategies that lead to improved spatial awareness in V-LLMs. Additionally, our
resulting model improves VQA across image and video domains, reduces undesired
hallucination, and generates better contextual object descriptions. Experiments
across 5 vision-language tasks involving 14 different datasets establish the
clear performance improvements achieved by our proposed framework.",CVPR
"Vision transformers have revolutionized computer vision, but their
computational demands present challenges for training and deployment. This
paper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel
method that leverages data lottery ticket selection and sparsity pruning to
accelerate vision transformer training while maintaining accuracy. Our approach
focuses on identifying and utilizing the most informative data subsets and
eliminating redundant model parameters to optimize the training process.
Through extensive experiments, we demonstrate the effectiveness of LOTUS in
achieving rapid convergence and high accuracy with significantly reduced
computational requirements. This work highlights the potential of combining
data selection and sparsity techniques for efficient vision transformer
training, opening doors for further research and development in this area.",CVPR
"We introduce a lightweight and accurate localization method that only
utilizes the geometry of 2D-3D lines. Given a pre-captured 3D map, our approach
localizes a panorama image, taking advantage of the holistic 360 view. The
system mitigates potential privacy breaches or domain discrepancies by avoiding
trained or hand-crafted visual descriptors. However, as lines alone can be
ambiguous, we express distinctive yet compact spatial contexts from
relationships between lines, namely the dominant directions of parallel lines
and the intersection between non-parallel lines. The resulting representations
are efficient in processing time and memory compared to conventional visual
descriptor-based methods. Given the groups of dominant line directions and
their intersections, we accelerate the search process to test thousands of pose
candidates in less than a millisecond without sacrificing accuracy. We
empirically show that the proposed 2D-3D matching can localize panoramas for
challenging scenes with similar structures, dramatic domain shifts or
illumination changes. Our fully geometric approach does not involve extensive
parameter tuning or neural network training, making it a practical algorithm
that can be readily deployed in the real world. Project page including the code
is available through this link: https://82magnolia.github.io/fgpl/.",CVPR
"Reconstructing 3D clothed human avatars from single images is a challenging
task, especially when encountering complex poses and loose clothing. Current
methods exhibit limitations in performance, largely attributable to their
dependence on insufficient 2D image features and inconsistent query methods.
Owing to this, we present the Global-correlated 3D-decoupling Transformer for
clothed Avatar reconstruction (GTA), a novel transformer-based architecture
that reconstructs clothed human avatars from monocular images. Our approach
leverages transformer architectures by utilizing a Vision Transformer model as
an encoder for capturing global-correlated image features. Subsequently, our
innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane
features, using learnable embeddings as queries for cross-plane generation. To
effectively enhance feature fusion with the tri-plane 3D feature and human body
prior, we propose a hybrid prior fusion strategy combining spatial and
prior-enhanced queries, leveraging the benefits of spatial localization and
human body prior knowledge. Comprehensive experiments on CAPE and THuman2.0
datasets illustrate that our method outperforms state-of-the-art approaches in
both geometry and texture reconstruction, exhibiting high robustness to
challenging poses and loose clothing, and producing higher-resolution textures.
Codes will be available at https://github.com/River-Zhang/GTA.",CVPR
"As an important and practical way to obtain high dynamic range (HDR) video,
HDR video reconstruction from sequences with alternating exposures is still
less explored, mainly due to the lack of large-scale real-world datasets.
Existing methods are mostly trained on synthetic datasets, which perform poorly
in real scenes. In this work, to facilitate the development of real-world HDR
video reconstruction, we present Real-HDRV, a large-scale real-world benchmark
dataset for HDR video reconstruction, featuring various scenes, diverse motion
patterns, and high-quality labels. Specifically, our dataset contains 500
LDRs-HDRs video pairs, comprising about 28,000 LDR frames and 4,000 HDR labels,
covering daytime, nighttime, indoor, and outdoor scenes. To our best knowledge,
our dataset is the largest real-world HDR video reconstruction dataset.
Correspondingly, we propose an end-to-end network for HDR video reconstruction,
where a novel two-stage strategy is designed to perform alignment sequentially.
Specifically, the first stage performs global alignment with the adaptively
estimated global offsets, reducing the difficulty of subsequent alignment. The
second stage implicitly performs local alignment in a coarse-to-fine manner at
the feature level using the adaptive separable convolution. Extensive
experiments demonstrate that: (1) models trained on our dataset can achieve
better performance on real scenes than those trained on synthetic datasets; (2)
our method outperforms previous state-of-the-art methods. Our dataset is
available at https://github.com/yungsyu99/Real-HDRV.",CVPR
"Open-vocabulary querying in 3D space is challenging but essential for scene
understanding tasks such as object localization and segmentation.
Language-embedded scene representations have made progress by incorporating
language features into 3D spaces. However, their efficacy heavily depends on
neural networks that are resource-intensive in training and rendering. Although
recent 3D Gaussians offer efficient and high-quality novel view synthesis,
directly embedding language features in them leads to prohibitive memory usage
and decreased performance. In this work, we introduce Language Embedded 3D
Gaussians, a novel scene representation for open-vocabulary query tasks.
Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we
propose a dedicated quantization scheme that drastically alleviates the memory
requirement, and a novel embedding procedure that achieves smoother yet high
accuracy query, countering the multi-view feature inconsistencies and the
high-frequency inductive bias in point-based representations. Our comprehensive
experiments show that our representation achieves the best visual quality and
language querying accuracy across current language-embedded representations,
while maintaining real-time rendering frame rates on a single desktop GPU.",CVPR
"We introduce GaussianAvatars, a new method to create photorealistic head
avatars that are fully controllable in terms of expression, pose, and
viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian
splats that are rigged to a parametric morphable face model. This combination
facilitates photorealistic rendering while allowing for precise animation
control via the underlying parametric model, e.g., through expression transfer
from a driving sequence or by manually changing the morphable model parameters.
We parameterize each splat by a local coordinate frame of a triangle and
optimize for explicit displacement offset to obtain a more accurate geometric
representation. During avatar reconstruction, we jointly optimize for the
morphable model parameters and Gaussian splat parameters in an end-to-end
fashion. We demonstrate the animation capabilities of our photorealistic avatar
in several challenging scenarios. For instance, we show reenactments from a
driving video, where our method outperforms existing works by a significant
margin.",CVPR
"While modeling people wearing tight-fitting clothing has made great strides
in recent years, loose-fitting clothing remains a challenge. We propose a
method that delivers realistic garment models from real-world images,
regardless of garment shape or deformation. To this end, we introduce a fitting
approach that utilizes shape and deformation priors learned from synthetic data
to accurately capture garment shapes and deformations, including large ones.
Not only does our approach recover the garment geometry accurately, it also
yields models that can be directly used by downstream applications such as
animation and simulation.",CVPR
"In recent years, the thriving development of research related to egocentric
videos has provided a unique perspective for the study of conversational
interactions, where both visual and audio signals play a crucial role. While
most prior work focus on learning about behaviors that directly involve the
camera wearer, we introduce the Ego-Exocentric Conversational Graph Prediction
problem, marking the first attempt to infer exocentric conversational
interactions from egocentric videos. We propose a unified multi-modal framework
-- Audio-Visual Conversational Attention (AV-CONV), for the joint prediction of
conversation behaviors -- speaking and listening -- for both the camera wearer
as well as all other social partners present in the egocentric video.
Specifically, we adopt the self-attention mechanism to model the
representations across-time, across-subjects, and across-modalities. To
validate our method, we conduct experiments on a challenging egocentric video
dataset that includes multi-speaker and multi-conversation scenarios. Our
results demonstrate the superior performance of our method compared to a series
of baselines. We also present detailed ablation studies to assess the
contribution of each component in our model. Check our project page at
https://vjwq.github.io/AV-CONV/.",CVPR
"Few-shot model compression aims to compress a large model into a more compact
one with only a tiny training set (even without labels). Block-level pruning
has recently emerged as a leading technique in achieving high accuracy and low
latency in few-shot CNN compression. But, few-shot compression for Vision
Transformers (ViT) remains largely unexplored, which presents a new challenge.
In particular, the issue of sparse compression exists in traditional CNN
few-shot methods, which can only produce very few compressed models of
different model sizes. This paper proposes a novel framework for few-shot ViT
compression named DC-ViT. Instead of dropping the entire block, DC-ViT
selectively eliminates the attention module while retaining and reusing
portions of the MLP module. DC-ViT enables dense compression, which outputs
numerous compressed models that densely populate the range of model complexity.
DC-ViT outperforms state-of-the-art few-shot compression methods by a
significant margin of 10 percentage points, along with lower latency in the
compression of ViT and its variants.",CVPR
"We introduce LightGlue, a deep neural network that learns to match local
features across images. We revisit multiple design decisions of SuperGlue, the
state of the art in sparse matching, and derive simple but effective
improvements. Cumulatively, they make LightGlue more efficient - in terms of
both memory and computation, more accurate, and much easier to train. One key
property is that LightGlue is adaptive to the difficulty of the problem: the
inference is much faster on image pairs that are intuitively easy to match, for
example because of a larger visual overlap or limited appearance change. This
opens up exciting prospects for deploying deep matchers in latency-sensitive
applications like 3D reconstruction. The code and trained models are publicly
available at https://github.com/cvg/LightGlue.",CVPR
"2D keypoints are commonly used as an additional cue to refine estimated 3D
human meshes. Current methods optimize the pose and shape parameters with a
reprojection loss on the provided 2D keypoints. Such an approach, while simple
and intuitive, has limited effectiveness because the optimal solution is hard
to find in ambiguous parameter space and may sacrifice depth. Additionally,
divergent gradients from distal joints complicate and deviate the refinement of
proximal joints in the kinematic chain. To address these, we introduce
Kinematic-Tree Rotation (KITRO), a novel mesh refinement strategy that
explicitly models depth and human kinematic-tree structure. KITRO treats
refinement from a bone-wise perspective. Unlike previous methods which perform
gradient-based optimizations, our method calculates bone directions in closed
form. By accounting for the 2D pose, bone length, and parent joint's depth, the
calculation results in two possible directions for each child joint. We then
use a decision tree to trace binary choices for all bones along the human
skeleton's kinematic-tree to select the most probable hypothesis. Our
experiments across various datasets and baseline models demonstrate that KITRO
significantly improves 3D joint estimation accuracy and achieves an ideal 2D
fit simultaneously. Our code available at: https://github.com/MartaYang/KITRO.",CVPR
"Customization techniques for text-to-image models have paved the way for a
wide range of previously unattainable applications, enabling the generation of
specific concepts across diverse contexts and styles. While existing methods
facilitate high-fidelity customization for individual concepts or a limited,
pre-defined set of them, they fall short of achieving scalability, where a
single model can seamlessly render countless concepts. In this paper, we
address a new problem called Modular Customization, with the goal of
efficiently merging customized models that were fine-tuned independently for
individual concepts. This allows the merged model to jointly synthesize
concepts in one image without compromising fidelity or incurring any additional
computational costs.
  To address this problem, we introduce Orthogonal Adaptation, a method
designed to encourage the customized models, which do not have access to each
other during fine-tuning, to have orthogonal residual weights. This ensures
that during inference time, the customized models can be summed with minimal
interference.
  Our proposed method is both simple and versatile, applicable to nearly all
optimizable weights in the model architecture. Through an extensive set of
quantitative and qualitative evaluations, our method consistently outperforms
relevant baselines in terms of efficiency and identity preservation,
demonstrating a significant leap toward scalable customization of diffusion
models.",CVPR
"Despite the great success of deep learning in stereo matching, recovering
accurate disparity maps is still challenging. Currently, L1 and cross-entropy
are the two most widely used losses for stereo network training. Compared with
the former, the latter usually performs better thanks to its probability
modeling and direct supervision to the cost volume. However, how to accurately
model the stereo ground-truth for cross-entropy loss remains largely
under-explored. Existing works simply assume that the ground-truth
distributions are uni-modal, which ignores the fact that most of the edge
pixels can be multi-modal. In this paper, a novel adaptive multi-modal
cross-entropy loss (ADL) is proposed to guide the networks to learn different
distribution patterns for each pixel. Moreover, we optimize the disparity
estimator to further alleviate the bleeding or misalignment artifacts in
inference. Extensive experimental results show that our method is generic and
can help classic stereo networks regain state-of-the-art performance. In
particular, GANet with our method ranks $1^{st}$ on both the KITTI 2015 and
2012 benchmarks among the published methods. Meanwhile, excellent
synthetic-to-realistic generalization performance can be achieved by simply
replacing the traditional loss with ours.",CVPR
"The vision and language generative models have been overgrown in recent
years. For video generation, various open-sourced models and public-available
services have been developed to generate high-quality videos. However, these
methods often use a few metrics, e.g., FVD or IS, to evaluate the performance.
We argue that it is hard to judge the large conditional generative models from
the simple metrics since these models are often trained on very large datasets
with multi-aspect abilities. Thus, we propose a novel framework and pipeline
for exhaustively evaluating the performance of the generated videos. Our
approach involves generating a diverse and comprehensive list of 700 prompts
for text-to-video generation, which is based on an analysis of real-world user
data and generated with the assistance of a large language model. Then, we
evaluate the state-of-the-art video generative models on our carefully designed
benchmark, in terms of visual qualities, content qualities, motion qualities,
and text-video alignment with 17 well-selected objective metrics. To obtain the
final leaderboard of the models, we further fit a series of coefficients to
align the objective metrics to the users' opinions. Based on the proposed human
alignment method, our final score shows a higher correlation than simply
averaging the metrics, showing the effectiveness of the proposed evaluation
method.",CVPR
"Extracting keypoint locations from input hand frames, known as 3D hand pose
estimation, is a critical task in various human-computer interaction
applications. Essentially, the 3D hand pose estimation can be regarded as a 3D
point subset generative problem conditioned on input frames. Thanks to the
recent significant progress on diffusion-based generative models, hand pose
estimation can also benefit from the diffusion model to estimate keypoint
locations with high quality. However, directly deploying the existing diffusion
models to solve hand pose estimation is non-trivial, since they cannot achieve
the complex permutation mapping and precise localization. Based on this
motivation, this paper proposes HandDiff, a diffusion-based hand pose
estimation model that iteratively denoises accurate hand pose conditioned on
hand-shaped image-point clouds. In order to recover keypoint permutation and
accurate location, we further introduce joint-wise condition and local detail
condition. Experimental results demonstrate that the proposed HandDiff
significantly outperforms the existing approaches on four challenging hand pose
benchmark datasets. Codes and pre-trained models are publicly available at
https://github.com/cwc1260/HandDiff.",CVPR
"As large language models (LLMs) have become increasingly compute and memory
intensive, parameter-efficient fine-tuning (PEFT) methods are now a common
strategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),
which adds trainable low-rank ""adapters"" to selected layers. Each adapter
consists of a low-rank matrix product, multiplicatively scaled by a
rank-dependent factor. This scaling factor, which divides adapters by a factor
of the rank, results in slowed learning and stunted performance for LoRA with
higher-rank adapters. Consequently, the use of LoRA in practice has generally
been limited to very low ranks. In this work, we study the impact of the
scaling factor on the learning process and prove that LoRA adapters should be
divided by a factor of the square root of the rank. Modifying LoRA with the
appropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)
method, easily provides for a fine-tuning compute/performance trade-off, where
larger ranks can be used to trade off increased computational resources during
training for better fine-tuning performance, with no change in inference
computing cost.",CVPR
"We present En3D, an enhanced generative scheme for sculpting high-quality 3D
human avatars. Unlike previous works that rely on scarce 3D datasets or limited
2D collections with imbalanced viewing angles and imprecise pose priors, our
approach aims to develop a zero-shot 3D generative scheme capable of producing
visually realistic, geometrically accurate and content-wise diverse 3D humans
without relying on pre-existing 3D or 2D assets. To address this challenge, we
introduce a meticulously crafted workflow that implements accurate physical
modeling to learn the enhanced 3D generative model from synthetic 2D data.
During inference, we integrate optimization modules to bridge the gap between
realistic appearances and coarse 3D shapes. Specifically, En3D comprises three
modules: a 3D generator that accurately models generalizable 3D humans with
realistic appearance from synthesized balanced, diverse, and structured human
images; a geometry sculptor that enhances shape quality using multi-view normal
constraints for intricate human anatomy; and a texturing module that
disentangles explicit texture maps with fidelity and editability, leveraging
semantical UV partitioning and a differentiable rasterizer. Experimental
results show that our approach significantly outperforms prior works in terms
of image quality, geometry accuracy and content diversity. We also showcase the
applicability of our generated avatars for animation and editing, as well as
the scalability of our approach for content-style free adaptation.",CVPR
"We present differentiable point-based inverse rendering, DPIR, an
analysis-by-synthesis method that processes images captured under diverse
illuminations to estimate shape and spatially-varying BRDF. To this end, we
adopt point-based rendering, eliminating the need for multiple samplings per
ray, typical of volumetric rendering, thus significantly enhancing the speed of
inverse rendering. To realize this idea, we devise a hybrid point-volumetric
representation for geometry and a regularized basis-BRDF representation for
reflectance. The hybrid geometric representation enables fast rendering through
point-based splatting while retaining the geometric details and stability
inherent to SDF-based representations. The regularized basis-BRDF mitigates the
ill-posedness of inverse rendering stemming from limited light-view angular
samples. We also propose an efficient shadow detection method using point-based
shadow map rendering. Our extensive evaluations demonstrate that DPIR
outperforms prior works in terms of reconstruction accuracy, computational
efficiency, and memory footprint. Furthermore, our explicit point-based
representation and rendering enables intuitive geometry and reflectance
editing.",CVPR
"Scene flow characterizes the 3D motion between two LiDAR scans captured by an
autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow
as point-wise unconstrained flow vectors that can be learned by either
large-scale training beforehand or time-consuming optimization at inference.
However, these methods do not take into account that objects in autonomous
driving often move rigidly. We incorporate this rigid-motion assumption into
our design, where the goal is to associate objects over scans and then estimate
the locally rigid transformations. We propose ICP-Flow, a learning-free flow
estimator. The core of our design is the conventional Iterative Closest Point
(ICP) algorithm, which aligns the objects over time and outputs the
corresponding rigid transformations. Crucially, to aid ICP, we propose a
histogram-based initialization that discovers the most likely translation, thus
providing a good starting point for ICP. The complete scene flow is then
recovered from the rigid transformations. We outperform state-of-the-art
baselines, including supervised models, on the Waymo dataset and perform
competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward
neural network, supervised by the pseudo labels from our model, and achieve top
performance among all models capable of real-time inference. We validate the
advantage of our model on scene flow estimation with longer temporal gaps, up
to 0.4 seconds where other models fail to deliver meaningful results.",CVPR
"This paper proposes to correct the rolling shutter (RS) distorted images by
estimating the distortion flow from the global shutter (GS) to RS directly.
Existing methods usually perform correction using the undistortion flow from
the RS to GS. They initially predict the flow from consecutive RS frames,
subsequently rescaling it as the displacement fields from the RS frame to the
underlying GS image using time-dependent scaling factors. Following this,
RS-aware forward warping is employed to convert the RS image into its GS
counterpart. Nevertheless, this strategy is prone to two shortcomings. First,
the undistortion flow estimation is rendered inaccurate by merely linear
scaling the flow, due to the complex non-linear motion nature. Second, RS-aware
forward warping often results in unavoidable artifacts. To address these
limitations, we introduce a new framework that directly estimates the
distortion flow and rectifies the RS image with the backward warping operation.
More specifically, we first propose a global correlation-based flow attention
mechanism to estimate the initial distortion flow and GS feature jointly, which
are then refined by the following coarse-to-fine decoder layers. Additionally,
a multi-distortion flow prediction strategy is integrated to mitigate the issue
of inaccurate flow estimation further. Experimental results validate the
effectiveness of the proposed method, which outperforms state-of-the-art
approaches on various benchmarks while maintaining high efficiency. The project
is available at \url{https://github.com/ljzycmd/DFRSC}.",CVPR
"Character animation in real-world scenarios necessitates a variety of
constraints, such as trajectories, key-frames, interactions, etc. Existing
methodologies typically treat single or a finite set of these constraint(s) as
separate control tasks. They are often specialized, and the tasks they address
are rarely extendable or customizable. We categorize these as solutions to the
close-set motion control problem. In response to the complexity of practical
motion control, we propose and attempt to solve the open-set motion control
problem. This problem is characterized by an open and fully customizable set of
motion control tasks. To address this, we introduce a new paradigm,
programmable motion generation. In this paradigm, any given motion control task
is broken down into a combination of atomic constraints. These constraints are
then programmed into an error function that quantifies the degree to which a
motion sequence adheres to them. We utilize a pre-trained motion generation
model and optimize its latent code to minimize the error function of the
generated motion. Consequently, the generated motion not only inherits the
prior of the generative model but also satisfies the required constraints.
Experiments show that we can generate high-quality motions when addressing a
wide range of unseen tasks. These tasks encompass motion control by motion
dynamics, geometric constraints, physical laws, interactions with scenes,
objects or the character own body parts, etc. All of these are achieved in a
unified approach, without the need for ad-hoc paired training data collection
or specialized network designs. During the programming of novel tasks, we
observed the emergence of new skills beyond those of the prior model. With the
assistance of large language models, we also achieved automatic programming. We
hope that this work will pave the way for the motion control of general AI
agents.",CVPR
"Monocular depth estimation is a fundamental computer vision task. Recovering
3D depth from a single image is geometrically ill-posed and requires scene
understanding, so it is not surprising that the rise of deep learning has led
to a breakthrough. The impressive progress of monocular depth estimators has
mirrored the growth in model capacity, from relatively modest CNNs to large
Transformer architectures. Still, monocular depth estimators tend to struggle
when presented with images with unfamiliar content and layout, since their
knowledge of the visual world is restricted by the data seen during training,
and challenged by zero-shot generalization to new domains. This motivates us to
explore whether the extensive priors captured in recent generative diffusion
models can enable better, more generalizable depth estimation. We introduce
Marigold, a method for affine-invariant monocular depth estimation that is
derived from Stable Diffusion and retains its rich prior knowledge. The
estimator can be fine-tuned in a couple of days on a single GPU using only
synthetic training data. It delivers state-of-the-art performance across a wide
range of datasets, including over 20% performance gains in specific cases.
Project page: https://marigoldmonodepth.github.io.",CVPR
"We are living in a world surrounded by diverse and ""smart"" devices with rich
modalities of sensing ability. Conveniently capturing the interactions between
us humans and these objects remains far-reaching. In this paper, we present
I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the
human and object in a novel setting: using a minimal amount of RGB camera and
object-mounted Inertial Measurement Unit (IMU). It combines general motion
inference and category-aware refinement. For the former, we introduce a
holistic human-object tracking method to fuse the IMU signals and the RGB
stream and progressively recover the human motions and subsequently the
companion object motions. For the latter, we tailor a category-aware motion
diffusion model, which is conditioned on both the raw IMU observations and the
results from the previous stage under over-parameterization representation. It
significantly refines the initial results and generates vivid body, hand, and
object motions. Moreover, we contribute a large dataset with ground truth human
and object motions, dense RGB inputs, and rich object-mounted IMU measurements.
Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid
capture setting. Our dataset and code will be released to the community.",CVPR
"Despite significant recent progress in the field of autonomous driving,
modern methods still struggle and can incur serious accidents when encountering
long-tail unforeseen events and challenging urban scenarios. On the one hand,
large language models (LLM) have shown impressive reasoning capabilities that
approach ""Artificial General Intelligence"". On the other hand, previous
autonomous driving methods tend to rely on limited-format inputs (e.g. sensor
data and navigation waypoints), restricting the vehicle's ability to understand
language information and interact with humans. To this end, this paper
introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous
driving framework. LMDrive uniquely processes and integrates multi-modal sensor
data with natural language instructions, enabling interaction with humans and
navigation software in realistic instructional settings. To facilitate further
research in language-based closed-loop autonomous driving, we also publicly
release the corresponding dataset which includes approximately 64K
instruction-following data clips, and the LangAuto benchmark that tests the
system's ability to handle complex instructions and challenging driving
scenarios. Extensive closed-loop experiments are conducted to demonstrate
LMDrive's effectiveness. To the best of our knowledge, we're the very first
work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes,
models, and datasets can be found at https://github.com/opendilab/LMDrive",CVPR
"Cooperative perception offers several benefits for enhancing the capabilities
of autonomous vehicles and improving road safety. Using roadside sensors in
addition to onboard sensors increases reliability and extends the sensor range.
External sensors offer higher situational awareness for automated vehicles and
prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion
model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object
detection and tracking task. Our dataset contains 2,000 labeled point clouds
and 5,000 labeled images from five roadside and four onboard sensors. It
includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled
eight categories and covered occlusion scenarios with challenging driving
maneuvers, like traffic violations, near-miss events, overtaking, and U-turns.
Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion
model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR
fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit
publicly available on our website:
https://tum-traffic-dataset.github.io/tumtraf-v2x.",CVPR
"This paper investigates the effective utilization of unlabeled data for
large-area cross-view geo-localization (CVGL), encompassing both unsupervised
and semi-supervised settings. Common approaches to CVGL rely on
ground-satellite image pairs and employ label-driven supervised training.
However, the cost of collecting precise cross-view image pairs hinders the
deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more
challenging to handle the significant imaging and spatial gaps between ground
and satellite images. To this end, we propose an unsupervised framework
including a cross-view projection to guide the model for retrieving initial
pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by
leveraging the fact that ``the perfectly paired ground-satellite image is
located in a unique and identical scene"". The framework exhibits competitive
performance compared with supervised works on three open-source benchmarks. Our
code and models will be released on https://github.com/liguopeng0923/UCVGL.",CVPR
"Regression tasks in computer vision, such as age estimation or counting, are
often formulated into classification by quantizing the target space into
classes. Yet real-world data is often imbalanced -- the majority of training
samples lie in a head range of target values, while a minority of samples span
a usually larger tail range. By selecting the class quantization, one can
adjust imbalanced regression targets into balanced classification outputs,
though there are trade-offs in balancing classification accuracy and
quantization error. To improve regression performance over the entire range of
data, we propose to construct hierarchical classifiers for solving imbalanced
regression tasks. The fine-grained classifiers limit the quantization error
while being modulated by the coarse predictions to ensure high accuracy.
Standard hierarchical classification approaches, however, when applied to the
regression problem, fail to ensure that predicted ranges remain consistent
across the hierarchy. As such, we propose a range-preserving distillation
process that can effectively learn a single classifier from the set of
hierarchical classifiers. Our novel hierarchical classification adjustment
(HCA) for imbalanced regression shows superior results on three diverse tasks:
age estimation, crowd counting and depth estimation. We will release the source
code upon acceptance.",CVPR
"The integration of an ensemble of deep learning models has been extensively
explored to enhance defense against adversarial attacks. The diversity among
sub-models increases the attack cost required to deceive the majority of the
ensemble, thereby improving the adversarial robustness. While existing
approaches mainly center on increasing diversity in feature representations or
dispersion of first-order gradients with respect to input, the limited
correlation between these diversity metrics and adversarial robustness
constrains the performance of ensemble adversarial defense. In this work, we
aim to enhance ensemble diversity by reducing attack transferability. We
identify second-order gradients, which depict the loss curvature, as a key
factor in adversarial robustness. Computing the Hessian matrix involved in
second-order gradients is computationally expensive. To address this, we
approximate the Hessian-vector product using differential approximation. Given
that low curvature provides better robustness, our ensemble model was designed
to consider the influence of curvature among different sub-models. We introduce
a novel regularizer to train multiple more-diverse low-curvature network
models. Extensive experiments across various datasets demonstrate that our
ensemble model exhibits superior robustness against a range of attacks,
underscoring the effectiveness of our approach.",CVPR
"While large-scale pre-trained text-to-image models can synthesize diverse and
high-quality human-centered images, novel challenges arise with a nuanced task
of ""identity fine editing"": precisely modifying specific features of a subject
while maintaining its inherent identity and context. Existing personalization
methods either require time-consuming optimization or learning additional
encoders, adept in ""identity re-contextualization"". However, they often
struggle with detailed and sensitive tasks like human face editing. To address
these challenges, we introduce DreamSalon, a noise-guided, staged-editing
framework, uniquely focusing on detailed image manipulations and
identity-context preservation. By discerning editing and boosting stages via
the frequency and gradient of predicted noises, DreamSalon first performs
detailed manipulations on specific features in the editing stage, guided by
high-frequency information, and then employs stochastic denoising in the
boosting stage to improve image quality. For more precise editing, DreamSalon
semantically mixes source and target textual prompts, guided by differences in
their embedding covariances, to direct the model's focus on specific
manipulation areas. Our experiments demonstrate DreamSalon's ability to
efficiently and faithfully edit fine details on human faces, outperforming
existing methods both qualitatively and quantitatively.",CVPR
"Despite noise and caption quality having been acknowledged as important
factors impacting vision-language contrastive pre-training, in this paper, we
show that the full potential of improving the training process by addressing
such issues is yet to be realized. Specifically, we firstly study and analyze
two issues affecting training: incorrect assignment of negative pairs, and low
caption quality and diversity. Then, we devise effective solutions for
addressing both problems, which essentially require training with multiple true
positive pairs. Finally, we propose training with sigmoid loss to address such
a requirement. We show very large gains over the current state-of-the-art for
both image recognition ($\sim +6\%$ on average over 11 datasets) and image
retrieval ($\sim +19\%$ on Flickr30k and $\sim +15\%$ on MSCOCO).",CVPR
"Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Despite the strong performance of
Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new
classes often results in the overwriting of old ones. Excessive modification of
the network causes forgetting, while minimal adjustments lead to an inadequate
fit for new classes. As a result, it is desired to figure out a way of
efficient model updating without harming former knowledge. In this paper, we
propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model
updating without conflict, we train a distinct lightweight adapter module for
each new task, aiming to create task-specific subspaces. These adapters span a
high-dimensional feature space, enabling joint decision-making across multiple
subspaces. As data evolves, the expanding subspaces render the old class
classifiers incompatible with new-stage spaces. Correspondingly, we design a
semantic-guided prototype complement strategy that synthesizes old classes' new
features without using any old class instance. Extensive experiments on seven
benchmark datasets verify EASE's state-of-the-art performance. Code is
available at: https://github.com/sun-hailong/CVPR24-Ease",CVPR
"This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. Demo page is available on
https://choijeongsoo.github.io/av2av.",CVPR
"Scene graph generation aims to capture detailed spatial and semantic
relationships between objects in an image, which is challenging due to
incomplete labelling, long-tailed relationship categories, and relational
semantic overlap. Existing Transformer-based methods either employ distinct
queries for objects and predicates or utilize holistic queries for relation
triplets and hence often suffer from limited capacity in learning low-frequency
relationships. In this paper, we present a new Transformer-based method, called
DSGG, that views scene graph detection as a direct graph prediction problem
based on a unique set of graph-aware queries. In particular, each graph-aware
query encodes a compact representation of both the node and all of its
relations in the graph, acquired through the utilization of a relaxed sub-graph
matching during the training process. Moreover, to address the problem of
relational semantic overlap, we utilize a strategy for relation distillation,
aiming to efficiently learn multiple instances of semantic relationships.
Extensive experiments on the VG and the PSG datasets show that our model
achieves state-of-the-art results, showing a significant improvement of 3.5\%
and 6.7\% in mR@50 and mR@100 for the scene-graph generation task and achieves
an even more substantial improvement of 8.5\% and 10.3\% in mR@50 and mR@100
for the panoptic scene graph generation task. Code is available at
\url{https://github.com/zeeshanhayder/DSGG}.",CVPR
"Anchor-based multi-view graph clustering (AMVGC) has received abundant
attention owing to its high efficiency and the capability to capture
complementary structural information across multiple views. Intuitively, a
high-quality anchor graph plays an essential role in the success of AMVGC.
However, the existing AMVGC methods only consider single-structure information,
i.e., local or global structure, which provides insufficient information for
the learning task. To be specific, the over-scattered global structure leads to
learned anchors failing to depict the cluster partition well. In contrast, the
local structure with an improper similarity measure results in potentially
inaccurate anchor assignment, ultimately leading to sub-optimal clustering
performance. To tackle the issue, we propose a novel anchor-based multi-view
graph clustering framework termed Efficient Multi-View Graph Clustering with
Local and Global Structure Preservation (EMVGC-LG). Specifically, a unified
framework with a theoretical guarantee is designed to capture local and global
information. Besides, EMVGC-LG jointly optimizes anchor construction and graph
learning to enhance the clustering quality. In addition, EMVGC-LG inherits the
linear complexity of existing AMVGC methods respecting the sample number, which
is time-economical and scales well with the data size. Extensive experiments
demonstrate the effectiveness and efficiency of our proposed method.",CVPR
"Human-centric video frame interpolation has great potential for improving
people's entertainment experiences and finding commercial applications in the
sports analysis industry, e.g., synthesizing slow-motion videos. Although there
are multiple benchmark datasets available in the community, none of them is
dedicated for human-centric scenarios. To bridge this gap, we introduce
SportsSloMo, a benchmark consisting of more than 130K video clips and 1M video
frames of high-resolution ($\geq$720p) slow-motion sports videos crawled from
YouTube. We re-train several state-of-the-art methods on our benchmark, and the
results show a decrease in their accuracy compared to other datasets. It
highlights the difficulty of our benchmark and suggests that it poses
significant challenges even for the best-performing methods, as human bodies
are highly deformable and occlusions are frequent in sports videos. To improve
the accuracy, we introduce two loss terms considering the human-aware priors,
where we add auxiliary supervision to panoptic segmentation and human keypoints
detection, respectively. The loss terms are model agnostic and can be easily
plugged into any video frame interpolation approaches. Experimental results
validate the effectiveness of our proposed loss terms, leading to consistent
performance improvement over 5 existing models, which establish strong baseline
models on our benchmark. The dataset and code can be found at:
https://neu-vi.github.io/SportsSlomo/.",CVPR
"Novel view synthesis aims to generate new view images of a given view image
collection. Recent attempts address this problem relying on 3D geometry priors
(e.g., shapes, sizes, and positions) learned from multi-view images. However,
such methods encounter the following limitations: 1) they require a set of
multi-view images as training data for a specific scene (e.g., face, car or
chair), which is often unavailable in many real-world scenarios; 2) they fail
to extract the geometry priors from single-view images due to the lack of
multi-view supervision. In this paper, we propose a Geometry-enhanced NeRF
(G-NeRF), which seeks to enhance the geometry priors by a geometry-guided
multi-view synthesis approach, followed by a depth-aware training. In the
synthesis process, inspired that existing 3D GAN models can unconditionally
synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D
GAN models, such as EG3D, as a free source to provide geometry priors through
synthesizing multi-view data. Simultaneously, to further improve the geometry
quality of the synthetic data, we introduce a truncation method to effectively
sample latent codes within 3D GAN models. To tackle the absence of multi-view
supervision for single-view images, we design the depth-aware training
approach, incorporating a depth-aware discriminator to guide geometry priors
through depth maps. Experiments demonstrate the effectiveness of our method in
terms of both qualitative and quantitative results.",CVPR
"The field of generative image inpainting and object insertion has made
significant progress with the recent advent of latent diffusion models.
Utilizing a precise object mask can greatly enhance these applications.
However, due to the challenges users encounter in creating high-fidelity masks,
there is a tendency for these methods to rely on more coarse masks (e.g.,
bounding box) for these applications. This results in limited control and
compromised background content preservation. To overcome these limitations, we
introduce SmartMask, which allows any novice user to create detailed masks for
precise object insertion. Combined with a ControlNet-Inpaint model, our
experiments demonstrate that SmartMask achieves superior object insertion
quality, preserving the background content more effectively than previous
methods. Notably, unlike prior works the proposed approach can also be used
even without user-mask guidance, which allows it to perform mask-free object
insertion at diverse positions and scales. Furthermore, we find that when used
iteratively with a novel instruction-tuning based planning model, SmartMask can
be used to design detailed layouts from scratch. As compared with user-scribble
based layout design, we observe that SmartMask allows for better quality
outputs with layout-to-image generation methods. Project page is available at
https://smartmask-gen.github.io",CVPR
"Multimodal large language models (MLLMs) have gained significant attention
due to their strong multimodal understanding capability. However, existing
works rely heavily on modality-specific encoders, which usually differ in
architecture and are limited to common modalities. In this paper, we present
OneLLM, an MLLM that aligns eight modalities to language using a unified
framework. We achieve this through a unified multimodal encoder and a
progressive multimodal alignment pipeline. In detail, we first train an image
projection module to connect a vision encoder with LLM. Then, we build a
universal projection module (UPM) by mixing multiple image projection modules
and dynamic routing. Finally, we progressively align more modalities to LLM
with the UPM. To fully leverage the potential of OneLLM in following
instructions, we also curated a comprehensive multimodal instruction dataset,
including 2M items from image, audio, video, point cloud, depth/normal map, IMU
and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,
encompassing tasks such as multimodal captioning, question answering and
reasoning, where it delivers excellent performance. Code, data, model and
online demo are available at https://github.com/csuhan/OneLLM",CVPR
"Interpreting camera data is key for autonomously acting systems, such as
autonomous vehicles. Vision systems that operate in real-world environments
must be able to understand their surroundings and need the ability to deal with
novel situations. This paper tackles open-world semantic segmentation, i.e.,
the variant of interpreting image data in which objects occur that have not
been seen during training. We propose a novel approach that performs accurate
closed-world semantic segmentation and, at the same time, can identify new
categories without requiring any additional training data. Our approach
additionally provides a similarity measure for every newly discovered class in
an image to a known category, which can be useful information in downstream
tasks such as planning or mapping. Through extensive experiments, we show that
our model achieves state-of-the-art results on classes known from training data
as well as for anomaly segmentation and can distinguish between different
unknown classes.",CVPR
"Recently, integrating video foundation models and large language models to
build a video understanding system can overcome the limitations of specific
pre-defined vision tasks. Yet, existing systems can only handle videos with
very few frames. For long videos, the computation complexity, memory cost, and
long-term temporal connection impose additional challenges. Taking advantage of
the Atkinson-Shiffrin memory model, with tokens in Transformers being employed
as the carriers of memory in combination with our specially designed memory
mechanism, we propose the MovieChat to overcome these challenges. MovieChat
achieves state-of-the-art performance in long video understanding, along with
the released MovieChat-1K benchmark with 1K long video and 14K manual
annotations for validation of the effectiveness of our method.",CVPR
"Multi-modality foundation models, as represented by GPT-4V, have brought a
new paradigm for low-level visual perception and understanding tasks, that can
respond to a broad range of natural human instructions in a model. While
existing foundation models have shown exciting potentials on low-level visual
tasks, their related abilities are still preliminary and need to be improved.
In order to enhance these models, we conduct a large-scale subjective
experiment collecting a vast number of real human feedbacks on low-level
vision. Each feedback follows a pathway that starts with a detailed description
on the low-level visual appearance (*e.g. clarity, color, brightness* of an
image, and ends with an overall conclusion, with an average length of 45 words.
The constructed **Q-Pathway** dataset includes 58K detailed human feedbacks on
18,973 images with diverse low-level appearance. Moreover, to enable foundation
models to robustly respond to diverse types of questions, we design a
GPT-participated conversion to process these feedbacks into diverse-format 200K
instruction-response pairs. Experimental results indicate that the
**Q-Instruct** consistently elevates low-level perception and understanding
abilities across several foundational models. We anticipate that our datasets
can pave the way for a future that general intelligence can perceive,
understand low-level visual appearance and evaluate visual quality like a
human. Our dataset, model zoo, and demo is published at:
https://q-future.github.io/Q-Instruct.",CVPR
"Although diffusion models are rising as a powerful solution for blind face
restoration, they are criticized for two problems: 1) slow training and
inference speed, and 2) failure in preserving identity and recovering
fine-grained facial details. In this work, we propose WaveFace to solve the
problems in the frequency domain, where low- and high-frequency components
decomposed by wavelet transformation are considered individually to maximize
authenticity as well as efficiency. The diffusion model is applied to recover
the low-frequency component only, which presents general information of the
original image but 1/16 in size. To preserve the original identity, the
generation is conditioned on the low-frequency component of low-quality images
at each denoising step. Meanwhile, high-frequency components at multiple
decomposition levels are handled by a unified network, which recovers complex
facial details in a single step. Evaluations on four benchmark datasets show
that: 1) WaveFace outperforms state-of-the-art methods in authenticity,
especially in terms of identity preservation, and 2) authentic images are
restored with the efficiency 10x faster than existing diffusion model-based BFR
methods.",CVPR
"Deep learning has achieved remarkable progress in various applications,
heightening the importance of safeguarding the intellectual property (IP) of
well-trained models. It entails not only authorizing usage but also ensuring
the deployment of models in authorized data domains, i.e., making models
exclusive to certain target domains. Previous methods necessitate concurrent
access to source training data and target unauthorized data when performing IP
protection, making them risky and inefficient for decentralized private data.
In this paper, we target a practical setting where only a well-trained source
model is available and investigate how we can realize IP protection. To achieve
this, we propose a novel MAsk Pruning (MAP) framework. MAP stems from an
intuitive hypothesis, i.e., there are target-related parameters in a
well-trained model, locating and pruning them is the key to IP protection.
Technically, MAP freezes the source model and learns a target-specific binary
mask to prevent unauthorized data usage while minimizing performance
degradation on authorized data. Moreover, we introduce a new metric aimed at
achieving a better balance between source and target performance degradation.
To verify the effectiveness and versatility, we have evaluated MAP in a variety
of scenarios, including vanilla source-available, practical source-free, and
challenging data-free. Extensive experiments indicate that MAP yields new
state-of-the-art performance.",CVPR
"Foundation segmentation models, while powerful, pose a significant risk: they
enable users to effortlessly extract any objects from any digital content with
a single click, potentially leading to copyright infringement or malicious
misuse. To mitigate this risk, we introduce a new task ""Anything Unsegmentable""
to grant any image ""the right to be unsegmented"". The ambitious pursuit of the
task is to achieve highly transferable adversarial attacks against all
prompt-based segmentation models, regardless of model parameterizations and
prompts. We highlight the non-transferable and heterogeneous nature of
prompt-specific adversarial noises. Our approach focuses on disrupting image
encoder features to achieve prompt-agnostic attacks. Intriguingly, targeted
feature attacks exhibit better transferability compared to untargeted ones,
suggesting the optimal update direction aligns with the image manifold. Based
on the observations, we design a novel attack named Unsegment Anything by
Simulating Deformation (UAD). Our attack optimizes a differentiable deformation
function to create a target deformed image, which alters structural information
while preserving achievable feature distance by adversarial example. Extensive
experiments verify the effectiveness of our approach, compromising a variety of
promptable segmentation models with different architectures and prompt
interfaces. We release the code at
https://github.com/jiahaolu97/anything-unsegmentable.",CVPR
"Researchers in natural science need reliable methods for quantifying animal
behavior. Recently, numerous computer vision methods emerged to automate the
process. However, observing wild species at remote locations remains a
challenging task due to difficult lighting conditions and constraints on power
supply and data storage. Event cameras offer unique advantages for
battery-dependent remote monitoring due to their low power consumption and high
dynamic range capabilities. We use this novel sensor to quantify a behavior in
Chinstrap penguins called ecstatic display. We formulate the problem as a
temporal action detection task, determining the start and end times of the
behavior. For this purpose, we recorded a colony of breeding penguins in
Antarctica for several weeks and labeled event data on 16 nests. The developed
method consists of a generator of candidate time intervals (proposals) and a
classifier of the actions within them. The experiments show that the event
cameras' natural response to motion is effective for continuous behavior
monitoring and detection, reaching a mean average precision (mAP) of 58% (which
increases to 63% in good weather conditions). The results also demonstrate the
robustness against various lighting conditions contained in the challenging
dataset. The low-power capabilities of the event camera allow it to record
significantly longer than with a conventional camera. This work pioneers the
use of event cameras for remote wildlife observation, opening new
interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/",CVPR
"Automatic text-to-3D generation that combines Score Distillation Sampling
(SDS) with the optimization of volume rendering has achieved remarkable
progress in synthesizing realistic 3D objects. Yet most existing text-to-3D
methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the
Janus issue, since it is hard to explicitly integrate 3D priors into implicit
3D representations. Besides, it is usually time-consuming for them to generate
elaborate 3D models with rich colors. In response, this paper proposes GSGEN, a
novel method that adopts Gaussian Splatting, a recent state-of-the-art
representation, to text-to-3D generation. GSGEN aims at generating high-quality
3D objects and addressing existing shortcomings by exploiting the explicit
nature of Gaussian Splatting that enables the incorporation of 3D prior.
Specifically, our method adopts a progressive optimization strategy, which
includes a geometry optimization stage and an appearance refinement stage. In
geometry optimization, a coarse representation is established under 3D point
cloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a
sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians
undergo an iterative appearance refinement to enrich texture details. In this
stage, we increase the number of Gaussians by compactness-based densification
to enhance continuity and improve fidelity. With these designs, our approach
can generate 3D assets with delicate details and accurate geometry. Extensive
evaluations demonstrate the effectiveness of our method, especially for
capturing high-frequency components. Our code is available at
https://github.com/gsgen3d/gsgen",CVPR
"Parameter-efficient transfer learning (PETL), i.e., fine-tuning a small
portion of parameters, is an effective strategy for adapting pre-trained models
to downstream domains. To further reduce the memory demand, recent PETL works
focus on the more valuable memory-efficient characteristic. In this paper, we
argue that the scalability, adaptability, and generalizability of
state-of-the-art methods are hindered by structural dependency and pertinency
on specific pre-trained backbones. To this end, we propose a new
memory-efficient PETL strategy, Universal Parallel Tuning (UniPT), to mitigate
these weaknesses. Specifically, we facilitate the transfer process via a
lightweight and learnable parallel network, which consists of: 1) A parallel
interaction module that decouples the sequential connections and processes the
intermediate activations detachedly from the pre-trained network. 2) A
confidence aggregation module that learns optimal strategies adaptively for
integrating cross-layer features. We evaluate UniPT with different backbones
(e.g., T5, VSE$\infty$, CLIP4Clip, Clip-ViL, and MDETR) on various
vision-and-language and pure NLP tasks. Extensive ablations on 18 datasets have
validated that UniPT can not only dramatically reduce memory consumption and
outperform the best competitor, but also achieve competitive performance over
other plain PETL methods with lower training memory overhead. Our code is
publicly available at: https://github.com/Paranioar/UniPT.",CVPR
"Refractive Index Tomography is the inverse problem of reconstructing the
continuously-varying 3D refractive index in a scene using 2D projected image
measurements. Although a purely refractive field is not directly visible, it
bends light rays as they travel through space, thus providing a signal for
reconstruction. The effects of such fields appear in many scientific computer
vision settings, ranging from refraction due to transparent cells in microscopy
to the lensing of distant galaxies caused by dark matter in astrophysics.
Reconstructing these fields is particularly difficult due to the complex
nonlinear effects of the refractive field on observed images. Furthermore,
while standard 3D reconstruction and tomography settings typically have access
to observations of the scene from many viewpoints, many refractive index
tomography problem settings only have access to images observed from a single
viewpoint. We introduce a method that leverages prior knowledge of light
sources scattered throughout the refractive medium to help disambiguate the
single-view refractive index tomography problem. We differentiably trace curved
rays through a neural field representation of the refractive field, and
optimize its parameters to best reproduce the observed image. We demonstrate
the efficacy of our approach by reconstructing simulated refractive fields,
analyze the effects of light source distribution on the recovered field, and
test our method on a simulated dark matter mapping problem where we
successfully recover the 3D refractive field caused by a realistic dark matter
distribution.",CVPR
"Multimodal summarization with multimodal output (MSMO) has emerged as a
promising research direction. Nonetheless, numerous limitations exist within
existing public MSMO datasets, including insufficient maintenance, data
inaccessibility, limited size, and the absence of proper categorization, which
pose significant challenges. To address these challenges and provide a
comprehensive dataset for this new direction, we have meticulously curated the
\textbf{MMSum} dataset. Our new dataset features (1) Human-validated summaries
for both video and textual content, providing superior human instruction and
labels for multimodal learning. (2) Comprehensively and meticulously arranged
categorization, spanning 17 principal categories and 170 subcategories to
encapsulate a diverse array of real-world scenarios. (3) Benchmark tests
performed on the proposed dataset to assess various tasks and methods,
including \textit{video summarization}, \textit{text summarization}, and
\textit{multimodal summarization}. To champion accessibility and collaboration,
we will release the \textbf{MMSum} dataset and the data collection tool as
fully open-source resources, fostering transparency and accelerating future
developments. Our project website can be found
at~\url{https://mmsum-dataset.github.io/}",CVPR
"The lightweight ""local-match-global"" matching introduced by SRe2L
successfully creates a distilled dataset with comprehensive information on the
full 224x224 ImageNet-1k. However, this one-sided approach is limited to a
particular backbone, layer, and statistics, which limits the improvement of the
generalization of a distilled dataset. We suggest that sufficient and various
""local-match-global"" matching are more precise and effective than a single one
and has the ability to create a distilled dataset with richer information and
better generalization. We call this perspective ""generalized matching"" and
propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this
work, which aims to create a synthetic dataset with densities, ensuring
consistency with the complete dataset across various backbones, layers, and
statistics. As experimentally demonstrated, G-VBSM is the first algorithm to
obtain strong performance across both small-scale and large-scale datasets.
Specifically, G-VBSM achieves a performance of 38.7% on CIFAR-100 with
128-width ConvNet, 47.6% on Tiny-ImageNet with ResNet18, and 31.4% on the full
224x224 ImageNet-1k with ResNet18, under images per class (IPC) 10, 50, and 10,
respectively. These results surpass all SOTA methods by margins of 3.9%, 6.5%,
and 10.1%, respectively.",CVPR
"Point-based interactive editing serves as an essential tool to complement the
controllability of existing generative models. A concurrent work,
DragDiffusion, updates the diffusion latent map in response to user inputs,
causing global latent map alterations. This results in imprecise preservation
of the original content and unsuccessful editing due to gradient vanishing. In
contrast, we present DragNoise, offering robust and accelerated editing without
retracing the latent map. The core rationale of DragNoise lies in utilizing the
predicted noise output of each U-Net as a semantic editor. This approach is
grounded in two critical observations: firstly, the bottleneck features of
U-Net inherently possess semantically rich features ideal for interactive
editing; secondly, high-level semantics, established early in the denoising
process, show minimal variation in subsequent stages. Leveraging these
insights, DragNoise edits diffusion semantics in a single denoising step and
efficiently propagates these changes, ensuring stability and efficiency in
diffusion editing. Comparative experiments reveal that DragNoise achieves
superior control and semantic retention, reducing the optimization time by over
50% compared to DragDiffusion. Our codes are available at
https://github.com/haofengl/DragNoise.",CVPR
"Document Presentation Attack Detection (DPAD) is an important measure in
protecting the authenticity of a document image. However, recent DPAD methods
demand additional resources, such as manual effort in collecting additional
data or knowing the parameters of acquisition devices. This work proposes a
DPAD method based on multi-modal disentangled traces (MMDT) without the above
drawbacks. We first disentangle the recaptured traces by a self-supervised
disentanglement and synthesis network to enhance the generalization capacity in
document images with different contents and layouts. Then, unlike the existing
DPAD approaches that rely only on data in the RGB domain, we propose to
explicitly employ the disentangled recaptured traces as new modalities in the
transformer backbone through adaptive multi-modal adapters to fuse RGB/trace
features efficiently. Visualization of the disentangled traces confirms the
effectiveness of the proposed method in different document contents. Extensive
experiments on three benchmark datasets demonstrate the superiority of our MMDT
method on representing forensic traces of recapturing distortion.",CVPR
"Deep learning has led to a dramatic leap on Single Image Super-Resolution
(SISR) performances in recent years. %Despite the substantial advancement%
While most existing work assumes a simple and fixed degradation model (e.g.,
bicubic downsampling), the research of Blind SR seeks to improve model
generalization ability with unknown degradation. Recently, Kong et al pioneer
the investigation of a more suitable training strategy for Blind SR using
Dropout. Although such method indeed brings substantial generalization
improvements via mitigating overfitting, we argue that Dropout simultaneously
introduces undesirable side-effect that compromises model's capacity to
faithfully reconstruct fine details. We show both the theoretical and
experimental analyses in our paper, and furthermore, we present another easy
yet effective training strategy that enhances the generalization ability of the
model by simply modulating its first and second-order features statistics.
Experimental results have shown that our method could serve as a model-agnostic
regularization and outperforms Dropout on seven benchmark datasets including
both synthetic and real-world scenarios.",CVPR
"Facial action unit (AU) intensity plays a pivotal role in quantifying
fine-grained expression behaviors, which is an effective condition for facial
expression manipulation. However, publicly available datasets containing
intensity annotations for multiple AUs remain severely limited, often featuring
a restricted number of subjects. This limitation places challenges to the AU
intensity manipulation in images due to disentanglement issues, leading
researchers to resort to other large datasets with pretrained AU intensity
estimators for pseudo labels. In addressing this constraint and fully
leveraging manual annotations of AU intensities for precise manipulation, we
introduce AUEditNet. Our proposed model achieves impressive intensity
manipulation across 12 AUs, trained effectively with only 18 subjects.
Utilizing a dual-branch architecture, our approach achieves comprehensive
disentanglement of facial attributes and identity without necessitating
additional loss functions or implementing with large batch sizes. This approach
offers a potential solution to achieve desired facial attribute editing despite
the dataset's limited subject count. Our experiments demonstrate AUEditNet's
superior accuracy in editing AU intensities, affirming its capability in
disentangling facial attributes and identity within a limited subject pool.
AUEditNet allows conditioning by either intensity values or target images,
eliminating the need for constructing AU combinations for specific facial
expression synthesis. Moreover, AU intensity estimation, as a downstream task,
validates the consistency between real and edited images, confirming the
effectiveness of our proposed AU intensity manipulation method.",CVPR
"Diffusion models have demonstrated exceptional efficacy in various generative
applications. While existing models focus on minimizing a weighted sum of
denoising score matching losses for data distribution modeling, their training
primarily emphasizes instance-level optimization, overlooking valuable
structural information within each mini-batch, indicative of pair-wise
relationships among samples. To address this limitation, we introduce
Structure-guided Adversarial training of Diffusion Models (SADM). In this
pioneering approach, we compel the model to learn manifold structures between
samples in each training batch. To ensure the model captures authentic manifold
structures in the data distribution, we advocate adversarial training of the
diffusion generator against a novel structure discriminator in a minimax game,
distinguishing real manifold structures from the generated ones. SADM
substantially improves existing diffusion transformers (DiT) and outperforms
existing methods in image generation and cross-domain fine-tuning tasks across
12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on
ImageNet for class-conditional image generation at resolutions of 256x256 and
512x512, respectively.",CVPR
"3D visual grounding is a challenging task that often requires direct and
dense supervision, notably the semantic label for each object in the scene. In
this paper, we instead study the naturally supervised setting that learns from
only 3D scene and QA pairs, where prior works underperform. We propose the
Language-Regularized Concept Learner (LARC), which uses constraints from
language as regularization to significantly improve the accuracy of
neuro-symbolic concept learners in the naturally supervised setting. Our
approach is based on two core insights: the first is that language constraints
(e.g., a word's relation to another) can serve as effective regularization for
structured representations in neuro-symbolic models; the second is that we can
query large language models to distill such constraints from language
properties. We show that LARC improves performance of prior works in naturally
supervised 3D visual grounding, and demonstrates a wide range of 3D visual
reasoning capabilities-from zero-shot composition, to data efficiency and
transferability. Our method represents a promising step towards regularizing
structured visual reasoning frameworks with language-based priors, for learning
in settings without dense supervision.",CVPR
"Joint camera pose and dense geometry estimation from a set of images or a
monocular video remains a challenging problem due to its computational
complexity and inherent visual ambiguities. Most dense incremental
reconstruction systems operate directly on image pixels and solve for their 3D
positions using multi-view geometry cues. Such pixel-level approaches suffer
from ambiguities or violations of multi-view consistency (e.g. caused by
textureless or specular surfaces).
  We address this issue with a new image representation which we call a
SuperPrimitive. SuperPrimitives are obtained by splitting images into
semantically correlated local regions and enhancing them with estimated surface
normal directions, both of which are predicted by state-of-the-art single image
neural networks. This provides a local geometry estimate per SuperPrimitive,
while their relative positions are adjusted based on multi-view observations.
  We demonstrate the versatility of our new representation by addressing three
3D reconstruction tasks: depth completion, few-view structure from motion, and
monocular dense visual odometry.",CVPR
"CLIP showcases exceptional cross-modal matching capabilities due to its
training on image-text contrastive learning tasks. However, without specific
optimization for unimodal scenarios, its performance in single-modality feature
extraction might be suboptimal. Despite this, some studies have directly used
CLIP's image encoder for tasks like few-shot classification, introducing a
misalignment between its pre-training objectives and feature extraction
methods. This inconsistency can diminish the quality of the image's feature
representation, adversely affecting CLIP's effectiveness in target tasks. In
this paper, we view text features as precise neighbors of image features in
CLIP's space and present a novel CrOss-moDal nEighbor Representation(CODER)
based on the distance structure between images and their neighbor texts. This
feature extraction method aligns better with CLIP's pre-training objectives,
thereby fully leveraging CLIP's robust cross-modal capabilities. The key to
construct a high-quality CODER lies in how to create a vast amount of
high-quality and diverse texts to match with images. We introduce the Auto Text
Generator(ATG) to automatically generate the required texts in a data-free and
training-free manner. We apply CODER to CLIP's zero-shot and few-shot image
classification tasks. Experiment results across various datasets and models
confirm CODER's effectiveness. Code is available
at:https://github.com/YCaigogogo/CVPR24-CODER.",CVPR
"Volumetric optical microscopy using non-diffracting beams enables rapid
imaging of 3D volumes by projecting them axially to 2D images but lacks crucial
depth information. Addressing this, we introduce MicroDiffusion, a pioneering
tool facilitating high-quality, depth-resolved 3D volume reconstruction from
limited 2D projections. While existing Implicit Neural Representation (INR)
models often yield incomplete outputs and Denoising Diffusion Probabilistic
Models (DDPM) excel at capturing details, our method integrates INR's
structural coherence with DDPM's fine-detail enhancement capabilities. We
pretrain an INR model to transform 2D axially-projected images into a
preliminary 3D volume. This pretrained INR acts as a global prior guiding
DDPM's generative process through a linear interpolation between INR outputs
and noise inputs. This strategy enriches the diffusion process with structured
3D information, enhancing detail and reducing noise in localized 2D images. By
conditioning the diffusion model on the closest 2D projection, MicroDiffusion
substantially enhances fidelity in resulting 3D reconstructions, surpassing INR
and standard DDPM outputs with unparalleled image quality and structural
fidelity. Our code and dataset are available at
https://github.com/UCSC-VLAA/MicroDiffusion.",CVPR
"Existing one-shot 4D head synthesis methods usually learn from monocular
videos with the aid of 3DMM reconstruction, yet the latter is evenly
challenging which restricts them from reasonable 4D head synthesis. We present
a method to learn one-shot 4D head synthesis via large-scale synthetic data.
The key is to first learn a part-wise 4D generative model from monocular images
via adversarial learning, to synthesize multi-view images of diverse identities
and full motions as training data; then leverage a transformer-based animatable
triplane reconstructor to learn 4D head reconstruction using the synthetic
data. A novel learning strategy is enforced to enhance the generalizability to
real images by disentangling the learning process of 3D reconstruction and
reenactment. Experiments demonstrate our superiority over the prior art.",CVPR
"In this paper, we democratise 3D content creation, enabling precise
generation of 3D shapes from abstract sketches while overcoming limitations
tied to drawing skills. We introduce a novel part-level modelling and alignment
framework that facilitates abstraction modelling and cross-modal
correspondence. Leveraging the same part-level decoder, our approach seamlessly
extends to sketch modelling by establishing correspondence between CLIPasso
edgemaps and projected 3D part regions, eliminating the need for a dataset
pairing human sketches and 3D shapes. Additionally, our method introduces a
seamless in-position editing process as a byproduct of cross-modal part-aligned
modelling. Operating in a low-dimensional implicit space, our approach
significantly reduces computational demands and processing time.",CVPR
"Understanding how the surrounding environment changes is crucial for
performing downstream tasks safely and reliably in autonomous driving
applications. Recent occupancy estimation techniques using only camera images
as input can provide dense occupancy representations of large-scale scenes
based on the current observation. However, they are mostly limited to
representing the current 3D space and do not consider the future state of
surrounding objects along the time axis. To extend camera-only occupancy
estimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark
for camera-only 4D occupancy forecasting, evaluating the surrounding scene
changes in a near future. We build our benchmark based on multiple publicly
available datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5,
which provides sequential occupancy states of general movable and static
objects, as well as their 3D backward centripetal flow. To establish this
benchmark for future research with comprehensive comparisons, we introduce four
baseline types from diverse camera-based perception and prediction
implementations, including a static-world occupancy model, voxelization of
point cloud prediction, 2D-3D instance-based prediction, and our proposed novel
end-to-end 4D occupancy forecasting network. Furthermore, the standardized
evaluation protocol for preset multiple tasks is also provided to compare the
performance of all the proposed baselines on present and future occupancy
estimation with respect to objects of interest in autonomous driving scenarios.
The dataset and our implementation of all four baselines in the proposed
Cam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.",CVPR
"The diffusion-based text-to-image model harbors immense potential in
transferring reference style. However, current encoder-based approaches
significantly impair the text controllability of text-to-image models while
transferring styles. In this paper, we introduce DEADiff to address this issue
using the following two strategies: 1) a mechanism to decouple the style and
semantics of reference images. The decoupled feature representations are first
extracted by Q-Formers which are instructed by different text descriptions.
Then they are injected into mutually exclusive subsets of cross-attention
layers for better disentanglement. 2) A non-reconstructive learning method. The
Q-Formers are trained using paired images rather than the identical target, in
which the reference image and the ground-truth image are with the same style or
semantics. We show that DEADiff attains the best visual stylization results and
optimal balance between the text controllability inherent in the text-to-image
model and style similarity to the reference image, as demonstrated both
quantitatively and qualitatively. Our project page is
https://tianhao-qi.github.io/DEADiff/.",CVPR
"In this paper, we explore the unique modality of sketch for explainability,
emphasising the profound impact of human strokes compared to conventional
pixel-oriented studies. Beyond explanations of network behavior, we discern the
genuine implications of explainability across diverse downstream sketch-related
tasks. We propose a lightweight and portable explainability solution -- a
seamless plugin that integrates effortlessly with any pre-trained model,
eliminating the need for re-training. Demonstrating its adaptability, we
present four applications: highly studied retrieval and generation, and
completely novel assisted drawing and sketch adversarial attacks. The
centrepiece to our solution is a stroke-level attribution map that takes
different forms when linked with downstream tasks. By addressing the inherent
non-differentiability of rasterisation, we enable explanations at both coarse
stroke level (SLA) and partial stroke level (P-SLA), each with its advantages
for specific downstream tasks.",CVPR
"In this paper, we delve into the creation of one-shot hand avatars, attaining
high-fidelity and drivable hand representations swiftly from a single image.
With the burgeoning domains of the digital human, the need for quick and
personalized hand avatar creation has become increasingly critical. Existing
techniques typically require extensive input data and may prove cumbersome or
even impractical in certain scenarios. To enhance accessibility, we present a
novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed
hand avatars from merely one image. OHTA tackles the inherent difficulties of
this data-limited problem by learning and utilizing data-driven hand priors.
Specifically, we design a hand prior model initially employed for 1) learning
various hand priors with available data and subsequently for 2) the inversion
and fitting of the target identity with prior knowledge. OHTA demonstrates the
capability to create high-fidelity hand avatars with consistent animatable
quality, solely relying on a single image. Furthermore, we illustrate the
versatility of OHTA through diverse applications, encompassing text-to-avatar
conversion, hand editing, and identity latent space manipulation.",CVPR
"Federated learning is a promising framework to train neural networks with
widely distributed data. However, performance degrades heavily with
heterogeneously distributed data. Recent work has shown this is due to the
final layer of the network being most prone to local bias, some finding success
freezing the final layer as an orthogonal classifier. We investigate the
training dynamics of the classifier by applying SVD to the weights motivated by
the observation that freezing weights results in constant singular values. We
find that there are differences when training in IID and non-IID settings.
Based on this finding, we introduce two regularization terms for local training
to continuously emulate IID settings: (1) variance in the dimension-wise
probability distribution of the classifier and (2) hyperspherical uniformity of
representations of the encoder. These regularizations promote local models to
act as if it were in an IID setting regardless of the local data distribution,
thus offsetting proneness to bias while being flexible to the data. On
extensive experiments in both label-shift and feature-shift settings, we verify
that our method achieves highest performance by a large margin especially in
highly non-IID cases in addition to being scalable to larger models and
datasets.",CVPR
"We present WinSyn, a unique dataset and testbed for creating high-quality
synthetic data with procedural modeling techniques. The dataset contains
high-resolution photographs of windows, selected from locations around the
world, with 89,318 individual window crops showcasing diverse geometric and
material characteristics. We evaluate a procedural model by training semantic
segmentation networks on both synthetic and real images and then comparing
their performances on a shared test set of real images. Specifically, we
measure the difference in mean Intersection over Union (mIoU) and determine the
effective number of real images to match synthetic data's training performance.
We design a baseline procedural model as a benchmark and provide 21,290
synthetically generated images. By tuning the procedural model, key factors are
identified which significantly influence the model's fidelity in replicating
real-world scenarios. Importantly, we highlight the challenge of procedural
modeling using current techniques, especially in their ability to replicate the
spatial semantics of real-world scenarios. This insight is critical because of
the potential of procedural models to bridge to hidden scene aspects such as
depth, reflectivity, material properties, and lighting conditions.",CVPR
"Despite the growing demand for accurate surface normal estimation models,
existing methods use general-purpose dense prediction models, adopting the same
inductive biases as other tasks. In this paper, we discuss the inductive biases
needed for surface normal estimation and propose to (1) utilize the per-pixel
ray direction and (2) encode the relationship between neighboring surface
normals by learning their relative rotation. The proposed method can generate
crisp - yet, piecewise smooth - predictions for challenging in-the-wild images
of arbitrary resolution and aspect ratio. Compared to a recent ViT-based
state-of-the-art model, our method shows a stronger generalization ability,
despite being trained on an orders of magnitude smaller dataset. The code is
available at https://github.com/baegwangbin/DSINE.",CVPR
"Dual-Camera Compressed Hyperspectral Imaging (DCCHI) offers the capability to
reconstruct 3D Hyperspectral Image (HSI) by fusing compressive and Panchromatic
(PAN) image, which has shown great potential for snapshot hyperspectral imaging
in practice. In this paper, we introduce a novel DCCHI reconstruction network,
the Intra-Inter Similarity Exploiting Transformer (In2SET). Our key insight is
to make full use of the PAN image to assist the reconstruction. To this end, we
propose using the intra-similarity within the PAN image as a proxy for
approximating the intra-similarity in the original HSI, thereby offering an
enhanced content prior for more accurate HSI reconstruction. Furthermore, we
aim to align the features from the underlying HSI with those of the PAN image,
maintaining semantic consistency and introducing new contextual information for
the reconstruction process. By integrating In2SET into a PAN-guided unrolling
framework, our method substantially enhances the spatial-spectral fidelity and
detail of the reconstructed images, providing a more comprehensive and accurate
depiction of the scene. Extensive experiments conducted on both real and
simulated datasets demonstrate that our approach consistently outperforms
existing state-of-the-art methods in terms of reconstruction quality and
computational complexity. Code will be released.",CVPR
"How do two sets of images differ? Discerning set-level differences is crucial
for understanding model behaviors and analyzing datasets, yet manually sifting
through thousands of images is impractical. To aid in this discovery process,
we explore the task of automatically describing the differences between two
$\textbf{sets}$ of images, which we term Set Difference Captioning. This task
takes in image sets $D_A$ and $D_B$, and outputs a description that is more
often true on $D_A$ than $D_B$. We outline a two-stage approach that first
proposes candidate difference descriptions from image sets and then re-ranks
the candidates by checking how well they can differentiate the two sets. We
introduce VisDiff, which first captions the images and prompts a language model
to propose candidate descriptions, then re-ranks these descriptions using CLIP.
To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image
sets with ground truth difference descriptions. We apply VisDiff to various
domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing
classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing
model failure modes (supervised ResNet), characterizing differences between
generative models (e.g., StableDiffusionV1 and V2), and discovering what makes
images memorable. Using VisDiff, we are able to find interesting and previously
unknown differences in datasets and models, demonstrating its utility in
revealing nuanced insights.",CVPR
"We propose SketchINR, to advance the representation of vector sketches with
implicit neural models. A variable length vector sketch is compressed into a
latent space of fixed dimension that implicitly encodes the underlying shape as
a function of time and strokes. The learned function predicts the $xy$ point
coordinates in a sketch at each time and stroke. Despite its simplicity,
SketchINR outperforms existing representations at multiple tasks: (i) Encoding
an entire sketch dataset into a fixed size latent vector, SketchINR gives
$60\times$ and $10\times$ data compression over raster and vector sketches,
respectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity
representation than other learned vector sketch representations, and is
uniquely able to scale to complex vector sketches such as FS-COCO. (iii)
SketchINR supports parallelisation that can decode/render $\sim$$100\times$
faster than other learned vector representations such as SketchRNN. (iv)
SketchINR, for the first time, emulates the human ability to reproduce a sketch
with varying abstraction in terms of number and complexity of strokes. As a
first look at implicit sketches, SketchINR's compact high-fidelity
representation will support future work in modelling long and complex sketches.",CVPR
"The prevalent approaches of unsupervised 3D object detection follow
cluster-based pseudo-label generation and iterative self-training processes.
However, the challenge arises due to the sparsity of LiDAR scans, which leads
to pseudo-labels with erroneous size and position, resulting in subpar
detection performance. To tackle this problem, this paper introduces a
Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object
detection. CPD first constructs Commonsense Prototype (CProto) characterized by
high-quality bounding box and dense points, based on commonsense intuition.
Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size
prior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely
scanned objects by the geometric knowledge from CProto. CPD outperforms
state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD),
PandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD
and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on
easy and moderate car classes, respectively. These achievements position CPD in
close proximity to fully supervised detectors, highlighting the significance of
our method. The code will be available at https://github.com/hailanyi/CPD.",CVPR
"Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.",CVPR
"Semantic segmentation models, while effective for in-distribution categories,
face challenges in real-world deployment due to encountering
out-of-distribution (OoD) objects. Detecting these OoD objects is crucial for
safety-critical applications. Existing methods rely on anomaly scores, but
choosing a suitable threshold for generating masks presents difficulties and
can lead to fragmentation and inaccuracy. This paper introduces a method to
convert anomaly \textbf{S}core \textbf{T}o segmentation \textbf{M}ask, called
S2M, a simple and effective framework for OoD detection in semantic
segmentation. Unlike assigning anomaly scores to pixels, S2M directly segments
the entire OoD object. By transforming anomaly scores into prompts for a
promptable segmentation model, S2M eliminates the need for threshold selection.
Extensive experiments demonstrate that S2M outperforms the state-of-the-art by
approximately 20% in IoU and 40% in mean F1 score, on average, across various
benchmarks including Fishyscapes, Segment-Me-If-You-Can, and RoadAnomaly
datasets.",CVPR
"We propose a novel self-supervised embedding to learn how actions sound from
narrated in-the-wild egocentric videos. Whereas existing methods rely on
curated data with known audio-visual correspondence, our multimodal
contrastive-consensus coding (MC3) embedding reinforces the associations
between audio, language, and vision when all modality pairs agree, while
diminishing those associations when any one pair does not. We show our approach
can successfully discover how the long tail of human actions sound from
egocentric video, outperforming an array of recent multimodal embedding
techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal
tasks.",CVPR
"Low-resource settings are well-established in natural language processing,
where many languages lack sufficient data for deep learning at scale. However,
low-resource problems are under-explored in computer vision. In this paper, we
address this gap and explore the challenges of low-resource image tasks with
vision foundation models. We first collect a benchmark of genuinely
low-resource image data, covering historic maps, circuit diagrams, and
mechanical drawings. These low-resource settings all share three challenges:
data scarcity, fine-grained differences, and the distribution shift from
natural images to the specialized domain of interest. While existing foundation
models have shown impressive generalizability, we find they cannot transfer
well to our low-resource tasks. To begin to tackle the challenges of
low-resource vision, we introduce one simple baseline per challenge.
Specifically, we i) enlarge the data space by generative models, ii) adopt the
best sub-kernels to encode local regions for fine-grained difference discovery
and iii) learn attention for specialized domains. Experiments on our three
low-resource tasks demonstrate our proposals already provide a better baseline
than transfer learning, data augmentation, and fine-grained methods. This
highlights the unique characteristics and challenges of low-resource vision for
foundation models that warrant further investigation. Project page:
https://xiaobai1217.github.io/Low-Resource-Vision/.",CVPR
"We propose SceneTex, a novel method for effectively generating high-quality
and style-consistent textures for indoor scenes using depth-to-image diffusion
priors. Unlike previous methods that either iteratively warp 2D views onto a
mesh surface or distillate diffusion latent features without accurate geometric
and style cues, SceneTex formulates the texture synthesis task as an
optimization problem in the RGB space where style and geometry consistency are
properly reflected. At its core, SceneTex proposes a multiresolution texture
field to implicitly encode the mesh appearance. We optimize the target texture
via a score-distillation-based objective function in respective RGB renderings.
To further secure the style consistency across views, we introduce a
cross-attention decoder to predict the RGB values by cross-attending to the
pre-sampled reference locations in each instance. SceneTex enables various and
accurate texture synthesis for 3D-FRONT scenes, demonstrating significant
improvements in visual quality and prompt fidelity over the prior texture
generation methods.",CVPR
"Prompt tuning represents a valuable technique for adapting pre-trained
visual-language models (VLM) to various downstream tasks. Recent advancements
in CoOp-based methods propose a set of learnable domain-shared or
image-conditional textual tokens to facilitate the generation of task-specific
textual classifiers. However, those textual tokens have a limited
generalization ability regarding unseen domains, as they cannot dynamically
adjust to the distribution of testing classes. To tackle this issue, we present
a novel Textual-based Class-aware Prompt tuning(TCP) that explicitly
incorporates prior knowledge about classes to enhance their discriminability.
The critical concept of TCP involves leveraging Textual Knowledge Embedding
(TKE) to map the high generalizability of class-level textual knowledge into
class-aware textual tokens. By seamlessly integrating these class-aware prompts
into the Text Encoder, a dynamic class-aware classifier is generated to enhance
discriminability for unseen domains. During inference, TKE dynamically
generates class-aware prompts related to the unseen classes. Comprehensive
evaluations demonstrate that TKE serves as a plug-and-play module effortlessly
combinable with existing methods. Furthermore, TCP consistently achieves
superior performance while demanding less training time.
Code:https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/",CVPR
"Existing photorealistic relightable hand models require extensive
identity-specific observations in different views, poses, and illuminations,
and face challenges in generalizing to natural illuminations and novel
identities. To bridge this gap, we present URHand, the first universal
relightable hand model that generalizes across viewpoints, poses,
illuminations, and identities. Our model allows few-shot personalization using
images captured with a mobile phone, and is ready to be photorealistically
rendered under novel illuminations. To simplify the personalization process
while retaining photorealism, we build a powerful universal relightable prior
based on neural relighting from multi-view images of hands captured in a light
stage with hundreds of identities. The key challenge is scaling the
cross-identity training while maintaining personalized fidelity and sharp
details without compromising generalization under natural illuminations. To
this end, we propose a spatially varying linear lighting model as the neural
renderer that takes physics-inspired shading as input feature. By removing
non-linear activations and bias, our specifically designed lighting model
explicitly keeps the linearity of light transport. This enables single-stage
training from light-stage data while generalizing to real-time rendering under
arbitrary continuous illuminations across diverse identities. In addition, we
introduce the joint learning of a physically based model and our neural
relighting model, which further improves fidelity and generalization. Extensive
experiments show that our approach achieves superior performance over existing
methods in terms of both quality and generalizability. We also demonstrate
quick personalization of URHand from a short phone scan of an unseen identity.",CVPR
"Monocular egocentric 3D human motion capture is a challenging and actively
researched problem. Existing methods use synchronously operating visual sensors
(e.g. RGB cameras) and often fail under low lighting and fast motions, which
can be restricting in many applications involving head-mounted devices. In
response to the existing limitations, this paper 1) introduces a new problem,
i.e., 3D human motion capture from an egocentric monocular event camera with a
fisheye lens, and 2) proposes the first approach to it called EventEgo3D
(EE3D). Event streams have high temporal resolution and provide reliable cues
for 3D human motion capture under high-speed human motions and rapidly changing
illumination. The proposed EE3D framework is specifically tailored for learning
with event streams in the LNES representation, enabling high 3D reconstruction
accuracy. We also design a prototype of a mobile head-mounted device with an
event camera and record a real dataset with event observations and the
ground-truth 3D human poses (in addition to the synthetic dataset). Our EE3D
demonstrates robustness and superior 3D accuracy compared to existing solutions
across various challenging experiments while supporting real-time 3D pose
update rates of 140Hz.",CVPR
"The advances in the Neural Radiance Fields (NeRF) research offer extensive
applications in diverse domains, but protecting their copyrights has not yet
been researched in depth. Recently, NeRF watermarking has been considered one
of the pivotal solutions for safely deploying NeRF-based 3D representations.
However, existing methods are designed to apply only to implicit or explicit
NeRF representations. In this work, we introduce an innovative watermarking
method that can be employed in both representations of NeRF. This is achieved
by fine-tuning NeRF to embed binary messages in the rendering process. In
detail, we propose utilizing the discrete wavelet transform in the NeRF space
for watermarking. Furthermore, we adopt a deferred back-propagation technique
and introduce a combination with the patch-wise loss to improve rendering
quality and bit accuracy with minimum trade-offs. We evaluate our method in
three different aspects: capacity, invisibility, and robustness of the embedded
watermarks in the 2D-rendered images. Our method achieves state-of-the-art
performance with faster training speed over the compared state-of-the-art
methods.",CVPR
"Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the
tracking-by-attention paradigm, utilizing track queries for identity-consistent
detection and object queries for identity-agnostic track spawning.
Tracking-by-attention, however, entangles detection and tracking queries in one
embedding for both the detection and tracking task, which is sub-optimal. Other
approaches resemble the tracking-by-detection paradigm, detecting objects using
decoupled track and detection queries followed by a subsequent association.
These methods, however, do not leverage synergies between the detection and
association task. Combining the strengths of both paradigms, we introduce
ADA-Track, a novel end-to-end framework for 3D MOT from multi-view cameras. We
introduce a learnable data association module based on edge-augmented
cross-attention, leveraging appearance and geometric features. Furthermore, we
integrate this association module into the decoder layer of a DETR-based 3D
detector, enabling simultaneous DETR-like query-to-image cross-attention for
detection and query-to-query cross-attention for data association. By stacking
these decoder layers, queries are refined for the detection and association
task alternately, effectively harnessing the task dependencies. We evaluate our
method on the nuScenes dataset and demonstrate the advantage of our approach
compared to the two previous paradigms. Code is available at
https://github.com/dsx0511/ADA-Track.",CVPR
"Logit knowledge distillation attracts increasing attention due to its
practicality in recent studies. However, it often suffers inferior performance
compared to the feature knowledge distillation. In this paper, we argue that
existing logit-based methods may be sub-optimal since they only leverage the
global logit output that couples multiple semantic knowledge. This may transfer
ambiguous knowledge to the student and mislead its learning. To this end, we
propose a simple but effective method, i.e., Scale Decoupled Distillation
(SDD), for logit knowledge distillation. SDD decouples the global logit output
into multiple local logit outputs and establishes distillation pipelines for
them. This helps the student to mine and inherit fine-grained and unambiguous
logit knowledge. Moreover, the decoupled knowledge can be further divided into
consistent and complementary logit knowledge that transfers the semantic
information and sample ambiguity, respectively. By increasing the weight of
complementary parts, SDD can guide the student to focus more on ambiguous
samples, improving its discrimination ability. Extensive experiments on several
benchmark datasets demonstrate the effectiveness of SDD for wide
teacher-student pairs, especially in the fine-grained classification task. Code
is available at: https://github.com/shicaiwei123/SDD-CVPR2024",CVPR
"Recovering the 3D scene geometry from a single view is a fundamental yet
ill-posed problem in computer vision. While classical depth estimation methods
infer only a 2.5D scene representation limited to the image plane, recent
approaches based on radiance fields reconstruct a full 3D representation.
However, these methods still struggle with occluded regions since inferring
geometry without visual observation requires (i) semantic knowledge of the
surroundings, and (ii) reasoning about spatial context. We propose KYN, a novel
method for single-view scene reconstruction that reasons about semantic and
spatial context to predict each point's density. We introduce a vision-language
modulation module to enrich point features with fine-grained semantic
information. We aggregate point representations across the scene through a
language-guided spatial attention mechanism to yield per-point density
predictions aware of the 3D semantic context. We show that KYN improves 3D
shape recovery compared to predicting density for each 3D point in isolation.
We achieve state-of-the-art results in scene and object reconstruction on
KITTI-360, and show improved zero-shot generalization compared to prior work.
Project page: https://ruili3.github.io/kyn.",CVPR
"Recent Text-to-Image (T2I) generation models such as Stable Diffusion and
Imagen have made significant progress in generating high-resolution images
based on text descriptions. However, many generated images still suffer from
issues such as artifacts/implausibility, misalignment with text descriptions,
and low aesthetic quality. Inspired by the success of Reinforcement Learning
with Human Feedback (RLHF) for large language models, prior works collected
human-provided scores as feedback on generated images and trained a reward
model to improve the T2I generation. In this paper, we enrich the feedback
signal by (i) marking image regions that are implausible or misaligned with the
text, and (ii) annotating which words in the text prompt are misrepresented or
missing on the image. We collect such rich human feedback on 18K generated
images (RichHF-18K) and train a multimodal transformer to predict the rich
feedback automatically. We show that the predicted rich human feedback can be
leveraged to improve image generation, for example, by selecting high-quality
training data to finetune and improve the generative models, or by creating
masks with predicted heatmaps to inpaint the problematic regions. Notably, the
improvements generalize to models (Muse) beyond those used to generate the
images on which human feedback data were collected (Stable Diffusion variants).
The RichHF-18K data set will be released in our GitHub repository:
https://github.com/google-research/google-research/tree/master/richhf_18k.",CVPR
"LiDAR-based 3D object detection plays an essential role in autonomous
driving. Existing high-performing 3D object detectors usually build dense
feature maps in the backbone network and prediction head. However, the
computational costs introduced by the dense feature maps grow quadratically as
the perception range increases, making these models hard to scale up to
long-range detection. Some recent works have attempted to construct fully
sparse detectors to solve this issue; nevertheless, the resulting models either
rely on a complex multi-stage pipeline or exhibit inferior performance. In this
work, we propose SAFDNet, a straightforward yet highly effective architecture,
tailored for fully sparse 3D object detection. In SAFDNet, an adaptive feature
diffusion strategy is designed to address the center feature missing problem.
We conducted extensive experiments on Waymo Open, nuScenes, and Argoverse2
datasets. SAFDNet performed slightly better than the previous SOTA on the first
two datasets but much better on the last dataset, which features long-range
detection, verifying the efficacy of SAFDNet in scenarios where long-range
detection is required. Notably, on Argoverse2, SAFDNet surpassed the previous
best hybrid detector HEDNet by 2.6% mAP while being 2.1x faster, and yielded
2.1% mAP gains over the previous best sparse detector FSDv2 while being 1.3x
faster. The code will be available at https://github.com/zhanggang001/HEDNet.",CVPR
"We investigate a fundamental aspect of machine vision: the measurement of
features, by revisiting clustering, one of the most classic approaches in
machine learning and data analysis. Existing visual feature extractors,
including ConvNets, ViTs, and MLPs, represent an image as rectangular regions.
Though prevalent, such a grid-style paradigm is built upon engineering practice
and lacks explicit modeling of data distribution. In this work, we propose
feature extraction with clustering (FEC), a conceptually elegant yet
surprisingly ad-hoc interpretable neural clustering framework, which views
feature extraction as a process of selecting representatives from data and thus
automatically captures the underlying data distribution. Given an image, FEC
alternates between grouping pixels into individual clusters to abstract
representatives and updating the deep features of pixels with current
representatives. Such an iterative working mechanism is implemented in the form
of several neural layers and the final representatives can be used for
downstream tasks. The cluster assignments across layers, which can be viewed
and inspected by humans, make the forward process of FEC fully transparent and
empower it with promising ad-hoc interpretability. Extensive experiments on
various visual recognition models and tasks verify the effectiveness,
generality, and interpretability of FEC. We expect this work will provoke a
rethink of the current de facto grid-style paradigm.",CVPR
"Our understanding of the generalization capabilities of neural networks (NNs)
is still incomplete. Prevailing explanations are based on implicit biases of
gradient descent (GD) but they cannot account for the capabilities of models
from gradient-free methods nor the simplicity bias recently observed in
untrained networks. This paper seeks other sources of generalization in NNs.
  Findings. To understand the inductive biases provided by architectures
independently from GD, we examine untrained, random-weight networks. Even
simple MLPs show strong inductive biases: uniform sampling in weight space
yields a very biased distribution of functions in terms of complexity. But
unlike common wisdom, NNs do not have an inherent ""simplicity bias"". This
property depends on components such as ReLUs, residual connections, and layer
normalizations. Alternative architectures can be built with a bias for any
level of complexity. Transformers also inherit all these properties from their
building blocks.
  Implications. We provide a fresh explanation for the success of deep learning
independent from gradient-based training. It points at promising avenues for
controlling the solutions implemented by trained models.",CVPR
"In this paper, we delve into the nuanced challenge of tailoring the Segment
Anything Models (SAMs) for integration with event data, with the overarching
objective of attaining robust and universal object segmentation within the
event-centric domain. One pivotal issue at the heart of this endeavor is the
precise alignment and calibration of embeddings derived from event-centric data
such that they harmoniously coincide with those originating from RGB imagery.
Capitalizing on the vast repositories of datasets with paired events and RGB
images, our proposition is to harness and extrapolate the profound knowledge
encapsulated within the pre-trained SAM framework. As a cornerstone to
achieving this, we introduce a multi-scale feature distillation methodology.
This methodology rigorously optimizes the alignment of token embeddings
originating from event data with their RGB image counterparts, thereby
preserving and enhancing the robustness of the overall architecture.
Considering the distinct significance that token embeddings from intermediate
layers hold for higher-level embeddings, our strategy is centered on accurately
calibrating the pivotal token embeddings. This targeted calibration is aimed at
effectively managing the discrepancies in high-level embeddings originating
from both the event and image domains. Extensive experiments on different
datasets demonstrate the effectiveness of the proposed distillation method.
Code in http://github.com/happychenpipi/EventSAM.",CVPR
"For privacy and security concerns, the need to erase unwanted information
from pre-trained vision models is becoming evident nowadays. In real-world
scenarios, erasure requests originate at any time from both users and model
owners. These requests usually form a sequence. Therefore, under such a
setting, selective information is expected to be continuously removed from a
pre-trained model while maintaining the rest. We define this problem as
continual forgetting and identify two key challenges. (i) For unwanted
knowledge, efficient and effective deleting is crucial. (ii) For remaining
knowledge, the impact brought by the forgetting procedure should be minimal. To
address them, we propose Group Sparse LoRA (GS-LoRA). Specifically, towards
(i), we use LoRA modules to fine-tune the FFN layers in Transformer blocks for
each forgetting task independently, and towards (ii), a simple group sparse
regularization is adopted, enabling automatic selection of specific LoRA groups
and zeroing out the others. GS-LoRA is effective, parameter-efficient,
data-efficient, and easy to implement. We conduct extensive experiments on face
recognition, object detection and image classification and demonstrate that
GS-LoRA manages to forget specific classes with minimal impact on other
classes. Codes will be released on \url{https://github.com/bjzhb666/GS-LoRA}.",CVPR
"Machine learning holds tremendous promise for transforming the fundamental
practice of scientific discovery by virtue of its data-driven nature. With the
ever-increasing stream of research data collection, it would be appealing to
autonomously explore patterns and insights from observational data for
discovering novel classes of phenotypes and concepts. However, in the
biomedical domain, there are several challenges inherently presented in the
cumulated data which hamper the progress of novel class discovery. The
non-i.i.d. data distribution accompanied by the severe imbalance among
different groups of classes essentially leads to ambiguous and biased semantic
representations. In this work, we present a geometry-constrained probabilistic
modeling treatment to resolve the identified issues. First, we propose to
parameterize the approximated posterior of instance embedding as a marginal von
MisesFisher distribution to account for the interference of distributional
latent bias. Then, we incorporate a suite of critical geometric properties to
impose proper constraints on the layout of constructed embedding space, which
in turn minimizes the uncontrollable risk for unknown class learning and
structuring. Furthermore, a spectral graph-theoretic method is devised to
estimate the number of potential novel classes. It inherits two intriguing
merits compared to existent approaches, namely high computational efficiency
and flexibility for taxonomy-adaptive estimation. Extensive experiments across
various biomedical scenarios substantiate the effectiveness and general
applicability of our method.",CVPR
"Deriving reliable region-word alignment from image-text pairs is critical to
learn object-level vision-language representations for open-vocabulary object
detection. Existing methods typically rely on pre-trained or self-trained
vision-language models for alignment, which are prone to limitations in
localization accuracy or generalization capabilities. In this paper, we propose
CoDet, a novel approach that overcomes the reliance on pre-aligned
vision-language space by reformulating region-word alignment as a co-occurring
object discovery problem. Intuitively, by grouping images that mention a shared
concept in their captions, objects corresponding to the shared concept shall
exhibit high co-occurrence among the group. CoDet then leverages visual
similarities to discover the co-occurring objects and align them with the
shared concept. Extensive experiments demonstrate that CoDet has superior
performances and compelling scalability in open-vocabulary detection, e.g., by
scaling up the visual backbone, CoDet achieves 37.0 $\text{AP}^m_{novel}$ and
44.7 $\text{AP}^m_{all}$ on OV-LVIS, surpassing the previous SoTA by 4.2
$\text{AP}^m_{novel}$ and 9.8 $\text{AP}^m_{all}$. Code is available at
https://github.com/CVMI-Lab/CoDet.",CVPR
"Image Quality Assessment (IQA) models benefit significantly from semantic
information, which allows them to treat different types of objects distinctly.
Currently, leveraging semantic information to enhance IQA is a crucial research
direction. Traditional methods, hindered by a lack of sufficiently annotated
data, have employed the CLIP image-text pretraining model as their backbone to
gain semantic awareness. However, the generalist nature of these pre-trained
Vision-Language (VL) models often renders them suboptimal for IQA-specific
tasks. Recent approaches have attempted to address this mismatch using prompt
technology, but these solutions have shortcomings. Existing prompt-based VL
models overly focus on incremental semantic information from text, neglecting
the rich insights available from visual data analysis. This imbalance limits
their performance improvements in IQA tasks. This paper introduces an
innovative multi-modal prompt-based methodology for IQA. Our approach employs
carefully crafted prompts that synergistically mine incremental semantic
information from both visual and linguistic data. Specifically, in the visual
branch, we introduce a multi-layer prompt structure to enhance the VL model's
adaptability. In the text branch, we deploy a dual-prompt scheme that steers
the model to recognize and differentiate between scene category and distortion
type, thereby refining the model's capacity to assess image quality. Our
experimental findings underscore the effectiveness of our method over existing
Blind Image Quality Assessment (BIQA) approaches. Notably, it demonstrates
competitive performance across various datasets. Our method achieves Spearman
Rank Correlation Coefficient (SRCC) values of 0.961(surpassing 0.946 in CSIQ)
and 0.941 (exceeding 0.930 in KADID), illustrating its robustness and accuracy
in diverse contexts.",CVPR
"Image-goal navigation is a challenging task that requires an agent to
navigate to a goal indicated by an image in unfamiliar environments. Existing
methods utilizing diverse scene memories suffer from inefficient exploration
since they use all historical observations for decision-making without
considering the goal-relevant fraction. To address this limitation, we present
MemoNav, a novel memory model for image-goal navigation, which utilizes a
working memory-inspired pipeline to improve navigation performance.
Specifically, we employ three types of navigation memory. The node features on
a map are stored in the short-term memory (STM), as these features are
dynamically updated. A forgetting module then retains the informative STM
fraction to increase efficiency. We also introduce long-term memory (LTM) to
learn global scene representations by progressively aggregating STM features.
Subsequently, a graph attention module encodes the retained STM and the LTM to
generate working memory (WM) which contains the scene features essential for
efficient navigation. The synergy among these three memory types boosts
navigation performance by enabling the agent to learn and leverage
goal-relevant scene features within a topological map. Our evaluation on
multi-goal tasks demonstrates that MemoNav significantly outperforms previous
methods across all difficulty levels in both Gibson and Matterport3D scenes.
Qualitative results further illustrate that MemoNav plans more efficient
routes.",CVPR
"Three-dimensional object detection is one of the key tasks in autonomous
driving. To reduce costs in practice, low-cost multi-view cameras for 3D object
detection are proposed to replace the expansive LiDAR sensors. However, relying
solely on cameras is difficult to achieve highly accurate and robust 3D object
detection. An effective solution to this issue is combining multi-view cameras
with the economical millimeter-wave radar sensor to achieve more reliable
multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a
radar-camera fusion 3D object detection method in the bird's eye view (BEV).
Specifically, we first design RadarBEVNet for radar BEV feature extraction.
RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section
(RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based
encoder and a transformer-based encoder are proposed to extract radar features,
with an injection and extraction module to facilitate communication between the
two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to
scattering the point feature in BEV. Besides, we present the Cross-Attention
Multi-layer Fusion module to automatically align the multi-modal BEV feature
from radar and camera with the deformable attention mechanism, and then fuse
the feature with channel and spatial fusion layers. Experimental results show
that RCBEVDet achieves new state-of-the-art radar-camera fusion results on
nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore,
RCBEVDet achieves better 3D detection results than all real-time camera-only
and radar-camera 3D object detectors with a faster inference speed at 21~28
FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.",CVPR
"When pre-trained models become rapidly larger, the cost of fine-tuning on
downstream tasks steadily increases, too. To economically fine-tune these
models, parameter-efficient transfer learning (PETL) is proposed, which only
tunes a tiny subset of trainable parameters to efficiently learn quality
representations. However, current PETL methods are facing the dilemma that
during training the GPU memory footprint is not effectively reduced as
trainable parameters. PETL will likely fail, too, if the full fine-tuning
encounters the out-of-GPU-memory issue. This phenomenon happens because
trainable parameters from these methods are generally entangled with the
backbone, such that a lot of intermediate states have to be stored in GPU
memory for gradient propagation. To alleviate this problem, we introduce
Disentangled Transfer Learning (DTL), which disentangles the trainable
parameters from the backbone using a lightweight Compact Side Network (CSN). By
progressively extracting task-specific information with a few low-rank linear
mappings and appropriately adding the information back to the backbone, CSN
effectively realizes knowledge transfer in various downstream tasks. We
conducted extensive experiments to validate the effectiveness of our method.
The proposed method not only reduces a large amount of GPU memory usage and
trainable parameters, but also outperforms existing PETL methods by a
significant margin in accuracy, achieving new state-of-the-art on several
standard benchmarks. The code is available at https://github.com/heekhero/DTL.",CVPR
"This work proposes TimeChat, a time-sensitive multimodal large language model
specifically designed for long video understanding. Our model incorporates two
key architectural contributions: (1) a timestamp-aware frame encoder that binds
visual content with the timestamp of each frame, and (2) a sliding video
Q-Former that produces a video token sequence of varying lengths to accommodate
videos of various durations. Additionally, we construct an instruction-tuning
dataset, encompassing 6 tasks and a total of 125K instances, to further enhance
TimeChat's instruction-following performance. Experiment results across various
video understanding tasks, such as dense captioning, temporal grounding, and
highlight detection, demonstrate TimeChat's strong zero-shot temporal
localization and reasoning capabilities. For example, it achieves +9.2 F1 score
and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)
on Charades-STA, compared to state-of-the-art video large language models,
holding the potential to serve as a versatile video assistant for long-form
video comprehension tasks and satisfy realistic user requirements.",CVPR
"We propose G-HOP, a denoising diffusion based generative prior for
hand-object interactions that allows modeling both the 3D object and a human
hand, conditioned on the object category. To learn a 3D spatial diffusion model
that can capture this joint distribution, we represent the human hand via a
skeletal distance field to obtain a representation aligned with the (latent)
signed distance field for the object. We show that this hand-object prior can
then serve as generic guidance to facilitate other tasks like reconstruction
from interaction clip and human grasp synthesis. We believe that our model,
trained by aggregating seven diverse real-world interaction datasets spanning
across 155 categories, represents a first approach that allows jointly
generating both hand and object. Our empirical evaluations demonstrate the
benefit of this joint prior in video-based reconstruction and human grasp
synthesis, outperforming current task-specific baselines.
  Project website: https://judyye.github.io/ghop-www",CVPR
"Large language models (LLMs) have shown remarkable text understanding
capabilities, which have been extended as Video LLMs to handle video data for
comprehending visual details. However, existing Video LLMs can only provide a
coarse description of the entire video, failing to capture the precise start
and end time boundary of specific events. In this paper, we solve this issue
via proposing VTimeLLM, a novel Video LLM designed for fine-grained video
moment understanding and reasoning with respect to time boundary. Specifically,
our VTimeLLM adopts a boundary-aware three-stage training strategy, which
respectively utilizes image-text pairs for feature alignment, multiple-event
videos to increase temporal-boundary awareness, and high-quality
video-instruction tuning to further improve temporal understanding ability as
well as align with human intents. Extensive experiments demonstrate that in
fine-grained time-related comprehension tasks for videos such as Temporal Video
Grounding and Dense Video Captioning, VTimeLLM significantly outperforms
existing Video LLMs. Besides, benefits from the fine-grained temporal
understanding of the videos further enable VTimeLLM to beat existing Video LLMs
in video dialogue benchmark, showing its superior cross-modal understanding and
reasoning abilities.",CVPR
"We propose a novel concept of dual and integrated latent topologies (DITTO in
short) for implicit 3D reconstruction from noisy and sparse point clouds. Most
existing methods predominantly focus on single latent type, such as point or
grid latents. In contrast, the proposed DITTO leverages both point and grid
latents (i.e., dual latent) to enhance their strengths, the stability of grid
latents and the detail-rich capability of point latents. Concretely, DITTO
consists of dual latent encoder and integrated implicit decoder. In the dual
latent encoder, a dual latent layer, which is the key module block composing
the encoder, refines both latents in parallel, maintaining their distinct
shapes and enabling recursive interaction. Notably, a newly proposed dynamic
sparse point transformer within the dual latent layer effectively refines point
latents. Then, the integrated implicit decoder systematically combines these
refined latents, achieving high-fidelity 3D reconstruction and surpassing
previous state-of-the-art methods on object- and scene-level datasets,
especially in thin and detailed structures.",CVPR
"We propose a method that can generate cinemagraphs automatically from a still
landscape image using a pre-trained StyleGAN. Inspired by the success of recent
unconditional video generation, we leverage a powerful pre-trained image
generator to synthesize high-quality cinemagraphs. Unlike previous approaches
that mainly utilize the latent space of a pre-trained StyleGAN, our approach
utilizes its deep feature space for both GAN inversion and cinemagraph
generation. Specifically, we propose multi-scale deep feature warping (MSDFW),
which warps the intermediate features of a pre-trained StyleGAN at different
resolutions. By using MSDFW, the generated cinemagraphs are of high resolution
and exhibit plausible looping animation. We demonstrate the superiority of our
method through user studies and quantitative comparisons with state-of-the-art
cinemagraph generation methods and a video generation method that uses a
pre-trained StyleGAN.",CVPR
"We delve into pseudo-labeling for semi-supervised monocular 3D object
detection (SSM3OD) and discover two primary issues: a misalignment between the
prediction quality of 3D and 2D attributes and the tendency of depth
supervision derived from pseudo-labels to be noisy, leading to significant
optimization conflicts with other reliable forms of supervision. We introduce a
novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach
features a Decoupled Pseudo-label Generation (DPG) module, designed to
efficiently generate pseudo-labels by separately processing 2D and 3D
attributes. This module incorporates a unique homography-based method for
identifying dependable pseudo-labels in BEV space, specifically for 3D
attributes. Additionally, we present a DepthGradient Projection (DGP) module to
mitigate optimization conflicts caused by noisy depth supervision of
pseudo-labels, effectively decoupling the depth gradient and removing
conflicting gradients. This dual decoupling strategy-at both the pseudo-label
generation and gradient levels-significantly improves the utilization of
pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark
demonstrate the superiority of our method over existing approaches.",CVPR
"Visual sound source localization poses a significant challenge in identifying
the semantic region of each sounding source within a video. Existing
self-supervised and weakly supervised source localization methods struggle to
accurately distinguish the semantic regions of each sounding object,
particularly in multi-source mixtures. These methods often rely on audio-visual
correspondence as guidance, which can lead to substantial performance drops in
complex multi-source localization scenarios. The lack of access to individual
source sounds in multi-source mixtures during training exacerbates the
difficulty of learning effective audio-visual correspondence for localization.
To address this limitation, in this paper, we propose incorporating the text
modality as an intermediate feature guide using tri-modal joint embedding
models (e.g., AudioCLIP) to disentangle the semantic audio-visual source
correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by
predicting the class of sounding entities in mixtures. Subsequently, the
textual representation of each sounding source is employed as guidance to
disentangle fine-grained audio-visual source correspondence from multi-source
mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables
our framework to handle a flexible number of sources and exhibits promising
zero-shot transferability to unseen classes during test time. Extensive
experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets
demonstrate significant performance improvements over state-of-the-art methods.",CVPR
"Open-vocabulary image segmentation aims to partition an image into semantic
regions according to arbitrary text descriptions. However, complex visual
scenes can be naturally decomposed into simpler parts and abstracted at
multiple levels of granularity, introducing inherent segmentation ambiguity.
Unlike existing methods that typically sidestep this ambiguity and treat it as
an external factor, our approach actively incorporates a hierarchical
representation encompassing different semantic-levels into the learning
process. We propose a decoupled text-image fusion mechanism and representation
learning modules for both ""things"" and ""stuff"". Additionally, we systematically
examine the differences that exist in the textual and visual features between
these types of categories. Our resulting model, named HIPIE, tackles
HIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within a
unified framework. Benchmarked on over 40 datasets, e.g., ADE20K, COCO,
Pascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves the
state-of-the-art results at various levels of image comprehension, including
semantic-level (e.g., semantic segmentation), instance-level (e.g.,
panoptic/referring segmentation and object detection), as well as part-level
(e.g., part/subpart segmentation) tasks. Our code is released at
https://github.com/berkeley-hipie/HIPIE.",CVPR
"Determining the relative pose of an object between two images is pivotal to
the success of generalizable object pose estimation. Existing approaches
typically approximate the continuous pose representation with a large number of
discrete pose hypotheses, which incurs a computationally expensive process of
scoring each hypothesis at test time. By contrast, we present a Deep Voxel
Matching Network (DVMNet) that eliminates the need for pose hypotheses and
computes the relative object pose in a single pass. To this end, we map the two
input RGB images, reference and query, to their respective voxelized 3D
representations. We then pass the resulting voxels through a pose estimation
module, where the voxels are aligned and the pose is computed in an end-to-end
fashion by solving a least-squares problem. To enhance robustness, we introduce
a weighted closest voxel algorithm capable of mitigating the impact of noisy
voxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse
datasets, demonstrating that our method delivers more accurate relative pose
estimates for novel objects at a lower computational cost compared to
state-of-the-art methods. Our code is released at:
https://github.com/sailor-z/DVMNet/.",CVPR
"We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D
radiance fields parameterized by 3D Gaussian primitives from pairs of images.
Our model features real-time and memory-efficient rendering for scalable
training as well as fast 3D reconstruction at inference time. To overcome local
minima inherent to sparse and locally supported representations, we predict a
dense probability distribution over 3D and sample Gaussian means from that
probability distribution. We make this sampling operation differentiable via a
reparameterization trick, allowing us to back-propagate gradients through the
Gaussian splatting representation. We benchmark our method on wide-baseline
novel view synthesis on the real-world RealEstate10k and ACID datasets, where
we outperform state-of-the-art light field transformers and accelerate
rendering by 2.5 orders of magnitude while reconstructing an interpretable and
editable 3D radiance field.",CVPR
"We present MicroCinema, a straightforward yet effective framework for
high-quality and coherent text-to-video generation. Unlike existing approaches
that align text prompts with video directly, MicroCinema introduces a
Divide-and-Conquer strategy which divides the text-to-video into a two-stage
process: text-to-image generation and image\&text-to-video generation. This
strategy offers two significant advantages. a) It allows us to take full
advantage of the recent advances in text-to-image models, such as Stable
Diffusion, Midjourney, and DALLE, to generate photorealistic and highly
detailed images. b) Leveraging the generated image, the model can allocate less
focus to fine-grained appearance details, prioritizing the efficient learning
of motion dynamics. To implement this strategy effectively, we introduce two
core designs. First, we propose the Appearance Injection Network, enhancing the
preservation of the appearance of the given image. Second, we introduce the
Appearance Noise Prior, a novel mechanism aimed at maintaining the capabilities
of pre-trained 2D diffusion models. These design elements empower MicroCinema
to generate high-quality videos with precise motion, guided by the provided
text prompts. Extensive experiments demonstrate the superiority of the proposed
framework. Concretely, MicroCinema achieves SOTA zero-shot FVD of 342.86 on
UCF-101 and 377.40 on MSR-VTT. See
https://wangyanhui666.github.io/MicroCinema.github.io/ for video samples.",CVPR
"Prompt learning has emerged as an effective and data-efficient technique in
large Vision-Language Models (VLMs). However, when adapting VLMs to specialized
domains such as remote sensing and medical imaging, domain prompt learning
remains underexplored. While large-scale domain-specific foundation models can
help tackle this challenge, their concentration on a single vision level makes
it challenging to prompt both vision and language modalities. To overcome this,
we propose to leverage domain-specific knowledge from domain-specific
foundation models to transfer the robust recognition ability of VLMs from
generalized to specialized domains, using quaternion networks. Specifically,
the proposed method involves using domain-specific vision features from
domain-specific foundation models to guide the transformation of generalized
contextual embeddings from the language branch into a specialized space within
the quaternion networks. Moreover, we present a hierarchical approach that
generates vision prompt features by analyzing intermodal relationships between
hierarchical language prompt features and domain-specific vision features. In
this way, quaternion networks can effectively mine the intermodal relationships
in the specific domain, facilitating domain-specific vision-language
contrastive learning. Extensive experiments on domain-specific datasets show
that our proposed method achieves new state-of-the-art results in prompt
learning.",CVPR
"This work proposes a novel learning framework for visual hand dynamics
analysis that takes into account the physiological aspects of hand motion. The
existing models, which are simplified joint-actuated systems, often produce
unnatural motions. To address this, we integrate a musculoskeletal system with
a learnable parametric hand model, MANO, to create a new model, MS-MANO. This
model emulates the dynamics of muscles and tendons to drive the skeletal
system, imposing physiologically realistic constraints on the resulting torque
trajectories. We further propose a simulation-in-the-loop pose refinement
framework, BioPR, that refines the initial estimated pose through a multi-layer
perceptron (MLP) network. Our evaluation of the accuracy of MS-MANO and the
efficacy of the BioPR is conducted in two separate parts. The accuracy of
MS-MANO is compared with MyoSuite, while the efficacy of BioPR is benchmarked
against two large-scale public datasets and two recent state-of-the-art
methods. The results demonstrate that our approach consistently improves the
baseline methods both quantitatively and qualitatively.",CVPR
"We propose a practical approach to JPEG image decoding, utilizing a local
implicit neural representation with continuous cosine formulation. The JPEG
algorithm significantly quantizes discrete cosine transform (DCT) spectra to
achieve a high compression rate, inevitably resulting in quality degradation
while encoding an image. We have designed a continuous cosine spectrum
estimator to address the quality degradation issue that restores the distorted
spectrum. By leveraging local DCT formulations, our network has the privilege
to exploit dequantization and upsampling simultaneously. Our proposed model
enables decoding compressed images directly across different quality factors
using a single pre-trained model without relying on a conventional JPEG
decoder. As a result, our proposed network achieves state-of-the-art
performance in flexible color image JPEG artifact removal tasks. Our source
code is available at https://github.com/WooKyoungHan/JDEC.",CVPR
"Referring Remote Sensing Image Segmentation (RRSIS) is a new challenge that
combines computer vision and natural language processing, delineating specific
regions in aerial images as described by textual queries. Traditional Referring
Image Segmentation (RIS) approaches have been impeded by the complex spatial
scales and orientations found in aerial imagery, leading to suboptimal
segmentation results. To address these challenges, we introduce the Rotated
Multi-Scale Interaction Network (RMSIN), an innovative approach designed for
the unique demands of RRSIS. RMSIN incorporates an Intra-scale Interaction
Module (IIM) to effectively address the fine-grained detail required at
multiple scales and a Cross-scale Interaction Module (CIM) for integrating
these details coherently across the network. Furthermore, RMSIN employs an
Adaptive Rotated Convolution (ARC) to account for the diverse orientations of
objects, a novel contribution that significantly enhances segmentation
accuracy. To assess the efficacy of RMSIN, we have curated an expansive dataset
comprising 17,402 image-caption-mask triplets, which is unparalleled in terms
of scale and variety. This dataset not only presents the model with a wide
range of spatial and rotational scenarios but also establishes a stringent
benchmark for the RRSIS task, ensuring a rigorous evaluation of performance.
Our experimental evaluations demonstrate the exceptional performance of RMSIN,
surpassing existing state-of-the-art models by a significant margin. All
datasets and code are made available at https://github.com/Lsan2401/RMSIN.",CVPR
"Notwithstanding offering convenience and entertainment to society, Deepfake
face swapping has caused critical privacy issues with the rapid development of
deep generative models. Due to imperceptible artifacts in high-quality
synthetic images, passive detection models against face swapping in recent
years usually suffer performance damping regarding the generalizability issue.
Therefore, several studies have been attempted to proactively protect the
original images against malicious manipulations by inserting invisible signals
in advance. However, the existing proactive defense approaches demonstrate
unsatisfactory results with respect to visual quality, detection accuracy, and
source tracing ability. In this study, to fulfill the research gap, we propose
the first robust identity perceptual watermarking framework that concurrently
performs detection and source tracing against Deepfake face swapping
proactively. We assign identity semantics regarding the image contents to the
watermarks and devise an unpredictable and nonreversible chaotic encryption
system to ensure watermark confidentiality. The watermarks are encoded and
recovered by jointly training an encoder-decoder framework along with
adversarial image manipulations. Falsification and source tracing are
accomplished by justifying the consistency between the content-matched identity
perceptual watermark and the recovered robust watermark from the image.
Extensive experiments demonstrate state-of-the-art detection performance on
Deepfake face swapping under both cross-dataset and cross-manipulation
settings.",CVPR
"Depth completion is a vital task for autonomous driving, as it involves
reconstructing the precise 3D geometry of a scene from sparse and noisy depth
measurements. However, most existing methods either rely only on 2D depth
representations or directly incorporate raw 3D point clouds for compensation,
which are still insufficient to capture the fine-grained 3D geometry of the
scene. To address this challenge, we introduce Tri-Perspective view
Decomposition (TPVD), a novel framework that can explicitly model 3D geometry.
In particular, (1) TPVD ingeniously decomposes the original point cloud into
three 2D views, one of which corresponds to the sparse depth input. (2) We
design TPV Fusion to update the 2D TPV features through recurrent 2D-3D-2D
aggregation, where a Distance-Aware Spherical Convolution (DASC) is applied.
(3) By adaptively choosing TPV affinitive neighbors, the newly proposed
Geometric Spatial Propagation Network (GSPN) further improves the geometric
consistency. As a result, our TPVD outperforms existing methods on KITTI,
NYUv2, and SUN RGBD. Furthermore, we build a novel depth completion dataset
named TOFDC, which is acquired by the time-of-flight (TOF) sensor and the color
camera on smartphones. Project page:
https://yanzq95.github.io/projectpage/TOFDC/index.html",CVPR
"Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have
recently gained significant popularity for creative Text-to-image generation.
Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE)
is of greater importance for application developers, which modify objects or
object properties in images by manipulating feature components in attention
layers during the generation process. However, little is known about what
semantic meanings these attention layers have learned and which parts of the
attention maps contribute to the success of image editing. In this paper, we
conduct an in-depth probing analysis and demonstrate that cross-attention maps
in Stable Diffusion often contain object attribution information that can
result in editing failures. In contrast, self-attention maps play a crucial
role in preserving the geometric and shape details of the source image during
the transformation to the target image. Our analysis offers valuable insights
into understanding cross and self-attention maps in diffusion models. Moreover,
based on our findings, we simplify popular image editing methods and propose a
more straightforward yet more stable and efficient tuning-free procedure that
only modifies self-attention maps of the specified attention layers during the
denoising process. Experimental results show that our simplified method
consistently surpasses the performance of popular approaches on multiple
datasets.",CVPR
"We propose RoHM, an approach for robust 3D human motion reconstruction from
monocular RGB(-D) videos in the presence of noise and occlusions. Most previous
approaches either train neural networks to directly regress motion in 3D or
learn data-driven motion priors and combine them with optimization at test
time. The former do not recover globally coherent motion and fail under
occlusions; the latter are time-consuming, prone to local minima, and require
manual tuning. To overcome these shortcomings, we exploit the iterative,
denoising nature of diffusion models. RoHM is a novel diffusion-based motion
model that, conditioned on noisy and occluded input data, reconstructs
complete, plausible motions in consistent global coordinates. Given the
complexity of the problem -- requiring one to address different tasks
(denoising and infilling) in different solution spaces (local and global
motion) -- we decompose it into two sub-tasks and learn two models, one for
global trajectory and one for local motion. To capture the correlations between
the two, we then introduce a novel conditioning module, combining it with an
iterative inference scheme. We apply RoHM to a variety of tasks -- from motion
reconstruction and denoising to spatial and temporal infilling. Extensive
experiments on three popular datasets show that our method outperforms
state-of-the-art approaches qualitatively and quantitatively, while being
faster at test time. The code is available at
https://sanweiliti.github.io/ROHM/ROHM.html.",CVPR
"We present MM-AU, a novel dataset for Multi-Modal Accident video
Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each
with temporally aligned text descriptions. We annotate over 2.23 million object
boxes and 58,650 pairs of video-based accident reasons, covering 58 accident
categories. MM-AU supports various accident understanding tasks, particularly
multimodal video diffusion to understand accident cause-effect chains for safe
driving. With MM-AU, we present an Abductive accident Video understanding
framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video
diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven
by an abductive CLIP model. This model involves a contrastive interaction loss
to learn the pair co-occurrence of normal, near-accident, accident frames with
the corresponding text descriptions, such as accident reasons, prevention
advice, and accident categories. OAVD enforces the causal region learning while
fixing the content of the original frame background in video generation, to
find the dominant cause-effect chain for certain accidents. Extensive
experiments verify the abductive ability of AdVersa-SD and the superiority of
OAVD against the state-of-the-art diffusion models. Additionally, we provide
careful benchmark evaluations for object detection and accident reason
answering since AdVersa-SD relies on precise object and accident reason
information.",CVPR
"We introduce a new task -- language-driven video inpainting, which uses
natural language instructions to guide the inpainting process. This approach
overcomes the limitations of traditional video inpainting methods that depend
on manually labeled binary masks, a process often tedious and labor-intensive.
We present the Remove Objects from Videos by Instructions (ROVI) dataset,
containing 5,650 videos and 9,091 inpainting results, to support training and
evaluation for this task. We also propose a novel diffusion-based
language-driven video inpainting framework, the first end-to-end baseline for
this task, integrating Multimodal Large Language Models to understand and
execute complex language-based inpainting requests effectively. Our
comprehensive results showcase the dataset's versatility and the model's
effectiveness in various language-instructed inpainting scenarios. We will make
datasets, code, and models publicly available.",CVPR
"Self-supervised pre-training has been proved to be effective in learning
transferable representations that benefit various visual tasks. This paper asks
this question: can self-supervised pre-training learn general facial
representations for various facial analysis tasks? Recent efforts toward this
goal are limited to treating each face image as a whole, i.e., learning
consistent facial representations at the image-level, which overlooks the
consistency of local facial representations (i.e., facial regions like eyes,
nose, etc). In this work, we make a first attempt to propose a novel
self-supervised facial representation learning framework to learn consistent
global and local facial representations, Facial Region Awareness (FRA).
Specifically, we explicitly enforce the consistency of facial regions by
matching the local facial representations across views, which are extracted
with learned heatmaps highlighting the facial regions. Inspired by the mask
prediction in supervised semantic segmentation, we obtain the heatmaps via
cosine similarity between the per-pixel projection of feature maps and facial
mask embeddings computed from learnable positional embeddings, which leverage
the attention mechanism to globally look up the facial image for facial
regions. To learn such heatmaps, we formulate the learning of facial mask
embeddings as a deep clustering problem by assigning the pixel features from
the feature maps to them. The transfer learning results on facial
classification and regression tasks show that our FRA outperforms previous
pre-trained models and more importantly, using ResNet as the unified backbone
for various tasks, our FRA achieves comparable or even better performance
compared with SOTA methods in facial analysis tasks.",CVPR
"We address the problem of synthesizing multi-view optical illusions: images
that change appearance upon a transformation, such as a flip or rotation. We
propose a simple, zero-shot method for obtaining these illusions from
off-the-shelf text-to-image diffusion models. During the reverse diffusion
process, we estimate the noise from different views of a noisy image, and then
combine these noise estimates together and denoise the image. A theoretical
analysis suggests that this method works precisely for views that can be
written as orthogonal transformations, of which permutations are a subset. This
leads to the idea of a visual anagram--an image that changes appearance under
some rearrangement of pixels. This includes rotations and flips, but also more
exotic pixel permutations such as a jigsaw rearrangement. Our approach also
naturally extends to illusions with more than two views. We provide both
qualitative and quantitative results demonstrating the effectiveness and
flexibility of our method. Please see our project webpage for additional
visualizations and results: https://dangeng.github.io/visual_anagrams/",CVPR
"Autonomous vehicle (AV) systems rely on robust perception models as a
cornerstone of safety assurance. However, objects encountered on the road
exhibit a long-tailed distribution, with rare or unseen categories posing
challenges to a deployed perception model. This necessitates an expensive
process of continuously curating and annotating data with significant human
effort. We propose to leverage recent advances in vision-language and large
language models to design an Automatic Data Engine (AIDE) that automatically
identifies issues, efficiently curates data, improves the model through
auto-labeling, and verifies the model through generation of diverse scenarios.
This process operates iteratively, allowing for continuous self-improvement of
the model. We further establish a benchmark for open-world detection on AV
datasets to comprehensively evaluate various learning paradigms, demonstrating
our method's superior performance at a reduced cost.",CVPR
"Current state-of-the-art point cloud-based perception methods usually rely on
large-scale labeled data, which requires expensive manual annotations. A
natural option is to explore the unsupervised methodology for 3D perception
tasks. However, such methods often face substantial performance-drop
difficulties. Fortunately, we found that there exist amounts of image-based
datasets and an alternative can be proposed, i.e., transferring the knowledge
in the 2D images to 3D point clouds. Specifically, we propose a novel approach
for the challenging cross-modal and cross-domain adaptation task by fully
exploring the relationship between images and point clouds and designing
effective feature alignment strategies. Without any 3D labels, our method
achieves state-of-the-art performance for 3D point cloud semantic segmentation
on SemanticKITTI by using the knowledge of KITTI360 and GTA5, compared to
existing unsupervised and weakly-supervised baselines.",CVPR
"Multi-domain generalization (mDG) is universally aimed to minimize the
discrepancy between training and testing distributions to enhance
marginal-to-label distribution mapping. However, existing mDG literature lacks
a general learning objective paradigm and often imposes constraints on static
target marginal distributions. In this paper, we propose to leverage a
$Y$-mapping to relax the constraint. We rethink the learning objective for mDG
and design a new \textbf{general learning objective} to interpret and analyze
most existing mDG wisdom. This general objective is bifurcated into two
synergistic amis: learning domain-independent conditional features and
maximizing a posterior. Explorations also extend to two effective
regularization terms that incorporate prior information and suppress invalid
causality, alleviating the issues that come with relaxed constraints. We
theoretically contribute an upper bound for the domain alignment of
domain-independent conditional features, disclosing that many previous mDG
endeavors actually \textbf{optimize partially the objective} and thus lead to
limited performance. As such, our study distills a general learning objective
into four practical components, providing a general, robust, and flexible
mechanism to handle complex domain shifts. Extensive empirical results indicate
that the proposed objective with $Y$-mapping leads to substantially better mDG
performance in various downstream tasks, including regression, segmentation,
and classification.",CVPR
"Recently, the advancement of self-supervised learning techniques, like masked
autoencoders (MAE), has greatly influenced visual representation learning for
images and videos. Nevertheless, it is worth noting that the predominant
approaches in existing masked image / video modeling rely excessively on
resource-intensive vision transformers (ViTs) as the feature encoder. In this
paper, we propose a new approach termed as \textbf{VideoMAC}, which combines
video masked autoencoders with resource-friendly ConvNets. Specifically,
VideoMAC employs symmetric masking on randomly sampled pairs of video frames.
To prevent the issue of mask pattern dissipation, we utilize ConvNets which are
implemented with sparse convolutional operators as encoders. Simultaneously, we
present a simple yet effective masked video modeling (MVM) approach, a dual
encoder architecture comprising an online encoder and an exponential moving
average target encoder, aimed to facilitate inter-frame reconstruction
consistency in videos. Additionally, we demonstrate that VideoMAC, empowering
classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the
benefits of MVM, outperforms ViT-based approaches on downstream tasks,
including video object segmentation (+\textbf{5.2\%} / \textbf{6.4\%}
$\mathcal{J}\&\mathcal{F}$), body part propagation (+\textbf{6.3\%} /
\textbf{3.1\%} mIoU), and human pose tracking (+\textbf{10.2\%} /
\textbf{11.1\%} PCK@0.1).",CVPR
"In the era where AI-generated content (AIGC) models can produce stunning and
lifelike images, the lingering shadow of unauthorized reproductions and
malicious tampering poses imminent threats to copyright integrity and
information security. Current image watermarking methods, while widely accepted
for safeguarding visual content, can only protect copyright and ensure
traceability. They fall short in localizing increasingly realistic image
tampering, potentially leading to trust crises, privacy violations, and legal
disputes. To solve this challenge, we propose an innovative proactive forensics
framework EditGuard, to unify copyright protection and tamper-agnostic
localization, especially for AIGC-based editing methods. It can offer a
meticulous embedding of imperceptible watermarks and precise decoding of
tampered areas and copyright information. Leveraging our observed fragility and
locality of image-into-image steganography, the realization of EditGuard can be
converted into a united image-bit steganography issue, thus completely
decoupling the training process from the tampering types. Extensive experiments
demonstrate that our EditGuard balances the tamper localization accuracy,
copyright recovery precision, and generalizability to various AIGC-based
tampering methods, especially for image forgery that is difficult for the naked
eye to detect. The project page is available at
https://xuanyuzhang21.github.io/project/editguard/.",CVPR
"Backdoors and adversarial examples are the two primary threats currently
faced by deep neural networks (DNNs). Both attacks attempt to hijack the model
behaviors with unintended outputs by introducing (small) perturbations to the
inputs. Backdoor attacks, despite the high success rates, often require a
strong assumption, which is not always easy to achieve in reality. Adversarial
example attacks, which put relatively weaker assumptions on attackers, often
demand high computational resources, yet do not always yield satisfactory
success rates when attacking mainstream black-box models in the real world.
These limitations motivate the following research question: can model hijacking
be achieved more simply, with a higher attack success rate and more reasonable
assumptions? In this paper, we propose CleanSheet, a new model hijacking attack
that obtains the high performance of backdoor attacks without requiring the
adversary to tamper with the model training process. CleanSheet exploits
vulnerabilities in DNNs stemming from the training data. Specifically, our key
idea is to treat part of the clean training data of the target model as
""poisoned data,"" and capture the characteristics of these data that are more
sensitive to the model (typically called robust features) to construct
""triggers."" These triggers can be added to any input example to mislead the
target model, similar to backdoor attacks. We validate the effectiveness of
CleanSheet through extensive experiments on 5 datasets, 79 normally trained
models, 68 pruned models, and 39 defensive models. Results show that CleanSheet
exhibits performance comparable to state-of-the-art backdoor attacks, achieving
an average attack success rate (ASR) of 97.5% on CIFAR-100 and 92.4% on GTSRB,
respectively. Furthermore, CleanSheet consistently maintains a high ASR, when
confronted with various mainstream backdoor defenses.",CVPR
"We present Multi-View Attentive Contextualization (MvACon), a simple yet
effective method for improving 2D-to-3D feature lifting in query-based
multi-view 3D (MV3D) object detection. Despite remarkable progress witnessed in
the field of query-based MV3D object detection, prior art often suffers from
either the lack of exploiting high-resolution 2D features in dense
attention-based lifting, due to high computational costs, or from
insufficiently dense grounding of 3D queries to multi-scale 2D features in
sparse attention-based lifting. Our proposed MvACon hits the two birds with one
stone using a representationally dense yet computationally sparse attentive
feature contextualization scheme that is agnostic to specific 2D-to-3D feature
lifting approaches. In experiments, the proposed MvACon is thoroughly tested on
the nuScenes benchmark, using both the BEVFormer and its recent 3D deformable
attention (DFA3D) variant, as well as the PETR, showing consistent detection
performance improvement, especially in enhancing performance in location,
orientation, and velocity prediction. It is also tested on the Waymo-mini
benchmark using BEVFormer with similar improvement. We qualitatively and
quantitatively show that global cluster-based contexts effectively encode dense
scene-level contexts for MV3D object detection. The promising results of our
proposed MvACon reinforces the adage in computer vision -- ``(contextualized)
feature matters"".",CVPR
"Self-supervised feature reconstruction methods have shown promising advances
in industrial image anomaly detection and localization. Despite this progress,
these methods still face challenges in synthesizing realistic and diverse
anomaly samples, as well as addressing the feature redundancy and pre-training
bias of pre-trained feature. In this work, we introduce RealNet, a feature
reconstruction network with realistic synthetic anomaly and adaptive feature
selection. It is incorporated with three key innovations: First, we propose
Strength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion
process-based synthesis strategy capable of generating samples with varying
anomaly strengths that mimic the distribution of real anomalous samples.
Second, we develop Anomaly-aware Features Selection (AFS), a method for
selecting representative and discriminative pre-trained feature subsets to
improve anomaly detection performance while controlling computational costs.
Third, we introduce Reconstruction Residuals Selection (RRS), a strategy that
adaptively selects discriminative residuals for comprehensive identification of
anomalous regions across multiple levels of granularity. We assess RealNet on
four benchmark datasets, and our results demonstrate significant improvements
in both Image AUROC and Pixel AUROC compared to the current state-o-the-art
methods. The code, data, and models are available at
https://github.com/cnulab/RealNet.",CVPR
"We introduce CyberDemo, a novel approach to robotic imitation learning that
leverages simulated human demonstrations for real-world tasks. By incorporating
extensive data augmentation in a simulated environment, CyberDemo outperforms
traditional in-domain real-world demonstrations when transferred to the real
world, handling diverse physical and visual conditions. Regardless of its
affordability and convenience in data collection, CyberDemo outperforms
baseline methods in terms of success rates across various tasks and exhibits
generalizability with previously unseen objects. For example, it can rotate
novel tetra-valve and penta-valve, despite human demonstrations only involving
tri-valves. Our research demonstrates the significant potential of simulated
human demonstrations for real-world dexterous manipulation tasks. More details
can be found at https://cyber-demo.github.io",CVPR
"We present InterHandGen, a novel framework that learns the generative prior
of two-hand interaction. Sampling from our model yields plausible and diverse
two-hand shapes in close interaction with or without an object. Our prior can
be incorporated into any optimization or learning methods to reduce ambiguity
in an ill-posed setup. Our key observation is that directly modeling the joint
distribution of multiple instances imposes high learning complexity due to its
combinatorial nature. Thus, we propose to decompose the modeling of joint
distribution into the modeling of factored unconditional and conditional single
instance distribution. In particular, we introduce a diffusion model that
learns the single-hand distribution unconditional and conditional to another
hand via conditioning dropout. For sampling, we combine anti-penetration and
classifier-free guidance to enable plausible generation. Furthermore, we
establish the rigorous evaluation protocol of two-hand synthesis, where our
method significantly outperforms baseline generative models in terms of
plausibility and diversity. We also demonstrate that our diffusion prior can
boost the performance of two-hand reconstruction from monocular in-the-wild
images, achieving new state-of-the-art accuracy.",CVPR
"Text-to-image (T2I) generative models have attracted significant attention
and found extensive applications within and beyond academic research. For
example, the Civitai community, a platform for T2I innovation, currently hosts
an impressive array of 74,492 distinct models. However, this diversity presents
a formidable challenge in selecting the most appropriate model and parameters,
a process that typically requires numerous trials. Drawing inspiration from the
tool usage research of large language models (LLMs), we introduce DiffAgent, an
LLM agent designed to screen the accurate selection in seconds via API calls.
DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to
accurately align T2I API responses with user input in accordance with human
preferences. To train and evaluate DiffAgent's capabilities, we present
DABench, a comprehensive dataset encompassing an extensive range of T2I APIs
from the community. Our evaluations reveal that DiffAgent not only excels in
identifying the appropriate T2I API but also underscores the effectiveness of
the SFTA training framework. Codes are available at
https://github.com/OpenGVLab/DiffAgent.",CVPR
"We contribute the Habitat Synthetic Scene Dataset, a dataset of 211
high-quality 3D scenes, and use it to test navigation agent generalization to
realistic 3D environments. Our dataset represents real interiors and contains a
diverse set of 18,656 models of real-world objects. We investigate the impact
of synthetic 3D scene dataset scale and realism on the task of training
embodied agents to find and navigate to objects (ObjectGoal navigation). By
comparing to synthetic 3D scene datasets from prior work, we find that scale
helps in generalization, but the benefits quickly saturate, making visual
fidelity and correlation to real-world scenes more important. Our experiments
show that agents trained on our smaller-scale dataset can match or outperform
agents trained on much larger datasets. Surprisingly, we observe that agents
trained on just 122 scenes from our dataset outperform agents trained on 10,000
scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in
real-world scanned environments.",CVPR
"We develop a theory for the representation of opaque solids as volumes.
Starting from a stochastic representation of opaque solids as random indicator
functions, we prove the conditions under which such solids can be modeled using
exponential volumetric transport. We also derive expressions for the volumetric
attenuation coefficient as a functional of the probability distributions of the
underlying indicator functions. We generalize our theory to account for
isotropic and anisotropic scattering at different parts of the solid, and for
representations of opaque solids as stochastic implicit surfaces. We derive our
volumetric representation from first principles, which ensures that it
satisfies physical constraints such as reciprocity and reversibility. We use
our theory to explain, compare, and correct previous volumetric
representations, as well as propose meaningful extensions that lead to improved
performance in 3D reconstruction tasks.",CVPR
"Zero-shot image captioning (IC) without well-paired image-text data can be
divided into two categories, training-free and text-only-training. Generally,
these two types of methods realize zero-shot IC by integrating pretrained
vision-language models like CLIP for image-text similarity evaluation and a
pre-trained language model (LM) for caption generation. The main difference
between them is whether using a textual corpus to train the LM. Though
achieving attractive performance w.r.t. some metrics, existing methods often
exhibit some common drawbacks. Training-free methods tend to produce
hallucinations, while text-only-training often lose generalization capability.
To move forward, in this paper, we propose a novel Memory-Augmented zero-shot
image Captioning framework (MeaCap). Specifically, equipped with a textual
memory, we introduce a retrieve-then-filter module to get key concepts that are
highly related to the image. By deploying our proposed memory-augmented
visual-related fusion score in a keywords-to-sentence LM, MeaCap can generate
concept-centered captions that keep high consistency with the image with fewer
hallucinations and more world-knowledge. The framework of MeaCap achieves the
state-of-the-art performance on a series of zero-shot IC settings. Our code is
available at https://github.com/joeyz0z/MeaCap.",CVPR
"Monocular 3D detection (M3D) aims for precise 3D object localization from a
single-view image which usually involves labor-intensive annotation of 3D
detection boxes. Weakly supervised M3D has recently been studied to obviate the
3D annotation process by leveraging many existing 2D annotations, but it often
requires extra training data such as LiDAR point clouds or multi-view images
which greatly degrades its applicability and usability in various applications.
We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that
exploits depth information to achieve M3D with a single-view image exclusively
without any 3D annotations or other training data. One key design in SKD-WM3D
is a self-knowledge distillation framework, which transforms image features
into 3D-like representations by fusing depth information and effectively
mitigates the inherent depth ambiguity in monocular scenarios with little
computational overhead in inference. In addition, we design an
uncertainty-aware distillation loss and a gradient-targeted transfer modulation
strategy which facilitate knowledge acquisition and knowledge transfer,
respectively. Extensive experiments show that SKD-WM3D surpasses the
state-of-the-art clearly and is even on par with many fully supervised methods.",CVPR
"We present ""SemCity,"" a 3D diffusion model for semantic scene generation in
real-world outdoor environments. Most 3D diffusion models focus on generating a
single object, synthetic indoor scenes, or synthetic outdoor scenes, while the
generation of real-world outdoor scenes is rarely addressed. In this paper, we
concentrate on generating a real-outdoor scene through learning a diffusion
model on a real-world outdoor dataset. In contrast to synthetic data,
real-outdoor datasets often contain more empty spaces due to sensor
limitations, causing challenges in learning real-outdoor distributions. To
address this issue, we exploit a triplane representation as a proxy form of
scene distributions to be learned by our diffusion model. Furthermore, we
propose a triplane manipulation that integrates seamlessly with our triplane
diffusion model. The manipulation improves our diffusion model's applicability
in a variety of downstream tasks related to outdoor scene generation such as
scene inpainting, scene outpainting, and semantic scene completion refinements.
In experimental results, we demonstrate that our triplane diffusion model shows
meaningful generation results compared with existing work in a real-outdoor
dataset, SemanticKITTI. We also show our triplane manipulation facilitates
seamlessly adding, removing, or modifying objects within a scene. Further, it
also enables the expansion of scenes toward a city-level scale. Finally, we
evaluate our method on semantic scene completion refinements where our
diffusion model enhances predictions of semantic scene completion networks by
learning scene distribution. Our code is available at
https://github.com/zoomin-lee/SemCity.",CVPR
"Event-based cameras provide accurate and high temporal resolution
measurements for performing computer vision tasks in challenging scenarios,
such as high-dynamic range environments and fast-motion maneuvers. Despite
their advantages, utilizing deep learning for event-based vision encounters a
significant obstacle due to the scarcity of annotated data caused by the
relatively recent emergence of event-based cameras. To overcome this
limitation, leveraging the knowledge available from annotated data obtained
with conventional frame-based cameras presents an effective solution based on
unsupervised domain adaptation. We propose a new algorithm tailored for
adapting a deep neural network trained on annotated frame-based data to
generalize well on event-based unannotated data. Our approach incorporates
uncorrelated conditioning and self-supervised learning in an adversarial
learning scheme to close the gap between the two source and target domains. By
applying self-supervised learning, the algorithm learns to align the
representations of event-based data with those from frame-based camera data,
thereby facilitating knowledge transfer.Furthermore, the inclusion of
uncorrelated conditioning ensures that the adapted model effectively
distinguishes between event-based and conventional data, enhancing its ability
to classify event-based images accurately.Through empirical experimentation and
evaluation, we demonstrate that our algorithm surpasses existing approaches
designed for the same purpose using two benchmarks. The superior performance of
our solution is attributed to its ability to effectively utilize annotated data
from frame-based cameras and transfer the acquired knowledge to the event-based
vision domain.",CVPR
"We propose a novel approach to the action segmentation task for long,
untrimmed videos, based on solving an optimal transport problem. By encoding a
temporal consistency prior into a Gromov-Wasserstein problem, we are able to
decode a temporally consistent segmentation from a noisy affinity/matching cost
matrix between video frames and action classes. Unlike previous approaches, our
method does not require knowing the action order for a video to attain temporal
consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can
be efficiently solved on GPUs using a few iterations of projected mirror
descent. We demonstrate the effectiveness of our method in an unsupervised
learning setting, where our method is used to generate pseudo-labels for
self-training. We evaluate our segmentation approach and unsupervised learning
pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly
datasets, yielding state-of-the-art results for the unsupervised video action
segmentation task.",CVPR
"Neural implicit representation of geometric shapes has witnessed considerable
advancements in recent years. However, common distance field based implicit
representations, specifically signed distance field (SDF) for watertight shapes
or unsigned distance field (UDF) for arbitrary shapes, routinely suffer from
degradation of reconstruction accuracy when converting to explicit surface
points and meshes. In this paper, we introduce a novel neural implicit
representation based on unsigned orthogonal distance fields (UODFs). In UODFs,
the minimal unsigned distance from any spatial point to the shape surface is
defined solely in one orthogonal direction, contrasting with the
multi-directional determination made by SDF and UDF. Consequently, every point
in the 3D UODFs can directly access its closest surface points along three
orthogonal directions. This distinctive feature leverages the accurate
reconstruction of surface points without interpolation errors. We verify the
effectiveness of UODFs through a range of reconstruction examples, extending
from simple watertight or non-watertight shapes to complex shapes that include
hollows, internal or assembling structures.",CVPR
"Given the power of vision transformers, a new learning paradigm, pre-training
and then prompting, makes it more efficient and effective to address downstream
visual recognition tasks. In this paper, we identify a novel security threat
towards such a paradigm from the perspective of backdoor attacks. Specifically,
an extra prompt token, called the switch token in this work, can turn the
backdoor mode on, i.e., converting a benign model into a backdoored one. Once
under the backdoor mode, a specific trigger can force the model to predict a
target class. It poses a severe risk to the users of cloud API, since the
malicious behavior can not be activated and detected under the benign mode,
thus making the attack very stealthy. To attack a pre-trained model, our
proposed attack, named SWARM, learns a trigger and prompt tokens including a
switch token. They are optimized with the clean loss which encourages the model
always behaves normally even the trigger presents, and the backdoor loss that
ensures the backdoor can be activated by the trigger when the switch is on.
Besides, we utilize the cross-mode feature distillation to reduce the effect of
the switch token on clean samples. The experiments on diverse visual
recognition tasks confirm the success of our switchable backdoor attack, i.e.,
achieving 95%+ attack success rate, and also being hard to be detected and
removed. Our code is available at https://github.com/20000yshust/SWARM.",CVPR
"Novel-view synthesis of specular objects like shiny metals or glossy paints
remains a significant challenge. Not only the glossy appearance but also global
illumination effects, including reflections of other objects in the
environment, are critical components to faithfully reproduce a scene. In this
paper, we present Neural Directional Encoding (NDE), a view-dependent
appearance encoding of neural radiance fields (NeRF) for rendering specular
objects. NDE transfers the concept of feature-grid-based spatial encoding to
the angular domain, significantly improving the ability to model high-frequency
angular signals. In contrast to previous methods that use encoding functions
with only angular input, we additionally cone-trace spatial features to obtain
a spatially varying directional encoding, which addresses the challenging
interreflection effects. Extensive experiments on both synthetic and real
datasets show that a NeRF model with NDE (1) outperforms the state of the art
on view synthesis of specular objects, and (2) works with small networks to
allow fast (real-time) inference. The project webpage and source code are
available at: \url{https://lwwu2.github.io/nde/}.",CVPR
"Generating novel views of an object from a single image is a challenging
task. It requires an understanding of the underlying 3D structure of the object
from an image and rendering high-quality, spatially consistent new views. While
recent methods for view synthesis based on diffusion have shown great progress,
achieving consistency among various view estimates and at the same time abiding
by the desired camera pose remains a critical problem yet to be solved. In this
work, we demonstrate a strikingly simple method, where we utilize a pre-trained
video diffusion model to solve this problem. Our key idea is that synthesizing
a novel view could be reformulated as synthesizing a video of a camera going
around the object of interest -- a scanning video -- which then allows us to
leverage the powerful priors that a video diffusion model would have learned.
Thus, to perform novel-view synthesis, we create a smooth camera trajectory to
the target view that we wish to render, and denoise using both a
view-conditioned diffusion model and a video diffusion model. By doing so, we
obtain a highly consistent novel view synthesis, outperforming the state of the
art.",CVPR
"Artifact-free super-resolution (SR) aims to translate low-resolution images
into their high-resolution counterparts with a strict integrity of the original
content, eliminating any distortions or synthetic details. While traditional
diffusion-based SR techniques have demonstrated remarkable abilities to enhance
image detail, they are prone to artifact introduction during iterative
procedures. Such artifacts, ranging from trivial noise to unauthentic textures,
deviate from the true structure of the source image, thus challenging the
integrity of the super-resolution process. In this work, we propose
Self-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that
delves into the latent space to effectively identify and mitigate the
propagation of artifacts. Our SARGD begins by using an artifact detector to
identify implausible pixels, creating a binary mask that highlights artifacts.
Following this, the Reality Guidance Refinement (RGR) process refines artifacts
by integrating this mask with realistic latent representations, improving
alignment with the original image. Nonetheless, initial realistic-latent
representations from lower-quality images result in over-smoothing in the final
output. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.
It dynamically computes a reality score, enhancing the sharpness of the
realistic latent. These alternating mechanisms collectively achieve
artifact-free super-resolution. Extensive experiments demonstrate the
superiority of our method, delivering detailed artifact-free high-resolution
images while reducing sampling steps by 2X. We release our code at
https://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.",CVPR
"Novel view synthesis for dynamic scenes is still a challenging problem in
computer vision and graphics. Recently, Gaussian splatting has emerged as a
robust technique to represent static scenes and enable high-quality and
real-time novel view synthesis. Building upon this technique, we propose a new
representation that explicitly decomposes the motion and appearance of dynamic
scenes into sparse control points and dense Gaussians, respectively. Our key
idea is to use sparse control points, significantly fewer in number than the
Gaussians, to learn compact 6 DoF transformation bases, which can be locally
interpolated through learned interpolation weights to yield the motion field of
3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF
transformations for each control point, which reduces learning complexities,
enhances learning abilities, and facilitates obtaining temporal and spatial
coherent motion patterns. Then, we jointly learn the 3D Gaussians, the
canonical space locations of control points, and the deformation MLP to
reconstruct the appearance, geometry, and dynamics of 3D scenes. During
learning, the location and number of control points are adaptively adjusted to
accommodate varying motion complexities in different regions, and an ARAP loss
following the principle of as rigid as possible is developed to enforce spatial
continuity and local rigidity of learned motions. Finally, thanks to the
explicit sparse motion representation and its decomposition from appearance,
our method can enable user-controlled motion editing while retaining
high-fidelity appearances. Extensive experiments demonstrate that our approach
outperforms existing approaches on novel view synthesis with a high rendering
speed and enables novel appearance-preserved motion editing applications.
Project page: https://yihua7.github.io/SC-GS-web/",CVPR
"With recent developments in Embodied Artificial Intelligence (EAI) research,
there has been a growing demand for high-quality, large-scale interactive scene
generation. While prior methods in scene synthesis have prioritized the
naturalness and realism of the generated scenes, the physical plausibility and
interactivity of scenes have been largely left unexplored. To address this
disparity, we introduce PhyScene, a novel method dedicated to generating
interactive 3D scenes characterized by realistic layouts, articulated objects,
and rich physical interactivity tailored for embodied agents. Based on a
conditional diffusion model for capturing scene layouts, we devise novel
physics- and interactivity-based guidance mechanisms that integrate constraints
from object collision, room layout, and object reachability. Through extensive
experiments, we demonstrate that PhyScene effectively leverages these guidance
functions for physically interactable scene synthesis, outperforming existing
state-of-the-art scene synthesis methods by a large margin. Our findings
suggest that the scenes generated by PhyScene hold considerable potential for
facilitating diverse skill acquisition among agents within interactive
environments, thereby catalyzing further advancements in embodied AI research.
Project website: http://physcene.github.io.",CVPR
"3D reconstruction from a single-view is challenging because of the ambiguity
from monocular cues and lack of information about occluded regions. Neural
radiance fields (NeRF), while popular for view synthesis and 3D reconstruction,
are typically reliant on multi-view images. Existing methods for single-view 3D
reconstruction with NeRF rely on either data priors to hallucinate views of
occluded regions, which may not be physically accurate, or shadows observed by
RGB cameras, which are difficult to detect in ambient light and low albedo
backgrounds. We propose using time-of-flight data captured by a single-photon
avalanche diode to overcome these limitations. Our method models two-bounce
optical paths with NeRF, using lidar transient data for supervision. By
leveraging the advantages of both NeRF and two-bounce light measured by lidar,
we demonstrate that we can reconstruct visible and occluded geometry without
data priors or reliance on controlled ambient lighting or scene albedo. In
addition, we demonstrate improved generalization under practical constraints on
sensor spatial- and temporal-resolution. We believe our method is a promising
direction as single-photon lidars become ubiquitous on consumer devices, such
as phones, tablets, and headsets.",CVPR
"Images suffer from heavy spatial redundancy because pixels in neighboring
regions are spatially correlated. Existing approaches strive to overcome this
limitation by reducing less meaningful image regions. However, current leading
methods rely on supervisory signals. They may compel models to preserve content
that aligns with labeled categories and discard content belonging to unlabeled
categories. This categorical inductive bias makes these methods less effective
in real-world scenarios. To address this issue, we propose a self-supervised
framework for image redundancy reduction called Learning to Rank Patches
(LTRP). We observe that image reconstruction of masked image modeling models is
sensitive to the removal of visible patches when the masking ratio is high
(e.g., 90\%). Building upon it, we implement LTRP via two steps: inferring the
semantic density score of each patch by quantifying variation between
reconstructions with and without this patch, and learning to rank the patches
with the pseudo score. The entire process is self-supervised, thus getting out
of the dilemma of categorical inductive bias. We design extensive experiments
on different datasets and tasks. The results demonstrate that LTRP outperforms
both supervised and other self-supervised methods due to the fair assessment of
image content.",CVPR
"Few-shot semantic segmentation (FSS) has achieved great success on segmenting
objects of novel classes, supported by only a few annotated samples. However,
existing FSS methods often underperform in the presence of domain shifts,
especially when encountering new domain styles that are unseen during training.
It is suboptimal to directly adapt or generalize the entire model to new
domains in the few-shot scenario. Instead, our key idea is to adapt a small
adapter for rectifying diverse target domain styles to the source domain.
Consequently, the rectified target domain features can fittingly benefit from
the well-optimized source domain segmentation model, which is intently trained
on sufficient source domain data. Training domain-rectifying adapter requires
sufficiently diverse target domains. We thus propose a novel local-global style
perturbation method to simulate diverse potential target domains by
perturbating the feature channel statistics of the individual images and
collective statistics of the entire source domain, respectively. Additionally,
we propose a cyclic domain alignment module to facilitate the adapter
effectively rectifying domains using a reverse domain rectification
supervision. The adapter is trained to rectify the image features from diverse
synthesized target domains to align with the source domain. During testing on
target domains, we start by rectifying the image features and then conduct
few-shot segmentation on the domain-rectified features. Extensive experiments
demonstrate the effectiveness of our method, achieving promising results on
cross-domain few-shot semantic segmentation tasks. Our code is available at
https://github.com/Matt-Su/DR-Adapter.",CVPR
"We present GigaPose, a fast, robust, and accurate method for CAD-based novel
object pose estimation in RGB images. GigaPose first leverages discriminative
""templates"", rendered images of the CAD models, to recover the out-of-plane
rotation and then uses patch correspondences to estimate the four remaining
parameters. Our approach samples templates in only a two-degrees-of-freedom
space instead of the usual three and matches the input image to the templates
using fast nearest-neighbor search in feature space, results in a speedup
factor of 35x compared to the state of the art. Moreover, GigaPose is
significantly more robust to segmentation errors. Our extensive evaluation on
the seven core datasets of the BOP challenge demonstrates that it achieves
state-of-the-art accuracy and can be seamlessly integrated with existing
refinement methods. Additionally, we show the potential of GigaPose with 3D
models predicted by recent work on 3D reconstruction from a single image,
relaxing the need for CAD models and making 6D pose object estimation much more
convenient. Our source code and trained models are publicly available at
https://github.com/nv-nguyen/gigaPose",CVPR
"Predicting the future motion of surrounding agents is essential for
autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed
environments. Context information, such as road maps and surrounding agents'
states, provides crucial geometric and semantic information for motion behavior
prediction. To this end, recent works explore two-stage prediction frameworks
where coarse trajectories are first proposed, and then used to select critical
context information for trajectory refinement. However, they either incur a
large amount of computation or bring limited improvement, if not both. In this
paper, we introduce a novel scenario-adaptive refinement strategy, named
SmartRefine, to refine prediction with minimal additional computation.
Specifically, SmartRefine can comprehensively adapt refinement configurations
based on each scenario's properties, and smartly chooses the number of
refinement iterations by introducing a quality score to measure the prediction
quality and remaining refinement potential of each scenario. SmartRefine is
designed as a generic and flexible approach that can be seamlessly integrated
into most state-of-the-art motion prediction models. Experiments on Argoverse
(1 & 2) show that our method consistently improves the prediction accuracy of
multiple state-of-the-art prediction models. Specifically, by adding
SmartRefine to QCNet, we outperform all published ensemble-free works on the
Argoverse 2 leaderboard (single agent track) at submission. Comprehensive
studies are also conducted to ablate design choices and explore the mechanism
behind multi-iteration refinement. Codes are available at
https://github.com/opendilab/SmartRefine/",CVPR
"In this study, we explore Transformer-based diffusion models for image and
video generation. Despite the dominance of Transformer architectures in various
fields due to their flexibility and scalability, the visual generative domain
primarily utilizes CNN-based U-Net architectures, particularly in
diffusion-based models. We introduce GenTron, a family of Generative models
employing Transformer-based diffusion, to address this gap. Our initial step
was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a
process involving thorough empirical exploration of the conditioning mechanism.
We then scale GenTron from approximately 900M to over 3B parameters, observing
significant improvements in visual quality. Furthermore, we extend GenTron to
text-to-video generation, incorporating novel motion-free guidance to enhance
video quality. In human evaluations against SDXL, GenTron achieves a 51.1% win
rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text
alignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench,
underscoring its strengths in compositional generation. We believe this work
will provide meaningful insights and serve as a valuable reference for future
research.",CVPR
"Current techniques for deep neural network (DNN) pruning often involve
intricate multi-step processes that require domain-specific expertise, making
their widespread adoption challenging. To address the limitation, the
Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for
additional fine-tuning steps by directly training and compressing a general DNN
from scratch. Nevertheless, the static design of optimizers (in OTO) can lead
to convergence issues of local optima. In this paper, we proposed the
Auto-Train-Once (ATO), an innovative network pruning algorithm designed to
automatically reduce the computational and storage costs of DNNs. During the
model training phase, our approach not only trains the target model but also
leverages a controller network as an architecture generator to guide the
learning of target model weights. Furthermore, we developed a novel stochastic
gradient algorithm that enhances the coordination between model training and
controller network training, thereby improving pruning performance. We provide
a comprehensive convergence analysis as well as extensive experiments, and the
results show that our approach achieves state-of-the-art performance across
various model architectures (including ResNet18, ResNet34, ResNet50, ResNet56,
and MobileNetv2) on standard benchmark datasets (CIFAR-10, CIFAR-100, and
ImageNet).",CVPR
"We present GigaPose, a fast, robust, and accurate method for CAD-based novel
object pose estimation in RGB images. GigaPose first leverages discriminative
""templates"", rendered images of the CAD models, to recover the out-of-plane
rotation and then uses patch correspondences to estimate the four remaining
parameters. Our approach samples templates in only a two-degrees-of-freedom
space instead of the usual three and matches the input image to the templates
using fast nearest-neighbor search in feature space, results in a speedup
factor of 35x compared to the state of the art. Moreover, GigaPose is
significantly more robust to segmentation errors. Our extensive evaluation on
the seven core datasets of the BOP challenge demonstrates that it achieves
state-of-the-art accuracy and can be seamlessly integrated with existing
refinement methods. Additionally, we show the potential of GigaPose with 3D
models predicted by recent work on 3D reconstruction from a single image,
relaxing the need for CAD models and making 6D pose object estimation much more
convenient. Our source code and trained models are publicly available at
https://github.com/nv-nguyen/gigaPose",CVPR
"Whole Slide Image (WSI) classification is often formulated as a Multiple
Instance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) have
demonstrated remarkable performance in WSI classification. However, existing
methods leverage coarse-grained pathogenetic descriptions for visual
representation supervision, which are insufficient to capture the complex
visual appearance of pathogenetic images, hindering the generalizability of
models on diverse downstream tasks. Additionally, processing high-resolution
WSIs can be computationally expensive. In this paper, we propose a novel
""Fine-grained Visual-Semantic Interaction"" (FiVE) framework for WSI
classification. It is designed to enhance the model's generalizability by
leveraging the interaction between localized visual patterns and fine-grained
pathological semantics. Specifically, with meticulously designed queries, we
start by utilizing a large language model to extract fine-grained pathological
descriptions from various non-standardized raw reports. The output descriptions
are then reconstructed into fine-grained labels used for training. By
introducing a Task-specific Fine-grained Semantics (TFS) module, we enable
prompts to capture crucial visual information in WSIs, which enhances
representation learning and augments generalization capabilities significantly.
Furthermore, given that pathological visual patterns are redundantly
distributed across tissue slices, we sample a subset of visual instances during
training. Our method demonstrates robust generalizability and strong
transferability, dominantly outperforming the counterparts on the TCGA Lung
Cancer dataset with at least 9.19% higher accuracy in few-shot experiments. The
code is available at: https://github.com/ls1rius/WSI_FiVE.",CVPR
"We concentrate on a novel human-centric image synthesis task, that is, given
only one reference facial photograph, it is expected to generate specific
individual images with diverse head positions, poses, facial expressions, and
illuminations in different contexts. To accomplish this goal, we argue that our
generative model should be capable of the following favorable characteristics:
(1) a strong visual and semantic understanding of our world and human society
for basic object and human image generation. (2) generalizable identity
preservation ability. (3) flexible and fine-grained head control. Recently,
large pre-trained text-to-image diffusion models have shown remarkable results,
serving as a powerful generative foundation. As a basis, we aim to unleash the
above two capabilities of the pre-trained model. In this work, we present a new
framework named CapHuman. We embrace the ""encode then learn to align"" paradigm,
which enables generalizable identity preservation for new individuals without
cumbersome tuning at inference. CapHuman encodes identity features and then
learns to align them into the latent space. Moreover, we introduce the 3D
facial prior to equip our model with control over the human head in a flexible
and 3D-consistent manner. Extensive qualitative and quantitative analyses
demonstrate our CapHuman can produce well-identity-preserved, photo-realistic,
and high-fidelity portraits with content-rich representations and various head
renditions, superior to established baselines. Code and checkpoint will be
released at https://github.com/VamosC/CapHuman.",CVPR
"In this paper, we introduce VoteCut, an innovative method for unsupervised
object discovery that leverages feature representations from multiple
self-supervised models. VoteCut employs normalized-cut based graph
partitioning, clustering and a pixel voting approach. Additionally, We present
CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels,
generated by VoteCut, and a novel soft target loss to refine segmentation
accuracy. Through rigorous evaluations across multiple datasets and several
unsupervised setups, our methods demonstrate significant improvements in
comparison to previous state-of-the-art models. Our ablation studies further
highlight the contributions of each component, revealing the robustness and
efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for
future advancements in image segmentation.",CVPR
"Text-to-image diffusion models have recently received increasing interest for
their astonishing ability to produce high-fidelity images from solely text
inputs. Subsequent research efforts aim to exploit and apply their capabilities
to real image editing. However, existing image-to-image methods are often
inefficient, imprecise, and of limited versatility. They either require
time-consuming fine-tuning, deviate unnecessarily strongly from the input
image, and/or lack support for multiple, simultaneous edits. To address these
issues, we introduce LEDITS++, an efficient yet versatile and precise textual
image manipulation technique. LEDITS++'s novel inversion approach requires no
tuning nor optimization and produces high-fidelity results with a few diffusion
steps. Second, our methodology supports multiple simultaneous edits and is
architecture-agnostic. Third, we use a novel implicit masking technique that
limits changes to relevant image regions. We propose the novel TEdBench++
benchmark as part of our exhaustive evaluation. Our results demonstrate the
capabilities of LEDITS++ and its improvements over previous methods. The
project page is available at https://leditsplusplus-project.static.hf.space .",CVPR
"Spiking Neural Networks (SNNs) have been widely praised for their high energy
efficiency and immense potential. However, comprehensive research that
critically contrasts and correlates SNNs with quantized Artificial Neural
Networks (ANNs) remains scant, often leading to skewed comparisons lacking
fairness towards ANNs. This paper introduces a unified perspective,
illustrating that the time steps in SNNs and quantized bit-widths of activation
values present analogous representations. Building on this, we present a more
pragmatic and rational approach to estimating the energy consumption of SNNs.
Diverging from the conventional Synaptic Operations (SynOps), we champion the
""Bit Budget"" concept. This notion permits an intricate discourse on
strategically allocating computational and storage resources between weights,
activation values, and temporal steps under stringent hardware constraints.
Guided by the Bit Budget paradigm, we discern that pivoting efforts towards
spike patterns and weight quantization, rather than temporal attributes,
elicits profound implications for model performance. Utilizing the Bit Budget
for holistic design consideration of SNNs elevates model performance across
diverse data types, encompassing static imagery and neuromorphic datasets. Our
revelations bridge the theoretical chasm between SNNs and quantized ANNs and
illuminate a pragmatic trajectory for future endeavors in energy-efficient
neural computations.",CVPR
"Successfully addressing a wide variety of tasks is a core ability of
autonomous agents, which requires flexibly adapting the underlying
decision-making strategies and, as we argue in this work, also adapting the
underlying perception modules. An analogical argument would be the human visual
system, which uses top-down signals to focus attention determined by the
current task. Similarly, in this work, we adapt pre-trained large vision models
conditioned on specific downstream tasks in the context of multi-task policy
learning. We introduce task-conditioned adapters that do not require finetuning
any pre-trained weights, combined with a single policy trained with behavior
cloning and capable of addressing multiple tasks. We condition the policy and
visual adapters on task embeddings, which can be selected at inference if the
task is known, or alternatively inferred from a set of example demonstrations.
To this end, we propose a new optimization-based estimator. We evaluate the
method on a wide variety of tasks of the CortexBench benchmark and show that,
compared to existing work, it can be addressed with a single policy. In
particular, we demonstrate that adapting visual features is a key design choice
and that the method generalizes to unseen tasks given visual demonstrations.",CVPR
"Video anomaly detection (VAD) with weak supervision has achieved remarkable
performance in utilizing video-level labels to discriminate whether a video
frame is normal or abnormal. However, current approaches are inherently limited
to a closed-set setting and may struggle in open-world applications where there
can be anomaly categories in the test data unseen during training. A few recent
studies attempt to tackle a more realistic setting, open-set VAD, which aims to
detect unseen anomalies given seen anomalies and normal videos. However, such a
setting focuses on predicting frame anomaly scores, having no ability to
recognize the specific categories of anomalies, despite the fact that this
ability is essential for building more informed video surveillance systems.
This paper takes a step further and explores open-vocabulary video anomaly
detection (OVVAD), in which we aim to leverage pre-trained large models to
detect and categorize seen and unseen anomalies. To this end, we propose a
model that decouples OVVAD into two mutually complementary tasks --
class-agnostic detection and class-specific classification -- and jointly
optimizes both tasks. Particularly, we devise a semantic knowledge injection
module to introduce semantic knowledge from large language models for the
detection task, and design a novel anomaly synthesis module to generate pseudo
unseen anomaly videos with the help of large vision generation models for the
classification task. These semantic knowledge and synthesis anomalies
substantially extend our model's capability in detecting and categorizing a
variety of seen and unseen anomalies. Extensive experiments on three
widely-used benchmarks demonstrate our model achieves state-of-the-art
performance on OVVAD task.",CVPR
"Recent works have shown that generative models leave traces of their
underlying generative process on the generated samples, broadly referred to as
fingerprints of a generative model, and have studied their utility in detecting
synthetic images from real ones. However, the extend to which these
fingerprints can distinguish between various types of synthetic image and help
identify the underlying generative process remain under-explored. In
particular, the very definition of a fingerprint remains unclear, to our
knowledge. To that end, in this work, we formalize the definition of artifact
and fingerprint in generative models, propose an algorithm for computing them
in practice, and finally study its effectiveness in distinguishing a large
array of different generative models. We find that using our proposed
definition can significantly improve the performance on the task of identifying
the underlying generative process from samples (model attribution) compared to
existing methods. Additionally, we study the structure of the fingerprints, and
observe that it is very predictive of the effect of different design choices on
the generative process.",CVPR
"In this work, we investigate the potential of a large language model (LLM) to
directly comprehend visual signals without the necessity of fine-tuning on
multi-modal datasets. The foundational concept of our method views an image as
a linguistic entity, and translates it to a set of discrete words derived from
the LLM's vocabulary. To achieve this, we present the Vision-to-Language
Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a
``foreign language'' with the combined aid of an encoder-decoder, the LLM
vocabulary, and a CLIP model. With this innovative image encoding, the LLM
gains the ability not only for visual comprehension but also for image
denoising and restoration in an auto-regressive fashion-crucially, without any
fine-tuning. We undertake rigorous experiments to validate our method,
encompassing understanding tasks like image recognition, image captioning, and
visual question answering, as well as image denoising tasks like inpainting,
outpainting, deblurring, and shift restoration. Code and models are available
at https://github.com/zh460045050/V2L-Tokenizer.",CVPR
"Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.",CVPR
"Video-based visual relation detection tasks, such as video scene graph
generation, play important roles in fine-grained video understanding. However,
current video visual relation detection datasets have two main limitations that
hinder the progress of research in this area. First, they do not explore
complex human-human interactions in multi-person scenarios. Second, the
relation types of existing datasets have relatively low-level semantics and can
be often recognized by appearance or simple prior information, without the need
for detailed spatio-temporal context reasoning. Nevertheless, comprehending
high-level interactions between humans is crucial for understanding complex
multi-person videos, such as sports and surveillance videos. To address this
issue, we propose a new video visual relation detection task: video human-human
interaction detection, and build a dataset named SportsHHI for it. SportsHHI
contains 34 high-level interaction classes from basketball and volleyball
sports. 118,075 human bounding boxes and 50,649 interaction instances are
annotated on 11,398 keyframes. To benchmark this, we propose a two-stage
baseline method and conduct extensive experiments to reveal the key factors for
a successful human-human interaction detector. We hope that SportsHHI can
stimulate research on human interaction understanding in videos and promote the
development of spatio-temporal context modeling techniques in video visual
relation detection.",CVPR
"We propose a computational imaging method for time-efficient light-field
acquisition that combines a coded aperture with an event-based camera.
Different from the conventional coded-aperture imaging method, our method
applies a sequence of coding patterns during a single exposure for an image
frame. The parallax information, which is related to the differences in coding
patterns, is recorded as events. The image frame and events, all of which are
measured in a single exposure, are jointly used to computationally reconstruct
a light field. We also designed an algorithm pipeline for our method that is
end-to-end trainable on the basis of deep optics and compatible with real
camera hardware. We experimentally showed that our method can achieve more
accurate reconstruction than several other imaging methods with a single
exposure. We also developed a hardware prototype with the potential to complete
the measurement on the camera within 22 msec and demonstrated that light fields
from real 3-D scenes can be obtained with convincing visual quality. Our
software and supplementary video are available from our project website.",CVPR
"Developing generalizable manipulation skills is a core challenge in embodied
AI. This includes generalization across diverse task configurations,
encompassing variations in object shape, density, friction coefficient, and
external disturbances such as forces applied to the robot. Rapid Motor
Adaptation (RMA) offers a promising solution to this challenge. It posits that
essential hidden variables influencing an agent's task performance, such as
object mass and shape, can be effectively inferred from the agent's action and
proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand
rotation, we use depth perception to develop agents tailored for rapid motor
adaptation in a variety of manipulation tasks. We evaluated our agents on four
challenging tasks from the Maniskill2 benchmark, namely pick-and-place
operations with hundreds of objects from the YCB and EGAD datasets, peg
insertion with precise position and orientation, and operating a variety of
faucets and handles, with customized environment variations. Empirical results
demonstrate that our agents surpass state-of-the-art methods like automatic
domain randomization and vision-based policies, obtaining better generalization
performance and sample efficiency.",CVPR
"Few-shot segmentation remains challenging due to the limitations of its
labeling information for unseen classes. Most previous approaches rely on
extracting high-level feature maps from the frozen visual encoder to compute
the pixel-wise similarity as a key prior guidance for the decoder. However,
such a prior representation suffers from coarse granularity and poor
generalization to new classes since these high-level feature maps have obvious
category bias. In this work, we propose to replace the visual prior
representation with the visual-text alignment capacity to capture more reliable
guidance and enhance the model generalization. Specifically, we design two
kinds of training-free prior information generation strategy that attempts to
utilize the semantic alignment capability of the Contrastive Language-Image
Pre-training model (CLIP) to locate the target class. Besides, to acquire more
accurate prior guidance, we build a high-order relationship of attention maps
and utilize it to refine the initial prior information. Experiments on both the
PASCAL-5{i} and COCO-20{i} datasets show that our method obtains a clearly
substantial improvement and reaches the new state-of-the-art performance.",CVPR
"Defocus blur is a persistent problem in microscope imaging that poses harm to
pathology interpretation and medical intervention in cell microscopy and
microscope surgery. To address this problem, a unified framework including the
multi-pyramid transformer (MPT) and extended frequency contrastive
regularization (EFCR) is proposed to tackle two outstanding challenges in
microscopy deblur: longer attention span and data deficiency. The MPT employs
an explicit pyramid structure at each network stage that integrates the
cross-scale window attention (CSWA), the intra-scale channel attention (ISCA),
and the feature-enhancing feed-forward network (FEFN) to capture long-range
cross-scale spatial interaction and global channel context. The EFCR addresses
the data deficiency problem by exploring latent deblur signals from different
frequency bands. It also enables deblur knowledge transfer to learn
cross-domain information from extra data, improving deblur performance for
labeled and unlabeled data. Extensive experiments and downstream task
validation show the framework achieves state-of-the-art performance across
multiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.",CVPR
"This paper addresses complex challenges in histopathological image analysis
through three key contributions. Firstly, it introduces a fast patch selection
method, FPS, for whole-slide image (WSI) analysis, significantly reducing
computational cost while maintaining accuracy. Secondly, it presents PathDino,
a lightweight histopathology feature extractor with a minimal configuration of
five Transformer blocks and only 9 million parameters, markedly fewer than
alternatives. Thirdly, it introduces a rotation-agnostic representation
learning paradigm using self-supervised learning, effectively mitigating
overfitting. We also show that our compact model outperforms existing
state-of-the-art histopathology-specific vision transformers on 12 diverse
datasets, including both internal datasets spanning four sites (breast, liver,
skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,
DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training
dataset of 6 million histopathology patches from The Cancer Genome Atlas
(TCGA), our approach demonstrates an average 8.5% improvement in patch-level
majority vote performance. These contributions provide a robust framework for
enhancing image analysis in digital pathology, rigorously validated through
extensive evaluation. Project Page:
https://kimialabmayo.github.io/PathDino-Page/",CVPR
"In the field of 3D object detection tasks, fusing heterogeneous features from
LiDAR and camera sensors into a unified Bird's Eye View (BEV) representation is
a widely adopted paradigm. However, existing methods are often compromised by
imprecise sensor calibration, resulting in feature misalignment in LiDAR-camera
BEV fusion. Moreover, such inaccuracies result in errors in depth estimation
for the camera branch, ultimately causing misalignment between LiDAR and camera
BEV features. In this work, we propose a novel ContrastAlign approach that
utilizes contrastive learning to enhance the alignment of heterogeneous
modalities, thereby improving the robustness of the fusion process.
Specifically, our approach includes the L-Instance module, which directly
outputs LiDAR instance features within LiDAR BEV features. Then, we introduce
the C-Instance module, which predicts camera instance features through RoI
(Region of Interest) pooling on the camera BEV features. We propose the
InstanceFusion module, which utilizes contrastive learning to generate similar
instance features across heterogeneous modalities. We then use graph matching
to calculate the similarity between the neighboring camera instance features
and the similarity instance features to complete the alignment of instance
features. Our method achieves state-of-the-art performance, with an mAP of
70.3%, surpassing BEVFusion by 1.8% on the nuScenes validation set.
Importantly, our method outperforms BEVFusion by 7.3% under conditions with
misalignment noise.",CVPR
"Labor-intensive labeling becomes a bottleneck in developing computer vision
algorithms based on deep learning. For this reason, dealing with imperfect
labels has increasingly gained attention and has become an active field of
study. We address learning with noisy labels (LNL) problem, which is formalized
as a task of finding a structured manifold in the midst of noisy data. In this
framework, we provide a proper objective function and an optimization algorithm
based on two expectation-maximization (EM) cycles. The separate networks
associated with the two EM cycles collaborate to optimize the objective
function, where one model is for distinguishing clean labels from corrupted
ones while the other is for refurbishing the corrupted labels. This approach
results in a non-collapsing LNL-flywheel model in the end. Experiments show
that our algorithm achieves state-of-the-art performance in multiple standard
benchmarks with substantial margins under various types of label noise.",CVPR
"In this paper, we target the adaptive source driven 3D scene editing task by
proposing a CustomNeRF model that unifies a text description or a reference
image as the editing prompt. However, obtaining desired editing results
conformed with the editing prompt is nontrivial since there exist two
significant challenges, including accurate editing of only foreground regions
and multi-view consistency given a single-view reference image. To tackle the
first challenge, we propose a Local-Global Iterative Editing (LGIE) training
scheme that alternates between foreground region editing and full-image
editing, aimed at foreground-only manipulation while preserving the background.
For the second challenge, we also design a class-guided regularization that
exploits class priors within the generation model to alleviate the
inconsistency problem among different views in image-driven editing. Extensive
experiments show that our CustomNeRF produces precise editing results under
various real scenes for both text- and image-driven settings.",CVPR
"Recently, model merging techniques have surfaced as a solution to combine
multiple single-talent models into a single multi-talent model. However,
previous endeavors in this field have either necessitated additional training
or fine-tuning processes, or require that the models possess the same
pre-trained initialization. In this work, we identify a common drawback in
prior works w.r.t. the inconsistency of unit similarity in the weight space and
the activation space. To address this inconsistency, we propose an innovative
model merging framework, coined as merging under dual-space constraints
(MuDSC). Specifically, instead of solely maximizing the objective of a single
space, we advocate for the exploration of permutation matrices situated in a
region with a unified high similarity in the dual space, achieved through the
linear combination of activation and weight similarity matrices. In order to
enhance usability, we have also incorporated adaptations for group structure,
including Multi-Head Attention and Group Normalization. Comprehensive
experimental comparisons demonstrate that MuDSC can significantly boost the
performance of merged models with various task combinations and architectures.
Furthermore, the visualization of the merged model within the multi-task loss
landscape reveals that MuDSC enables the merged model to reside in the
overlapping segment, featuring a unified lower loss for each task. Our code is
publicly available at https://github.com/zju-vipa/training_free_model_merging.",CVPR
"We present an efficient text-to-video generation framework based on latent
diffusion models, termed MagicVideo. MagicVideo can generate smooth video clips
that are concordant with the given text descriptions. Due to a novel and
efficient 3D U-Net design and modeling video distributions in a low-dimensional
space, MagicVideo can synthesize video clips with 256x256 spatial resolution on
a single GPU card, which takes around 64x fewer computations than the Video
Diffusion Models (VDM) in terms of FLOPs. In specific, unlike existing works
that directly train video models in the RGB space, we use a pre-trained VAE to
map video clips into a low-dimensional latent space and learn the distribution
of videos' latent codes via a diffusion model. Besides, we introduce two new
designs to adapt the U-Net denoiser trained on image tasks to video data: a
frame-wise lightweight adaptor for the image-to-video distribution adjustment
and a directed temporal attention module to capture temporal dependencies
across frames. Thus, we can exploit the informative weights of convolution
operators from a text-to-image model for accelerating video training. To
ameliorate the pixel dithering in the generated videos, we also propose a novel
VideoVAE auto-encoder for better RGB reconstruction. We conduct extensive
experiments and demonstrate that MagicVideo can generate high-quality video
clips with either realistic or imaginary content. Refer to
\url{https://magicvideo.github.io/#} for more examples.",CVPR
"Despite the progress of learning-based methods for 6D object pose estimation,
the trade-off between accuracy and scalability for novel objects still exists.
Specifically, previous methods for novel objects do not make good use of the
target object's 3D shape information since they focus on generalization by
processing the shape indirectly, making them less effective. We present
GenFlow, an approach that enables both accuracy and generalization to novel
objects with the guidance of the target object's shape. Our method predicts
optical flow between the rendered image and the observed image and refines the
6D pose iteratively. It boosts the performance by a constraint of the 3D shape
and the generalizable geometric knowledge learned from an end-to-end
differentiable system. We further improve our model by designing a cascade
network architecture to exploit the multi-scale correlations and coarse-to-fine
refinement. GenFlow ranked first on the unseen object pose estimation
benchmarks in both the RGB and RGB-D cases. It also achieves performance
competitive with existing state-of-the-art methods for the seen object pose
estimation without any fine-tuning.",CVPR
"Creating multi-view wire art (MVWA), a static 3D sculpture with diverse
interpretations from different viewpoints, is a complex task even for skilled
artists. In response, we present DreamWire, an AI system enabling everyone to
craft MVWA easily. Users express their vision through text prompts or
scribbles, freeing them from intricate 3D wire organisation. Our approach
synergises 3D B\'ezier curves, Prim's algorithm, and knowledge distillation
from diffusion models or their variants (e.g., ControlNet). This blend enables
the system to represent 3D wire art, ensuring spatial continuity and overcoming
data scarcity. Extensive evaluation and analysis are conducted to shed insight
on the inner workings of the proposed system, including the trade-off between
connectivity and visual aesthetics.",CVPR
"In the realm of computer vision and robotics, embodied agents are expected to
explore their environment and carry out human instructions. This necessitates
the ability to fully understand 3D scenes given their first-person observations
and contextualize them into language for interaction. However, traditional
research focuses more on scene-level input and output setups from a global
view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric
3D perception dataset and benchmark for holistic 3D scene understanding. It
encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language
prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which
partially align with LVIS, and dense semantic occupancy with 80 common
categories. Building upon this database, we introduce a baseline framework
named Embodied Perceptron. It is capable of processing an arbitrary number of
multi-modal inputs and demonstrates remarkable 3D perception capabilities, both
within the two series of benchmarks we set up, i.e., fundamental 3D perception
tasks and language-grounded tasks, and in the wild. Codes, datasets, and
benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.",CVPR
"6D pose estimation pipelines that rely on RGB-only or RGB-D data show
limitations for photometrically challenging objects with e.g. textureless
surfaces, reflections or transparency. A supervised learning-based method
utilising complementary polarisation information as input modality is proposed
to overcome such limitations. This supervised approach is then extended to a
self-supervised paradigm by leveraging physical characteristics of polarised
light, thus eliminating the need for annotated real data. The methods achieve
significant advancements in pose estimation by leveraging geometric information
from polarised light and incorporating shape priors and invertible physical
constraints.",CVPR
"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene representation from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, which not
only reduces storage requirements but also offers potential privacy protection.
Inspired by this, to take one step further, our approach builds upon the
powerful 3D scene representation capabilities of neural radiance fields (NeRF).
Specifically, we formulate the physical imaging process of SCI as part of the
training of NeRF, allowing us to exploit its impressive performance in
capturing complex scene structures. To assess the effectiveness of our method,
we conduct extensive evaluations using both synthetic data and real data
captured by our SCI system. Extensive experimental results demonstrate that our
proposed approach surpasses the state-of-the-art methods in terms of image
reconstruction and novel view image synthesis. Moreover, our method also
exhibits the ability to restore high frame-rate multi-view consistent images by
leveraging SCI and the rendering capabilities of NeRF. The code is available at
https://github.com/WU-CVGL/SCINeRF.",CVPR
"Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a
target domain, with only access to unlabeled target training data and the
source model pre-trained on a supervised source domain. Relying on pseudo
labeling and/or auxiliary supervision, conventional methods are inevitably
error-prone. To mitigate this limitation, in this work we for the first time
explore the potentials of off-the-shelf vision-language (ViL) multimodal models
(e.g.,CLIP) with rich whilst heterogeneous knowledge. We find that directly
applying the ViL model to the target domain in a zero-shot fashion is
unsatisfactory, as it is not specialized for this particular task but largely
generic. To make it task specific, we propose a novel Distilling multimodal
Foundation model(DIFO)approach. Specifically, DIFO alternates between two steps
during adaptation: (i) Customizing the ViL model by maximizing the mutual
information with the target model in a prompt learning manner, (ii) Distilling
the knowledge of this customized ViL model to the target model. For more
fine-grained and reliable distillation, we further introduce two effective
regularization terms, namely most-likely category encouragement and predictive
consistency. Extensive experiments show that DIFO significantly outperforms the
state-of-the-art alternatives. Code is here",CVPR
"Diffusion models have emerged as the de facto paradigm for video generation.
However, their reliance on web-scale data of varied quality often yields
results that are visually unappealing and misaligned with the textual prompts.
To tackle this problem, we propose InstructVideo to instruct text-to-video
diffusion models with human feedback by reward fine-tuning. InstructVideo has
two key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by
generating through the full DDIM sampling chain, we recast reward fine-tuning
as editing. By leveraging the diffusion process to corrupt a sampled video,
InstructVideo requires only partial inference of the DDIM sampling chain,
reducing fine-tuning cost while improving fine-tuning efficiency. 2) To
mitigate the absence of a dedicated video reward model for human preferences,
we repurpose established image reward models, e.g., HPSv2. To this end, we
propose Segmental Video Reward, a mechanism to provide reward signals based on
segmental sparse sampling, and Temporally Attenuated Reward, a method that
mitigates temporal modeling degradation during fine-tuning. Extensive
experiments, both qualitative and quantitative, validate the practicality and
efficacy of using image reward models in InstructVideo, significantly enhancing
the visual quality of generated videos without compromising generalization
capabilities. Code and models will be made publicly available.",CVPR
"Data-Free Meta-Learning (DFML) aims to extract knowledge from a collection of
pre-trained models without requiring the original data, presenting practical
benefits in contexts constrained by data privacy concerns. Current DFML methods
primarily focus on the data recovery from these pre-trained models. However,
they suffer from slow recovery speed and overlook gaps inherent in
heterogeneous pre-trained models. In response to these challenges, we introduce
the Faster and Better Data-Free Meta-Learning (FREE) framework, which contains:
(i) a meta-generator for rapidly recovering training tasks from pre-trained
models; and (ii) a meta-learner for generalizing to new unseen tasks.
Specifically, within the module Faster Inversion via Meta-Generator, each
pre-trained model is perceived as a distinct task. The meta-generator can
rapidly adapt to a specific task in just five steps, significantly accelerating
the data recovery. Furthermore, we propose Better Generalization via
Meta-Learner and introduce an implicit gradient alignment algorithm to optimize
the meta-learner. This is achieved as aligned gradient directions alleviate
potential conflicts among tasks from heterogeneous pre-trained models.
Empirical experiments on multiple benchmarks affirm the superiority of our
approach, marking a notable speed-up (20$\times$) and performance enhancement
(1.42\% $\sim$ 4.78\%) in comparison to the state-of-the-art.",CVPR
"Being able to understand visual scenes is a precursor for many downstream
tasks, including autonomous driving, robotics, and other vision-based
approaches. A common approach enabling the ability to reason over visual data
is Scene Graph Generation (SGG); however, many existing approaches assume
undisturbed vision, i.e., the absence of real-world corruptions such as fog,
snow, smoke, as well as non-uniform perturbations like sun glare or water
drops. In this work, we propose a novel SGG benchmark containing procedurally
generated weather corruptions and other transformations over the Visual Genome
dataset. Further, we introduce a corresponding approach, Hierarchical Knowledge
Enhanced Robust Scene Graph Generation (HiKER-SGG), providing a strong baseline
for scene graph generation under such challenging setting. At its core,
HiKER-SGG utilizes a hierarchical knowledge graph in order to refine its
predictions from coarse initial estimates to detailed predictions. In our
extensive experiments, we show that HiKER-SGG does not only demonstrate
superior performance on corrupted images in a zero-shot manner, but also
outperforms current state-of-the-art methods on uncorrupted SGG tasks. Code is
available at https://github.com/zhangce01/HiKER-SGG.",CVPR
"Recently, the advent of Large Visual-Language Models (LVLMs) has received
increasing attention across various domains, particularly in the field of
visual document understanding (VDU). Different from conventional
vision-language tasks, VDU is specifically concerned with text-rich scenarios
containing abundant document elements. Nevertheless, the importance of
fine-grained features remains largely unexplored within the community of LVLMs,
leading to suboptimal performance in text-rich scenarios. In this paper, we
abbreviate it as the fine-grained feature collapse issue. With the aim of
filling this gap, we propose a contrastive learning framework, termed Document
Object COntrastive learning (DoCo), specifically tailored for the downstream
tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the
features of document objects and align them to the visual features generated by
the vision encoder of LVLM, which enhances visual representation in text-rich
scenarios. It can represent that the contrastive learning between the visual
holistic representations and the multimodal fine-grained features of document
objects can assist the vision encoder in acquiring more effective visual cues,
thereby enhancing the comprehension of text-rich documents in LVLMs. We also
demonstrate that the proposed DoCo serves as a plug-and-play pre-training
method, which can be employed in the pre-training of various LVLMs without
inducing any increase in computational complexity during the inference process.
Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs
equipped with our proposed DoCo can achieve superior performance and mitigate
the gap between VDU and generic vision-language tasks.",CVPR
"We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.",CVPR
"Recent methods such as Score Distillation Sampling (SDS) and Variational
Score Distillation (VSD) using 2D diffusion models for text-to-3D generation
have demonstrated impressive generation quality. However, the long generation
time of such algorithms significantly degrades the user experience. To tackle
this problem, we propose DreamPropeller, a drop-in acceleration algorithm that
can be wrapped around any existing text-to-3D generation pipeline based on
score distillation. Our framework generalizes Picard iterations, a classical
algorithm for parallel sampling an ODE path, and can account for non-ODE paths
such as momentum-based gradient updates and changes in dimensions during the
optimization process as in many cases of 3D generation. We show that our
algorithm trades parallel compute for wallclock time and empirically achieves
up to 4.7x speedup with a negligible drop in generation quality for all tested
frameworks.",CVPR
"Recently, lightweight Vision Transformers (ViTs) demonstrate superior
performance and lower latency, compared with lightweight Convolutional Neural
Networks (CNNs), on resource-constrained mobile devices. Researchers have
discovered many structural connections between lightweight ViTs and lightweight
CNNs. However, the notable architectural disparities in the block structure,
macro, and micro designs between them have not been adequately examined. In
this study, we revisit the efficient design of lightweight CNNs from ViT
perspective and emphasize their promising prospect for mobile devices.
Specifically, we incrementally enhance the mobile-friendliness of a standard
lightweight CNN, \ie, MobileNetV3, by integrating the efficient architectural
designs of lightweight ViTs. This ends up with a new family of pure lightweight
CNNs, namely RepViT. Extensive experiments show that RepViT outperforms
existing state-of-the-art lightweight ViTs and exhibits favorable latency in
various vision tasks. Notably, on ImageNet, RepViT achieves over 80\% top-1
accuracy with 1.0 ms latency on an iPhone 12, which is the first time for a
lightweight model, to the best of our knowledge. Besides, when RepViT meets
SAM, our RepViT-SAM can achieve nearly 10$\times$ faster inference than the
advanced MobileSAM. Codes and models are available at
\url{https://github.com/THU-MIG/RepViT}.",CVPR
"The emerging conditional coding-based neural video codec (NVC) shows
superiority over commonly-used residual coding-based codec and the latest NVC
already claims to outperform the best traditional codec. However, there still
exist critical problems blocking the practicality of NVC. In this paper, we
propose a powerful conditional coding-based NVC that solves two critical
problems via feature modulation. The first is how to support a wide quality
range in a single model. Previous NVC with this capability only supports about
3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent
feature of the current frame via the learnable quantization scaler. During the
training, we specially design the uniform quantization parameter sampling
mechanism to improve the harmonization of encoding and quantization. This
results in a better learning of the quantization scaler and helps our NVC
support about 11.4 dB PSNR range. The second is how to make NVC still work
under a long prediction chain. We expose that the previous SOTA NVC has an
obvious quality degradation problem when using a large intra-period setting. To
this end, we propose modulating the temporal feature with a periodically
refreshing mechanism to boost the quality. %Besides solving the above two
problems, we also design a single model that can support both RGB and YUV
colorspaces. Notably, under single intra-frame setting, our codec can achieve
29.7\% bitrate saving over previous SOTA NVC with 16\% MACs reduction. Our
codec serves as a notable landmark in the journey of NVC evolution. The codes
are at https://github.com/microsoft/DCVC.",CVPR
"Category-agnostic pose estimation (CAPE) aims to predict keypoints for
arbitrary classes given a few support images annotated with keypoints. Existing
methods only rely on the features extracted at support keypoints to predict or
refine the keypoints on query image, but a few support feature vectors are
local and inadequate for CAPE. Considering that human can quickly perceive
potential keypoints of arbitrary objects, we propose a novel framework for CAPE
based on such potential keypoints (named as meta-points). Specifically, we
maintain learnable embeddings to capture inherent information of various
keypoints, which interact with image feature maps to produce meta-points
without any support. The produced meta-points could serve as meaningful
potential keypoints for CAPE. Due to the inevitable gap between inherency and
annotation, we finally utilize the identities and details offered by support
keypoints to assign and refine meta-points to desired keypoints in query image.
In addition, we propose a progressive deformable point decoder and a slacked
regression loss for better prediction and supervision. Our novel framework not
only reveals the inherency of keypoints but also outperforms existing methods
of CAPE. Comprehensive experiments and in-depth studies on large-scale MP-100
dataset demonstrate the effectiveness of our framework.",CVPR
"As recent advances in mobile camera technology have enabled the capability to
capture high-resolution images, such as 4K images, the demand for an efficient
deblurring model handling large motion has increased. In this paper, we
discover that the image residual errors, i.e., blur-sharp pixel differences,
can be grouped into some categories according to their motion blur type and how
complex their neighboring pixels are. Inspired by this, we decompose the
deblurring (regression) task into blur pixel discretization (pixel-level blur
classification) and discrete-to-continuous conversion (regression with blur
class map) tasks. Specifically, we generate the discretized image residual
errors by identifying the blur pixels and then transform them to a continuous
form, which is computationally more efficient than naively solving the original
regression problem with continuous values. Here, we found that the
discretization result, i.e., blur segmentation map, remarkably exhibits visual
similarity with the image residual errors. As a result, our efficient model
shows comparable performance to state-of-the-art methods in realistic
benchmarks, while our method is up to 10 times computationally more efficient.",CVPR
"Recently, Vision Transformer has achieved great success in recovering missing
details in low-resolution sequences, i.e., the video super-resolution (VSR)
task. Despite its superiority in VSR accuracy, the heavy computational burden
as well as the large memory footprint hinder the deployment of
Transformer-based VSR models on constrained devices. In this paper, we address
the above issue by proposing a novel feature-level masked processing framework:
VSR with Masked Intra and inter frame Attention (MIA-VSR). The core of MIA-VSR
is leveraging feature-level temporal continuity between adjacent frames to
reduce redundant computations and make more rational use of previously enhanced
SR features. Concretely, we propose an intra-frame and inter-frame attention
block which takes the respective roles of past features and input features into
consideration and only exploits previously enhanced features to provide
supplementary information. In addition, an adaptive block-wise mask prediction
module is developed to skip unimportant computations according to feature
similarity between adjacent frames. We conduct detailed ablation studies to
validate our contributions and compare the proposed method with recent
state-of-the-art VSR approaches. The experimental results demonstrate that
MIA-VSR improves the memory and computation efficiency over state-of-the-art
methods, without trading off PSNR accuracy. The code is available at
https://github.com/LabShuHangGU/MIA-VSR.",CVPR
"Image-language models with prompt learning have shown remarkable advances in
numerous downstream vision tasks. Nevertheless, conventional prompt learning
methods overfit their training distribution and lose the generalization ability
on test distributions. To improve generalization across various distribution
shifts, we propose any-shift prompting: a general probabilistic inference
framework that considers the relationship between training and test
distributions during prompt learning. We explicitly connect training and test
distributions in the latent space by constructing training and test prompts in
a hierarchical architecture. Within this framework, the test prompt exploits
the distribution relationships to guide the generalization of the CLIP
image-language model from training to any test distribution. To effectively
encode the distribution information and their relationships, we further
introduce a transformer inference network with a pseudo-shift training
mechanism. The network generates the tailored test prompt with both training
and test information in a feedforward pass, avoiding extra training costs at
test time. Extensive experiments on twenty-three datasets demonstrate the
effectiveness of any-shift prompting on the generalization over various
distribution shifts.",CVPR
"Current diffusion or flow-based generative models for 3D shapes divide to
two: distilling pre-trained 2D image diffusion models, and training directly on
3D shapes. When training a diffusion or flow models on 3D shapes a crucial
design choice is the shape representation. An effective shape representation
needs to adhere three design principles: it should allow an efficient
conversion of large 3D datasets to the representation form; it should provide a
good tradeoff of approximation power versus number of parameters; and it should
have a simple tensorial form that is compatible with existing powerful neural
architectures. While standard 3D shape representations such as volumetric grids
and point clouds do not adhere to all these principles simultaneously, we
advocate in this paper a new representation that does. We introduce Mosaic-SDF
(M-SDF): a simple 3D shape representation that approximates the Signed Distance
Function (SDF) of a given shape by using a set of local grids spread near the
shape's boundary. The M-SDF representation is fast to compute for each shape
individually making it readily parallelizable; it is parameter efficient as it
only covers the space around the shape's boundary; and it has a simple matrix
form, compatible with Transformer-based architectures. We demonstrate the
efficacy of the M-SDF representation by using it to train a 3D generative flow
model including class-conditioned generation with the 3D Warehouse dataset, and
text-to-3D generation using a dataset of about 600k caption-shape pairs.",CVPR
"Computer vision models normally witness degraded performance when deployed in
real-world scenarios, due to unexpected changes in inputs that were not
accounted for during training. Data augmentation is commonly used to address
this issue, as it aims to increase data variety and reduce the distribution gap
between training and test data. However, common visual augmentations might not
guarantee extensive robustness of computer vision models. In this paper, we
propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique
targeting augmentation in the frequency domain and filling the augmentation gap
left by visual augmentations. We demonstrate the utility of augmentation via
Fourier-basis additive noise in a straightforward and efficient adversarial
setting. Our results show that AFA benefits the robustness of models against
common corruptions, OOD generalization, and consistency of performance of
models against increasing perturbations, with negligible deficit to the
standard performance of models. It can be seamlessly integrated with other
augmentation techniques to further boost performance. Code and models can be
found at: https://github.com/nis-research/afa-augment",CVPR
"Pseudo-label-based semi-supervised learning (SSL) algorithms trained on a
class-imbalanced set face two cascading challenges: 1) Classifiers tend to be
biased towards majority classes, and 2) Biased pseudo-labels are used for
training. It is difficult to appropriately re-balance the classifiers in SSL
because the class distribution of an unlabeled set is often unknown and could
be mismatched with that of a labeled set. We propose a novel class-imbalanced
SSL algorithm called class-distribution-mismatch-aware debiasing (CDMAD). For
each iteration of training, CDMAD first assesses the classifier's biased degree
towards each class by calculating the logits on an image without any patterns
(e.g., solid color image), which can be considered irrelevant to the training
set. CDMAD then refines biased pseudo-labels of the base SSL algorithm by
ensuring the classifier's neutrality. CDMAD uses these refined pseudo-labels
during the training of the base SSL algorithm to improve the quality of the
representations. In the test phase, CDMAD similarly refines biased class
predictions on test samples. CDMAD can be seen as an extension of post-hoc
logit adjustment to address a challenge of incorporating the unknown class
distribution of the unlabeled set for re-balancing the biased classifier under
class distribution mismatch. CDMAD ensures Fisher consistency for the balanced
error. Extensive experiments verify the effectiveness of CDMAD.",CVPR
"Local feature detection and description play an important role in many
computer vision tasks, which are designed to detect and describe keypoints in
""any scene"" and ""any downstream task"". Data-driven local feature learning
methods need to rely on pixel-level correspondence for training, which is
challenging to acquire at scale, thus hindering further improvements in
performance. In this paper, we propose SAMFeat to introduce SAM (segment
anything model), a fundamental model trained on 11 million images, as a teacher
to guide local feature learning and thus inspire higher performance on limited
datasets. To do so, first, we construct an auxiliary task of Pixel Semantic
Relational Distillation (PSRD), which distillates feature relations with
category-agnostic semantic information learned by the SAM encoder into a local
feature learning network, to improve local feature description using semantic
discrimination. Second, we develop a technique called Weakly Supervised
Contrastive Learning Based on Semantic Grouping (WSC), which utilizes semantic
groupings derived from SAM as weakly supervised signals, to optimize the metric
space of local descriptors. Third, we design an Edge Attention Guidance (EAG)
to further improve the accuracy of local feature detection and description by
prompting the network to pay more attention to the edge region guided by SAM.
SAMFeat's performance on various tasks such as image matching on HPatches, and
long-term visual localization on Aachen Day-Night showcases its superiority
over previous local features. The release code is available at
https://github.com/vignywang/SAMFeat.",CVPR
"Federated learning achieves effective performance in modeling decentralized
data. In practice, client data are not well-labeled, which makes it potential
for federated unsupervised learning (FUSL) with non-IID data. However, the
performance of existing FUSL methods suffers from insufficient representations,
i.e., (1) representation collapse entanglement among local and global models,
and (2) inconsistent representation spaces among local models. The former
indicates that representation collapse in local model will subsequently impact
the global model and other local models. The latter means that clients model
data representation with inconsistent parameters due to the deficiency of
supervision signals. In this work, we propose FedU2 which enhances generating
uniform and unified representation in FUSL with non-IID data. Specifically,
FedU2 consists of flexible uniform regularizer (FUR) and efficient unified
aggregator (EUA). FUR in each client avoids representation collapse via
dispersing samples uniformly, and EUA in server promotes unified representation
by constraining consistent client model updating. To extensively validate the
performance of FedU2, we conduct both cross-device and cross-silo evaluation
experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.",CVPR
"Reconstructing 3D clothed human involves creating a detailed geometry of
individuals in clothing, with applications ranging from virtual try-on, movies,
to games. To enable practical and widespread applications, recent advances
propose to generate a clothed human from an RGB image. However, they struggle
to reconstruct detailed and robust avatars simultaneously. We empirically find
that the high-frequency (HF) and low-frequency (LF) information from a
parametric model has the potential to enhance geometry details and improve
robustness to noise, respectively. Based on this, we propose HiLo, namely
clothed human reconstruction with high- and low-frequency information, which
contains two components. 1) To recover detailed geometry using HF information,
we propose a progressive HF Signed Distance Function to enhance the detailed 3D
geometry of a clothed human. We analyze that our progressive learning manner
alleviates large gradients that hinder model convergence. 2) To achieve robust
reconstruction against inaccurate estimation of the parametric model by using
LF information, we propose a spatial interaction implicit function. This
function effectively exploits the complementary spatial information from a
low-resolution voxel grid of the parametric model. Experimental results
demonstrate that HiLo outperforms the state-of-the-art methods by 10.43% and
9.54% in terms of Chamfer distance on the Thuman2.0 and CAPE datasets,
respectively. Additionally, HiLo demonstrates robustness to noise from the
parametric model, challenging poses, and various clothing styles.",CVPR
"DETR accomplishes end-to-end object detection through iteratively generating
multiple object candidates based on image features and promoting one candidate
for each ground-truth object. The traditional training procedure using
one-to-one supervision in the original DETR lacks direct supervision for the
object detection candidates.
  We aim at improving the DETR training efficiency by explicitly supervising
the candidate generation procedure through mixing one-to-one supervision and
one-to-many supervision. Our approach, namely MS-DETR, is simple, and places
one-to-many supervision to the object queries of the primary decoder that is
used for inference. In comparison to existing DETR variants with one-to-many
supervision, such as Group DETR and Hybrid DETR, our approach does not need
additional decoder branches or object queries. The object queries of the
primary decoder in our approach directly benefit from one-to-many supervision
and thus are superior in object candidate prediction. Experimental results show
that our approach outperforms related DETR variants, such as DN-DETR, Hybrid
DETR, and Group DETR, and the combination with related DETR variants further
improves the performance.",CVPR
"With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.",CVPR
"In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR)
refers to computational reconstruction of an unknown 3D magnetic resonance
volume from stacks of 2D slices corrupted by motion. While promising, current
SVR methods require multiple slice stacks for accurate 3D reconstruction,
leading to long scans and limiting their use in time-sensitive applications
such as fetal fMRI. Here, we propose a SVR method that overcomes the
shortcomings of previous work and produces state-of-the-art reconstructions in
the presence of extreme inter-slice motion. Inspired by the recent success of
single-view depth estimation methods, we formulate SVR as a single-stack motion
estimation task and train a fully convolutional network to predict a motion
stack for a given slice stack, producing a 3D reconstruction as a byproduct of
the predicted motion. Extensive experiments on the SVR of adult and fetal
brains demonstrate that our fully convolutional method is twice as accurate as
previous SVR methods. Our code is available at github.com/seannz/svr.",CVPR
"This paper, for the first time, explores text-to-image diffusion models for
Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal
discovery: the capacity of text-to-image diffusion models to seamlessly bridge
the gap between sketches and photos. This proficiency is underpinned by their
robust cross-modal capabilities and shape bias, findings that are substantiated
through our pilot studies. In order to harness pre-trained diffusion models
effectively, we introduce a straightforward yet powerful strategy focused on
two key aspects: selecting optimal feature layers and utilising visual and
textual prompts. For the former, we identify which layers are most enriched
with information and are best suited for the specific retrieval requirements
(category-level or fine-grained). Then we employ visual and textual prompts to
guide the model's feature extraction process, enabling it to generate more
discriminative and contextually relevant cross-modal representations. Extensive
experiments on several benchmark datasets validate significant performance
improvements.",CVPR
"Text-to-image (T2I) generative models have recently emerged as a powerful
tool, enabling the creation of photo-realistic images and giving rise to a
multitude of applications. However, the effective integration of T2I models
into fundamental image classification tasks remains an open question. A
prevalent strategy to bolster image classification performance is through
augmenting the training set with synthetic images generated by T2I models. In
this study, we scrutinize the shortcomings of both current generative and
conventional data augmentation techniques. Our analysis reveals that these
methods struggle to produce images that are both faithful (in terms of
foreground objects) and diverse (in terms of background contexts) for
domain-specific concepts. To tackle this challenge, we introduce an innovative
inter-class data augmentation method known as Diff-Mix
(https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by
performing image translations between classes. Our empirical results
demonstrate that Diff-Mix achieves a better balance between faithfulness and
diversity, leading to a marked improvement in performance across diverse image
classification scenarios, including few-shot, conventional, and long-tail
classifications for domain-specific datasets.",CVPR
"Real-world objects and environments are predominantly composed of edge
features, including straight lines and curves. Such edges are crucial elements
for various applications, such as CAD modeling, surface meshing, lane mapping,
etc. However, existing traditional methods only prioritize lines over curves
for simplicity in geometric modeling. To this end, we introduce EMAP, a new
method for learning 3D edge representations with a focus on both lines and
curves. Our method implicitly encodes 3D edge distance and direction in
Unsigned Distance Functions (UDF) from multi-view edge maps. On top of this
neural representation, we propose an edge extraction algorithm that robustly
abstracts parametric 3D edges from the inferred edge points and their
directions. Comprehensive evaluations demonstrate that our method achieves
better 3D edge reconstruction on multiple challenging datasets. We further show
that our learned UDF field enhances neural surface reconstruction by capturing
more details.",CVPR
"Generative AI (GenAI) is transforming creative workflows through the
capability to synthesize and manipulate images via high-level prompts. Yet
creatives are not well supported to receive recognition or reward for the use
of their content in GenAI training. To this end, we propose ProMark, a causal
attribution technique to attribute a synthetically generated image to its
training data concepts like objects, motifs, templates, artists, or styles. The
concept information is proactively embedded into the input training images
using imperceptible watermarks, and the diffusion models (unconditional or
conditional) are trained to retain the corresponding watermarks in generated
images. We show that we can embed as many as $2^{16}$ unique watermarks into
the training data, and each training image can contain more than one watermark.
ProMark can maintain image quality whilst outperforming correlation-based
attribution. Finally, several qualitative examples are presented, providing the
confidence that the presence of the watermark conveys a causative relationship
between training data and synthetic images.",CVPR
"Recent works on text-to-3d generation show that using only 2D diffusion
supervision for 3D generation tends to produce results with inconsistent
appearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals
with extra legs). Existing methods mainly address this issue by retraining
diffusion models with images rendered from 3D data to ensure multi-view
consistency while struggling to balance 2D generation quality with 3D
consistency. In this paper, we present a new framework Sculpt3D that equips the
current pipeline with explicit injection of 3D priors from retrieved reference
objects without re-training the 2D diffusion model. Specifically, we
demonstrate that high-quality and diverse 3D geometry can be guaranteed by
keypoints supervision through a sparse ray sampling approach. Moreover, to
ensure accurate appearances of different views, we further modulate the output
of the 2D diffusion model to the correct patterns of the template views without
altering the generated object's style. These two decoupled designs effectively
harness 3D information from reference objects to generate 3D objects while
preserving the generation quality of the 2D diffusion model. Extensive
experiments show our method can largely improve the multi-view consistency
while retaining fidelity and diversity. Our project page is available at:
https://stellarcheng.github.io/Sculpt3D/.",CVPR
"Two primary input modalities prevail in image retrieval: sketch and text.
While text is widely used for inter-category retrieval tasks, sketches have
been established as the sole preferred modality for fine-grained image
retrieval due to their ability to capture intricate visual details. In this
paper, we question the reliance on sketches alone for fine-grained image
retrieval by simultaneously exploring the fine-grained representation
capabilities of both sketch and text, orchestrating a duet between the two. The
end result enables precise retrievals previously unattainable, allowing users
to pose ever-finer queries and incorporate attributes like colour and
contextual cues from text. For this purpose, we introduce a novel
compositionality framework, effectively combining sketches and text using
pre-trained CLIP models, while eliminating the need for extensive fine-grained
textual descriptions. Last but not least, our system extends to novel
applications in composed image retrieval, domain attribute transfer, and
fine-grained generation, providing solutions for various real-world scenarios.",CVPR
"In this paper, we explore the capability of an agent to construct a logical
sequence of action steps, thereby assembling a strategic procedural plan. This
plan is crucial for navigating from an initial visual observation to a target
visual outcome, as depicted in real-life instructional videos. Existing works
have attained partial success by extensively leveraging various sources of
information available in the datasets, such as heavy intermediate visual
observations, procedural names, or natural language step-by-step instructions,
for features or supervision signals. However, the task remains formidable due
to the implicit causal constraints in the sequencing of steps and the
variability inherent in multiple feasible plans. To tackle these intricacies
that previous efforts have overlooked, we propose to enhance the capabilities
of the agent by infusing it with procedural knowledge. This knowledge, sourced
from training procedure plans and structured as a directed weighted graph,
equips the agent to better navigate the complexities of step sequencing and its
potential variations. We coin our approach KEPP, a novel Knowledge-Enhanced
Procedure Planning system, which harnesses a probabilistic procedural knowledge
graph extracted from training data, effectively acting as a comprehensive
textbook for the training domain. Experimental evaluations across three
widely-used datasets under settings of varying complexity reveal that KEPP
attains superior, state-of-the-art results while requiring only minimal
supervision.",CVPR
"3D head avatars built with neural implicit volumetric representations have
achieved unprecedented levels of photorealism. However, the computational cost
of these methods remains a significant barrier to their widespread adoption,
particularly in real-time applications such as virtual reality and
teleconferencing. While attempts have been made to develop fast neural
rendering approaches for static scenes, these methods cannot be simply employed
to support realistic facial expressions, such as in the case of a dynamic
facial performance. To address these challenges, we propose a novel fast 3D
neural implicit head avatar model that achieves real-time rendering while
maintaining fine-grained controllability and high rendering quality. Our key
idea lies in the introduction of local hash table blendshapes, which are
learned and attached to the vertices of an underlying face parametric model.
These per-vertex hash-tables are linearly merged with weights predicted via a
CNN, resulting in expression dependent embeddings. Our novel representation
enables efficient density and color predictions using a lightweight MLP, which
is further accelerated by a hierarchical nearest neighbor search method.
Extensive experiments show that our approach runs in real-time while achieving
comparable rendering quality to state-of-the-arts and decent results on
challenging expressions.",CVPR
"Estimating relative camera poses between images has been a central problem in
computer vision. Methods that find correspondences and solve for the
fundamental matrix offer high precision in most cases. Conversely, methods
predicting pose directly using neural networks are more robust to limited
overlap and can infer absolute translation scale, but at the expense of reduced
precision. We show how to combine the best of both methods; our approach yields
results that are both precise and robust, while also accurately inferring
translation scales. At the heart of our model lies a Transformer that (1)
learns to balance between solved and learned pose estimations, and (2) provides
a prior to guide a solver. A comprehensive analysis supports our design choices
and demonstrates that our method adapts flexibly to various feature extractors
and correspondence estimators, showing state-of-the-art performance in 6DoF
pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free
Relocalization.",CVPR
"In this paper, we address web-scale visual entity recognition, specifically
the task of mapping a given query image to one of the 6 million existing
entities in Wikipedia. One way of approaching a problem of such scale is using
dual-encoder models (eg CLIP), where all the entity names and query images are
embedded into a unified space, paving the way for an approximate k-NN search.
Alternatively, it is also possible to re-purpose a captioning model to directly
generate the entity names for a given image. In contrast, we introduce a novel
Generative Entity Recognition (GER) framework, which given an input image
learns to auto-regressively decode a semantic and discriminative ``code''
identifying the target entity. Our experiments demonstrate the efficacy of this
GER paradigm, showcasing state-of-the-art performance on the challenging OVEN
benchmark. GER surpasses strong captioning, dual-encoder, visual matching and
hierarchical classification baselines, affirming its advantage in tackling the
complexities of web-scale recognition.",CVPR
"In this paper, we propose a novel abstraction-aware sketch-based image
retrieval framework capable of handling sketch abstraction at varied levels.
Prior works had mainly focused on tackling sub-factors such as drawing style
and order, we instead attempt to model abstraction as a whole, and propose
feature-level and retrieval granularity-level designs so that the system builds
into its DNA the necessary means to interpret abstraction. On learning
abstraction-aware features, we for the first-time harness the rich semantic
embedding of pre-trained StyleGAN model, together with a novel
abstraction-level mapper that deciphers the level of abstraction and
dynamically selects appropriate dimensions in the feature matrix
correspondingly, to construct a feature matrix embedding that can be freely
traversed to accommodate different levels of abstraction. For granularity-level
abstraction understanding, we dictate that the retrieval model should not treat
all abstraction-levels equally and introduce a differentiable surrogate Acc.@q
loss to inject that understanding into the system. Different to the
gold-standard triplet loss, our Acc.@q loss uniquely allows a sketch to
narrow/broaden its focus in terms of how stringent the evaluation should be -
the more abstract a sketch, the less stringent (higher q). Extensive
experiments depict our method to outperform existing state-of-the-arts in
standard SBIR tasks along with challenging scenarios like early retrieval,
forensic sketch-photo matching, and style-invariant retrieval.",CVPR
"Diffusion-based text-to-video generation has witnessed impressive progress in
the past year yet still falls behind text-to-image generation. One of the key
reasons is the limited scale of publicly available data (e.g., 10M video-text
pairs in WebVid10M vs. 5B image-text pairs in LAION), considering the high cost
of video captioning. Instead, it could be far easier to collect unlabeled clips
from video platforms like YouTube. Motivated by this, we come up with a novel
text-to-video generation framework, termed TF-T2V, which can directly learn
with text-free videos. The rationale behind is to separate the process of text
decoding from that of temporal modeling. To this end, we employ a content
branch and a motion branch, which are jointly optimized with weights shared.
Following such a pipeline, we study the effect of doubling the scale of
training set (i.e., video-only WebVid10M) with some randomly collected
text-free videos and are encouraged to observe the performance improvement (FID
from 9.67 to 8.19 and FVD from 484 to 441), demonstrating the scalability of
our approach. We also find that our model could enjoy sustainable performance
gain (FID from 8.19 to 7.64 and FVD from 441 to 366) after reintroducing some
text labels for training. Finally, we validate the effectiveness and
generalizability of our ideology on both native text-to-video generation and
compositional video synthesis paradigms. Code and models will be publicly
available at https://tf-t2v.github.io/.",CVPR
"We address the problem of generating realistic 3D human-object interactions
(HOIs) driven by textual prompts. To this end, we take a modular design and
decompose the complex task into simpler sub-tasks. We first develop a
dual-branch diffusion model (HOI-DM) to generate both human and object motions
conditioned on the input text, and encourage coherent motions by a
cross-attention communication module between the human and object motion
generation branches. We also develop an affordance prediction diffusion model
(APDM) to predict the contacting area between the human and object during the
interactions driven by the textual prompt. The APDM is independent of the
results by the HOI-DM and thus can correct potential errors by the latter.
Moreover, it stochastically generates the contacting points to diversify the
generated motions. Finally, we incorporate the estimated contacting points into
the classifier-guidance to achieve accurate and close contact between humans
and objects. To train and evaluate our approach, we annotate BEHAVE dataset
with text descriptions. Experimental results on BEHAVE and OMOMO demonstrate
that our approach produces realistic HOIs with various interactions and
different types of objects.",CVPR
"Recently, dataset distillation has paved the way towards efficient machine
learning, especially for image datasets. However, the distillation for videos,
characterized by an exclusive temporal dimension, remains an underexplored
domain. In this work, we provide the first systematic study of video
distillation and introduce a taxonomy to categorize temporal compression. Our
investigation reveals that the temporal information is usually not well learned
during distillation, and the temporal dimension of synthetic data contributes
little. The observations motivate our unified framework of disentangling the
dynamic and static information in the videos. It first distills the videos into
still images as static memory and then compensates the dynamic and motion
information with a learnable dynamic memory block. Our method achieves
state-of-the-art on video datasets at different scales, with a notably smaller
memory storage budget. Our code is available at
https://github.com/yuz1wan/video_distillation.",CVPR
"We present Readout Guidance, a method for controlling text-to-image diffusion
models with learned signals. Readout Guidance uses readout heads, lightweight
networks trained to extract signals from the features of a pre-trained, frozen
diffusion model at every timestep. These readouts can encode single-image
properties, such as pose, depth, and edges; or higher-order properties that
relate multiple images, such as correspondence and appearance similarity.
Furthermore, by comparing the readout estimates to a user-defined target, and
back-propagating the gradient through the readout head, these estimates can be
used to guide the sampling process. Compared to prior methods for conditional
generation, Readout Guidance requires significantly fewer added parameters and
training samples, and offers a convenient and simple recipe for reproducing
different forms of conditional control under a single framework, with a single
architecture and sampling procedure. We showcase these benefits in the
applications of drag-based manipulation, identity-consistent generation, and
spatially aligned control. Project page: https://readout-guidance.github.io.",CVPR
"Recently, the rise of query-based Transformer decoders is reshaping
camera-based 3D object detection. These query-based decoders are surpassing the
traditional dense BEV (Bird's Eye View)-based methods. However, we argue that
dense BEV frameworks remain important due to their outstanding abilities in
depth estimation and object localization, depicting 3D scenes accurately and
comprehensively. This paper aims to address the drawbacks of the existing dense
BEV-based 3D object detectors by introducing our proposed enhanced components,
including a CRF-modulated depth estimation module enforcing object-level
consistencies, a long-term temporal aggregation module with extended receptive
fields, and a two-stage object decoder combining perspective techniques with
CRF-modulated depth embedding. These enhancements lead to a ""modernized"" dense
BEV framework dubbed BEVNeXt. On the nuScenes benchmark, BEVNeXt outperforms
both BEV-based and query-based frameworks under various settings, achieving a
state-of-the-art result of 64.2 NDS on the nuScenes test set.",CVPR
"This paper unravels the potential of sketches for diffusion models,
addressing the deceptive promise of direct sketch control in generative AI. We
importantly democratise the process, enabling amateur sketches to generate
precise images, living up to the commitment of ""what you sketch is what you
get"". A pilot study underscores the necessity, revealing that deformities in
existing models stem from spatial-conditioning. To rectify this, we propose an
abstraction-aware framework, utilising a sketch adapter, adaptive time-step
sampling, and discriminative guidance from a pre-trained fine-grained
sketch-based image retrieval model, working synergistically to reinforce
fine-grained sketch-photo association. Our approach operates seamlessly during
inference without the need for textual prompts; a simple, rough sketch akin to
what you and I can create suffices! We welcome everyone to examine results
presented in the paper and its supplementary. Contributions include
democratising sketch control, introducing an abstraction-aware framework, and
leveraging discriminative guidance, validated through extensive experiments.",CVPR
"The autonomous driving community has shown significant interest in 3D
occupancy prediction, driven by its exceptional geometric perception and
general object recognition capabilities. To achieve this, current works try to
construct a Tri-Perspective View (TPV) or Occupancy (OCC) representation
extending from the Bird-Eye-View perception. However, compressed views like TPV
representation lose 3D geometry information while raw and sparse OCC
representation requires heavy but redundant computational costs. To address the
above limitations, we propose Compact Occupancy TRansformer (COTR), with a
geometry-aware occupancy encoder and a semantic-aware group decoder to
reconstruct a compact 3D OCC representation. The occupancy encoder first
generates a compact geometrical OCC feature through efficient explicit-implicit
view transformation. Then, the occupancy decoder further enhances the semantic
discriminability of the compact OCC representation by a coarse-to-fine semantic
grouping strategy. Empirical experiments show that there are evident
performance gains across multiple baselines, e.g., COTR outperforms baselines
with a relative improvement of 8%-15%, demonstrating the superiority of our
method.",CVPR
"Prompt learning in pretrained visual-language models has shown remarkable
flexibility across various downstream tasks. Leveraging its inherent
lightweight nature, recent research attempted to integrate the powerful
pretrained models into federated learning frameworks to simultaneously reduce
communication costs and promote local training on insufficient data. Despite
these efforts, current federated prompt learning methods lack specialized
designs to systematically address severe data heterogeneities, e.g., data
distribution with both label and feature shifts involved. To address this
challenge, we present Federated Prompts Cooperation via Optimal Transport
(FedOTP), which introduces efficient collaborative prompt learning strategies
to capture diverse category traits on a per-client basis. Specifically, for
each client, we learn a global prompt to extract consensus knowledge among
clients, and a local prompt to capture client-specific category
characteristics. Unbalanced Optimal Transport is then employed to align local
visual features with these prompts, striking a balance between global consensus
and local personalization. By relaxing one of the equality constraints, FedOTP
enables prompts to focus solely on the core regions of image patches. Extensive
experiments on datasets with various types of heterogeneities have demonstrated
that our FedOTP outperforms the state-of-the-art methods.",CVPR
"Domain generalization (DG) seeks to learn robust models that generalize well
under unknown distribution shifts. As a critical aspect of DG, optimizer
selection has not been explored in depth. Currently, most DG methods follow the
widely used benchmark, DomainBed, and utilize Adam as the default optimizer for
all datasets. However, we reveal that Adam is not necessarily the optimal
choice for the majority of current DG methods and datasets. Based on the
perspective of loss landscape flatness, we propose a novel approach,
Flatness-Aware Minimization for Domain Generalization (FAD), which can
efficiently optimize both zeroth-order and first-order flatness simultaneously
for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD)
generalization error and convergence. Our experimental results demonstrate the
superiority of FAD on various DG datasets. Additionally, we confirm that FAD is
capable of discovering flatter optima in comparison to other zeroth-order and
first-order flatness-aware optimization methods.",CVPR
"Humans can infer 3D structure from 2D images of an object based on past
experience and improve their 3D understanding as they see more images. Inspired
by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel
view synthesis from an arbitrary number of unposed images. Given a few unposed
images of an object, we adapt a pre-trained view-conditioned diffusion model
together with the camera poses of the images via test-time fine-tuning. The
adapted diffusion model and the obtained camera poses are then utilized as
instance-specific priors for 3D reconstruction and novel view synthesis. We
show that as the number of input images increases, the performance of our
approach improves, bridging the gap between optimization-based prior-less 3D
reconstruction methods and single-image-to-3D diffusion-based methods. We
demonstrate our system on real images as well as standard synthetic benchmarks.
Our ablation studies confirm that this adaption behavior is key for more
accurate 3D understanding.",CVPR
"Concerns for the privacy of individuals captured in public imagery have led
to privacy-preserving action recognition. Existing approaches often suffer from
issues arising through obfuscation being applied globally and a lack of
interpretability. Global obfuscation hides privacy sensitive regions, but also
contextual regions important for action recognition. Lack of interpretability
erodes trust in these new technologies. We highlight the limitations of current
paradigms and propose a solution: Human selected privacy templates that yield
interpretability by design, an obfuscation scheme that selectively hides
attributes and also induces temporal consistency, which is important in action
recognition. Our approach is architecture agnostic and directly modifies input
imagery, while existing approaches generally require architecture training. Our
approach offers more flexibility, as no retraining is required, and outperforms
alternatives on three widely used datasets.",CVPR
"Depression Recognition (DR) poses a considerable challenge, especially in the
context of the growing concerns surrounding privacy. Traditional automatic
diagnosis of DR technology necessitates the use of facial images, undoubtedly
expose the patient identity features and poses privacy risks. In order to
mitigate the potential risks associated with the inappropriate disclosure of
patient facial images, we design a new imaging system to erase the identity
information of captured facial images while retain disease-relevant features.
It is irreversible for identity information recovery while preserving essential
disease-related characteristics necessary for accurate DR. More specifically,
we try to record a de-identified facial image (erasing the identifiable
features as much as possible) by a learnable lens, which is optimized in
conjunction with the following DR task as well as a range of face analysis
related auxiliary tasks in an end-to-end manner. These aforementioned
strategies form our final Optical deep Depression Recognition network
(OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets
demonstrate that our OpticalDR has achieved state-of-the-art privacy protection
performance with an average AUC of 0.51 on popular facial recognition models,
and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and
7.89/8.82 on AVEC 2014, respectively.",CVPR
"We address the problem of generating realistic 3D motions of humans
interacting with objects in a scene. Our key idea is to create a neural
interaction field attached to a specific object, which outputs the distance to
the valid interaction manifold given a human pose as input. This interaction
field guides the sampling of an object-conditioned human motion diffusion
model, so as to encourage plausible contacts and affordance semantics. To
support interactions with scarcely available data, we propose an automated
synthetic data pipeline. For this, we seed a pre-trained motion model, which
has priors for the basics of human movement, with interaction-specific anchor
poses extracted from limited motion capture data. Using our guided diffusion
model trained on generated synthetic data, we synthesize realistic motions for
sitting and lifting with several objects, outperforming alternative approaches
in terms of motion quality and successful action completion. We call our
framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.",CVPR
"The Vision Transformer has emerged as a powerful tool for image
classification tasks, surpassing the performance of convolutional neural
networks (CNNs). Recently, many researchers have attempted to understand the
robustness of Transformers against adversarial attacks. However, previous
researches have focused solely on perturbations in the spatial domain. This
paper proposes an additional perspective that explores the adversarial
robustness of Transformers against frequency-selective perturbations in the
spectral domain. To facilitate comparison between these two domains, an attack
framework is formulated as a flexible tool for implementing attacks on images
in the spatial and spectral domains. The experiments reveal that Transformers
rely more on phase and low frequency information, which can render them more
vulnerable to frequency-selective attacks than CNNs. This work offers new
insights into the properties and adversarial robustness of Transformers.",CVPR
"Content-aware graphic layout generation aims to automatically arrange visual
elements along with a given content, such as an e-commerce product image. In
this paper, we argue that the current layout generation approaches suffer from
the limited training data for the high-dimensional layout structure. We show
that a simple retrieval augmentation can significantly improve the generation
quality. Our model, which is named Retrieval-Augmented Layout Transformer
(RALF), retrieves nearest neighbor layout examples based on an input image and
feeds these results into an autoregressive generator. Our model can apply
retrieval augmentation to various controllable generation tasks and yield
high-quality layouts within a unified architecture. Our extensive experiments
show that RALF successfully generates content-aware layouts in both constrained
and unconstrained settings and significantly outperforms the baselines.",CVPR
"While fine-tuning is a de facto standard method for training deep neural
networks, it still suffers from overfitting when using small target datasets.
Previous methods improve fine-tuning performance by maintaining knowledge of
the source datasets or introducing regularization terms such as contrastive
loss. However, these methods require auxiliary source information (e.g., source
labels or datasets) or heavy additional computations. In this paper, we propose
a simple method called adaptive random feature regularization (AdaRand).
AdaRand helps the feature extractors of training models to adaptively change
the distribution of feature vectors for downstream classification tasks without
auxiliary source information and with reasonable computation costs. To this
end, AdaRand minimizes the gap between feature vectors and random reference
vectors that are sampled from class conditional Gaussian distributions.
Furthermore, AdaRand dynamically updates the conditional distribution to follow
the currently updated feature extractors and balance the distance between
classes in feature spaces. Our experiments show that AdaRand outperforms the
other fine-tuning regularization, which requires auxiliary source information
and heavy computation costs.",CVPR
"Promptly identifying procedural errors from egocentric videos in an online
setting is highly challenging and valuable for detecting mistakes as soon as
they happen. This capability has a wide range of applications across various
fields, such as manufacturing and healthcare. The nature of procedural mistakes
is open-set since novel types of failures might occur, which calls for
one-class classifiers trained on correctly executed procedures. However, no
technique can currently detect open-set procedural mistakes online. We propose
PREGO, the first online one-class classification model for mistake detection in
PRocedural EGOcentric videos. PREGO is based on an online action recognition
component to model the current action, and a symbolic reasoning module to
predict the next actions. Mistake detection is performed by comparing the
recognized current action with the expected future one. We evaluate PREGO on
two procedural egocentric video datasets, Assembly101 and Epic-tent, which we
adapt for online benchmarking of procedural mistake detection to establish
suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets,
respectively.",CVPR
"The popularity of large-scale pre-training has promoted the development of
medical foundation models. However, some studies have shown that although
foundation models exhibit strong general feature extraction capabilities, their
performance on specific tasks is still inferior to task-specific methods. In
this paper, we explore a new perspective called ``Knowledge Decomposition'' to
improve the performance on specific medical tasks, which deconstruct the
foundation model into multiple lightweight expert models, each dedicated to a
particular task, with the goal of improving specialization while concurrently
mitigating resource expenditure. To accomplish the above objective, we design a
novel framework named Low-Rank Knowledge Decomposition (LoRKD), which
explicitly separates graidents by incorporating low-rank expert modules and the
efficient knowledge separation convolution. Extensive experimental results
demonstrate that the decomposed models perform well in terms of performance and
transferability, even surpassing the original foundation models.",CVPR
"We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian
Splatting (GS) that leverages forward mapping volume rendering to achieve
photorealistic novel view synthesis and relighting results. Unlike previous
works that use implicit neural representations and volume rendering (e.g.
NeRF), which suffer from low expressive power and high computational
complexity, we extend GS, a top-performance representation for novel view
synthesis, to estimate scene geometry, surface material, and environment
illumination from multi-view images captured under unknown lighting conditions.
There are two main problems when introducing GS to inverse rendering: 1) GS
does not support producing plausible normal natively; 2) forward mapping (e.g.
rasterization and splatting) cannot trace the occlusion like backward mapping
(e.g. ray tracing). To address these challenges, our GS-IR proposes an
efficient optimization scheme that incorporates a depth-derivation-based
regularization for normal estimation and a baking-based occlusion to model
indirect lighting. The flexible and expressive GS representation allows us to
achieve fast and compact geometry reconstruction, photorealistic novel view
synthesis, and effective physically-based rendering. We demonstrate the
superiority of our method over baseline methods through qualitative and
quantitative evaluations on various challenging scenes.",CVPR
"In this paper, we propose a new multi-modal task, namely audio-visual
instance segmentation (AVIS), in which the goal is to identify, segment, and
track individual sounding object instances in audible videos, simultaneously.
To our knowledge, it is the first time that instance segmentation has been
extended into the audio-visual domain. To better facilitate this research, we
construct the first audio-visual instance segmentation benchmark (AVISeg).
Specifically, AVISeg consists of 1,258 videos with an average duration of 62.6
seconds from YouTube and public audio-visual datasets, where 117 videos have
been annotated by using an interactive semi-automatic labeling tool based on
the Segment Anything Model (SAM). In addition, we present a simple baseline
model for the AVIS task. Our new model introduces an audio branch and a
cross-modal fusion module to Mask2Former to locate all sounding objects.
Finally, we evaluate the proposed method using two backbones on AVISeg. We
believe that AVIS will inspire the community towards a more comprehensive
multi-modal understanding.",CVPR
"Multi-Object Tracking (MOT) remains a vital component of intelligent video
analysis, which aims to locate targets and maintain a consistent identity for
each target throughout a video sequence. Existing works usually learn a
discriminative feature representation, such as motion and appearance, to
associate the detections across frames, which are easily affected by mutual
occlusion and background clutter in practice. In this paper, we propose a
simple yet effective two-stage feature learning paradigm to jointly learn
single-shot and multi-shot features for different targets, so as to achieve
robust data association in the tracking process. For the detections without
being associated, we design a novel single-shot feature learning module to
extract discriminative features of each detection, which can efficiently
associate targets between adjacent frames. For the tracklets being lost several
frames, we design a novel multi-shot feature learning module to extract
discriminative features of each tracklet, which can accurately refind these
lost targets after a long period. Once equipped with a simple data association
logic, the resulting VisualTracker can perform robust MOT based on the
single-shot and multi-shot feature representations. Extensive experimental
results demonstrate that our method has achieved significant improvements on
MOT17 and MOT20 datasets while reaching state-of-the-art performance on
DanceTrack dataset.",CVPR
"The authentic 3D hand avatar with every identifiable information, such as
hand shapes and textures, is necessary for immersive experiences in AR/VR. In
this paper, we present a universal hand model (UHM), which 1) can universally
represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can
be adapted to each person with a short phone scan for the authentic hand
avatar. For effective universal hand modeling, we perform tracking and modeling
at the same time, while previous 3D hand models perform them separately. The
conventional separate pipeline suffers from the accumulated errors from the
tracking stage, which cannot be recovered in the modeling stage. On the other
hand, ours does not suffer from the accumulated errors while having a much more
concise overall pipeline. We additionally introduce a novel image matching loss
function to address a skin sliding during the tracking and modeling, while
existing works have not focused on it much. Finally, using learned priors from
our UHM, we effectively adapt our UHM to each person's short phone scan for the
authentic hand avatar.",CVPR
"Synthesizing natural human motions that enable a 3D human avatar to walk and
reach for arbitrary goals in 3D space remains an unsolved problem with many
applications. Existing methods (data-driven or using reinforcement learning)
are limited in terms of generalization and motion naturalness. A primary
obstacle is the scarcity of training data that combines locomotion with goal
reaching. To address this, we introduce WANDR, a data-driven model that takes
an avatar's initial pose and a goal's 3D position and generates natural human
motions that place the end effector (wrist) on the goal location. To solve
this, we introduce novel intention features that drive rich goal-oriented
movement. Intention guides the agent to the goal, and interactively adapts the
generation to novel situations without needing to define sub-goals or the
entire motion path. Crucially, intention allows training on datasets that have
goal-oriented motions as well as those that do not. WANDR is a conditional
Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE
datasets. We evaluate our method extensively and demonstrate its ability to
generate natural and long-term motions that reach 3D goals and generalize to
unseen goal locations. Our models and code are available for research purposes
at wandr.is.tue.mpg.de.",CVPR
"Though significant progress in human pose and shape recovery from monocular
RGB images has been made in recent years, obtaining 3D human motion with high
accuracy and temporal consistency from videos remains challenging. Existing
video-based methods tend to reconstruct human motion from global image
features, which lack detailed representation capability and limit the
reconstruction accuracy. In this paper, we propose a Temporal-Aware Refining
Network (TAR), to synchronously explore temporal-aware global and local image
features for accurate pose and shape recovery. First, a global transformer
encoder is introduced to obtain temporal global features from static feature
sequences. Second, a bidirectional ConvGRU network takes the sequence of
high-resolution feature maps as input, and outputs temporal local feature maps
that maintain high resolution and capture the local motion of the human body.
Finally, a recurrent refinement module iteratively updates estimated SMPL
parameters by leveraging both global and local temporal information to achieve
accurate and smooth results. Extensive experiments demonstrate that our TAR
obtains more accurate results than previous state-of-the-art methods on popular
benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.",CVPR
"Video Transformers have become the prevalent solution for various video
downstream tasks with superior expressive power and flexibility. However, these
video transformers suffer from heavy computational costs induced by the massive
number of tokens across the entire video frames, which has been the major
barrier to training the model. Further, the patches irrelevant to the main
contents, e.g., backgrounds, degrade the generalization performance of models.
To tackle these issues, we propose training free token merging for lightweight
video Transformer (vid-TLDR) that aims to enhance the efficiency of video
Transformers by merging the background tokens without additional training. For
vid-TLDR, we introduce a novel approach to capture the salient regions in
videos only with the attention map. Further, we introduce the saliency-aware
token merging strategy by dropping the background tokens and sharpening the
object scores. Our experiments show that vid-TLDR significantly mitigates the
computational complexity of video Transformers while achieving competitive
performance compared to the base model without vid-TLDR. Code is available at
https://github.com/mlvlab/vid-TLDR.",CVPR
"Pre-trained models with large-scale training data, such as CLIP and Stable
Diffusion, have demonstrated remarkable performance in various high-level
computer vision tasks such as image understanding and generation from language
descriptions. Yet, their potential for low-level tasks such as image
restoration remains relatively unexplored. In this paper, we explore such
models to enhance image restoration. As off-the-shelf features (OSF) from
pre-trained models do not directly serve image restoration, we propose to learn
an additional lightweight module called Pre-Train-Guided Refinement Module
(PTG-RM) to refine restoration results of a target restoration network with
OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying
Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention
(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,
while PTG-CSA enhances spatial-channel attention for restoration-related
learning. Extensive experiments demonstrate that PTG-RM, with its compact size
($<$1M parameters), effectively enhances restoration performance of various
models across different tasks, including low-light enhancement, deraining,
deblurring, and denoising.",CVPR
"We have recently seen tremendous progress in photo-real human modeling and
rendering. Yet, efficiently rendering realistic human performance and
integrating it into the rasterization pipeline remains challenging. In this
paper, we present HiFi4G, an explicit and compact Gaussian-based approach for
high-fidelity human performance rendering from dense footage. Our core
intuition is to marry the 3D Gaussian representation with non-rigid tracking,
achieving a compact and compression-friendly representation. We first propose a
dual-graph mechanism to obtain motion priors, with a coarse deformation graph
for effective initialization and a fine-grained Gaussian graph to enforce
subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with
adaptive spatial-temporal regularizers to effectively balance the non-rigid
prior and Gaussian updating. We also present a companion compression scheme
with residual compensation for immersive experiences on various platforms. It
achieves a substantial compression rate of approximately 25 times, with less
than 2MB of storage per frame. Extensive experiments demonstrate the
effectiveness of our approach, which significantly outperforms existing
approaches in terms of optimization speed, rendering quality, and storage
overhead.",CVPR
"Although effective deepfake detection models have been developed in recent
years, recent studies have revealed that these models can result in unfair
performance disparities among demographic groups, such as race and gender. This
can lead to particular groups facing unfair targeting or exclusion from
detection, potentially allowing misclassified deepfakes to manipulate public
opinion and undermine trust in the model. The existing method for addressing
this problem is providing a fair loss function. It shows good fairness
performance for intra-domain evaluation but does not maintain fairness for
cross-domain testing. This highlights the significance of fairness
generalization in the fight against deepfakes. In this work, we propose the
first method to address the fairness generalization problem in deepfake
detection by simultaneously considering features, loss, and optimization
aspects. Our method employs disentanglement learning to extract demographic and
domain-agnostic forgery features, fusing them to encourage fair learning across
a flattened loss landscape. Extensive experiments on prominent deepfake
datasets demonstrate our method's effectiveness, surpassing state-of-the-art
approaches in preserving fairness during cross-domain deepfake detection. The
code is available at https://github.com/Purdue-M2/Fairness-Generalization",CVPR
"Existing super-resolution (SR) models primarily focus on restoring local
texture details, often neglecting the global semantic information within the
scene. This oversight can lead to the omission of crucial semantic details or
the introduction of inaccurate textures during the recovery process. In our
work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering
SR models with the capacity to comprehend low-resolution images. We achieve
this by marrying image appearance and language understanding to generate a
cognitive embedding, which not only activates prior information from large
text-to-image diffusion models but also facilitates the generation of
high-quality reference images to optimize the SR process. To further improve
image fidelity, we propose a novel condition injection scheme called
""All-in-Attention"", consolidating all conditional information into a single
module. Consequently, our method successfully restores semantically correct and
photorealistic details, demonstrating state-of-the-art performance across
multiple benchmarks. Code: https://github.com/VINHYU/CoSeR",CVPR
"General image fusion aims at integrating important information from
multi-source images. However, due to the significant cross-task gap, the
respective fusion mechanism varies considerably in practice, resulting in
limited performance across subtasks. To handle this problem, we propose a novel
task-customized mixture of adapters (TC-MoA) for general image fusion,
adaptively prompting various fusion tasks in a unified model. We borrow the
insight from the mixture of experts (MoE), taking the experts as efficient
tuning adapters to prompt a pre-trained foundation model. These adapters are
shared across different tasks and constrained by mutual information
regularization, ensuring compatibility with different tasks while
complementarity for multi-source images. The task-specific routing networks
customize these adapters to extract task-specific information from different
sources with dynamic dominant intensity, performing adaptive visual feature
prompt fusion. Notably, our TC-MoA controls the dominant intensity bias for
different fusion tasks, successfully unifying multiple fusion tasks in a single
model. Extensive experiments show that TC-MoA outperforms the competing
approaches in learning commonalities while retaining compatibility for general
image fusion (multi-modal, multi-exposure, and multi-focus), and also
demonstrating striking controllability on more generalization experiments. The
code is available at https://github.com/YangSun22/TC-MoA .",CVPR
"Referring video segmentation relies on natural language expressions to
identify and segment objects, often emphasizing motion clues. Previous works
treat a sentence as a whole and directly perform identification at the
video-level, mixing up static image-level cues with temporal motion cues.
However, image-level features cannot well comprehend motion cues in sentences,
and static cues are not crucial for temporal perception. In fact, static cues
can sometimes interfere with temporal perception by overshadowing motion cues.
In this work, we propose to decouple video-level referring expression
understanding into static and motion perception, with a specific emphasis on
enhancing temporal comprehension. Firstly, we introduce an
expression-decoupling module to make static cues and motion cues perform their
distinct role, alleviating the issue of sentence embeddings overlooking motion
cues. Secondly, we propose a hierarchical motion perception module to capture
temporal information effectively across varying timescales. Furthermore, we
employ contrastive learning to distinguish the motions of visually similar
objects. These contributions yield state-of-the-art performance across five
datasets, including a remarkable $\textbf{9.2%}$ $\mathcal{J\&F}$ improvement
on the challenging $\textbf{MeViS}$ dataset. Code is available at
https://github.com/heshuting555/DsHmp.",CVPR
"Multi-target multi-camera tracking is a crucial task that involves
identifying and tracking individuals over time using video streams from
multiple cameras. This task has practical applications in various fields, such
as visual surveillance, crowd behavior analysis, and anomaly detection.
However, due to the difficulty and cost of collecting and labeling data,
existing datasets for this task are either synthetically generated or
artificially constructed within a controlled camera network setting, which
limits their ability to model real-world dynamics and generalize to diverse
camera configurations. To address this issue, we present MTMMC, a real-world,
large-scale dataset that includes long video sequences captured by 16
multi-modal cameras in two different environments - campus and factory - across
various time, weather, and season conditions. This dataset provides a
challenging test-bed for studying multi-camera tracking under diverse
real-world complexities and includes an additional input modality of spatially
aligned and temporally synchronized RGB and thermal cameras, which enhances the
accuracy of multi-camera tracking. MTMMC is a super-set of existing datasets,
benefiting independent fields such as person detection, re-identification, and
multiple object tracking. We provide baselines and new learning setups on this
dataset and set the reference scores for future studies. The datasets, models,
and test server will be made publicly available.",CVPR
"Recent self-training techniques have shown notable improvements in
unsupervised domain adaptation for 3D object detection (3D UDA). These
techniques typically select pseudo labels, i.e., 3D boxes, to supervise models
for the target domain. However, this selection process inevitably introduces
unreliable 3D boxes, in which 3D points cannot be definitively assigned as
foreground or background. Previous techniques mitigate this by reweighting
these boxes as pseudo labels, but these boxes can still poison the training
process. To resolve this problem, in this paper, we propose a novel pseudo
label refinery framework. Specifically, in the selection process, to improve
the reliability of pseudo boxes, we propose a complementary augmentation
strategy. This strategy involves either removing all points within an
unreliable box or replacing it with a high-confidence box. Moreover, the point
numbers of instances in high-beam datasets are considerably higher than those
in low-beam datasets, also degrading the quality of pseudo labels during the
training process. We alleviate this issue by generating additional proposals
and aligning RoI features across different domains. Experimental results
demonstrate that our method effectively enhances the quality of pseudo labels
and consistently surpasses the state-of-the-art methods on six autonomous
driving benchmarks. Code will be available at
https://github.com/Zhanwei-Z/PERE.",CVPR
"Single-source domain generalization (SDG) for object detection is a
challenging yet essential task as the distribution bias of the unseen domain
degrades the algorithm performance significantly. However, existing methods
attempt to extract domain-invariant features, neglecting that the biased data
leads the network to learn biased features that are non-causal and poorly
generalizable. To this end, we propose an Unbiased Faster R-CNN (UFR) for
generalizable feature learning. Specifically, we formulate SDG in object
detection from a causal perspective and construct a Structural Causal Model
(SCM) to analyze the data bias and feature bias in the task, which are caused
by scene confounders and object attribute confounders. Based on the SCM, we
design a Global-Local Transformation module for data augmentation, which
effectively simulates domain diversity and mitigates the data bias.
Additionally, we introduce a Causal Attention Learning module that incorporates
a designed attention invariance loss to learn image-level features that are
robust to scene confounders. Moreover, we develop a Causal Prototype Learning
module with an explicit instance constraint and an implicit prototype
constraint, which further alleviates the negative impact of object attribute
confounders. Experimental results on five scenes demonstrate the prominent
generalization ability of our method, with an improvement of 3.9% mAP on the
Night-Clear scene.",CVPR
"Federated learning (FL) has emerged as a powerful paradigm for learning from
decentralized data, and federated domain generalization further considers the
test dataset (target domain) is absent from the decentralized training data
(source domains). However, most existing FL methods assume that domain labels
are provided during training, and their evaluation imposes explicit constraints
on the number of domains, which must strictly match the number of clients.
Because of the underutilization of numerous edge devices and additional
cross-client domain annotations in the real world, such restrictions may be
impractical and involve potential privacy leaks. In this paper, we propose an
efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a
method that tackles the above restrictions by learning adaptive prompts for
domain generalization in a distributed manner. Specifically, we first design
two types of prompts, i.e., global prompt to capture general knowledge across
all clients and domain prompts to capture domain-specific knowledge. They
eliminate the restriction on the one-to-one mapping between source domains and
local clients. Furthermore, a dynamic query metric is introduced to
automatically search the suitable domain label for each sample, which includes
two-substep text-image alignments based on prompt tuning without
labor-intensive annotation. Extensive experiments on multiple datasets
demonstrate that our DiPrompT achieves superior domain generalization
performance over state-of-the-art FL methods when domain labels are not
provided, and even outperforms many centralized learning methods using domain
labels.",CVPR
"We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.",CVPR
"Generative 3D part assembly involves understanding part relationships and
predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work
often focus on the geometry of individual parts, neglecting part-whole
hierarchies of objects. Leveraging two key observations: 1) super-part poses
provide strong hints about part poses, and 2) predicting super-part poses is
easier due to fewer superparts, we propose a part-whole-hierarchy message
passing network for efficient 3D part assembly. We first introduce super-parts
by grouping geometrically similar parts without any semantic labels. Then we
employ a part-whole hierarchical encoder, wherein a super-part encoder predicts
latent super-part poses based on input parts. Subsequently, we transform the
point cloud using the latent poses, feeding it to the part encoder for
aggregating super-part information and reasoning about part relationships to
predict all part poses. In training, only ground-truth part poses are required.
During inference, the predicted latent poses of super-parts enhance
interpretability. Experimental results on the PartNet dataset show that our
method achieves state-of-the-art performance in part and connectivity accuracy
and enables an interpretable hierarchical part assembly. Code is available at
https://github.com/pkudba/3DHPA.",CVPR
"Estimating the 3D structure of the human body from natural scenes is a
fundamental aspect of visual perception. 3D human pose estimation is a vital
step in advancing fields like AIGC and human-robot interaction, serving as a
crucial technique for understanding and interacting with human actions in
real-world settings. However, the current datasets, often collected under
single laboratory conditions using complex motion capture equipment and
unvarying backgrounds, are insufficient. The absence of datasets on variable
conditions is stalling the progress of this crucial task. To facilitate the
development of 3D pose estimation, we present FreeMan, the first large-scale,
multi-view dataset collected under the real-world conditions. FreeMan was
captured by synchronizing 8 smartphones across diverse scenarios. It comprises
11M frames from 8000 sequences, viewed from different perspectives. These
sequences cover 40 subjects across 10 different scenarios, each with varying
lighting conditions. We have also established an semi-automated pipeline
containing error detection to reduce the workload of manual check and ensure
precise annotation. We provide comprehensive evaluation baselines for a range
of tasks, underlining the significant challenges posed by FreeMan. Further
evaluations of standard indoor/outdoor human sensing datasets reveal that
FreeMan offers robust representation transferability in real and complex
scenes. Code and data are available at https://wangjiongw.github.io/freeman.",CVPR
"The human ability to easily solve multimodal tasks in context (i.e., with
only a few demonstrations or simple instructions), is what current multimodal
systems have largely struggled to imitate. In this work, we demonstrate that
the task-agnostic in-context learning capabilities of large multimodal models
can be significantly enhanced by effective scaling-up. We introduce Emu2, a
generative multimodal model with 37 billion parameters, trained on large-scale
multimodal sequences with a unified autoregressive objective. Emu2 exhibits
strong multimodal in-context learning abilities, even emerging to solve tasks
that require on-the-fly reasoning, such as visual prompting and object-grounded
generation. The model sets a new record on multiple multimodal understanding
tasks in few-shot settings. When instruction-tuned to follow specific
instructions, Emu2 further achieves new state-of-the-art on challenging tasks
such as question answering benchmarks for large multimodal models and
open-ended subject-driven generation. These achievements demonstrate that Emu2
can serve as a base model and general-purpose interface for a wide range of
multimodal tasks. Code and models are publicly available to facilitate future
research.",CVPR
"This paper presents a neural architecture MVDiffusion++ for 3D object
reconstruction that synthesizes dense and high-resolution views of an object
given one or a few images without camera poses. MVDiffusion++ achieves superior
flexibility and scalability with two surprisingly simple ideas: 1) A
``pose-free architecture'' where standard self-attention among 2D latent
features learns 3D consistency across an arbitrary number of conditional and
generation views without explicitly using camera pose information; and 2) A
``view dropout strategy'' that discards a substantial number of output views
during training, which reduces the training-time memory footprint and enables
dense and high-resolution view synthesis at test time. We use the Objaverse for
training and the Google Scanned Objects for evaluation with standard novel view
synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly
outperforms the current state of the arts. We also demonstrate a text-to-3D
application example by combining MVDiffusion++ with a text-to-image generative
model. The project page is at https://mvdiffusion-plusplus.github.io.",CVPR
"Transformer recently emerged as the de facto model for computer vision tasks
and has also been successfully applied to shadow removal. However, these
existing methods heavily rely on intricate modifications to the attention
mechanisms within the transformer blocks while using a generic patch embedding.
As a result, it often leads to complex architectural designs requiring
additional computation resources. In this work, we aim to explore the efficacy
of incorporating shadow information within the early processing stage.
Accordingly, we propose a transformer-based framework with a novel patch
embedding that is tailored for shadow removal, dubbed ShadowMaskFormer.
Specifically, we present a simple and effective mask-augmented patch embedding
to integrate shadow information and promote the model's emphasis on acquiring
knowledge for shadow regions. Extensive experiments conducted on the ISTD,
ISTD+, and SRD benchmark datasets demonstrate the efficacy of our method
against state-of-the-art approaches while using fewer model parameters.",CVPR
"Ultra-fine-grained visual categorization (Ultra-FGVC) aims at distinguishing
highly similar sub-categories within fine-grained objects, such as different
soybean cultivars. Compared to traditional fine-grained visual categorization,
Ultra-FGVC encounters more hurdles due to the small inter-class and large
intra-class variation. Given these challenges, relying on human annotation for
Ultra-FGVC is impractical. To this end, our work introduces a novel task termed
Ultra-Fine-Grained Novel Class Discovery (UFG-NCD), which leverages partially
annotated data to identify new categories of unlabeled images for Ultra-FGVC.
To tackle this problem, we devise a Region-Aligned Proxy Learning (RAPL)
framework, which comprises a Channel-wise Region Alignment (CRA) module and a
Semi-Supervised Proxy Learning (SemiPL) strategy. The CRA module is designed to
extract and utilize discriminative features from local regions, facilitating
knowledge transfer from labeled to unlabeled classes. Furthermore, SemiPL
strengthens representation learning and knowledge transfer with proxy-guided
supervised learning and proxy-guided contrastive learning. Such techniques
leverage class distribution information in the embedding space, improving the
mining of subtle differences between labeled and unlabeled ultra-fine-grained
classes. Extensive experiments demonstrate that RAPL significantly outperforms
baselines across various datasets, indicating its effectiveness in handling the
challenges of UFG-NCD. Code is available at
https://github.com/SSDUT-Caiyq/UFG-NCD.",CVPR
"Comprehensive capturing of human motions requires both accurate captures of
complex poses and precise localization of the human within scenes. Most of the
HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However,
solely using these modalities or a combination of them may not be adequate for
HPE, particularly for complex and fast movements. For holistic human motion
understanding, we present RELI11D, a high-quality multimodal human motion
dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records
the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours
of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event
steams. Through extensive experiments, we demonstrate that the RELI11D presents
considerable challenges and opportunities as it contains many rapid and complex
motions that require precise location. To address the challenge of integrating
different modalities, we propose LEIR, a multimodal baseline that effectively
utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention
fusion strategy. We show that LEIR exhibits promising results for rapid motions
and daily motions and that utilizing the characteristics of multiple modalities
can indeed improve HPE performance. Both the dataset and source code will be
released publicly to the research community, fostering collaboration and
enabling further exploration in this field.",CVPR
"We propose Lodge, a network capable of generating extremely long dance
sequences conditioned on given music. We design Lodge as a two-stage coarse to
fine diffusion architecture, and propose the characteristic dance primitives
that possess significant expressiveness as intermediate representations between
two diffusion models. The first stage is global diffusion, which focuses on
comprehending the coarse-level music-dance correlation and production
characteristic dance primitives. In contrast, the second-stage is the local
diffusion, which parallelly generates detailed motion sequences under the
guidance of the dance primitives and choreographic rules. In addition, we
propose a Foot Refine Block to optimize the contact between the feet and the
ground, enhancing the physical realism of the motion. Our approach can
parallelly generate dance sequences of extremely long length, striking a
balance between global choreographic patterns and local motion quality and
expressiveness. Extensive experiments validate the efficacy of our method.",CVPR
"Neural radiance fields (NeRFs) are promising 3D representations for scenes,
objects, and humans. However, most existing methods require multi-view inputs
and per-scene training, which limits their real-life applications. Moreover,
current methods focus on single-subject cases, leaving scenes of interacting
hands that involve severe inter-hand occlusions and challenging view variations
remain unsolved. To tackle these issues, this paper proposes a generalizable
visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,
given an image of interacting hands as input, our VA-NeRF first obtains a
mesh-based representation of hands and extracts their corresponding geometric
and textural features. Subsequently, a feature fusion module that exploits the
visibility of query points and mesh vertices is introduced to adaptively merge
features of both hands, enabling the recovery of features in unseen areas.
Additionally, our VA-NeRF is optimized together with a novel discriminator
within an adversarial learning paradigm. In contrast to conventional
discriminators that predict a single real/fake label for the synthesized image,
the proposed discriminator generates a pixel-wise visibility map, providing
fine-grained supervision for unseen areas and encouraging the VA-NeRF to
improve the visual quality of synthesized images. Experiments on the
Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms
conventional NeRFs significantly. Project Page:
\url{https://github.com/XuanHuang0/VANeRF}.",CVPR
"Humans effortlessly interpret images by parsing them into part-whole
hierarchies; deep learning excels in learning multi-level feature spaces, but
they often lack explicit coding of part-whole relations, a prominent property
of medical imaging. To overcome this limitation, we introduce Adam-v2, a new
self-supervised learning framework extending Adam [79] by explicitly
incorporating part-whole hierarchies into its learning objectives through three
key branches: (1) Localizability, acquiring discriminative representations to
distinguish different anatomical patterns; (2) Composability, learning each
anatomical structure in a parts-to-whole manner; and (3) Decomposability,
comprehending each anatomical structure in a whole-to-parts manner.
Experimental results across 10 tasks, compared to 11 baselines in zero-shot,
few-shot transfer, and full fine-tuning settings, showcase Adam-v2's superior
performance over large-scale medical models and existing SSL methods across
diverse downstream tasks. The higher generality and robustness of Adam-v2's
representations originate from its explicit construction of hierarchies for
distinct anatomical structures from unlabeled medical images. Adam-v2 preserves
a semantic balance of anatomical diversity and harmony in its embedding,
yielding representations that are both generic and semantically meaningful, yet
overlooked in existing SSL methods. All code and pretrained models are
available at https://github.com/JLiangLab/Eden.",CVPR
"With advancements in domain generalized stereo matching networks, models
pre-trained on synthetic data demonstrate strong robustness to unseen domains.
However, few studies have investigated the robustness after fine-tuning them in
real-world scenarios, during which the domain generalization ability can be
seriously degraded. In this paper, we explore fine-tuning stereo matching
networks without compromising their robustness to unseen domains. Our
motivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for
fine-tuning: GT degrades, but PL preserves the domain generalization ability.
Empirically, we find the difference between GT and PL implies valuable
information that can regularize networks during fine-tuning. We also propose a
framework to utilize this difference for fine-tuning, consisting of a frozen
Teacher, an exponential moving average (EMA) Teacher, and a Student network.
The core idea is to utilize the EMA Teacher to measure what the Student has
learned and dynamically improve GT and PL for fine-tuning. We integrate our
framework with state-of-the-art networks and evaluate its effectiveness on
several real-world datasets. Extensive experiments show that our method
effectively preserves the domain generalization ability during fine-tuning.",CVPR
"Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.",CVPR
"In this study, we focus on the problem of 3D human mesh recovery from a
single image under obscured conditions. Most state-of-the-art methods aim to
improve 2D alignment technologies, such as spatial averaging and 2D joint
sampling. However, they tend to neglect the crucial aspect of 3D alignment by
improving 3D representations. Furthermore, recent methods struggle to separate
the target human from occlusion or background in crowded scenes as they
optimize the 3D space of target human with 3D joint coordinates as local
supervision. To address these issues, a desirable method would involve a
framework for fusing 2D and 3D features and a strategy for optimizing the 3D
space globally. Therefore, this paper presents 3D JOint contrastive learning
with TRansformers (JOTR) framework for handling occluded 3D human mesh
recovery. Our method includes an encoder-decoder transformer architecture to
fuse 2D and 3D representations for achieving 2D$\&$3D aligned results in a
coarse-to-fine manner and a novel 3D joint contrastive learning approach for
adding explicitly global supervision for the 3D feature space. The contrastive
learning approach includes two contrastive losses: joint-to-joint contrast for
enhancing the similarity of semantically similar voxels (i.e., human joints),
and joint-to-non-joint contrast for ensuring discrimination from others (e.g.,
occlusions and background). Qualitative and quantitative analyses demonstrate
that our method outperforms state-of-the-art competitors on both
occlusion-specific and standard benchmarks, significantly improving the
reconstruction of occluded humans.",CVPR
"In this paper, we present CCEdit, a versatile generative video editing
framework based on diffusion models. Our approach employs a novel trident
network structure that separates structure and appearance control, ensuring
precise and creative editing capabilities. Utilizing the foundational
ControlNet architecture, we maintain the structural integrity of the video
during editing. The incorporation of an additional appearance branch enables
users to exert fine-grained control over the edited key frame. These two side
branches seamlessly integrate into the main branch, which is constructed upon
existing text-to-image (T2I) generation models, through learnable temporal
layers. The versatility of our framework is demonstrated through a diverse
range of choices in both structure representations and personalized T2I models,
as well as the option to provide the edited key frame. To facilitate
comprehensive evaluation, we introduce the BalanceCC benchmark dataset,
comprising 100 videos and 4 target prompts for each video. Our extensive user
studies compare CCEdit with eight state-of-the-art video editing methods. The
outcomes demonstrate CCEdit's substantial superiority over all other methods.",CVPR
"This paper introduces CN-RMA, a novel approach for 3D indoor object detection
from multi-view images. We observe the key challenge as the ambiguity of image
and 3D correspondence without explicit geometry to provide occlusion
information. To address this issue, CN-RMA leverages the synergy of 3D
reconstruction networks and 3D object detection networks, where the
reconstruction network provides a rough Truncated Signed Distance Function
(TSDF) and guides image features to vote to 3D space correctly in an end-to-end
manner. Specifically, we associate weights to sampled points of each ray
through ray marching, representing the contribution of a pixel in an image to
corresponding 3D locations. Such weights are determined by the predicted signed
distances so that image features vote only to regions near the reconstructed
surface. Our method achieves state-of-the-art performance in 3D object
detection from multi-view images, as measured by mAP@0.25 and mAP@0.5 on the
ScanNet and ARKitScenes datasets. The code and models are released at
https://github.com/SerCharles/CN-RMA.",CVPR
"Vision-language foundation models have shown remarkable performance in
various zero-shot settings such as image retrieval, classification, or
captioning. But so far, those models seem to fall behind when it comes to
zero-shot localization of referential expressions and objects in images. As a
result, they need to be fine-tuned for this task. In this paper, we show that
pretrained vision-language (VL) models allow for zero-shot open-vocabulary
object localization without any fine-tuning. To leverage those capabilities, we
propose a Grounding Everything Module (GEM) that generalizes the idea of
value-value attention introduced by CLIPSurgery to a self-self attention path.
We show that the concept of self-self attention corresponds to clustering, thus
enforcing groups of tokens arising from the same object to be similar while
preserving the alignment with the language space. To further guide the group
formation, we propose a set of regularizations that allows the model to finally
generalize across datasets and backbones. We evaluate the proposed GEM
framework on various benchmark tasks and datasets for semantic segmentation. It
shows that GEM not only outperforms other training-free open-vocabulary
localization methods, but also achieves state-of-the-art results on the
recently proposed OpenImagesV7 large-scale segmentation benchmark.",CVPR
"We developed a tool for visualizing and analyzing large pre-trained vision
models by mapping them onto the brain, thus exposing their hidden inside. Our
innovation arises from a surprising usage of brain encoding: predicting brain
fMRI measurements in response to images. We report two findings. First,
explicit mapping between the brain and deep-network features across dimensions
of space, layers, scales, and channels is crucial. This mapping method,
FactorTopy, is plug-and-play for any deep-network; with it, one can paint a
picture of the network onto the brain (literally!). Second, our visualization
shows how different training methods matter: they lead to remarkable
differences in hierarchical organization and scaling behavior, growing with
more data or network capacity. It also provides insight into fine-tuning: how
pre-trained models change when adapting to small datasets. We found brain-like
hierarchically organized network suffer less from catastrophic forgetting after
fine-tuned.",CVPR
"As natural image understanding moves towards the pretrain-finetune era,
research in pathology imaging is concurrently evolving. Despite the predominant
focus on pretraining pathological foundation models, how to adapt foundation
models to downstream tasks is little explored. For downstream adaptation, we
propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the
Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework
designed to efficiently adapt pathological or even visual foundation models to
pathology-specific tasks via multi-modal prompt tuning. The proposed framework
leverages Task-specific Visual Prompts and Task-specific Textual Prompts to
identify task-relevant features, along with Instance-specific Visual Prompts
for encoding single pathological image features. Results across multiple
datasets at both patch-level and WSI-level demonstrate its superior performance
over single-modality prompt tuning approaches. Significantly, PathoTune
facilitates the direct adaptation of natural visual foundation models to
pathological tasks, drastically outperforming pathological foundation models
with simple linear probing. The code will be available upon acceptance.",CVPR
"Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.",CVPR
"This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical
agent for multi-task robotic manipulation. HDP factorises a manipulation policy
into a hierarchical structure: a high-level task-planning agent which predicts
a distant next-best end-effector pose (NBP), and a low-level goal-conditioned
diffusion policy which generates optimal motion trajectories. The factorised
policy representation allows HDP to tackle both long-horizon task planning
while generating fine-grained low-level actions. To generate context-aware
motion trajectories while satisfying robot kinematics constraints, we present a
novel kinematics-aware goal-conditioned control agent, Robot Kinematics
Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the
end-effector pose and joint position trajectories, and distill the accurate but
kinematics-unaware end-effector pose diffuser to the kinematics-aware but less
accurate joint position diffuser via differentiable kinematics. Empirically, we
show that HDP achieves a significantly higher success rate than the
state-of-the-art methods in both simulation and real-world.",CVPR
"Large-scale visual-language pre-trained models have achieved significant
success in various video tasks. However, most existing methods follow an ""adapt
then align"" paradigm, which adapts pre-trained image encoders to model
video-level representations and utilizes one-hot or text embedding of the
action labels for supervision. This paradigm overlooks the challenge of mapping
from static images to complicated activity concepts. In this paper, we propose
a novel ""Align before Adapt"" (ALT) paradigm. Prior to adapting to video
representation learning, we exploit the entity-to-region alignments for each
frame. The alignments are fulfilled by matching the region-aware image
embeddings to an offline-constructed text corpus. With the aligned entities, we
feed their text embeddings to a transformer-based video adapter as the queries,
which can help extract the semantics of the most important entities from a
video to a vector. This paradigm reuses the visual-language alignment of VLP
during adaptation and tries to explain an action by the underlying entities.
This helps understand actions by bridging the gap with complex activity
semantics, particularly when facing unfamiliar or unseen categories. ALT
demonstrates competitive performance while maintaining remarkably low
computational costs. In fully supervised experiments, it achieves 88.1% top-1
accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the
previous state-of-the-art methods in both zero-shot and few-shot experiments,
emphasizing its superior generalizability across various learning scenarios.",CVPR
"Despite advancements in text-to-image generation (T2I), prior methods often
face text-image misalignment problems such as relation confusion in generated
images. Existing solutions involve cross-attention manipulation for better
compositional understanding or integrating large language models for improved
layout planning. However, the inherent alignment capabilities of T2I models are
still inadequate. By reviewing the link between generative and discriminative
modeling, we posit that T2I models' discriminative abilities may reflect their
text-image alignment proficiency during generation. In this light, we advocate
bolstering the discriminative abilities of T2I models to achieve more precise
text-to-image alignment for generation. We present a discriminative adapter
built on T2I models to probe their discriminative abilities on two
representative tasks and leverage discriminative fine-tuning to improve their
text-image alignment. As a bonus of the discriminative adapter, a
self-correction mechanism can leverage discriminative gradients to better align
generated images to text prompts during inference. Comprehensive evaluations
across three benchmark datasets, including both in-distribution and
out-of-distribution scenarios, demonstrate our method's superior generation
performance. Meanwhile, it achieves state-of-the-art discriminative performance
on the two discriminative tasks compared to other generative models.",CVPR
"Contrastive Language-Image Pre-training (CLIP) has become a promising
language-supervised visual pre-training framework. This paper aims to distill
small CLIP models supervised by a large teacher CLIP model. We propose several
distillation strategies, including relation, feature, gradient and contrastive
paradigms, to examine the effectiveness of CLIP-Knowledge Distillation (KD). We
show that a simple feature mimicry with Mean Squared Error loss works
surprisingly well. Moreover, interactive contrastive learning across teacher
and student encoders is also effective in performance improvement. We explain
that the success of CLIP-KD can be attributed to maximizing the feature
similarity between teacher and student. The unified method is applied to
distill several student models trained on CC3M+12M. CLIP-KD improves student
CLIP models consistently over zero-shot ImageNet classification and cross-modal
retrieval benchmarks. When using ViT-L/14 pretrained on Laion-400M as the
teacher, CLIP-KD achieves 57.5\% and 55.4\% zero-shot top-1 ImageNet accuracy
over ViT-B/16 and ResNet-50, surpassing the original CLIP without KD by 20.5\%
and 20.1\% margins, respectively. Our code is released on
https://github.com/winycg/CLIP-KD.",CVPR
"The remarkable efficacy of text-to-image diffusion models has motivated
extensive exploration of their potential application in video domains.
Zero-shot methods seek to extend image diffusion models to videos without
necessitating model training. Recent methods mainly focus on incorporating
inter-frame correspondence into attention mechanisms. However, the soft
constraint imposed on determining where to attend to valid features can
sometimes be insufficient, resulting in temporal inconsistency. In this paper,
we introduce FRESCO, intra-frame correspondence alongside inter-frame
correspondence to establish a more robust spatial-temporal constraint. This
enhancement ensures a more consistent transformation of semantically similar
content across frames. Beyond mere attention guidance, our approach involves an
explicit update of features to achieve high spatial-temporal consistency with
the input video, significantly improving the visual coherence of the resulting
translated videos. Extensive experiments demonstrate the effectiveness of our
proposed framework in producing high-quality, coherent videos, marking a
notable improvement over existing zero-shot methods.",CVPR
"Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
in various multi-modal tasks. Nevertheless, their performance in fine-grained
image understanding tasks is still limited. To address this issue, this paper
proposes a new framework to enhance the fine-grained image understanding
abilities of MLLMs. Specifically, we present a new method for constructing the
instruction tuning dataset at a low cost by leveraging annotations in existing
datasets. A self-consistent bootstrapping method is also introduced to extend
existing dense object annotations into high-quality
referring-expression-bounding-box pairs. These methods enable the generation of
high-quality instruction data which includes a wide range of fundamental
abilities essential for fine-grained image perception. Moreover, we argue that
the visual encoder should be tuned during instruction tuning to mitigate the
gap between full image perception and fine-grained image perception.
Experimental results demonstrate the superior performance of our method. For
instance, our model exhibits a 5.2% accuracy improvement over Qwen-VL on GQA
and surpasses the accuracy of Kosmos-2 by 24.7% on RefCOCO_val. We have also
attained the top rank on the leaderboard of MMBench. This promising performance
is achieved by training on only publicly available data, making it easily
reproducible. The models, datasets, and codes are publicly available at
https://github.com/SY-Xuan/Pink.",CVPR
"High-definition (HD) maps have played an integral role in the development of
modern autonomous vehicle (AV) stacks, albeit with high associated labeling and
maintenance costs. As a result, many recent works have proposed methods for
estimating HD maps online from sensor data, enabling AVs to operate outside of
previously-mapped regions. However, current online map estimation approaches
are developed in isolation of their downstream tasks, complicating their
integration in AV stacks. In particular, they do not produce uncertainty or
confidence estimates. In this work, we extend multiple state-of-the-art online
map estimation methods to additionally estimate uncertainty and show how this
enables more tightly integrating online mapping with trajectory forecasting. In
doing so, we find that incorporating uncertainty yields up to 50% faster
training convergence and up to 15% better prediction performance on the
real-world nuScenes driving dataset.",CVPR
"Can we synthesize 3D humans interacting with scenes without learning from any
3D human-scene interaction data? We propose GenZI, the first zero-shot approach
to generating 3D human-scene interactions. Key to GenZI is our distillation of
interaction priors from large vision-language models (VLMs), which have learned
a rich semantic space of 2D human-scene compositions. Given a natural language
description and a coarse point location of the desired interaction in a 3D
scene, we first leverage VLMs to imagine plausible 2D human interactions
inpainted into multiple rendered views of the scene. We then formulate a robust
iterative optimization to synthesize the pose and shape of a 3D human model in
the scene, guided by consistency with the 2D interaction hypotheses. In
contrast to existing learning-based approaches, GenZI circumvents the
conventional need for captured 3D interaction data, and allows for flexible
control of the 3D interaction synthesis with easy-to-use text prompts.
Extensive experiments show that our zero-shot approach has high flexibility and
generality, making it applicable to diverse scene types, including both indoor
and outdoor environments.",CVPR
"Human pose and shape (HPS) estimation with lensless imaging is not only
beneficial to privacy protection but also can be used in covert surveillance
scenarios due to the small size and simple structure of this device. However,
this task presents significant challenges due to the inherent ambiguity of the
captured measurements and lacks effective methods for directly estimating human
pose and shape from lensless data. In this paper, we propose the first
end-to-end framework to recover 3D human poses and shapes from lensless
measurements to our knowledge. We specifically design a multi-scale lensless
feature decoder to decode the lensless measurements through the optically
encoded mask for efficient feature extraction. We also propose a double-head
auxiliary supervision mechanism to improve the estimation accuracy of human
limb ends. Besides, we establish a lensless imaging system and verify the
effectiveness of our method on various datasets acquired by our lensless
imaging system.",CVPR
"We study visually grounded VideoQA in response to the emerging trends of
utilizing pretraining techniques for video-language understanding.
Specifically, by forcing vision-language models (VLMs) to answer questions and
simultaneously provide visual evidence, we seek to ascertain the extent to
which the predictions of such techniques are genuinely anchored in relevant
video content, versus spurious correlations from language or irrelevant visual
context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with
10.5$K$ temporal grounding (or location) labels tied to the original QA pairs.
With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through
post-hoc attention analysis, we find that these models are extremely weak in
substantiating the answers despite their strong QA performance. This exposes
the limitation of current VLMs in making reliable predictions. As a remedy, we
further explore and propose a grounded-QA method via Gaussian mask optimization
and cross-modal learning. Experiments with different backbones demonstrate that
this grounding mechanism improves both grounding and QA. With these efforts, we
aim to push towards trustworthy VLMs in VQA systems. Our dataset and code are
available at https://github.com/doc-doc/NExT-GQA.",CVPR
"This paper introduces a versatile paradigm for integrating multi-view
reflectance (optional) and normal maps acquired through photometric stereo. Our
approach employs a pixel-wise joint re-parameterization of reflectance and
normal, considering them as a vector of radiances rendered under simulated,
varying illumination. This re-parameterization enables the seamless integration
of reflectance and normal maps as input data in neural volume rendering-based
3D reconstruction while preserving a single optimization objective. In
contrast, recent multi-view photometric stereo (MVPS) methods depend on
multiple, potentially conflicting objectives. Despite its apparent simplicity,
our proposed approach outperforms state-of-the-art approaches in MVPS
benchmarks across F-score, Chamfer distance, and mean angular error metrics.
Notably, it significantly improves the detailed 3D reconstruction of areas with
high curvature or low visibility.",CVPR
"Predicting future human pose is a fundamental application for machine
intelligence, which drives robots to plan their behavior and paths ahead of
time to seamlessly accomplish human-robot collaboration in real-world 3D
scenarios. Despite encouraging results, existing approaches rarely consider the
effects of the external scene on the motion sequence, leading to pronounced
artifacts and physical implausibilities in the predictions. To address this
limitation, this work introduces a novel multi-modal sense-informed motion
prediction approach, which conditions high-fidelity generation on two modal
information: external 3D scene, and internal human gaze, and is able to
recognize their salience for future human activity. Furthermore, the gaze
information is regarded as the human intention, and combined with both motion
and scene features, we construct a ternary intention-aware attention to
supervise the generation to match where the human wants to reach. Meanwhile, we
introduce semantic coherence-aware attention to explicitly distinguish the
salient point clouds and the underlying ones, to ensure a reasonable
interaction of the generated sequence with the 3D scene. On two real-world
benchmarks, the proposed method achieves state-of-the-art performance both in
3D human pose and trajectory prediction.",CVPR
"Learning commonsense reasoning from visual contexts and scenes in real-world
is a crucial step toward advanced artificial intelligence. However, existing
video reasoning benchmarks are still inadequate since they were mainly designed
for factual or situated reasoning and rarely involve broader knowledge in the
real world. Our work aims to delve deeper into reasoning evaluations,
specifically within dynamic, open-world, and structured context knowledge. We
propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K
situations with instance-level annotations depicted in the videos. The
reasoning process is required to understand and apply situated knowledge and
general knowledge for problem-solving. To create such a dataset, we propose an
automatic and scalable generation method to generate question-answer pairs,
knowledge graphs, and rationales by instructing the combinations of LLMs and
MLLMs. Concretely, we first extract observable situated entities, relations,
and processes from videos for situated knowledge and then extend to open-world
knowledge beyond the visible content. The task generation is facilitated
through multiple dialogues as iterations and subsequently corrected and refined
by our designed self-promptings and demonstrations. With a corpus of both
explicit situated facts and implicit commonsense, we generate associated
question-answer pairs and reasoning processes, finally followed by manual
reviews for quality assurance. We evaluated recent mainstream large
vision-language models on the benchmark and found several insightful
conclusions. For more information, please refer to our benchmark at
www.bobbywu.com/SOKBench.",CVPR
"In this paper, we revisit techniques for uncertainty estimation within deep
neural networks and consolidate a suite of techniques to enhance their
reliability. Our investigation reveals that an integrated application of
diverse techniques--spanning model regularization, classifier and
optimization--substantially improves the accuracy of uncertainty predictions in
image classification tasks. The synergistic effect of these techniques
culminates in our novel SURE approach. We rigorously evaluate SURE against the
benchmark of failure prediction, a critical testbed for uncertainty estimation
efficacy. Our results showcase a consistently better performance than models
that individually deploy each technique, across various datasets and model
architectures. When applied to real-world challenges, such as data corruption,
label noise, and long-tailed class distribution, SURE exhibits remarkable
robustness, delivering results that are superior or on par with current
state-of-the-art specialized methods. Particularly on Animal-10N and Food-101N
for learning with noisy labels, SURE achieves state-of-the-art performance
without any task-specific adjustments. This work not only sets a new benchmark
for robust uncertainty estimation but also paves the way for its application in
diverse, real-world scenarios where reliability is paramount. Our code is
available at \url{https://yutingli0606.github.io/SURE/}.",CVPR
"In this paper, we present ShapeMatcher, a unified self-supervised learning
framework for joint shape canonicalization, segmentation, retrieval and
deformation. Given a partially-observed object in an arbitrary pose, we first
canonicalize the object by extracting point-wise affine-invariant features,
disentangling inherent structure of the object with its pose and size. These
learned features are then leveraged to predict semantically consistent part
segmentation and corresponding part centers. Next, our lightweight retrieval
module aggregates the features within each part as its retrieval token and
compare all the tokens with source shapes from a pre-established database to
identify the most geometrically similar shape. Finally, we deform the retrieved
shape in the deformation module to tightly fit the input object by harnessing
part center guided neural cage deformation. The key insight of ShapeMaker is
the simultaneous training of the four highly-associated processes:
canonicalization, segmentation, retrieval, and deformation, leveraging
cross-task consistency losses for mutual supervision. Extensive experiments on
synthetic datasets PartNet, ComplementMe, and real-world dataset Scan2CAD
demonstrate that ShapeMaker surpasses competitors by a large margin.",CVPR
"DiffusionAvatars synthesizes a high-fidelity 3D head avatar of a person,
offering intuitive control over both pose and expression. We propose a
diffusion-based neural renderer that leverages generic 2D priors to produce
compelling images of faces. For coarse guidance of the expression and head
pose, we render a neural parametric head model (NPHM) from the target
viewpoint, which acts as a proxy geometry of the person. Additionally, to
enhance the modeling of intricate facial expressions, we condition
DiffusionAvatars directly on the expression codes obtained from NPHM via
cross-attention. Finally, to synthesize consistent surface details across
different viewpoints and expressions, we rig learnable spatial features to the
head's surface via TriPlane lookup in NPHM's canonical space. We train
DiffusionAvatars on RGB videos and corresponding fitted NPHM meshes of a person
and test the obtained avatars in both self-reenactment and animation scenarios.
Our experiments demonstrate that DiffusionAvatars generates temporally
consistent and visually appealing videos for novel poses and expressions of a
person, outperforming existing approaches.",CVPR
"Promptly identifying procedural errors from egocentric videos in an online
setting is highly challenging and valuable for detecting mistakes as soon as
they happen. This capability has a wide range of applications across various
fields, such as manufacturing and healthcare. The nature of procedural mistakes
is open-set since novel types of failures might occur, which calls for
one-class classifiers trained on correctly executed procedures. However, no
technique can currently detect open-set procedural mistakes online. We propose
PREGO, the first online one-class classification model for mistake detection in
PRocedural EGOcentric videos. PREGO is based on an online action recognition
component to model the current action, and a symbolic reasoning module to
predict the next actions. Mistake detection is performed by comparing the
recognized current action with the expected future one. We evaluate PREGO on
two procedural egocentric video datasets, Assembly101 and Epic-tent, which we
adapt for online benchmarking of procedural mistake detection to establish
suitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets,
respectively.",CVPR
"Test-time adaptation (TTA) aims to improve model generalizability when test
data diverges from training distribution, offering the distinct advantage of
not requiring access to training data and processes, especially valuable in the
context of large pre-trained models. However, current TTA methods fail to
address the fundamental issue: covariate shift, i.e., the decreased
generalizability can be attributed to the model's reliance on the marginal
distribution of the training data, which may impair model calibration and
introduce confirmation bias. To address this, we propose a novel energy-based
perspective, enhancing the model's perception of target data distributions
without requiring access to training data or processes. Building on this
perspective, we introduce $\textbf{T}$est-time $\textbf{E}$nergy
$\textbf{A}$daptation ($\textbf{TEA}$), which transforms the trained classifier
into an energy-based model and aligns the model's distribution with the test
data's, enhancing its ability to perceive test distributions and thus improving
overall generalizability. Extensive experiments across multiple tasks,
benchmarks and architectures demonstrate TEA's superior generalization
performance against state-of-the-art methods. Further in-depth analyses reveal
that TEA can equip the model with a comprehensive perception of test
distribution, ultimately paving the way toward improved generalization and
calibration.",CVPR
"Binary neural networks utilize 1-bit quantized weights and activations to
reduce both the model's storage demands and computational burden. However,
advanced binary architectures still incorporate millions of inefficient and
nonhardware-friendly full-precision multiplication operations. A&B BNN is
proposed to directly remove part of the multiplication operations in a
traditional BNN and replace the rest with an equal number of bit operations,
introducing the mask layer and the quantized RPReLU structure based on the
normalizer-free network architecture. The mask layer can be removed during
inference by leveraging the intrinsic characteristics of BNN with
straightforward mathematical transformations to avoid the associated
multiplication operations. The quantized RPReLU structure enables more
efficient bit operations by constraining its slope to be integer powers of 2.
Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10,
CIFAR-100, and ImageNet datasets, respectively, which are competitive with the
state-of-the-art. Ablation studies have verified the efficacy of the quantized
RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to
using a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers
an innovative approach for hardware-friendly network architecture.",CVPR
"Surveillance videos are an essential component of daily life with various
critical applications, particularly in public security. However, current
surveillance video tasks mainly focus on classifying and localizing anomalous
events. Existing methods are limited to detecting and classifying the
predefined events with unsatisfactory semantic understanding, although they
have obtained considerable performance. To address this issue, we propose a new
research direction of surveillance video-and-language understanding, and
construct the first multimodal surveillance video dataset. We manually annotate
the real-world surveillance dataset UCF-Crime with fine-grained event content
and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation), contains
23,542 sentences, with an average length of 20 words, and its annotated videos
are as long as 110.7 hours. Furthermore, we benchmark SOTA models for four
multimodal tasks on this newly created dataset, which serve as new baselines
for surveillance video-and-language understanding. Through our experiments, we
find that mainstream models used in previously publicly available datasets
perform poorly on surveillance video, which demonstrates the new challenges in
surveillance video-and-language understanding. To validate the effectiveness of
our UCA, we conducted experiments on multimodal anomaly detection. The results
demonstrate that our multimodal surveillance learning can improve the
performance of conventional anomaly detection tasks. All the experiments
highlight the necessity of constructing this dataset to advance surveillance
AI. The link to our dataset is provided at:
https://xuange923.github.io/Surveillance-Video-Understanding.",CVPR
"The widespread adoption of face recognition has led to increasing privacy
concerns, as unauthorized access to face images can expose sensitive personal
information. This paper explores face image protection against viewing and
recovery attacks. Inspired by image compression, we propose creating a visually
uninformative face image through feature subtraction between an original face
and its model-produced regeneration. Recognizable identity features within the
image are encouraged by co-training a recognition model on its high-dimensional
feature representation. To enhance privacy, the high-dimensional representation
is crafted through random channel shuffling, resulting in randomized
recognizable images devoid of attacker-leverageable texture details. We distill
our methodologies into a novel privacy-preserving face recognition method,
MinusFace. Experiments demonstrate its high recognition accuracy and effective
privacy protection. Its code is available at https://github.com/Tencent/TFace.",CVPR
"We introduce One-shot Open Affordance Learning (OOAL), where a model is
trained with just one example per base object category, but is expected to
identify novel objects and affordances. While vision-language models excel at
recognizing novel objects and scenes, they often struggle to understand finer
levels of granularity such as affordances. To handle this issue, we conduct a
comprehensive analysis of existing foundation models, to explore their inherent
understanding of affordances and assess the potential for data-limited
affordance learning. We then propose a vision-language framework with simple
and effective designs that boost the alignment between visual features and
affordance text embeddings. Experiments on two affordance segmentation
benchmarks show that the proposed method outperforms state-of-the-art models
with less than 1% of the full training data, and exhibits reasonable
generalization capability on unseen objects and affordances.",CVPR
"We propose a framework for automatic colorization that allows for iterative
editing and modifications. The core of our framework lies in an imagination
module: by understanding the content within a grayscale image, we utilize a
pre-trained image generation model to generate multiple images that contain the
same content. These images serve as references for coloring, mimicking the
process of human experts. As the synthesized images can be imperfect or
different from the original grayscale image, we propose a Reference Refinement
Module to select the optimal reference composition. Unlike most previous
end-to-end automatic colorization algorithms, our framework allows for
iterative and localized modifications of the colorization results because we
explicitly model the coloring samples. Extensive experiments demonstrate the
superiority of our framework over existing automatic colorization algorithms in
editability and flexibility. Project page:
https://xy-cong.github.io/imagine-colorization.",CVPR
"Lidars and cameras are critical sensors that provide complementary
information for 3D detection in autonomous driving. While most prevalent
methods progressively downscale the 3D point clouds and camera images and then
fuse the high-level features, the downscaled features inevitably lose low-level
detailed information. In this paper, we propose Fine-Grained Lidar-Camera
Fusion (FGFusion) that make full use of multi-scale features of image and point
cloud and fuse them in a fine-grained way. First, we design a dual pathway
hierarchy structure to extract both high-level semantic and low-level detailed
features of the image. Second, an auxiliary network is introduced to guide
point cloud features to better learn the fine-grained spatial information.
Finally, we propose multi-scale fusion (MSF) to fuse the last N feature maps of
image and point cloud. Extensive experiments on two popular autonomous driving
benchmarks, i.e. KITTI and Waymo, demonstrate the effectiveness of our method.",CVPR
"We study the underexplored but fundamental vision problem of machine
understanding of abstract freehand scene sketches. We introduce a sketch
encoder that results in semantically-aware feature space, which we evaluate by
testing its performance on a semantic sketch segmentation task. To train our
model we rely only on the availability of bitmap sketches with their brief
captions and do not require any pixel-level annotations. To obtain
generalization to a large set of sketches and categories, we build on a vision
transformer encoder pretrained with the CLIP model. We freeze the text encoder
and perform visual-prompt tuning of the visual encoder branch while introducing
a set of critical modifications. Firstly, we augment the classical key-query
(k-q) self-attention blocks with value-value (v-v) self-attention blocks.
Central to our model is a two-level hierarchical network design that enables
efficient semantic disentanglement: The first level ensures holistic scene
sketch encoding, and the second level focuses on individual categories. We,
then, in the second level of the hierarchy, introduce a cross-attention between
textual and visual branches. Our method outperforms zero-shot CLIP pixel
accuracy of segmentation results by 37 points, reaching an accuracy of $85.5\%$
on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us
to identify further improvements needed over our method to reconcile machine
and human understanding of scene sketches.",CVPR
"This paper proposes a fine-grained self-localization method for outdoor
robotics that utilizes a flexible number of onboard cameras and readily
accessible satellite images. The proposed method addresses limitations in
existing cross-view localization methods that struggle to handle noise sources
such as moving objects and seasonal variations. It is the first sparse
visual-only method that enhances perception in dynamic environments by
detecting view-consistent key points and their corresponding deep features from
ground and satellite views, while removing off-the-ground objects and
establishing homography transformation between the two views. Moreover, the
proposed method incorporates a spatial embedding approach that leverages camera
intrinsic and extrinsic information to reduce the ambiguity of purely visual
matching, leading to improved feature matching and overall pose estimation
accuracy. The method exhibits strong generalization and is robust to
environmental changes, requiring only geo-poses as ground truth. Extensive
experiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate that
our proposed method outperforms existing state-of-the-art methods, achieving
median spatial accuracy errors below $0.5$ meters along the lateral and
longitudinal directions, and a median orientation accuracy error below 2
degrees.",CVPR
"The scarcity of ground-truth labels poses one major challenge in developing
optical flow estimation models that are both generalizable and robust. While
current methods rely on data augmentation, they have yet to fully exploit the
rich information available in labeled video sequences. We propose OCAI, a
method that supports robust frame interpolation by generating intermediate
video frames alongside optical flows in between. Utilizing a forward warping
approach, OCAI employs occlusion awareness to resolve ambiguities in pixel
values and fills in missing values by leveraging the forward-backward
consistency of optical flows. Additionally, we introduce a teacher-student
style semi-supervised learning method on top of the interpolated frames. Using
a pair of unlabeled frames and the teacher model's predicted optical flow, we
generate interpolated frames and flows to train a student model. The teacher's
weights are maintained using Exponential Moving Averaging of the student. Our
evaluations demonstrate perceptually superior interpolation quality and
enhanced optical flow accuracy on established benchmarks such as Sintel and
KITTI.",CVPR
"As pretrained text-to-image diffusion models become increasingly powerful,
recent efforts have been made to distill knowledge from these text-to-image
pretrained models for optimizing a text-guided 3D model. Most of the existing
methods generate a holistic 3D model from a plain text input. This can be
problematic when the text describes a complex scene with multiple objects,
because the vectorized text embeddings are inherently unable to capture a
complex description with multiple entities and relationships. Holistic 3D
modeling of the entire scene further prevents accurate grounding of text
entities and concepts. To address this limitation, we propose GraphDreamer, a
novel framework to generate compositional 3D scenes from scene graphs, where
objects are represented as nodes and their interactions as edges. By exploiting
node and edge information in scene graphs, our method makes better use of the
pretrained text-to-image diffusion model and is able to fully disentangle
different objects without image-level supervision. To facilitate modeling of
object-wise relationships, we use signed distance fields as representation and
impose a constraint to avoid inter-penetration of objects. To avoid manual
scene graph creation, we design a text prompt for ChatGPT to generate scene
graphs based on text inputs. We conduct both qualitative and quantitative
experiments to validate the effectiveness of GraphDreamer in generating
high-fidelity compositional 3D scenes with disentangled object entities.",CVPR
"Generative Zero-shot learning (ZSL) learns a generator to synthesize visual
samples for unseen classes, which is an effective way to advance ZSL. However,
existing generative methods rely on the conditions of Gaussian noise and the
predefined semantic prototype, which limit the generator only optimized on
specific seen classes rather than characterizing each visual instance,
resulting in poor generalizations (\textit{e.g.}, overfitting to seen classes).
To address this issue, we propose a novel Visual-Augmented Dynamic Semantic
prototype method (termed VADS) to boost the generator to learn accurate
semantic-visual mapping by fully exploiting the visual-augmented knowledge into
semantic conditions. In detail, VADS consists of two modules: (1) Visual-aware
Domain Knowledge Learning module (VDKL) learns the local bias and global prior
of the visual features (referred to as domain visual knowledge), which replace
pure Gaussian noise to provide richer prior noise information; (2)
Vision-Oriented Semantic Updation module (VOSU) updates the semantic prototype
according to the visual representations of the samples. Ultimately, we
concatenate their output as a dynamic semantic prototype, which serves as the
condition of the generator. Extensive experiments demonstrate that our VADS
achieves superior CZSL and GZSL performances on three prominent datasets and
outperforms other state-of-the-art methods with averaging increases by 6.4\%,
5.9\% and 4.2\% on SUN, CUB and AWA2, respectively.",CVPR
"While image diffusion models have made significant progress in text-driven 3D
content creation, they often fail to accurately capture the intended meaning of
text prompts, especially for view information. This limitation leads to the
Janus problem, where multi-faced 3D models are generated under the guidance of
such diffusion models. In this paper, we propose a robust high-quality 3D
content generation pipeline by exploiting orthogonal-view image guidance.
First, we introduce a novel 2D diffusion model that generates an image
consisting of four orthogonal-view sub-images based on the given text prompt.
Then, the 3D content is created using this diffusion model. Notably, the
generated orthogonal-view image provides strong geometric structure priors and
thus improves 3D consistency. As a result, it effectively resolves the Janus
problem and significantly enhances the quality of 3D content creation.
Additionally, we present a 3D synthesis fusion network that can further improve
the details of the generated 3D contents. Both quantitative and qualitative
evaluations demonstrate that our method surpasses previous text-to-3D
techniques. Project page: https://efficientdreamer.github.io.",CVPR
"Image-based virtual try-on is an increasingly important task for online
shopping. It aims to synthesize images of a specific person wearing a specified
garment. Diffusion model-based approaches have recently become popular, as they
are excellent at image synthesis tasks. However, these approaches usually
employ additional image encoders and rely on the cross-attention mechanism for
texture transfer from the garment to the person image, which affects the
try-on's efficiency and fidelity. To address these issues, we propose an
Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the
fidelity of the results and introduces no additional image encoders.
Accordingly, we make contributions from two aspects. First, we propose to
concatenate the masked person and reference garment images along the spatial
dimension and utilize the resulting image as the input for the diffusion
model's denoising UNet. This enables the original self-attention layers
contained in the diffusion model to achieve efficient and accurate texture
transfer. Second, we propose a novel diffusion-based method that predicts a
precise inpainting mask based on the person and reference garment images,
further enhancing the reliability of the try-on results. In addition, we
integrate mask prediction and image synthesis into a single compact model. The
experimental results show that our approach can be applied to various try-on
tasks, e.g., garment-to-person and person-to-person try-ons, and significantly
outperforms state-of-the-art methods on popular VITON, VITON-HD databases.",CVPR
"The long-tailed distribution problem in medical image analysis reflects a
high prevalence of common conditions and a low prevalence of rare ones, which
poses a significant challenge in developing a unified model capable of
identifying rare or novel tumor categories not encountered during training. In
this paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT)
based on query-disentangling and self-prompting to segment unseen tumor
categories beyond the training set. ZePT disentangles the object queries into
two subsets and trains them in two stages. Initially, it learns a set of
fundamental queries for organ segmentation through an object-aware feature
grouping strategy, which gathers organ-level visual features. Subsequently, it
refines the other set of advanced queries that focus on the auto-generated
visual prompts for unseen tumor segmentation. Moreover, we introduce
query-knowledge alignment at the feature level to enhance each query's
discriminative representation and generalizability. Extensive experiments on
various tumor segmentation tasks demonstrate the performance superiority of
ZePT, which surpasses the previous counterparts and evidence the promising
ability for zero-shot tumor segmentation in real-world settings.",CVPR
"While large language models (LLMs) excel in a simulated world of texts, they
struggle to interact with the more realistic world without perceptions of other
modalities such as visual or audio signals. Although vision-language models
(VLMs) integrate LLM modules (1) aligned with static image features, and (2)
may possess prior knowledge of world dynamics (as demonstrated in the text
world), they have not been trained in an embodied visual world and thus cannot
align with its dynamics. On the other hand, training an embodied agent in a
noisy visual world without expert guidance is often challenging and
inefficient. In this paper, we train a VLM agent living in a visual world using
an LLM agent excelling in a parallel text world (but inapplicable to the visual
world). Specifically, we distill LLM's reflection outcomes (improved actions by
analyzing mistakes) in a text world's tasks to finetune the VLM on the same
tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA)
quickly adapting to the visual world dynamics. Such cross-modality imitation
learning between the two parallel worlds enables EMMA to generalize to a broad
scope of new tasks without any further guidance from the LLM expert. Extensive
evaluations on the ALFWorld benchmark highlight EMMA's superior performance to
SOTA VLM-based agents across diverse tasks, e.g., 20%-70% improvement in the
success rate.",CVPR
"Recently, 3D Gaussian Splatting has demonstrated impressive novel view
synthesis results, reaching high fidelity and efficiency. However, strong
artifacts can be observed when changing the sampling rate, \eg, by changing
focal length or camera distance. We find that the source for this phenomenon
can be attributed to the lack of 3D frequency constraints and the usage of a 2D
dilation filter. To address this problem, we introduce a 3D smoothing filter
which constrains the size of the 3D Gaussian primitives based on the maximal
sampling frequency induced by the input views, eliminating high-frequency
artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip
filter, which simulates a 2D box filter, effectively mitigates aliasing and
dilation issues. Our evaluation, including scenarios such a training on
single-scale images and testing on multiple scales, validates the effectiveness
of our approach.",CVPR
"Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in
image synthesis, which aims to represent an image with a discrete token
sequence. Existing studies effectively address this problem by learning a
discrete codebook from scratch and in a code-independent manner to quantize
continuous representations into discrete tokens. However, learning a codebook
from scratch and in a code-independent manner is highly challenging, which may
be a key reason causing codebook collapse, i.e., some code vectors can rarely
be optimized without regard to the relationship between codes and good codebook
priors such that die off finally. In this paper, inspired by pretrained
language models, we find that these language models have actually pretrained a
superior codebook via a large number of text corpus, but such information is
rarely exploited in VQIM. To this end, we propose a novel codebook transfer
framework with part-of-speech, called VQCT, which aims to transfer a
well-trained codebook from pretrained language models to VQIM for robust
codebook learning. Specifically, we first introduce a pretrained codebook from
language models and part-of-speech knowledge as priors. Then, we construct a
vision-related codebook with these priors for achieving codebook transfer.
Finally, a novel codebook transfer network is designed to exploit abundant
semantic relationships between codes contained in pretrained codebooks for
robust VQIM codebook learning. Experimental results on four datasets show that
our VQCT method achieves superior VQIM performance over previous
state-of-the-art methods.",CVPR
"We estimate the radiance field of large-scale dynamic areas from multiple
vehicle captures under varying environmental conditions. Previous works in this
domain are either restricted to static environments, do not scale to more than
a single short video, or struggle to separately represent dynamic object
instances. To this end, we present a novel, decomposable radiance field
approach for dynamic urban environments. We propose a multi-level neural scene
graph representation that scales to thousands of images from dozens of
sequences with hundreds of fast-moving objects. To enable efficient training
and rendering of our representation, we develop a fast composite ray sampling
and rendering scheme. To test our approach in urban driving scenarios, we
introduce a new, novel view synthesis benchmark. We show that our approach
outperforms prior art by a significant margin on both established and our
proposed benchmark while being faster in training and rendering.",CVPR
"Multi-view representation learning aims to derive robust representations that
are both view-consistent and view-specific from diverse data sources. This
paper presents an in-depth analysis of existing approaches in this domain,
highlighting a commonly overlooked aspect: the redundancy between
view-consistent and view-specific representations. To this end, we propose an
innovative framework for multi-view representation learning, which incorporates
a technique we term 'distilled disentangling'. Our method introduces the
concept of masked cross-view prediction, enabling the extraction of compact,
high-quality view-consistent representations from various sources without
incurring extra computational overhead. Additionally, we develop a distilled
disentangling module that efficiently filters out consistency-related
information from multi-view representations, resulting in purer view-specific
representations. This approach significantly reduces redundancy between
view-consistent and view-specific representations, enhancing the overall
efficiency of the learning process. Our empirical evaluations reveal that
higher mask ratios substantially improve the quality of view-consistent
representations. Moreover, we find that reducing the dimensionality of
view-consistent representations relative to that of view-specific
representations further refines the quality of the combined representations.
Our code is accessible at: https://github.com/Guanzhou-Ke/MRDD.",CVPR
"Absolute Pose Regressors (APRs) directly estimate camera poses from monocular
images, but their accuracy is unstable for different queries. Uncertainty-aware
APRs provide uncertainty information on the estimated pose, alleviating the
impact of these unreliable predictions. However, existing uncertainty modelling
techniques are often coupled with a specific APR architecture, resulting in
suboptimal performance compared to state-of-the-art (SOTA) APR methods. This
work introduces a novel APR-agnostic framework, HR-APR, that formulates
uncertainty estimation as cosine similarity estimation between the query and
database features. It does not rely on or affect APR network architecture,
which is flexible and computationally efficient. In addition, we take advantage
of the uncertainty for pose refinement to enhance the performance of APR. The
extensive experiments demonstrate the effectiveness of our framework, reducing
27.4\% and 15.2\% of computational overhead on the 7Scenes and Cambridge
Landmarks datasets while maintaining the SOTA accuracy in single-image APRs.",CVPR
"Scene text recognition (STR) in the wild frequently encounters challenges
when coping with domain variations, font diversity, shape deformations, etc. A
straightforward solution is performing model fine-tuning tailored to a specific
scenario, but it is computationally intensive and requires multiple model
copies for various scenarios. Recent studies indicate that large language
models (LLMs) can learn from a few demonstration examples in a training-free
manner, termed ""In-Context Learning"" (ICL). Nevertheless, applying LLMs as a
text recognizer is unacceptably resource-consuming. Moreover, our pilot
experiments on LLMs show that ICL fails in STR, mainly attributed to the
insufficient incorporation of contextual information from diverse samples in
the training stage. To this end, we introduce E$^2$STR, a STR model trained
with context-rich scene text sequences, where the sequences are generated via
our proposed in-context training strategy. E$^2$STR demonstrates that a
regular-sized model is sufficient to achieve effective ICL capabilities in STR.
Extensive experiments show that E$^2$STR exhibits remarkable training-free
adaptation in various scenarios and outperforms even the fine-tuned
state-of-the-art approaches on public benchmarks. The code is released at
https://github.com/bytedance/E2STR .",CVPR
"Recently, there has been a surge in face personalization techniques,
benefiting from the advanced capabilities of pretrained text-to-image diffusion
models. Among these, a notable method is Textual Inversion, which generates
personalized images by inverting given images into textual embeddings. However,
methods based on Textual Inversion still struggle with balancing the trade-off
between reconstruction quality and editability. In this study, we examine this
issue through the lens of initialization. Upon closely examining traditional
initialization methods, we identified a significant disparity between the
initial and learned embeddings in terms of both scale and orientation. The
scale of the learned embedding can be up to 100 times greater than that of the
initial embedding. Such a significant change in the embedding could increase
the risk of overfitting, thereby compromising the editability. Driven by this
observation, we introduce a novel initialization method, termed Cross
Initialization, that significantly narrows the gap between the initial and
learned embeddings. This method not only improves both reconstruction and
editability but also reduces the optimization steps from 5000 to 320.
Furthermore, we apply a regularization term to keep the learned embedding close
to the initial embedding. We show that when combined with Cross Initialization,
this regularization term can effectively improve editability. We provide
comprehensive empirical evidence to demonstrate the superior performance of our
method compared to the baseline methods. Notably, in our experiments, Cross
Initialization is the only method that successfully edits an individual's
facial expression. Additionally, a fast version of our method allows for
capturing an input image in roughly 26 seconds, while surpassing the baseline
methods in terms of both reconstruction and editability. Code will be made
publicly available.",CVPR
"Recent text-to-image diffusion models have reached an unprecedented level in
generating high-quality images. However, their exclusive reliance on textual
prompts often falls short in precise control of image compositions. In this
paper, we propose LoCo, a training-free approach for layout-to-image Synthesis
that excels in producing high-quality images aligned with both textual prompts
and layout instructions. Specifically, we introduce a Localized Attention
Constraint (LAC), leveraging semantic affinity between pixels in self-attention
maps to create precise representations of desired objects and effectively
ensure the accurate placement of objects in designated regions. We further
propose a Padding Token Constraint (PTC) to leverage the semantic information
embedded in previously neglected padding tokens, improving the consistency
between object appearance and layout instructions. LoCo seamlessly integrates
into existing text-to-image and layout-to-image models, enhancing their
performance in spatial control and addressing semantic failures observed in
prior methods. Extensive experiments showcase the superiority of our approach,
surpassing existing state-of-the-art training-free layout-to-image methods both
qualitatively and quantitatively across multiple benchmarks.",CVPR
"Current instruction-based editing methods, such as InstructPix2Pix, often
fail to produce satisfactory results in complex scenarios due to their
dependence on the simple CLIP text encoder in diffusion models. To rectify
this, this paper introduces SmartEdit, a novel approach to instruction-based
image editing that leverages Multimodal Large Language Models (MLLMs) to
enhance their understanding and reasoning capabilities. However, direct
integration of these elements still faces challenges in situations requiring
complex reasoning. To mitigate this, we propose a Bidirectional Interaction
Module that enables comprehensive bidirectional information interactions
between the input image and the MLLM output. During training, we initially
incorporate perception data to boost the perception and understanding
capabilities of diffusion models. Subsequently, we demonstrate that a small
amount of complex instruction editing data can effectively stimulate
SmartEdit's editing capabilities for more complex instructions. We further
construct a new evaluation dataset, Reason-Edit, specifically tailored for
complex instruction-based image editing. Both quantitative and qualitative
results on this evaluation dataset indicate that our SmartEdit surpasses
previous methods, paving the way for the practical application of complex
instruction-based image editing.",CVPR
"Group fairness for Graph Neural Networks (GNNs), which emphasizes algorithmic
decisions neither favoring nor harming certain groups defined by sensitive
attributes (e.g., race and gender), has gained considerable attention. In
particular, the objective of group fairness is to ensure that the decisions
made by GNNs are independent of the sensitive attribute. To achieve this
objective, most existing approaches involve eliminating sensitive attribute
information in node representations or algorithmic decisions. However, such
ways may also eliminate task-related information due to its inherent
correlation with the sensitive attribute, leading to a sacrifice in utility. In
this work, we focus on improving the fairness of GNNs while preserving
task-related information and propose a fair GNN framework named FairSAD.
Instead of eliminating sensitive attribute information, FairSAD enhances the
fairness of GNNs via Sensitive Attribute Disentanglement (SAD), which separates
the sensitive attribute-related information into an independent component to
mitigate its impact. Additionally, FairSAD utilizes a channel masking mechanism
to adaptively identify the sensitive attribute-related component and
subsequently decorrelates it. Overall, FairSAD minimizes the impact of the
sensitive attribute on GNN outcomes rather than eliminating sensitive
attributes, thereby preserving task-related information associated with the
sensitive attribute. Furthermore, experiments conducted on several real-world
datasets demonstrate that FairSAD outperforms other state-of-the-art methods by
a significant margin in terms of both fairness and utility performance. Our
source code is available at https://github.com/ZzoomD/FairSAD.",CVPR
"In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that
empowers the Segment Anything Model (SAM) to utilize annotated reference images
as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM
can utilize annotated reference images to comprehend specific objects and
perform segmentation of specific objects in target image. It is note that the
VRP encoder can support a variety of annotation formats for reference images,
including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}.
VRP-SAM achieves a breakthrough within the SAM framework by extending its
versatility and applicability while preserving SAM's inherent strengths, thus
enhancing user-friendliness. To enhance the generalization ability of VRP-SAM,
the VRP encoder adopts a meta-learning strategy. To validate the effectiveness
of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO
datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual
reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM
demonstrates strong generalization capabilities, allowing it to perform
segmentation of unseen objects and enabling cross-domain segmentation. The
source code and models will be available at
\url{https://github.com/syp2ysy/VRP-SAM}",CVPR
"The task of online mapping is to predict a local map using current sensor
observations, e.g. from lidar and camera, without relying on a pre-built map.
State-of-the-art methods are based on supervised learning and are trained
predominantly using two datasets: nuScenes and Argoverse 2. However, these
datasets revisit the same geographic locations across training, validation, and
test sets. Specifically, over $80$% of nuScenes and $40$% of Argoverse 2
validation and test samples are less than $5$ m from a training sample. At test
time, the methods are thus evaluated more on how well they localize within a
memorized implicit map built from the training data than on extrapolating to
unseen locations. Naturally, this data leakage causes inflated performance
numbers and we propose geographically disjoint data splits to reveal the true
performance in unseen environments. Experimental results show that methods
perform considerably worse, some dropping more than $45$ mAP, when trained and
evaluated on proper data splits. Additionally, a reassessment of prior design
choices reveals diverging conclusions from those based on the original split.
Notably, the impact of lifting methods and the support from auxiliary tasks
(e.g., depth supervision) on performance appears less substantial or follows a
different trajectory than previously perceived. Splits can be found at
https://github.com/LiljaAdam/geographical-splits",CVPR
"Reconstructing outdoor 3D scenes from temporal observations is a challenge
that recent work on neural fields has offered a new avenue for. However,
existing methods that recover scene properties, such as geometry, appearance,
or radiance, solely from RGB captures often fail when handling poorly-lit or
texture-deficient regions. Similarly, recovering scenes with scanning LiDAR
sensors is also difficult due to their low angular sampling rate which makes
recovering expansive real-world scenes difficult. Tackling these gaps, we
introduce Gated Fields - a neural scene reconstruction method that utilizes
active gated video sequences. To this end, we propose a neural rendering
approach that seamlessly incorporates time-gated capture and illumination. Our
method exploits the intrinsic depth cues in the gated videos, achieving precise
and dense geometry reconstruction irrespective of ambient illumination
conditions. We validate the method across day and night scenarios and find that
Gated Fields compares favorably to RGB and LiDAR reconstruction methods. Our
code and datasets are available at https://light.princeton.edu/gatedfields/.",CVPR
"Rigging and skinning clothed human avatars is a challenging task and
traditionally requires a lot of manual work and expertise. Recent methods
addressing it either generalize across different characters or focus on
capturing the dynamics of a single character observed under different pose
configurations. However, the former methods typically predict solely static
skinning weights, which perform poorly for highly articulated poses, and the
latter ones either require dense 3D character scans in different poses or
cannot generate an explicit mesh with vertex correspondence over time. To
address these challenges, we propose a fully automated approach for creating a
fully rigged character with pose-dependent skinning weights, which can be
solely learned from multi-view video. Therefore, we first acquire a rigged
template, which is then statically skinned. Next, a coordinate-based MLP learns
a skinning weights field parameterized over the position in a canonical pose
space and the respective pose. Moreover, we introduce our pose- and
view-dependent appearance field allowing us to differentiably render and
supervise the posed mesh using multi-view imagery. We show that our approach
outperforms state-of-the-art while not relying on dense 4D scans.",CVPR
"Recently, leveraging large language models (LLMs) or multimodal large
language models (MLLMs) for document understanding has been proven very
promising. However, previous works that employ LLMs/MLLMs for document
understanding have not fully explored and utilized the document layout
information, which is vital for precise document understanding. In this paper,
we propose LayoutLLM, an LLM/MLLM based method for document understanding. The
core of LayoutLLM is a layout instruction tuning strategy, which is specially
designed to enhance the comprehension and utilization of document layouts. The
proposed layout instruction tuning strategy consists of two components:
Layout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture
the characteristics of document layout in Layout-aware Pre-training, three
groups of pre-training tasks, corresponding to document-level, region-level and
segment-level information, are introduced. Furthermore, a novel module called
layout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on
regions relevant to the question and generate accurate answers. LayoutCoT is
effective for boosting the performance of document understanding. Meanwhile, it
brings a certain degree of interpretability, which could facilitate manual
inspection and correction. Experiments on standard benchmarks show that the
proposed LayoutLLM significantly outperforms existing methods that adopt
open-source 7B LLMs/MLLMs for document understanding. The training data of the
LayoutLLM is publicly available at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM",CVPR
"Choreographers determine what the dances look like, while cameramen determine
the final presentation of dances. Recently, various methods and datasets have
showcased the feasibility of dance synthesis. However, camera movement
synthesis with music and dance remains an unsolved challenging problem due to
the scarcity of paired data. Thus, we present DCM, a new multi-modal 3D
dataset, which for the first time combines camera movement with dance motion
and music audio. This dataset encompasses 108 dance sequences (3.2 hours) of
paired dance-camera-music data from the anime community, covering 4 music
genres. With this dataset, we uncover that dance camera movement is
multifaceted and human-centric, and possesses multiple influencing factors,
making dance camera synthesis a more challenging task compared to camera or
dance synthesis alone. To overcome these difficulties, we propose
DanceCamera3D, a transformer-based diffusion model that incorporates a novel
body attention loss and a condition separation strategy. For evaluation, we
devise new metrics measuring camera movement quality, diversity, and dancer
fidelity. Utilizing these metrics, we conduct extensive experiments on our DCM
dataset, providing both quantitative and qualitative evidence showcasing the
effectiveness of our DanceCamera3D model. Code and video demos are available at
https://github.com/Carmenw1203/DanceCamera3D-Official.",CVPR
"Pre-trained Vision Language Models (VLMs) have demonstrated notable progress
in various zero-shot tasks, such as classification and retrieval. Despite their
performance, because improving performance on new tasks requires task-specific
knowledge, their adaptation is essential. While labels are needed for the
adaptation, acquiring them is typically expensive. To overcome this challenge,
active learning, a method of achieving a high performance by obtaining labels
for a small number of samples from experts, has been studied. Active learning
primarily focuses on selecting unlabeled samples for labeling and leveraging
them to train models. In this study, we pose the question, ""how can the
pre-trained VLMs be adapted under the active learning framework?"" In response
to this inquiry, we observe that (1) simply applying a conventional active
learning framework to pre-trained VLMs even may degrade performance compared to
random selection because of the class imbalance in labeling candidates, and (2)
the knowledge of VLMs can provide hints for achieving the balance before
labeling. Based on these observations, we devise a novel active learning
framework for VLMs, denoted as PCB. To assess the effectiveness of our
approach, we conduct experiments on seven different real-world datasets, and
the results demonstrate that PCB surpasses conventional active learning and
random sampling methods. Code will be available in
https://github.com/kaist-dmlab/pcb .",CVPR
"Large foundation models, known for their strong zero-shot generalization,
have excelled in visual and language applications. However, applying them to
medical image segmentation, a domain with diverse imaging types and target
labels, remains an open challenge. Current approaches, such as adapting
interactive segmentation models like Segment Anything Model (SAM), require user
prompts for each sample during inference. Alternatively, transfer learning
methods like few/one-shot models demand labeled samples, leading to high costs.
This paper introduces a new paradigm toward the universal medical image
segmentation, termed 'One-Prompt Segmentation.' One-Prompt Segmentation
combines the strengths of one-shot and interactive methods. In the inference
stage, with just \textbf{one prompted sample}, it can adeptly handle the unseen
task in a single forward pass. We train One-Prompt Model on 64 open-source
medical datasets, accompanied by the collection of over 3,000 clinician-labeled
prompts. Tested on 14 previously unseen tasks, the One-Prompt Model showcases
superior zero-shot segmentation capabilities, outperforming a wide range of
related methods. The code and annotated data will be publicly released.",CVPR
"We present an approach that can reconstruct hands in 3D from monocular input.
Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based
architecture and can analyze hands with significantly increased accuracy and
robustness compared to previous work. The key to HaMeR's success lies in
scaling up both the data used for training and the capacity of the deep network
for hand reconstruction. For training data, we combine multiple datasets that
contain 2D or 3D hand annotations. For the deep model, we use a large scale
Vision Transformer architecture. Our final model consistently outperforms the
previous baselines on popular 3D hand pose benchmarks. To further evaluate the
effect of our design in non-controlled settings, we annotate existing
in-the-wild datasets with 2D hand keypoint annotations. On this newly collected
dataset of annotations, HInt, we demonstrate significant improvements over
existing baselines. We make our code, data and models available on the project
website: https://geopavlakos.github.io/hamer/.",CVPR
"The robust generalization of models to rare, in-distribution (ID) samples
drawn from the long tail of the training distribution and to
out-of-training-distribution (OOD) samples is one of the major challenges of
current deep learning methods. For image classification, this manifests in the
existence of adversarial attacks, the performance drops on distorted images,
and a lack of generalization to concepts such as sketches. The current
understanding of generalization in neural networks is very limited, but some
biases that differentiate models from human vision have been identified and
might be causing these limitations. Consequently, several attempts with varying
success have been made to reduce these biases during training to improve
generalization. We take a step back and sanity-check these attempts. Fixing the
architecture to the well-established ResNet-50, we perform a large-scale study
on 48 ImageNet models obtained via different training methods to understand how
and if these biases - including shape bias, spectral biases, and critical bands
- interact with generalization. Our extensive study results reveal that
contrary to previous findings, these biases are insufficient to accurately
predict the generalization of a model holistically. We provide access to all
checkpoints and evaluation code at
https://github.com/paulgavrikov/biases_vs_generalization",CVPR
"Understanding data visualizations like charts and plots requires reasoning
about both visual elements and numerics. Although strong in extractive
questions, current chart visual question answering (chart VQA) models suffer on
complex reasoning questions. In this work, we address the lack of reasoning
ability by data augmentation. We leverage Large Language Models (LLMs), which
have shown to have strong reasoning ability, as an automatic data annotator
that generates question-answer annotations for chart images. The key innovation
in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data
generator learns to decompose the complex question into step-by-step
sub-questions (rationales), which are then used to derive the final answer
using external tools, i.e. Python. This step-wise generation procedure is
trained on synthetic data generated using a template-based QA generation
pipeline. Experimental results highlight the significance of the proposed
step-by-step generation. By training with the LLM-augmented data (LAMENDA), we
significantly enhance the chart VQA models, achieving the state-of-the-art
accuracy on the ChartQA and PlotQA datasets. In particular, our approach
improves the accuracy of the previous state-of-the-art approach from 38% to 54%
on the human-written questions in the ChartQA dataset, which needs strong
reasoning. We hope our work underscores the potential of synthetic data and
encourages further exploration of data augmentation using LLMs for
reasoning-heavy tasks.",CVPR
"Neural Radiance Fields (NeRF) revolutionize the realm of visual media by
providing photorealistic Free-Viewpoint Video (FVV) experiences, offering
viewers unparalleled immersion and interactivity. However, the technology's
significant storage requirements and the computational complexity involved in
generation and rendering currently limit its broader application. To close this
gap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel
technology that significantly reduces the storage size for Free-Viewpoint Video
(FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a
hybrid representation with tri-planes and voxel grids to support scaling up to
long-duration sequences and scenes with complex motions or rapid changes. We
propose a group training scheme tailored to achieving high training efficiency
and yielding temporally consistent, low-entropy scene representations.
Leveraging these properties of the representations, we introduce a compression
pipeline with off-the-shelf video codecs, achieving an order of magnitude less
storage size compared to the state-of-the-art. Our experiments demonstrate that
TeTriRF can achieve competitive quality with a higher compression rate.",CVPR
"We introduce Infinigen, a procedural generator of photorealistic 3D scenes of
the natural world. Infinigen is entirely procedural: every asset, from shape to
texture, is generated from scratch via randomized mathematical rules, using no
external source and allowing infinite variation and composition. Infinigen
offers broad coverage of objects and scenes in the natural world including
plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and
snow. Infinigen can be used to generate unlimited, diverse training data for a
wide range of computer vision tasks including object detection, semantic
segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a
useful resource for computer vision research and beyond. Please visit
https://infinigen.org for videos, code and pre-generated data.",CVPR
"Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.",CVPR
"Automatic web navigation aims to build a web agent that can follow language
instructions to execute complex and diverse tasks on real-world websites.
Existing work primarily takes HTML documents as input, which define the
contents and action spaces (i.e., actionable elements and operations) of
webpages. Nevertheless, HTML documents may not provide a clear task-related
context for each element, making it hard to select the right (sequence of)
actions. In this paper, we propose to contextualize HTML elements through their
""dual views"" in webpage screenshots: each HTML element has its corresponding
bounding box and visual content in the screenshot. We build upon the insight --
web developers tend to arrange task-related elements nearby on webpages to
enhance user experiences -- and propose to contextualize each element with its
neighbor elements, using both textual and visual features. The resulting
representations of HTML elements are more informative for the agent to take
action. We validate our method on the recently released Mind2Web dataset, which
features diverse navigation domains and tasks on real-world websites. Our
method consistently outperforms the baseline in all the scenarios, including
cross-task, cross-website, and cross-domain ones.",CVPR
"Domain adaptive object detection aims to adapt detection models to domains
where annotated data is unavailable. Existing methods have been proposed to
address the domain gap using the semi-supervised student-teacher framework.
However, a fundamental issue arises from the class imbalance in the labelled
training set, which can result in inaccurate pseudo-labels. The relationship
between classes, especially where one class is a majority and the other
minority, has a large impact on class bias. We propose Class-Aware Teacher
(CAT) to address the class bias issue in the domain adaptation setting. In our
work, we approximate the class relationships with our Inter-Class Relation
module (ICRm) and exploit it to reduce the bias within the model. In this way,
we are able to apply augmentations to highly related classes, both inter- and
intra-domain, to boost the performance of minority classes while having minimal
impact on majority classes. We further reduce the bias by implementing a
class-relation weight to our classification loss. Experiments conducted on
various datasets and ablation studies show that our method is able to address
the class bias in the domain adaptation setting. On the Cityscapes to Foggy
Cityscapes dataset, we attained a 52.5 mAP, a substantial improvement over the
51.2 mAP achieved by the state-of-the-art method.",CVPR
"Human hands are highly articulated and versatile at handling objects. Jointly
estimating the 3D poses of a hand and the object it manipulates from a
monocular camera is challenging due to frequent occlusions. Thus, existing
methods often rely on intermediate 3D shape representations to increase
performance. These representations are typically explicit, such as 3D point
clouds or meshes, and thus provide information in the direct surroundings of
the intermediate hand pose estimate. To address this, we introduce HOISDF, a
Signed Distance Field (SDF) guided hand-object pose estimation network, which
jointly exploits hand and object SDFs to provide a global, implicit
representation over the complete reconstruction volume. Specifically, the role
of the SDFs is threefold: equip the visual encoder with implicit shape
information, help to encode hand-object interactions, and guide the hand and
object pose regression via SDF-based sampling and by augmenting the feature
representations. We show that HOISDF achieves state-of-the-art results on
hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available
at https://github.com/amathislab/HOISDF",CVPR
"Object State Changes (OSCs) are pivotal for video understanding. While humans
can effortlessly generalize OSC understanding from familiar to unknown objects,
current approaches are confined to a closed vocabulary. Addressing this gap, we
introduce a novel open-world formulation for the video OSC problem. The goal is
to temporally localize the three stages of an OSC -- the object's initial
state, its transitioning state, and its end state -- whether or not the object
has been observed during training. Towards this end, we develop VidOSC, a
holistic learning approach that: (1) leverages text and vision-language models
for supervisory signals to obviate manually labeling OSC training data, and (2)
abstracts fine-grained shared state representations from objects to enhance
generalization. Furthermore, we present HowToChange, the first open-world
benchmark for video OSC localization, which offers an order of magnitude
increase in the label space and annotation volume compared to the best existing
benchmark. Experimental results demonstrate the efficacy of our approach, in
both traditional closed-world and open-world scenarios.",CVPR
"Dense depth maps have been used as a key element of visual perception tasks.
There have been tremendous efforts to enhance the depth quality, ranging from
optimization-based to learning-based methods. Despite the remarkable progress
for a long time, their applicability in the real world is limited due to
systematic measurement biases such as density, sensing pattern, and scan range.
It is well-known that the biases make it difficult for these methods to achieve
their generalization. We observe that learning a joint representation for input
modalities (e.g., images and depth), which most recent methods adopt, is
sensitive to the biases. In this work, we disentangle those modalities to
mitigate the biases with prompt engineering. For this, we design a novel depth
prompt module to allow the desirable feature representation according to new
depth distributions from either sensor types or scene configurations. Our depth
prompt can be embedded into foundation models for monocular depth estimation.
Through this embedding process, our method helps the pretrained model to be
free from restraint of depth scan range and to provide absolute scale depth
maps. We demonstrate the effectiveness of our method through extensive
evaluations. Source code is publicly available at
https://github.com/JinhwiPark/DepthPrompting .",CVPR
"Chain-of-thought (CoT) prompting has been shown to empirically improve the
accuracy of large language models (LLMs) on various question answering tasks.
While understanding why CoT prompting is effective is crucial to ensuring that
this phenomenon is a consequence of desired model behavior, little work has
addressed this; nonetheless, such an understanding is a critical prerequisite
for responsible model deployment. We address this question by leveraging
gradient-based feature attribution methods which produce saliency scores that
capture the influence of input tokens on model output. Specifically, we probe
several open-source LLMs to investigate whether CoT prompting affects the
relative importances they assign to particular input tokens. Our results
indicate that while CoT prompting does not increase the magnitude of saliency
scores attributed to semantically relevant tokens in the prompt compared to
standard few-shot prompting, it increases the robustness of saliency scores to
question perturbations and variations in model output.",CVPR
"Dataset distillation has emerged as a promising approach in deep learning,
enabling efficient training with small synthetic datasets derived from larger
real ones. Particularly, distribution matching-based distillation methods
attract attention thanks to its effectiveness and low computational cost.
However, these methods face two primary limitations: the dispersed feature
distribution within the same class in synthetic datasets, reducing class
discrimination, and an exclusive focus on mean feature consistency, lacking
precision and comprehensiveness. To address these challenges, we introduce two
novel constraints: a class centralization constraint and a covariance matching
constraint. The class centralization constraint aims to enhance class
discrimination by more closely clustering samples within classes. The
covariance matching constraint seeks to achieve more accurate feature
distribution matching between real and synthetic datasets through local feature
covariance matrices, particularly beneficial when sample sizes are much smaller
than the number of features. Experiments demonstrate notable improvements with
these constraints, yielding performance boosts of up to 6.6% on CIFAR10, 2.9%
on SVHN, 2.5% on CIFAR100, and 2.5% on TinyImageNet, compared to the
state-of-the-art relevant methods. In addition, our method maintains robust
performance in cross-architecture settings, with a maximum performance drop of
1.7% on four architectures. Code is available at
https://github.com/VincenDen/IID.",CVPR
"Despite significant progress in single image-based 3D human mesh recovery,
accurately and smoothly recovering 3D human motion from a video remains
challenging. Existing video-based methods generally recover human mesh by
estimating the complex pose and shape parameters from coupled image features,
whose high complexity and low representation ability often result in
inconsistent pose motion and limited shape patterns. To alleviate this issue,
we introduce 3D pose as the intermediary and propose a Pose and Mesh
Co-Evolution network (PMCE) that decouples this task into two parts: 1)
video-based 3D human pose estimation and 2) mesh vertices regression from the
estimated 3D pose and temporal image feature. Specifically, we propose a
two-stream encoder that estimates mid-frame 3D pose and extracts a temporal
image feature from the input image sequence. In addition, we design a
co-evolution decoder that performs pose and mesh interactions with the
image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the
human body shape. Extensive experiments demonstrate that the proposed PMCE
outperforms previous state-of-the-art methods in terms of both per-frame
accuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M,
and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.",CVPR
"Neural radiance fields provide state-of-the-art view synthesis quality but
tend to be slow to render. One reason is that they make use of volume
rendering, thus requiring many samples (and model queries) per ray at render
time. Although this representation is flexible and easy to optimize, most
real-world objects can be modeled more efficiently with surfaces instead of
volumes, requiring far fewer samples per ray. This observation has spurred
considerable progress in surface representations such as signed distance
functions, but these may struggle to model semi-opaque and thin structures. We
propose a method, HybridNeRF, that leverages the strengths of both
representations by rendering most objects as surfaces while modeling the
(typically) small fraction of challenging regions volumetrically. We evaluate
HybridNeRF against the challenging Eyeful Tower dataset along with other
commonly used view synthesis datasets. When comparing to state-of-the-art
baselines, including recent rasterization-based approaches, we improve error
rates by 15-30% while achieving real-time framerates (at least 36 FPS) for
virtual-reality resolutions (2Kx2K).",CVPR
"Point cloud classification refers to the process of assigning semantic labels
or categories to individual points within a point cloud data structure. Recent
works have explored the extension of pre-trained CLIP to 3D recognition. In
this direction, CLIP-based point cloud models like PointCLIP, CLIP2Point have
become state-of-the-art methods in the few-shot setup. Although these methods
show promising performance for some classes like airplanes, desks, guitars,
etc, the performance for some classes like the cup, flower pot, sink,
nightstand, etc is still far from satisfactory. This is due to the fact that
the adapter of CLIP-based models is trained using randomly sampled N-way K-shot
data in the standard supervised learning setup. In this paper, we propose a
novel meta-episodic learning framework for CLIP-based point cloud
classification, addressing the challenges of limited training examples and
sampling unknown classes. Additionally, we introduce dynamic task sampling
within the episode based on performance memory. This sampling strategy
effectively addresses the challenge of sampling unknown classes, ensuring that
the model learns from a diverse range of classes and promotes the exploration
of underrepresented categories. By dynamically updating the performance memory,
we adaptively prioritize the sampling of classes based on their performance,
enhancing the model's ability to handle challenging and real-world scenarios.
Experiments show an average performance gain of 3-6\% on ModelNet40 and
ScanobjectNN datasets in a few-shot setup.",CVPR
"We have recently seen tremendous progress in realistic text-to-motion
generation. Yet, the existing methods often fail or produce implausible motions
with unseen text inputs, which limits the applications. In this paper, we
present OMG, a novel framework, which enables compelling motion generation from
zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the
pretrain-then-finetune paradigm into the text-to-motion generation. At the
pre-training stage, our model improves the generation ability by learning the
rich out-of-domain inherent motion traits. To this end, we scale up a large
unconditional diffusion model up to 1B parameters, so as to utilize the massive
unlabeled motion data up to over 20M motion instances. At the subsequent
fine-tuning stage, we introduce motion ControlNet, which incorporates text
prompts as conditioning information, through a trainable copy of the
pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block.
MoC block adaptively recognizes various ranges of the sub-motions with a
cross-attention mechanism and processes them separately with the
text-token-specific experts. Such a design effectively aligns the CLIP token
embeddings of text prompts to various ranges of compact and expressive motion
features. Extensive experiments demonstrate that our OMG achieves significant
improvements over the state-of-the-art methods on zero-shot text-to-motion
generation. Project page: https://tr3e.github.io/omg-page.",CVPR
"Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
clients have heterogeneous data distributions. This data heterogeneity causes
the model to forget the global knowledge acquired from previously sampled
clients after being trained on local datasets. Although the introduction of
proximal objectives in local updates helps to preserve global knowledge, it can
also hinder local learning by interfering with local objectives. To address
this problem, we propose a novel method, Federated Stabilized Orthogonal
Learning (FedSOL), which adopts an orthogonal learning strategy to balance the
two conflicting objectives. FedSOL is designed to identify gradients of local
objectives that are inherently orthogonal to directions affecting the proximal
objective. Specifically, FedSOL targets parameter regions where learning on the
local objective is minimally influenced by proximal weight perturbations. Our
experiments demonstrate that FedSOL consistently achieves state-of-the-art
performance across various scenarios.",CVPR
"State-of-the-art neural implicit surface representations have achieved
impressive results in indoor scene reconstruction by incorporating monocular
geometric priors as additional supervision. However, we have observed that
multi-view inconsistency between such priors poses a challenge for high-quality
reconstructions. In response, we present NC-SDF, a neural signed distance field
(SDF) 3D reconstruction framework with view-dependent normal compensation (NC).
Specifically, we integrate view-dependent biases in monocular normal priors
into the neural implicit representation of the scene. By adaptively learning
and correcting the biases, our NC-SDF effectively mitigates the adverse impact
of inconsistent supervision, enhancing both the global consistency and local
details in the reconstructions. To further refine the details, we introduce an
informative pixel sampling strategy to pay more attention to intricate geometry
with higher information content. Additionally, we design a hybrid geometry
modeling approach to improve the neural implicit representation. Experiments on
synthetic and real-world datasets demonstrate that NC-SDF outperforms existing
approaches in terms of reconstruction quality.",CVPR
"This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method
for better handling various downstream computer vision tasks. While
self-supervised pre-training approaches, e.g., Masked Autoencoder, have shown
success in transfer learning, task-specific sub-architectures are still
required to be appended for different downstream tasks, which cannot enjoy the
benefits of large-scale pre-training. GLID overcomes this challenge by allowing
the pre-trained generalist encoder-decoder to be fine-tuned on various vision
tasks with minimal task-specific architecture modifications. In the GLID
training scheme, pre-training pretext task and other downstream tasks are
modeled as ""query-to-answer"" problems, including the pre-training pretext task
and other downstream tasks. We pre-train a task-agnostic encoder-decoder with
query-mask pairs. During fine-tuning, GLID maintains the pre-trained
encoder-decoder and queries, only replacing the topmost linear transformation
layer with task-specific linear heads. This minimizes the pretrain-finetune
architecture inconsistency and enables the pre-trained model to better adapt to
downstream tasks. GLID achieves competitive performance on various vision
tasks, including object detection, image segmentation, pose estimation, and
depth estimation, outperforming or matching specialist models such as
Mask2Former, DETR, ViTPose, and BinsFormer.",CVPR
"Ensuring the legal usage of deep models is crucial to promoting trustable,
accountable, and responsible artificial intelligence innovation. Current
passport-based methods that obfuscate model functionality for license-to-use
and ownership verifications suffer from capacity and quality constraints, as
they require retraining the owner model for new users. They are also vulnerable
to advanced Expanded Residual Block ambiguity attacks. We propose
Steganographic Passport, which uses an invertible steganographic network to
decouple license-to-use from ownership verification by hiding the user's
identity images into the owner-side passport and recovering them from their
respective user-side passports. An irreversible and collision-resistant hash
function is used to avoid exposing the owner-side passport from the derived
user-side passports and increase the uniqueness of the model signature. To
safeguard both the passport and model's weights against advanced ambiguity
attacks, an activation-level obfuscation is proposed for the verification
branch of the owner's model. By jointly training the verification and
deployment branches, their weights become tightly coupled. The proposed method
supports agile licensing of deep models by providing a strong ownership proof
and license accountability without requiring a separate model retraining for
the admission of every new user. Experiment results show that our
Steganographic Passport outperforms other passport-based deep model protection
methods in robustness against various known attacks.",CVPR
"The success of denoising diffusion models in representing rich data
distributions over 2D raster images has prompted research on extending them to
other data representations, such as vector graphics. Unfortunately due to their
variable structure and scarcity of vector training data, directly applying
diffusion models on this domain remains a challenging problem. Using
workarounds like optimization via Score Distillation Sampling (SDS) is also
fraught with difficulty, as vector representations are non trivial to directly
optimize and tend to result in implausible geometries such as redundant or
self-intersecting shapes. NIVeL addresses these challenges by reinterpreting
the problem on an alternative, intermediate domain which preserves the
desirable properties of vector graphics -- mainly sparsity of representation
and resolution-independence. This alternative domain is based on neural
implicit fields expressed in a set of decomposable, editable layers. Based on
our experiments, NIVeL produces text-to-vector graphics results of
significantly better quality than the state-of-the-art.",CVPR
"Large multimodal models (LMMs) have evolved from large language models (LLMs)
to integrate multiple input modalities, such as visual inputs. This integration
augments the capacity of LLMs for tasks requiring visual comprehension and
reasoning. However, the extent and limitations of their enhanced abilities are
not fully understood, especially when it comes to real-world tasks. To address
this gap, we introduce GlitchBench, a novel benchmark derived from video game
quality assurance tasks, to test and evaluate the reasoning capabilities of
LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios
from video games and aims to challenge both the visual and linguistic reasoning
powers of LMMs in detecting and interpreting out-of-the-ordinary events. We
evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents
a new challenge for these models. Code and data are available at:
https://glitchbench.github.io/",CVPR
"Although Vision Transformer (ViT) has achieved significant success in
computer vision, it does not perform well in dense prediction tasks due to the
lack of inner-patch information interaction and the limited diversity of
feature scale. Most existing studies are devoted to designing vision-specific
transformers to solve the above problems, which introduce additional
pre-training costs. Therefore, we present a plain, pre-training-free, and
feature-enhanced ViT backbone with Convolutional Multi-scale feature
interaction, named ViT-CoMer, which facilitates bidirectional interaction
between CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has
the following advantages: (1) We inject spatial pyramid multi-receptive field
convolutional features into the ViT architecture, which effectively alleviates
the problems of limited local information interaction and single-feature
representation in ViT. (2) We propose a simple and efficient CNN-Transformer
bidirectional fusion interaction module that performs multi-scale fusion across
hierarchical features, which is beneficial for handling dense prediction tasks.
(3) We evaluate the performance of ViT-CoMer across various dense prediction
tasks, different frameworks, and multiple advanced pre-training. Notably, our
ViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and
62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art
methods. We hope ViT-CoMer can serve as a new backbone for dense prediction
tasks to facilitate future research. The code will be released at
https://github.com/Traffic-X/ViT-CoMer.",CVPR
"Although neural radiance fields (NeRFs) have achieved triumphs in image novel
view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS
methods employ a simple shift from image NVS methods while ignoring the dynamic
nature and the large-scale reconstruction problem of LiDAR point clouds. In
light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for
novel space-time LiDAR view synthesis. In consideration of the sparsity and
large-scale characteristics, we design a 4D hybrid representation combined with
multi-planar and grid features to achieve effective reconstruction in a
coarse-to-fine manner. Furthermore, we introduce geometric constraints derived
from point clouds to improve temporal consistency. For the realistic synthesis
of LiDAR point clouds, we incorporate the global optimization of ray-drop
probability to preserve cross-region patterns. Extensive experiments on
KITTI-360 and NuScenes datasets demonstrate the superiority of our method in
accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes
are available at https://github.com/ispc-lab/LiDAR4D.",CVPR
"Test-time adaptation (TTA) has emerged as a viable solution to adapt
pre-trained models to domain shifts using unlabeled test data. However, TTA
faces challenges of adaptation failures due to its reliance on blind adaptation
to unknown test samples in dynamic scenarios. Traditional methods for
out-of-distribution performance estimation are limited by unrealistic
assumptions in the TTA context, such as requiring labeled data or re-training
models. To address this issue, we propose AETTA, a label-free accuracy
estimation algorithm for TTA. We propose the prediction disagreement as the
accuracy estimate, calculated by comparing the target model prediction with
dropout inferences. We then improve the prediction disagreement to extend the
applicability of AETTA under adaptation failures. Our extensive evaluation with
four baselines and six TTA methods demonstrates that AETTA shows an average of
19.8%p more accurate estimation compared with the baselines. We further
demonstrate the effectiveness of accuracy estimation with a model recovery case
study, showcasing the practicality of our model recovery based on accuracy
estimation. The source code is available at https://github.com/taeckyung/AETTA.",CVPR
"Dataset distillation is the technique of synthesizing smaller condensed
datasets from large original datasets while retaining necessary information to
persist the effect. In this paper, we approach the dataset distillation problem
from a novel perspective: we regard minimizing the prediction discrepancy on
the real data distribution between models, which are respectively trained on
the large original dataset and on the small distilled dataset, as a conduit for
condensing information from the raw data into the distilled version. An
adversarial framework is proposed to solve the problem efficiently. In contrast
to existing distillation methods involving nested optimization or long-range
gradient unrolling, our approach hinges on single-level optimization. This
ensures the memory efficiency of our method and provides a flexible tradeoff
between time and memory budgets, allowing us to distil ImageNet-1K using a
minimum of only 6.5GB of GPU memory. Under the optimal tradeoff strategy, it
requires only 2.5$\times$ less memory and 5$\times$ less runtime compared to
the state-of-the-art. Empirically, our method can produce synthetic datasets
just 10% the size of the original, yet achieve, on average, 94% of the test
accuracy of models trained on the full original datasets including ImageNet-1K,
significantly surpassing state-of-the-art. Additionally, extensive tests reveal
that our distilled datasets excel in cross-architecture generalization
capabilities.",CVPR
"Real-time rendering of photorealistic and controllable human avatars stands
as a cornerstone in Computer Vision and Graphics. While recent advances in
neural implicit rendering have unlocked unprecedented photorealism for digital
avatars, real-time performance has mostly been demonstrated for static scenes
only. To address this, we propose ASH, an animatable Gaussian splatting
approach for photorealistic rendering of dynamic humans in real-time. We
parameterize the clothed human as animatable 3D Gaussians, which can be
efficiently splatted into image space to generate the final rendering. However,
naively learning the Gaussian parameters in 3D space poses a severe challenge
in terms of compute. Instead, we attach the Gaussians onto a deformable
character model, and learn their parameters in 2D texture space, which allows
leveraging efficient 2D convolutional architectures that easily scale with the
required number of Gaussians. We benchmark ASH with competing methods on
pose-controllable avatars, demonstrating that our method outperforms existing
real-time methods by a large margin and shows comparable or even better results
than offline methods.",CVPR
"In recent years, there has been a significant shift in the field of digital
avatar research, towards modeling, animating and reconstructing clothed human
representations, as a key step towards creating realistic avatars. However,
current 3D cloth generation methods are garment specific or trained completely
on synthetic data, hence lacking fine details and realism. In this work, we
make a step towards automatic realistic garment design and propose
Design2Cloth, a high fidelity 3D generative model trained on a real world
dataset from more than 2000 subject scans. To provide vital contribution to the
fashion industry, we developed a user-friendly adversarial model capable of
generating diverse and detailed clothes simply by drawing a 2D cloth mask.
Under a series of both qualitative and quantitative experiments, we showcase
that Design2Cloth outperforms current state-of-the-art cloth generative models
by a large margin. In addition to the generative properties of our network, we
showcase that the proposed method can be used to achieve high quality
reconstructions from single in-the-wild images and 3D scans. Dataset, code and
pre-trained model will become publicly available.",CVPR
"Active Domain Adaptation (ADA) aims to maximally boost model adaptation in a
new target domain by actively selecting a limited number of target data to
annotate.This setting neglects the more practical scenario where training data
are collected from multiple sources. This motivates us to target a new and
challenging setting of knowledge transfer that extends ADA from a single source
domain to multiple source domains, termed Multi-source Active Domain Adaptation
(MADA). Not surprisingly, we find that most traditional ADA methods cannot work
directly in such a setting, mainly due to the excessive domain gap introduced
by all the source domains and thus their uncertainty-aware sample selection can
easily become miscalibrated under the multi-domain shifts. Considering this, we
propose a Dynamic integrated uncertainty valuation framework(Detective) that
comprehensively consider the domain shift between multi-source domains and
target domain to detect the informative target samples. Specifically, the
leverages a dynamic Domain Adaptation(DA) model that learns how to adapt the
model's parameters to fit the union of multi-source domains. This enables an
approximate single-source domain modeling by the dynamic model. We then
comprehensively measure both domain uncertainty and predictive uncertainty in
the target domain to detect informative target samples using evidential deep
learning, thereby mitigating uncertainty miscalibration. Furthermore, we
introduce a contextual diversity-aware calculator to enhance the diversity of
the selected samples. Experiments demonstrate that our solution outperforms
existing methods by a considerable margin on three domain adaptation
benchmarks.",CVPR
"White balance (WB) algorithms in many commercial cameras assume single and
uniform illumination, leading to undesirable results when multiple lighting
sources with different chromaticities exist in the scene. Prior research on
multi-illuminant WB typically predicts illumination at the pixel level without
fully grasping the scene's actual lighting conditions, including the number and
color of light sources. This often results in unnatural outcomes lacking in
overall consistency. To handle this problem, we present a deep white balancing
model that leverages the slot attention, where each slot is in charge of
representing individual illuminants. This design enables the model to generate
chromaticities and weight maps for individual illuminants, which are then fused
to compose the final illumination map. Furthermore, we propose the
centroid-matching loss, which regulates the activation of each slot based on
the color range, thereby enhancing the model to separate illumination more
effectively. Our method achieves the state-of-the-art performance on both
single- and multi-illuminant WB benchmarks, and also offers additional
information such as the number of illuminants in the scene and their
chromaticity. This capability allows for illumination editing, an application
not feasible with prior methods.",CVPR
"Recent advances in large video-language models have displayed promising
outcomes in video comprehension. Current approaches straightforwardly convert
video into language tokens and employ large language models for multi-modal
tasks. However, this method often leads to the generation of irrelevant
content, commonly known as ""hallucination"", as the length of the text increases
and the impact of the video diminishes. To address this problem, we propose
Vista-LLaMA, a novel framework that maintains the consistent distance between
all visual tokens and any language tokens, irrespective of the generated text
length. Vista-LLaMA omits relative position encoding when determining attention
weights between visual and text tokens, retaining the position encoding for
text and text tokens. This amplifies the effect of visual tokens on text
generation, especially when the relative distance is longer between visual and
text tokens. The proposed attention mechanism significantly reduces the chance
of producing irrelevant text related to the video content. Furthermore, we
present a sequential visual projector that projects the current video frame
into tokens of language space with the assistance of the previous frame. This
approach not only captures the temporal relationship within the video, but also
allows less visual tokens to encompass the entire video. Our approach
significantly outperforms various previous methods (e.g., Video-ChatGPT,
MovieChat) on four challenging open-ended video question answering benchmarks.
We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot
MSRVTT-QA, setting a new state-of-the-art performance. This project is
available at https://jinxxian.github.io/Vista-LLaMA.",CVPR
"4D medical images, which represent 3D images with temporal information, are
crucial in clinical practice for capturing dynamic changes and monitoring
long-term disease progression. However, acquiring 4D medical images poses
challenges due to factors such as radiation exposure and imaging duration,
necessitating a balance between achieving high temporal resolution and
minimizing adverse effects. Given these circumstances, not only is data
acquisition challenging, but increasing the frame rate for each dataset also
proves difficult. To address this challenge, this paper proposes a simple yet
effective Unsupervised Volumetric Interpolation framework, UVI-Net. This
framework facilitates temporal interpolation without the need for any
intermediate frames, distinguishing it from the majority of other existing
unsupervised methods. Experiments on benchmark datasets demonstrate significant
improvements across diverse evaluation metrics compared to unsupervised and
supervised baselines. Remarkably, our approach achieves this superior
performance even when trained with a dataset as small as one, highlighting its
exceptional robustness and efficiency in scenarios with sparse supervision.
This positions UVI-Net as a compelling alternative for 4D medical imaging,
particularly in settings where data availability is limited. The source code is
available at https://github.com/jungeun122333/UVI-Net.",CVPR
"Multi-agent collaborative perception as a potential application for
vehicle-to-everything communication could significantly improve the perception
performance of autonomous vehicles over single-agent perception. However,
several challenges remain in achieving pragmatic information sharing in this
emerging research. In this paper, we propose SCOPE, a novel collaborative
perception framework that aggregates the spatio-temporal awareness
characteristics across on-road agents in an end-to-end manner. Specifically,
SCOPE has three distinct strengths: i) it considers effective semantic cues of
the temporal context to enhance current representations of the target agent;
ii) it aggregates perceptually critical spatial information from heterogeneous
agents and overcomes localization errors via multi-scale feature interactions;
iii) it integrates multi-source representations of the target agent based on
their complementary contributions by an adaptive fusion paradigm. To thoroughly
evaluate SCOPE, we consider both real-world and simulated scenarios of
collaborative 3D object detection tasks on three datasets. Extensive
experiments demonstrate the superiority of our approach and the necessity of
the proposed components.",CVPR
"Visual interactivity understanding within visual scenes presents a
significant challenge in computer vision. Existing methods focus on complex
interactivities while leveraging a simple relationship model. These methods,
however, struggle with a diversity of appearance, situation, position,
interaction, and relation in videos. This limitation hinders the ability to
fully comprehend the interplay within the complex visual dynamics of subjects.
In this paper, we delve into interactivities understanding within visual
content by deriving scene graph representations from dense interactivities
among humans and objects. To achieve this goal, we first present a new dataset
containing Appearance-Situation-Position-Interaction-Relation predicates, named
ASPIRe, offering an extensive collection of videos marked by a wide range of
interactivities. Then, we propose a new approach named Hierarchical
Interlacement Graph (HIG), which leverages a unified layer and graph within a
hierarchical structure to provide deep insights into scene changes across five
distinct tasks. Our approach demonstrates superior performance to other methods
through extensive experiments conducted in various scenarios.",CVPR
"Curation methods for massive vision-language datasets trade off between
dataset size and quality. However, even the highest quality of available
curated captions are far too short to capture the rich visual detail in an
image. To show the value of dense and highly-aligned image-text pairs, we
collect the Densely Captioned Images (DCI) dataset, containing 8012 natural
images human-annotated with mask-aligned descriptions averaging above 1000
words each. With precise and reliable captions associated with specific parts
of an image, we can evaluate vision-language models' (VLMs) understanding of
image content with a novel task that matches each caption with its
corresponding subcrop. As current models are often limited to 77 text tokens,
we also introduce a summarized version (sDCI) in which each caption length is
limited. We show that modern techniques that make progress on standard
benchmarks do not correspond with significant improvement on our sDCI based
benchmark. Lastly, we finetune CLIP using sDCI and show significant
improvements over the baseline despite a small training set. By releasing the
first human annotated dense image captioning dataset, we hope to enable the
development of new benchmarks or fine-tuning recipes for the next generation of
VLMs to come.",CVPR
"Computer vision techniques play a central role in the perception stack of
autonomous vehicles. Such methods are employed to perceive the vehicle
surroundings given sensor data. 3D LiDAR sensors are commonly used to collect
sparse 3D point clouds from the scene. However, compared to human perception,
such systems struggle to deduce the unseen parts of the scene given those
sparse point clouds. In this matter, the scene completion task aims at
predicting the gaps in the LiDAR measurements to achieve a more complete scene
representation. Given the promising results of recent diffusion models as
generative models for images, we propose extending them to achieve scene
completion from a single 3D LiDAR scan. Previous works used diffusion models
over range images extracted from LiDAR data, directly applying image-based
diffusion methods. Distinctly, we propose to directly operate on the points,
reformulating the noising and denoising diffusion process such that it can
efficiently work at scene scale. Together with our approach, we propose a
regularization loss to stabilize the noise predicted during the denoising
process. Our experimental evaluation shows that our method can complete the
scene given a single LiDAR scan as input, producing a scene with more details
compared to state-of-the-art scene completion methods. We believe that our
proposed diffusion process formulation can support further research in
diffusion models applied to scene-scale point cloud data.",CVPR
"Contemporary models for generating images show remarkable quality and
versatility. Swayed by these advantages, the research community repurposes them
to generate videos. Since video content is highly redundant, we argue that
naively bringing advances of image models to the video generation domain
reduces motion fidelity, visual quality and impairs scalability. In this work,
we build Snap Video, a video-first model that systematically addresses these
challenges. To do that, we first extend the EDM framework to take into account
spatially and temporally redundant pixels and naturally support video
generation. Second, we show that a U-Net - a workhorse behind image generation
- scales poorly when generating videos, requiring significant computational
overhead. Hence, we propose a new transformer-based architecture that trains
3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us
to efficiently train a text-to-video model with billions of parameters for the
first time, reach state-of-the-art results on a number of benchmarks, and
generate videos with substantially higher quality, temporal consistency, and
motion complexity. The user studies showed that our model was favored by a
large margin over the most recent methods. See our website at
https://snap-research.github.io/snapvideo/.",CVPR
"A significant challenge facing current optical flow methods is the difficulty
in generalizing them well to the real world. This is mainly due to the high
cost of hand-crafted datasets, and existing self-supervised methods are limited
by indirect loss and occlusions, resulting in fuzzy outcomes. To address this
challenge, we introduce a novel optical flow training framework: automatic data
factory (ADF). ADF only requires RGB images as input to effectively train the
optical flow network on the target data domain. Specifically, we use advanced
Nerf technology to reconstruct scenes from photo groups collected by a
monocular camera, and then calculate optical flow labels between camera pose
pairs based on the rendering results. To eliminate erroneous labels caused by
defects in the scene reconstructed by Nerf, we screened the generated labels
from multiple aspects, such as optical flow matching accuracy, radiation field
confidence, and depth consistency. The filtered labels can be directly used for
network supervision. Experimentally, the generalization ability of ADF on KITTI
surpasses existing self-supervised optical flow and monocular scene flow
algorithms. In addition, ADF achieves impressive results in real-world
zero-point generalization evaluations and surpasses most supervised methods.",CVPR
"Industrial anomaly detection (IAD) has garnered significant attention and
experienced rapid development. However, the recent development of IAD approach
has encountered certain difficulties due to dataset limitations. On the one
hand, most of the state-of-the-art methods have achieved saturation (over 99%
in AUROC) on mainstream datasets such as MVTec, and the differences of methods
cannot be well distinguished, leading to a significant gap between public
datasets and actual application scenarios. On the other hand, the research on
various new practical anomaly detection settings is limited by the scale of the
dataset, posing a risk of overfitting in evaluation results. Therefore, we
propose a large-scale, Real-world, and multi-view Industrial Anomaly Detection
dataset, named Real-IAD, which contains 150K high-resolution images of 30
different objects, an order of magnitude larger than existing datasets. It has
a larger range of defect area and ratio proportions, making it more challenging
than previous datasets. To make the dataset closer to real application
scenarios, we adopted a multi-view shooting method and proposed sample-level
evaluation metrics. In addition, beyond the general unsupervised anomaly
detection setting, we propose a new setting for Fully Unsupervised Industrial
Anomaly Detection (FUIAD) based on the observation that the yield rate in
industrial production is usually greater than 60%, which has more practical
application value. Finally, we report the results of popular IAD methods on the
Real-IAD dataset, providing a highly challenging benchmark to promote the
development of the IAD field.",CVPR
"Despite the commercial abundance of UAVs, aerial data acquisition remains
challenging, and the existing Asia and North America-centric open-source UAV
datasets are small-scale or low-resolution and lack diversity in scene
contextuality. Additionally, the color content of the scenes, solar-zenith
angle, and population density of different geographies influence the data
diversity. These two factors conjointly render suboptimal aerial-visual
perception of the deep neural network (DNN) models trained primarily on the
ground-view data, including the open-world foundational models.
  To pave the way for a transformative era of aerial detection, we present
Multiview Aerial Visual RECognition or MAVREC, a video dataset where we record
synchronized scenes from different perspectives -- ground camera and
drone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard
2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million
annotated bounding boxes. This makes MAVREC the largest ground and aerial-view
dataset, and the fourth largest among all drone-based datasets across all
modalities and tasks. Through our extensive benchmarking on MAVREC, we
recognize that augmenting object detectors with ground-view images from the
corresponding geographical location is a superior pre-training strategy for
aerial detection. Building on this strategy, we benchmark MAVREC with a
curriculum-based semi-supervised object detection approach that leverages
labeled (ground and aerial) and unlabeled (only aerial) images to enhance the
aerial detection. We publicly release the MAVREC dataset:
https://mavrec.github.io.",CVPR
"Recovering dense and long-range pixel motion in videos is a challenging
problem. Part of the difficulty arises from the 3D-to-2D projection process,
leading to occlusions and discontinuities in the 2D motion domain. While 2D
motion can be intricate, we posit that the underlying 3D motion can often be
simple and low-dimensional. In this work, we propose to estimate point
trajectories in 3D space to mitigate the issues caused by image projection. Our
method, named SpatialTracker, lifts 2D pixels to 3D using monocular depth
estimators, represents the 3D content of each frame efficiently using a
triplane representation, and performs iterative updates using a transformer to
estimate 3D trajectories. Tracking in 3D allows us to leverage
as-rigid-as-possible (ARAP) constraints while simultaneously learning a
rigidity embedding that clusters pixels into different rigid parts. Extensive
evaluation shows that our approach achieves state-of-the-art tracking
performance both qualitatively and quantitatively, particularly in challenging
scenarios such as out-of-plane rotation.",CVPR
"Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability
to perceive and understand multi-modal signals. However, most of the existing
MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text
pairs, leading to insufficient extraction and reasoning of visual knowledge. To
address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal
Large Language Model (LION), which empowers the MLLM by injecting visual
knowledge in two levels. 1) Progressive incorporation of fine-grained
spatial-aware visual knowledge. We design a vision aggregator cooperated with
region-level vision-language (VL) tasks to incorporate fine-grained
spatial-aware visual knowledge into the MLLM. To alleviate the conflict between
image-level and region-level VL tasks during incorporation, we devise a
dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This
progressive incorporation scheme contributes to the mutual promotion between
these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual
evidence. We facilitate the MLLM with high-level semantic visual evidence by
leveraging diverse image tags. To mitigate the potential influence caused by
imperfect predicted tags, we propose a soft prompting method by embedding a
learnable token into the tailored text instruction. Comprehensive experiments
on several multi-modal benchmarks demonstrate the superiority of our model
(e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over
InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).",CVPR
"Universal Domain Adaptation (UniDA) targets knowledge transfer in the
presence of both covariate and label shifts. Recently, Source-free Universal
Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to
source data, which tends to be more practical due to data protection policies.
The main challenge lies in determining whether covariate-shifted samples belong
to target-private unknown categories. Existing methods tackle this either
through hand-crafted thresholding or by developing time-consuming iterative
clustering strategies. In this paper, we propose a new idea of LEArning
Decomposition (LEAD), which decouples features into source-known and -unknown
components to identify target-private data. Technically, LEAD initially
leverages the orthogonal decomposition analysis for feature decomposition.
Then, LEAD builds instance-level decision boundaries to adaptively identify
target-private data. Extensive experiments across various UniDA scenarios have
demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA
scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and
reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD
is also appealing in that it is complementary to most existing methods. The
code is available at https://github.com/ispc-lab/LEAD.",CVPR
"We present a framework for generating full-bodied photorealistic avatars that
gesture according to the conversational dynamics of a dyadic interaction. Given
speech audio, we output multiple possibilities of gestural motion for an
individual, including face, body, and hands. The key behind our method is in
combining the benefits of sample diversity from vector quantization with the
high-frequency details obtained through diffusion to generate more dynamic,
expressive motion. We visualize the generated motion using highly
photorealistic avatars that can express crucial nuances in gestures (e.g.
sneers and smirks). To facilitate this line of research, we introduce a
first-of-its-kind multi-view conversational dataset that allows for
photorealistic reconstruction. Experiments show our model generates appropriate
and diverse gestures, outperforming both diffusion- and VQ-only methods.
Furthermore, our perceptual evaluation highlights the importance of
photorealism (vs. meshes) in accurately assessing subtle motion details in
conversational gestures. Code and dataset available online.",CVPR
"Video Motion Magnification (VMM) aims to reveal subtle and imperceptible
motion information of objects in the macroscopic world. Prior methods directly
model the motion field from the Eulerian perspective by Representation Learning
that separates shape and texture or Multi-domain Learning from phase
fluctuations. Inspired by the frequency spectrum, we observe that the
low-frequency components with stable energy always possess spatial structure
and less noise, making them suitable for modeling the subtle motion field. To
this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion
Magnification with a Multi-level Isomorphic Architecture to capture multi-level
high-frequency details and a stable low-frequency structure (motion field) in
video space. Since high-frequency details and subtle motions are susceptible to
information degradation due to their inherent subtlety and unavoidable external
interference from noise, we carefully design Sparse High/Low-pass Filters to
enhance the integrity of details and motion structures, and a Sparse Frequency
Mixer to promote seamless recoupling. Besides, we innovatively design a
contrastive regularization for this task to strengthen the model's ability to
discriminate irrelevant features, reducing undesired motion magnification.
Extensive experiments on both Real-world and Synthetic Datasets show that our
FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\times$
and boosts inference speed by 1.68$\times$ than the latest method. Our code is
available at https://github.com/Jiafei127/FD4MM.",CVPR
"Skeleton-based action recognition has attracted lots of research attention.
Recently, to build an accurate skeleton-based action recognizer, a variety of
works have been proposed. Among them, some works use large model architectures
as backbones of their recognizers to boost the skeleton data representation
capability, while some other works pre-train their recognizers on external data
to enrich the knowledge. In this work, we observe that large language models
which have been extensively used in various natural language processing tasks
generally hold both large model architectures and rich implicit knowledge.
Motivated by this, we propose a novel LLM-AR framework, in which we investigate
treating the Large Language Model as an Action Recognizer. In our framework, we
propose a linguistic projection process to project each input action signal
(i.e., each skeleton sequence) into its ``sentence format'' (i.e., an ``action
sentence''). Moreover, we also incorporate our framework with several designs
to further facilitate this linguistic projection process. Extensive experiments
demonstrate the efficacy of our proposed framework.",CVPR
"Point cloud analysis has achieved outstanding performance by transferring
point cloud pre-trained models. However, existing methods for model adaptation
usually update all model parameters, i.e., full fine-tuning paradigm, which is
inefficient as it relies on high computational costs (e.g., training GPU
memory) and massive storage space. In this paper, we aim to study
parameter-efficient transfer learning for point cloud analysis with an ideal
trade-off between task performance and parameter efficiency. To achieve this
goal, we freeze the parameters of the default pre-trained models and then
propose the Dynamic Adapter, which generates a dynamic scale for each token,
considering the token significance to the downstream task. We further
seamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing
Internal Prompts, capturing the instance-specific features for interaction.
Extensive experiments conducted on five challenging datasets demonstrate that
the proposed DAPT achieves superior performance compared to the full
fine-tuning counterparts while significantly reducing the trainable parameters
and training GPU memory by 95% and 35%, respectively. Code is available at
https://github.com/LMD0311/DAPT.",CVPR
"The ability to learn from context with novel concepts, and deliver
appropriate responses are essential in human conversations. Despite current
Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being
trained on mega-scale datasets, recognizing unseen images or understanding
novel concepts in a training-free manner remains a challenge. In-Context
Learning (ICL) explores training-free few-shot learning, where models are
encouraged to ``learn to learn"" from limited tasks and generalize to unseen
tasks. In this work, we propose link-context learning (LCL), which emphasizes
""reasoning from cause and effect"" to augment the learning capabilities of
MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal
relationship between the support set and the query set. By providing
demonstrations with causal links, LCL guides the model to discern not only the
analogy but also the underlying causal associations between data points, which
empowers MLLMs to recognize unseen images and understand novel concepts more
effectively. To facilitate the evaluation of this novel approach, we introduce
the ISEKAI dataset, comprising exclusively of unseen generated image-label
pairs designed for link-context learning. Extensive experiments show that our
LCL-MLLM exhibits strong link-context learning capabilities to novel concepts
over vanilla MLLMs. Code and data will be released at
https://github.com/isekai-portal/Link-Context-Learning.",CVPR
"Diffusion models (DMs) embark a new era of generative modeling and offer more
opportunities for efficient generating high-quality and realistic data samples.
However, their widespread use has also brought forth new challenges in model
security, which motivates the creation of more effective adversarial attackers
on DMs to understand its vulnerability. We propose CAAT, a simple but generic
and efficient approach that does not require costly training to effectively
fool latent diffusion models (LDMs). The approach is based on the observation
that cross-attention layers exhibits higher sensitivity to gradient change,
allowing for leveraging subtle perturbations on published images to
significantly corrupt the generated images. We show that a subtle perturbation
on an image can significantly impact the cross-attention layers, thus changing
the mapping between text and image during the fine-tuning of customized
diffusion models. Extensive experiments demonstrate that CAAT is compatible
with diverse diffusion models and outperforms baseline attack methods in a more
effective (more noise) and efficient (twice as fast as Anti-DreamBooth and
Mist) manner.",CVPR
"Existing depth sensors are imperfect and may provide inaccurate depth values
in challenging scenarios, such as in the presence of transparent or reflective
objects. In this work, we present a general framework that leverages
polarization imaging to improve inaccurate depth measurements from various
depth sensors. Previous polarization-based depth enhancement methods focus on
utilizing pure physics-based formulas for a single sensor. In contrast, our
method first adopts a learning-based strategy where a neural network is trained
to estimate a dense and complete depth map from polarization data and a sensor
depth map from different sensors. To further improve the performance, we
propose a Polarization Prompt Fusion Tuning (PPFT) strategy to effectively
utilize RGB-based models pre-trained on large-scale datasets, as the size of
the polarization dataset is limited to train a strong model from scratch. We
conducted extensive experiments on a public dataset, and the results
demonstrate that the proposed method performs favorably compared to existing
depth enhancement baselines. Code and demos are available at
https://lastbasket.github.io/PPFT/.",CVPR
"Generative models can produce impressively realistic images. This paper
demonstrates that generated images have geometric features different from those
of real images. We build a set of collections of generated images, prequalified
to fool simple, signal-based classifiers into believing they are real. We then
show that prequalified generated images can be identified reliably by
classifiers that only look at geometric properties. We use three such
classifiers. All three classifiers are denied access to image pixels, and look
only at derived geometric features. The first classifier looks at the
perspective field of the image, the second looks at lines detected in the
image, and the third looks at relations between detected objects and shadows.
Our procedure detects generated images more reliably than SOTA local signal
based detectors, for images from a number of distinct generators. Saliency maps
suggest that the classifiers can identify geometric problems reliably. We
conclude that current generators cannot reliably reproduce geometric properties
of real images.",CVPR
"Recently, we have witnessed the explosive growth of various volumetric
representations in modeling animatable head avatars. However, due to the
diversity of frameworks, there is no practical method to support high-level
applications like 3D head avatar editing across different representations. In
this paper, we propose a generic avatar editing approach that can be
universally applied to various 3DMM driving volumetric head avatars. To achieve
this goal, we design a novel expression-aware modification generative model,
which enables lift 2D editing from a single image to a consistent 3D
modification field. To ensure the effectiveness of the generative modification
process, we develop several techniques, including an expression-dependent
modification distillation scheme to draw knowledge from the large-scale head
avatar model and 2D facial texture editing tools, implicit latent space
guidance to enhance model convergence, and a segmentation-based loss reweight
strategy for fine-grained texture inversion. Extensive experiments demonstrate
that our method delivers high-quality and consistent results across multiple
expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/",CVPR
"Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running sampling multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
We demonstrate the effectiveness of this method on top of the latent
token-based Muse text-to-image model. The MRF richly encodes the compatibility
among image tokens at different spatial locations to improve quality and
significantly reduce the required number of Muse sampling steps. Inference with
the MRF is significantly cheaper, and its parameters can be quickly learned
through back-propagation by modeling MRF inference as a differentiable
neural-network layer. Our full model, MarkovGen, uses this proposed MRF model
to both speed up Muse by 1.5X and produce higher quality images by decreasing
undesirable image artifacts.",CVPR
"Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest
towards editable implicit 3D representations has surged over the last years.
However, editing implicit or hybrid representations as used for NeRFs is
difficult due to the entanglement of appearance and geometry encoded in the
model parameters. Despite these challenges, recent research has shown first
promising steps towards photorealistic and non-photorealistic appearance edits.
The main open issues of related work include limited interactivity, a lack of
support for local edits and large memory requirements, rendering them less
useful in practice. We address these limitations with LAENeRF, a unified
framework for photorealistic and non-photorealistic appearance editing of
NeRFs. To tackle local editing, we leverage a voxel grid as starting point for
region selection. We learn a mapping from expected ray terminations to final
output color, which can optionally be supervised by a style loss, resulting in
a framework which can perform photorealistic and non-photorealistic appearance
editing of selected regions. Relying on a single point per ray for our mapping,
we limit memory requirements and enable fast optimization. To guarantee
interactivity, we compose the output color using a set of learned, modifiable
base colors, composed with additive layer mixing. Compared to concurrent work,
LAENeRF enables recoloring and stylization while keeping processing time low.
Furthermore, we demonstrate that our approach surpasses baseline methods both
quantitatively and qualitatively.",CVPR
"Understanding the world in first-person view is fundamental in Augmented
Reality (AR). This immersive perspective brings dramatic visual changes and
unique challenges compared to third-person views. Synthetic data has empowered
third-person-view vision models, but its application to embodied egocentric
perception tasks remains largely unexplored. A critical challenge lies in
simulating natural human movements and behaviors that effectively steer the
embodied cameras to capture a faithful egocentric representation of the 3D
world. To address this challenge, we introduce EgoGen, a new synthetic data
generator that can produce accurate and rich ground-truth training data for
egocentric perception tasks. At the heart of EgoGen is a novel human motion
synthesis model that directly leverages egocentric visual inputs of a virtual
human to sense the 3D environment. Combined with collision-avoiding motion
primitives and a two-stage reinforcement learning approach, our motion
synthesis model offers a closed-loop solution where the embodied perception and
movement of the virtual human are seamlessly coupled. Compared to previous
works, our model eliminates the need for a pre-defined global path, and is
directly applicable to dynamic environments. Combined with our easy-to-use and
scalable data generation pipeline, we demonstrate EgoGen's efficacy in three
tasks: mapping and localization for head-mounted cameras, egocentric camera
tracking, and human mesh recovery from egocentric views. EgoGen will be fully
open-sourced, offering a practical solution for creating realistic egocentric
training data and aiming to serve as a useful tool for egocentric computer
vision research. Refer to our project page: https://ego-gen.github.io/.",CVPR
"Domain adaptation for object detection typically entails transferring
knowledge from one visible domain to another visible domain. However, there are
limited studies on adapting from the visible to the thermal domain, because the
domain gap between the visible and thermal domains is much larger than
expected, and traditional domain adaptation can not successfully facilitate
learning in this situation. To overcome this challenge, we propose a
Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training
paradigms for each domain. Specifically, we segregate the source and target
training sets for building dual-teachers and successively deploy exponential
moving average to the student model to individual teachers of each domain. The
framework further incorporates a zigzag learning method between dual teachers,
facilitating a gradual transition from the visible to thermal domains during
training. We validate the superiority of our method through newly designed
experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.
Source code is available at https://github.com/EdwardDo69/D3T .",CVPR
"We present Bayesian Diffusion Models (BDM), a prediction algorithm that
performs effective Bayesian inference by tightly coupling the top-down (prior)
information with the bottom-up (data-driven) procedure via joint diffusion
processes. We show the effectiveness of BDM on the 3D shape reconstruction
task. Compared to prototypical deep learning data-driven approaches trained on
paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM
brings in rich prior information from standalone labels (e.g. point clouds) to
improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian
frameworks where explicit prior and likelihood are required for the inference,
BDM performs seamless information fusion via coupled diffusion processes with
learned gradient computation networks. The specialty of our BDM lies in its
capability to engage the active and effective information exchange and fusion
of the top-down and bottom-up processes where each itself is a diffusion
process. We demonstrate state-of-the-art results on both synthetic and
real-world benchmarks for 3D shape reconstruction.",CVPR
"We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.",CVPR
"In autonomous driving, predicting future events in advance and evaluating the
foreseeable risks empowers autonomous vehicles to better plan their actions,
enhancing safety and efficiency on the road. To this end, we propose Drive-WM,
the first driving world model compatible with existing end-to-end planning
models. Through a joint spatial-temporal modeling facilitated by view
factorization, our model generates high-fidelity multiview videos in driving
scenes. Building on its powerful generation ability, we showcase the potential
of applying the world model for safe driving planning for the first time.
Particularly, our Drive-WM enables driving into multiple futures based on
distinct driving maneuvers, and determines the optimal trajectory according to
the image-based rewards. Evaluation on real-world driving datasets verifies
that our method could generate high-quality, consistent, and controllable
multiview videos, opening up possibilities for real-world simulations and safe
planning.",CVPR
"Recent advances in vision-language models like Stable Diffusion have shown
remarkable power in creative image synthesis and editing.However, most existing
text-to-image editing methods encounter two obstacles: First, the text prompt
needs to be carefully crafted to achieve good results, which is not intuitive
or user-friendly. Second, they are insensitive to local edits and can
irreversibly affect non-edited regions, leaving obvious editing traces. To
tackle these problems, we propose a Zero-shot instructiON-guided local image
Editing approach, termed ZONE. We first convert the editing intent from the
user-provided instruction (e.g., ""make his tie blue"") into specific image
editing regions through InstructPix2Pix. We then propose a Region-IoU scheme
for precise image layer extraction from an off-the-shelf segment model. We
further develop an edge smoother based on FFT for seamless blending between the
layer and the image.Our method allows for arbitrary manipulation of a specific
region with a single instruction while preserving the rest. Extensive
experiments demonstrate that our ZONE achieves remarkable local editing results
and user-friendliness, outperforming state-of-the-art methods. Code is
available at https://github.com/lsl001006/ZONE.",CVPR
"Diffusion models have achieved remarkable image generation quality surpassing
previous generative models. However, a notable limitation of diffusion models,
in comparison to GANs, is their difficulty in smoothly interpolating between
two image samples, due to their highly unstructured latent space. Such a smooth
interpolation is intriguing as it naturally serves as a solution for the image
morphing task with many applications. In this work, we present DiffMorpher, the
first approach enabling smooth and natural image interpolation using diffusion
models. Our key idea is to capture the semantics of the two images by fitting
two LoRAs to them respectively, and interpolate between both the LoRA
parameters and the latent noises to ensure a smooth semantic transition, where
correspondence automatically emerges without the need for annotation. In
addition, we propose an attention interpolation and injection technique and a
new sampling schedule to further enhance the smoothness between consecutive
images. Extensive experiments demonstrate that DiffMorpher achieves starkly
better image morphing effects than previous methods across a variety of object
categories, bridging a critical functional gap that distinguished diffusion
models from GANs.",CVPR
"We present InstructDiffusion, a unifying and generic framework for aligning
computer vision tasks with human instructions. Unlike existing approaches that
integrate prior knowledge and pre-define the output space (e.g., categories and
coordinates) for each vision task, we cast diverse vision tasks into a
human-intuitive image-manipulating process whose output space is a flexible and
interactive pixel space. Concretely, the model is built upon the diffusion
process and is trained to predict pixels according to user instructions, such
as encircling the man's left shoulder in red or applying a blue mask to the
left car. InstructDiffusion could handle a variety of vision tasks, including
understanding tasks (such as segmentation and keypoint detection) and
generative tasks (such as editing and enhancement). It even exhibits the
ability to handle unseen tasks and outperforms prior methods on novel datasets.
This represents a significant step towards a generalist modeling interface for
vision tasks, advancing artificial general intelligence in the field of
computer vision.",CVPR
"Motion capture from a limited number of body-worn sensors, such as inertial
measurement units (IMUs) and pressure insoles, has important applications in
health, human performance, and entertainment. Recent work has focused on
accurately reconstructing whole-body motion from a specific sensor
configuration using six IMUs. While a common goal across applications is to use
the minimal number of sensors to achieve required accuracy, the optimal
arrangement of the sensors might differ from application to application. We
propose a single diffusion model, DiffusionPoser, which reconstructs human
motion in real-time from an arbitrary combination of sensors, including IMUs
placed at specified locations, and, pressure insoles. Unlike existing methods,
our model grants users the flexibility to determine the number and arrangement
of sensors tailored to the specific activity of interest, without the need for
retraining. A novel autoregressive inferencing scheme ensures real-time motion
reconstruction that closely aligns with measured sensor signals. The generative
nature of DiffusionPoser ensures realistic behavior, even for
degrees-of-freedom not directly measured. Qualitative results can be found on
our website: https://diffusionposer.github.io/.",CVPR
"Representation learning of pathology whole-slide images (WSIs) has been has
primarily relied on weak supervision with Multiple Instance Learning (MIL).
However, the slide representations resulting from this approach are highly
tailored to specific clinical tasks, which limits their expressivity and
generalization, particularly in scenarios with limited data. Instead, we
hypothesize that morphological redundancy in tissue can be leveraged to build a
task-agnostic slide representation in an unsupervised fashion. To this end, we
introduce PANTHER, a prototype-based approach rooted in the Gaussian mixture
model that summarizes the set of WSI patches into a much smaller set of
morphological prototypes. Specifically, each patch is assumed to have been
generated from a mixture distribution, where each mixture component represents
a morphological exemplar. Utilizing the estimated mixture parameters, we then
construct a compact slide representation that can be readily used for a wide
range of downstream tasks. By performing an extensive evaluation of PANTHER on
subtyping and survival tasks using 13 datasets, we show that 1) PANTHER
outperforms or is on par with supervised MIL baselines and 2) the analysis of
morphological prototypes brings new qualitative and quantitative insights into
model interpretability.",CVPR
"Existing text-to-image generative models reflect or even amplify societal
biases ingrained in their training data. This is especially concerning for
human image generation where models are biased against certain demographic
groups. Existing attempts to rectify this issue are hindered by the inherent
limitations of the pre-trained models and fail to substantially improve
demographic diversity. In this work, we introduce Fair Retrieval Augmented
Generation (FairRAG), a novel framework that conditions pre-trained generative
models on reference images retrieved from an external image database to improve
fairness in human generation. FairRAG enables conditioning through a
lightweight linear module that projects reference images into the textual
space. To enhance fairness, FairRAG applies simple-yet-effective debiasing
strategies, providing images from diverse demographic groups during the
generative process. Extensive experiments demonstrate that FairRAG outperforms
existing methods in terms of demographic diversity, image-text alignment, and
image fidelity while incurring minimal computational overhead during inference.",CVPR
"Multi-modal learning that combines pathological images with genomic data has
significantly enhanced the accuracy of survival prediction. Nevertheless,
existing methods have not fully utilized the inherent hierarchical structure
within both whole slide images (WSIs) and transcriptomic data, from which
better intra-modal representations and inter-modal integration could be
derived. Moreover, many existing studies attempt to improve multi-modal
representations through attention mechanisms, which inevitably lead to high
complexity when processing high-dimensional WSIs and transcriptomic data.
Recently, a structured state space model named Mamba emerged as a promising
approach for its superior performance in modeling long sequences with low
complexity. In this study, we propose Mamba with multi-grained multi-modal
interaction (SurvMamba) for survival prediction. SurvMamba is implemented with
a Hierarchical Interaction Mamba (HIM) module that facilitates efficient
intra-modal interactions at different granularities, thereby capturing more
detailed local features as well as rich global representations. In addition, an
Interaction Fusion Mamba (IFM) module is used for cascaded inter-modal
interactive fusion, yielding more comprehensive features for survival
prediction. Comprehensive evaluations on five TCGA datasets demonstrate that
SurvMamba outperforms other existing methods in terms of performance and
computational cost.",CVPR
"We introduce SODA, a self-supervised diffusion model, designed for
representation learning. The model incorporates an image encoder, which
distills a source view into a compact representation, that, in turn, guides the
generation of related novel views. We show that by imposing a tight bottleneck
between the encoder and a denoising decoder, and leveraging novel view
synthesis as a self-supervised objective, we can turn diffusion models into
strong representation learners, capable of capturing visual semantics in an
unsupervised manner. To the best of our knowledge, SODA is the first diffusion
model to succeed at ImageNet linear-probe classification, and, at the same
time, it accomplishes reconstruction, editing and synthesis tasks across a wide
range of datasets. Further investigation reveals the disentangled nature of its
emergent latent space, that serves as an effective interface to control and
manipulate the model's produced images. All in all, we aim to shed light on the
exciting and promising potential of diffusion models, not only for image
generation, but also for learning rich and robust representations.",CVPR
"We propose a method to efficiently equip the Segment Anything Model (SAM)
with the ability to generate regional captions. SAM presents strong
generalizability to segment anything while is short for semantic understanding.
By introducing a lightweight query-based feature mixer, we align the
region-specific features with the embedding space of language models for later
caption generation. As the number of trainable parameters is small (typically
in the order of tens of millions), it costs less computation, less memory
usage, and less communication bandwidth, resulting in both fast and scalable
training. To address the scarcity problem of regional caption data, we propose
to first pre-train our model on objection detection and segmentation tasks. We
call this step weak supervision pretraining since the pre-training data only
contains category names instead of full-sentence descriptions. The weak
supervision pretraining allows us to leverage many publicly available object
detection and segmentation datasets. We conduct extensive experiments to
demonstrate the superiority of our method and validate each design choice. This
work serves as a stepping stone towards scaling up regional captioning data and
sheds light on exploring efficient ways to augment SAM with regional semantics.
The project page, along with the associated code, can be accessed via
https://xk-huang.github.io/segment-caption-anything/.",CVPR
"Estimating the 6D object pose from a single RGB image often involves noise
and indeterminacy due to challenges such as occlusions and cluttered
backgrounds. Meanwhile, diffusion models have shown appealing performance in
generating high-quality images from random noise with high indeterminacy
through step-by-step denoising. Inspired by their denoising capability, we
propose a novel diffusion-based framework (6D-Diff) to handle the noise and
indeterminacy in object pose estimation for better performance. In our
framework, to establish accurate 2D-3D correspondence, we formulate 2D
keypoints detection as a reverse diffusion (denoising) process. To facilitate
such a denoising process, we design a Mixture-of-Cauchy-based forward diffusion
process and condition the reverse process on the object features. Extensive
experiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our
framework.",CVPR
"Advancements in 3D instance segmentation have traditionally been tethered to
the availability of annotated datasets, limiting their application to a narrow
spectrum of object categories. Recent efforts have sought to harness
vision-language models like CLIP for open-set semantic reasoning, yet these
methods struggle to distinguish between objects of the same categories and rely
on specific prompts that are not universally applicable. In this paper, we
introduce SAI3D, a novel zero-shot 3D instance segmentation approach that
synergistically leverages geometric priors and semantic cues derived from
Segment Anything Model (SAM). Our method partitions a 3D scene into geometric
primitives, which are then progressively merged into 3D instance segmentations
that are consistent with the multi-view SAM masks. Moreover, we design a
hierarchical region-growing algorithm with a dynamic thresholding mechanism,
which largely improves the robustness of finegrained 3D scene parsing.Empirical
evaluations on ScanNet, Matterport3D and the more challenging ScanNet++
datasets demonstrate the superiority of our approach. Notably, SAI3D
outperforms existing open-vocabulary baselines and even surpasses
fully-supervised methods in class-agnostic segmentation on ScanNet++. Our
project page is at https://yd-yin.github.io/SAI3D.",CVPR
"The pre-trained vision-language model, exemplified by CLIP, advances
zero-shot semantic segmentation by aligning visual features with class
embeddings through a transformer decoder to generate semantic masks. Despite
its effectiveness, prevailing methods within this paradigm encounter
challenges, including overfitting on seen classes and small fragmentation in
masks. To mitigate these issues, we propose a Language-Driven Visual Consensus
(LDVC) approach, fostering improved alignment of semantic and visual
information.Specifically, we leverage class embeddings as anchors due to their
discrete and abstract nature, steering vision features toward class embeddings.
Moreover, to circumvent noisy alignments from the vision part due to its
redundant nature, we introduce route attention into self-attention for finding
visual consensus, thereby enhancing semantic consistency within the same
object. Equipped with a vision-language prompting strategy, our approach
significantly boosts the generalization capacity of segmentation models for
unseen classes. Experimental results underscore the effectiveness of our
approach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the
COCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.",CVPR
"Many instruments performing optical and non-optical imaging and sensing, such
as Optical Coherence Tomography (OCT), Magnetic Resonance Imaging or
Fourier-transform spectrometry, produce digital signals containing modulations,
sine-like components, which only after Fourier transformation give information
about the structure or characteristics of the investigated object. Due to the
fundamental physics-related limitations of such methods, the distribution of
these signal components is often nonlinear and, when not properly compensated,
leads to the resolution, precision or quality drop in the final image. Here, we
propose an innovative approach that has the potential to allow cleaning of the
signal from the nonlinearities but most of all, it now allows to switch the
given order off, leaving all others intact. The latter provides a tool for more
in-depth analysis of the nonlinearity-inducing properties of the investigated
object, which can lead to applications in early disease detection or more
sensitive sensing of chemical compounds. We consider OCT signals and
nonlinearities up to the third order. In our approach, we propose two neural
networks: one to remove solely the second-order nonlinearity and the other for
removing solely the third-order nonlinearity. The input of the networks is a
novel two-dimensional data structure with all the information needed for the
network to infer a nonlinearity-free signal. We describe the developed networks
and present the results for second-order and third-order nonlinearity removal
in OCT data representing the images of various objects: a mirror, glass, and
fruits.",CVPR
"Model stealing attacks have become a serious concern for deep learning
models, where an attacker can steal a trained model by querying its black-box
API. This can lead to intellectual property theft and other security and
privacy risks. The current state-of-the-art defenses against model stealing
attacks suggest adding perturbations to the prediction probabilities. However,
they suffer from heavy computations and make impracticable assumptions about
the adversary. They often require the training of auxiliary models. This can be
time-consuming and resource-intensive which hinders the deployment of these
defenses in real-world applications. In this paper, we propose a simple yet
effective and efficient defense alternative. We introduce a heuristic approach
to perturb the output probabilities. The proposed defense can be easily
integrated into models without additional training. We show that our defense is
effective in defending against three state-of-the-art stealing attacks. We
evaluate our approach on large and quantized (i.e., compressed) Convolutional
Neural Networks (CNNs) trained on several vision datasets. Our technique
outperforms the state-of-the-art defenses with a $\times37$ faster inference
latency without requiring any additional model and with a low impact on the
model's performance. We validate that our defense is also effective for
quantized CNNs targeting edge devices.",CVPR
"Several unsupervised image segmentation approaches have been proposed which
eliminate the need for dense manually-annotated segmentation masks; current
models separately handle either semantic segmentation (e.g., STEGO) or
class-agnostic instance segmentation (e.g., CutLER), but not both (i.e.,
panoptic segmentation). We propose an Unsupervised Universal Segmentation model
(U2Seg) adept at performing various image segmentation tasks -- instance,
semantic and panoptic -- using a novel unified framework. U2Seg generates
pseudo semantic labels for these segmentation tasks via leveraging
self-supervised models followed by clustering; each cluster represents
different semantic and/or instance membership of pixels. We then self-train the
model on these pseudo semantic labels, yielding substantial performance gains
over specialized methods tailored to each task: a +2.6 AP$^{\text{box}}$ boost
vs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc
increase (vs. STEGO) in unsupervised semantic segmentation on COCOStuff.
Moreover, our method sets up a new baseline for unsupervised panoptic
segmentation, which has not been previously explored. U2Seg is also a strong
pretrained model for few-shot segmentation, surpassing CutLER by +5.0
AP$^{\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCO
labels. We hope our simple yet effective method can inspire more research on
unsupervised universal image segmentation.",CVPR
"Recent text-to-3D methods employing diffusion models have made significant
advancements in 3D human generation. However, these approaches face challenges
due to the limitations of text-to-image diffusion models, which lack an
understanding of 3D structures. Consequently, these methods struggle to achieve
high-quality human generation, resulting in smooth geometry and cartoon-like
appearances. In this paper, we propose HumanNorm, a novel approach for
high-quality and realistic 3D human generation. The main idea is to enhance the
model's 2D perception of 3D geometry by learning a normal-adapted diffusion
model and a normal-aligned diffusion model. The normal-adapted diffusion model
can generate high-fidelity normal maps corresponding to user prompts with
view-dependent and body-aware text. The normal-aligned diffusion model learns
to generate color images aligned with the normal maps, thereby transforming
physical geometry details into realistic appearance. Leveraging the proposed
normal diffusion model, we devise a progressive geometry generation strategy
and a multi-step Score Distillation Sampling (SDS) loss to enhance the
performance of 3D human generation. Comprehensive experiments substantiate
HumanNorm's ability to generate 3D humans with intricate geometry and realistic
appearances. HumanNorm outperforms existing text-to-3D methods in both geometry
and texture quality. The project page of HumanNorm is
https://humannorm.github.io/.",CVPR
"Self-supervised landmark estimation is a challenging task that demands the
formation of locally distinct feature representations to identify sparse facial
landmarks in the absence of annotated data. To tackle this task, existing
state-of-the-art (SOTA) methods (1) extract coarse features from backbones that
are trained with instance-level self-supervised learning (SSL) paradigms, which
neglect the dense prediction nature of the task, (2) aggregate them into
memory-intensive hypercolumn formations, and (3) supervise lightweight
projector networks to naively establish full local correspondences among all
pairs of spatial features. In this paper, we introduce SCE-MAE, a framework
that (1) leverages the MAE, a region-level SSL method that naturally better
suits the landmark prediction task, (2) operates on the vanilla feature map
instead of on expensive hypercolumns, and (3) employs a Correspondence
Approximation and Refinement Block (CARB) that utilizes a simple density peak
clustering algorithm and our proposed Locality-Constrained Repellence Loss to
directly hone only select local correspondences. We demonstrate through
extensive experiments that SCE-MAE is highly effective and robust,
outperforming existing SOTA methods by large margins of approximately 20%-44%
on the landmark matching and approximately 9%-15% on the landmark detection
tasks.",CVPR
"Equipping a deep model the abaility of few-shot learning, i.e., learning
quickly from only few examples, is a core challenge for artificial
intelligence. Gradient-based meta-learning approaches effectively address the
challenge by learning how to learn novel tasks. Its key idea is learning a deep
model in a bi-level optimization manner, where the outer-loop process learns a
shared gradient descent algorithm (i.e., its hyperparameters), while the
inner-loop process leverage it to optimize a task-specific model by using only
few labeled data. Although these existing methods have shown superior
performance, the outer-loop process requires calculating second-order
derivatives along the inner optimization path, which imposes considerable
memory burdens and the risk of vanishing gradients. Drawing inspiration from
recent progress of diffusion models, we find that the inner-loop gradient
descent process can be actually viewed as a reverse process (i.e., denoising)
of diffusion where the target of denoising is model weights but the origin
data. Based on this fact, in this paper, we propose to model the gradient
descent optimizer as a diffusion model and then present a novel
task-conditional diffusion-based meta-learning, called MetaDiff, that
effectively models the optimization process of model weights from Gaussion
noises to target weights in a denoising manner. Thanks to the training
efficiency of diffusion models, our MetaDiff do not need to differentiate
through the inner-loop path such that the memory burdens and the risk of
vanishing gradients can be effectvely alleviated. Experiment results show that
our MetaDiff outperforms the state-of-the-art gradient-based meta-learning
family in few-shot learning tasks.",CVPR
"3D human generation is increasingly significant in various applications.
However, the direct use of 2D generative methods in 3D generation often results
in losing local details, while methods that reconstruct geometry from generated
images struggle with global view consistency. In this work, we introduce
Joint2Human, a novel method that leverages 2D diffusion models to generate
detailed 3D human geometry directly, ensuring both global structure and local
details. To achieve this, we employ the Fourier occupancy field (FOF)
representation, enabling the direct generation of 3D shapes as preliminary
results with 2D generative models. With the proposed high-frequency enhancer
and the multi-view recarving strategy, our method can seamlessly integrate the
details from different views into a uniform global shape. To better utilize the
3D human prior and enhance control over the generated geometry, we introduce a
compact spherical embedding of 3D joints. This allows for an effective guidance
of pose during the generation process. Additionally, our method can generate 3D
humans guided by textual inputs. Our experimental results demonstrate the
capability of our method to ensure global structure, local details, high
resolution, and low computational cost simultaneously. More results and the
code can be found on our project page at
http://cic.tju.edu.cn/faculty/likun/projects/Joint2Human.",CVPR
"Learning generalized face anti-spoofing (FAS) models against presentation
attacks is essential for the security of face recognition systems. Previous FAS
methods usually encourage models to extract discriminative features, of which
the distances within the same class (bonafide or attack) are pushed close while
those between bonafide and attack are pulled away. However, these methods are
designed based on Euclidean distance, which lacks generalization ability for
unseen attack detection due to poor hierarchy embedding ability. According to
the evidence that different spoofing attacks are intrinsically hierarchical, we
propose to learn richer hierarchical and discriminative spoofing cues in
hyperbolic space. Specifically, for unimodal FAS learning, the feature
embeddings are projected into the Poincar\'e ball, and then the hyperbolic
binary logistic regression layer is cascaded for classification. To further
improve generalization, we conduct hyperbolic contrastive learning for the
bonafide only while relaxing the constraints on diverse spoofing attacks. To
alleviate the vanishing gradient problem in hyperbolic space, a new feature
clipping method is proposed to enhance the training stability of hyperbolic
models. Besides, we further design a multimodal FAS framework with Euclidean
multimodal feature decomposition and hyperbolic multimodal feature fusion &
classification. Extensive experiments on three benchmark datasets (i.e., WMCA,
PADISI-Face, and SiW-M) with diverse attack types demonstrate that the proposed
method can bring significant improvement compared to the Euclidean baselines on
unseen attack detection. In addition, the proposed framework is also
generalized well on four benchmark datasets (i.e., MSU-MFSD, IDIAP
REPLAY-ATTACK, CASIA-FASD, and OULU-NPU) with a limited number of attack types.",CVPR
"We present NARUTO, a neural active reconstruction system that combines a
hybrid neural representation with uncertainty learning, enabling high-fidelity
surface reconstruction. Our approach leverages a multi-resolution hash-grid as
the mapping backbone, chosen for its exceptional convergence speed and capacity
to capture high-frequency local features.The centerpiece of our work is the
incorporation of an uncertainty learning module that dynamically quantifies
reconstruction uncertainty while actively reconstructing the environment. By
harnessing learned uncertainty, we propose a novel uncertainty aggregation
strategy for goal searching and efficient path planning. Our system
autonomously explores by targeting uncertain observations and reconstructs
environments with remarkable completeness and fidelity. We also demonstrate the
utility of this uncertainty-aware approach by enhancing SOTA neural SLAM
systems through an active ray sampling strategy. Extensive evaluations of
NARUTO in various environments, using an indoor scene simulator, confirm its
superior performance and state-of-the-art status in active reconstruction, as
evidenced by its impressive results on benchmark datasets like Replica and
MP3D.",CVPR
"Social interaction is a fundamental aspect of human behavior and
communication. The way individuals position themselves in relation to others,
also known as proxemics, conveys social cues and affects the dynamics of social
interaction. Reconstructing such interaction from images presents challenges
because of mutual occlusion and the limited availability of large training
datasets. To address this, we present a novel approach that learns a prior over
the 3D proxemics two people in close social interaction and demonstrate its use
for single-view 3D reconstruction. We start by creating 3D training data of
interacting people using image datasets with contact annotations. We then model
the proxemics using a novel denoising diffusion model called BUDDI that learns
the joint distribution over the poses of two people in close social
interaction. Sampling from our generative proxemics model produces realistic 3D
human interactions, which we validate through a perceptual study. We use BUDDI
in reconstructing two people in close proximity from a single image without any
contact annotation via an optimization approach that uses the diffusion model
as a prior. Our approach recovers accurate and plausible 3D social interactions
from noisy initial estimates, outperforming state-of-the-art methods. Our code,
data, and model are availableat our project website at: muelea.github.io/buddi.",CVPR
"Image signal processing (ISP) pipeline plays a fundamental role in digital
cameras, which converts raw Bayer sensor data to RGB images. However,
ISP-generated images usually suffer from imperfections due to the compounded
degradations that stem from sensor noises, demosaicing noises, compression
artifacts, and possibly adverse effects of erroneous ISP hyperparameter
settings such as ISO and gamma values. In a general sense, these ISP
imperfections can be considered as degradations. The highly complex mechanisms
of ISP degradations, some of which are even unknown, pose great challenges to
the generalization capability of deep neural networks (DNN) for image
restoration and to their adaptability to downstream tasks. To tackle the
issues, we propose a novel DNN approach to learn degradation-independent
representations (DiR) through the refinement of a self-supervised learned
baseline representation. The proposed DiR learning technique has remarkable
domain generalization capability and consequently, it outperforms
state-of-the-art methods across various downstream tasks, including blind image
restoration, object detection, and instance segmentation, as verified in our
experiments.",CVPR
"Existing approaches to unsupervised video instance segmentation typically
rely on motion estimates and experience difficulties tracking small or
divergent motions. We present VideoCutLER, a simple method for unsupervised
multi-instance video segmentation without using motion-based learning signals
like optical flow or training on natural videos. Our key insight is that using
high-quality pseudo masks and a simple video synthesis method for model
training is surprisingly sufficient to enable the resulting video model to
effectively segment and track multiple instances across video frames. We show
the first competitive unsupervised learning results on the challenging
YouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous
state-of-the-art by a large margin. VideoCutLER can also serve as a strong
pretrained model for supervised video instance segmentation tasks, exceeding
DINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.",CVPR
"In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.",CVPR
"Although polygon meshes have been a standard representation in geometry
processing, their irregular and combinatorial nature hinders their suitability
for learning-based applications. In this work, we introduce a novel learnable
mesh representation through a set of local 3D sample Points and their
associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,
which we denote PoNQ. A global mesh is directly derived from PoNQ by
efficiently leveraging the knowledge of the local quadric errors. Besides
marking the first use of QEM within a neural shape representation, our
contribution guarantees both topological and geometrical properties by ensuring
that a PoNQ mesh does not self-intersect and is always the boundary of a
volume. Notably, our representation does not rely on a regular grid, is
supervised directly by the target surface alone, and also handles open surfaces
with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ
through a learning-based mesh prediction from SDF grids and show that our
method surpasses recent state-of-the-art techniques in terms of both surface
and edge-based metrics.",CVPR
"The increasing prominence of e-commerce has underscored the importance of
Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D
realm and rely heavily on extensive data for training. Research on 3D VTON
primarily centers on garment-body shape compatibility, a topic extensively
covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion
model has now been adapted for 3D editing via multi-viewpoint editing. In this
work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating
Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless
transition from 2D to 3D VTON, we propose, for the first time, the use of only
images as editing prompts for 3D editing. To further address issues, e.g., face
blurring, garment inaccuracy, and degraded viewpoint quality during editing, we
devise a three-stage refinement strategy to gradually mitigate potential
issues. Furthermore, we introduce a new editing strategy termed Edit Recall
Reconstruction (ERR) to tackle the limitations of previous editing strategies
in leading to complex geometric changes. Our comprehensive experiments
demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D
VTON while also establishing a novel starting point for image-prompting 3D
scene editing.",CVPR
"It is well known that many open-released foundational diffusion models have
difficulty in generating images that substantially depart from average
brightness, despite such images being present in the training data. This is due
to an inconsistency: while denoising starts from pure Gaussian noise during
inference, the training noise schedule retains residual data even in the final
timestep distribution, due to difficulties in numerical conditioning in
mainstream formulation, leading to unintended bias during inference. To
mitigate this issue, certain $\epsilon$-prediction models are combined with an
ad-hoc offset-noise methodology. In parallel, some contemporary models have
adopted zero-terminal SNR noise schedules together with
$\mathbf{v}$-prediction, which necessitate major alterations to pre-trained
models. However, such changes risk destabilizing a large multitude of
community-driven applications anchored on these pre-trained models. In light of
this, our investigation revisits the fundamental causes, leading to our
proposal of an innovative and principled remedy, called One More Step (OMS). By
integrating a compact network and incorporating an additional simple yet
effective step during inference, OMS elevates image fidelity and harmonizes the
dichotomy between training and inference, while preserving original model
parameters. Once trained, various pre-trained diffusion models with the same
latent domain can share the same OMS module.",CVPR
"With the immense growth of dataset sizes and computing resources in recent
years, so-called foundation models have become popular in NLP and vision tasks.
In this work, we propose to explore foundation models for the task of keypoint
detection on 3D shapes. A unique characteristic of keypoint detection is that
it requires semantic and geometric awareness while demanding high localization
accuracy. To address this problem, we propose, first, to back-project features
from large pre-trained 2D vision models onto 3D shapes and employ them for this
task. We show that we obtain robust 3D features that contain rich semantic
information and analyze multiple candidate features stemming from different 2D
foundation models. Second, we employ a keypoint candidate optimization module
which aims to match the average observed distribution of keypoints on the shape
and is guided by the back-projected features. The resulting approach achieves a
new state of the art for few-shot keypoint detection on the KeyPointNet
dataset, almost doubling the performance of the previous best methods.",CVPR
"How to effectively explore multi-scale representations of rain streaks is
important for image deraining. In contrast to existing Transformer-based
methods that depend mostly on single-scale rain appearance, we develop an
end-to-end multi-scale Transformer that leverages the potentially useful
features in various scales to facilitate high-quality image reconstruction. To
better explore the common degradation representations from spatially-varying
rain streaks, we incorporate intra-scale implicit neural representations based
on pixel coordinates with the degraded inputs in a closed-loop design, enabling
the learned features to facilitate rain removal and improve the robustness of
the model in complex scenarios. To ensure richer collaborative representation
from different scales, we embed a simple yet effective inter-scale
bidirectional feedback operation into our multi-scale Transformer by performing
coarse-to-fine and fine-to-coarse information communication. Extensive
experiments demonstrate that our approach, named as NeRD-Rain, performs
favorably against the state-of-the-art ones on both synthetic and real-world
benchmark datasets. The source code and trained models are available at
https://github.com/cschenxiang/NeRD-Rain.",CVPR
"Text-to-image diffusion models produce high quality images but do not offer
control over individual instances in the image. We introduce InstanceDiffusion
that adds precise instance-level control to text-to-image diffusion models.
InstanceDiffusion supports free-form language conditions per instance and
allows flexible ways to specify instance locations such as simple single
points, scribbles, bounding boxes or intricate instance segmentation masks, and
combinations thereof. We propose three major changes to text-to-image models
that enable precise instance-level control. Our UniFusion block enables
instance-level conditions for text-to-image models, the ScaleU block improves
image fidelity, and our Multi-instance Sampler improves generations for
multiple instances. InstanceDiffusion significantly surpasses specialized
state-of-the-art models for each location condition. Notably, on the COCO
dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$
for box inputs, and 25.4% IoU for mask inputs.",CVPR
"Both limited annotation and domain shift are prevalent challenges in medical
image segmentation. Traditional semi-supervised segmentation and unsupervised
domain adaptation methods address one of these issues separately. However, the
coexistence of limited annotation and domain shift is quite common, which
motivates us to introduce a novel and challenging scenario: Mixed Domain
Semi-supervised medical image Segmentation (MiDSS). In this scenario, we handle
data from multiple medical centers, with limited annotations available for a
single domain and a large amount of unlabeled data from multiple domains. We
found that the key to solving the problem lies in how to generate reliable
pseudo labels for the unlabeled data in the presence of domain shift with
labeled data. To tackle this issue, we employ Unified Copy-Paste (UCP) between
images to construct intermediate domains, facilitating the knowledge transfer
from the domain of labeled data to the domains of unlabeled data. To fully
utilize the information within the intermediate domain, we propose a symmetric
Guidance training strategy (SymGD), which additionally offers direct guidance
to unlabeled data by merging pseudo labels from intermediate samples.
Subsequently, we introduce a Training Process aware Random Amplitude MixUp
(TP-RAM) to progressively incorporate style-transition components into
intermediate samples. Compared with existing state-of-the-art approaches, our
method achieves a notable 13.57% improvement in Dice score on Prostate dataset,
as demonstrated on three public datasets. Our code is available at
https://github.com/MQinghe/MiDSS .",CVPR
"Faithfully modeling the space of articulations is a crucial task that allows
recovery and generation of realistic poses, and remains a notorious challenge.
To this end, we introduce Neural Riemannian Distance Fields (NRDFs),
data-driven priors modeling the space of plausible articulations, represented
as the zero-level-set of a neural field in a high-dimensional
product-quaternion space. To train NRDFs only on positive examples, we
introduce a new sampling algorithm, ensuring that the geodesic distances follow
a desired distribution, yielding a principled distance field learning paradigm.
We then devise a projection algorithm to map any random pose onto the level-set
by an adaptive-step Riemannian optimizer, adhering to the product manifold of
joint rotations at all times. NRDFs can compute the Riemannian gradient via
backpropagation and by mathematical analogy, are related to Riemannian flow
matching, a recent generative model. We conduct a comprehensive evaluation of
NRDF against other pose priors in various downstream tasks, i.e., pose
generation, image-based pose estimation, and solving inverse kinematics,
highlighting NRDF's superior performance. Besides humans, NRDF's versatility
extends to hand and animal poses, as it can effectively represent any
articulation.",CVPR
"The widespread adoption of face recognition has led to increasing privacy
concerns, as unauthorized access to face images can expose sensitive personal
information. This paper explores face image protection against viewing and
recovery attacks. Inspired by image compression, we propose creating a visually
uninformative face image through feature subtraction between an original face
and its model-produced regeneration. Recognizable identity features within the
image are encouraged by co-training a recognition model on its high-dimensional
feature representation. To enhance privacy, the high-dimensional representation
is crafted through random channel shuffling, resulting in randomized
recognizable images devoid of attacker-leverageable texture details. We distill
our methodologies into a novel privacy-preserving face recognition method,
MinusFace. Experiments demonstrate its high recognition accuracy and effective
privacy protection. Its code is available at https://github.com/Tencent/TFace.",CVPR
"Generating human motions from textual descriptions has gained growing
research interest due to its wide range of applications. However, only a few
works consider human-scene interactions together with text conditions, which is
crucial for visual and physical realism. This paper focuses on the task of
generating human motions in 3D indoor scenes given text descriptions of the
human-scene interactions. This task presents challenges due to the
multi-modality nature of text, scene, and motion, as well as the need for
spatial reasoning. To address these challenges, we propose a new approach that
decomposes the complex problem into two more manageable sub-problems: (1)
language grounding of the target object and (2) object-centric motion
generation. For language grounding of the target object, we leverage the power
of large language models. For motion generation, we design an object-centric
scene representation for the generative model to focus on the target object,
thereby reducing the scene complexity and facilitating the modeling of the
relationship between human motions and the object. Experiments demonstrate the
better motion quality of our approach compared to baselines and validate our
design choices.",CVPR
"Realistic 3D human generation from text prompts is a desirable yet
challenging task. Existing methods optimize 3D representations like mesh or
neural fields via score distillation sampling (SDS), which suffers from
inadequate fine details or excessive training time. In this paper, we propose
an efficient yet effective framework, HumanGaussian, that generates
high-quality 3D humans with fine-grained geometry and realistic appearance. Our
key insight is that 3D Gaussian Splatting is an efficient renderer with
periodic Gaussian shrinkage or growing, where such adaptive density control can
be naturally guided by intrinsic human structures. Specifically, 1) we first
propose a Structure-Aware SDS that simultaneously optimizes human appearance
and geometry. The multi-modal score function from both RGB and depth space is
leveraged to distill the Gaussian densification and pruning process. 2)
Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS
into a noisier generative score and a cleaner classifier score, which well
addresses the over-saturation issue. The floating artifacts are further
eliminated based on Gaussian size in a prune-only phase to enhance generation
smoothness. Extensive experiments demonstrate the superior efficiency and
competitive quality of our framework, rendering vivid 3D humans under diverse
scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian",CVPR
"Current open-source Large Multimodal Models (LMMs) excel at tasks such as
open-vocabulary language grounding and segmentation but can suffer under false
premises when queries imply the existence of something that is not actually
present in the image. We observe that existing methods that fine-tune an LMM to
segment images significantly degrade their ability to reliably determine
(""see"") if an object is present and to interact naturally with humans (""say""),
a form of catastrophic forgetting. In this work, we propose a cascading and
joint training approach for LMMs to solve this task, avoiding catastrophic
forgetting of previous skills. Our resulting model can ""see"" by detecting
whether objects are present in an image, ""say"" by telling the user if they are
not, proposing alternative queries or correcting semantic errors in the query,
and finally ""segment"" by outputting the mask of the desired objects if they
exist. Additionally, we introduce a novel False Premise Correction benchmark
dataset, an extension of existing RefCOCO(+/g) referring segmentation datasets
(which we call FP-RefCOCO(+/g)). The results show that our method not only
detects false premises up to 55% better than existing approaches, but under
false premise conditions produces relative cIOU improvements of more than 31%
over baselines, and produces natural language feedback judged helpful up to 67%
of the time.",CVPR
"To synthesize high-fidelity samples, diffusion models typically require
auxiliary data to guide the generation process. However, it is impractical to
procure the painstaking patch-level annotation effort required in specialized
domains like histopathology and satellite imagery; it is often performed by
domain experts and involves hundreds of millions of patches. Modern-day
self-supervised learning (SSL) representations encode rich semantic and visual
information. In this paper, we posit that such representations are expressive
enough to act as proxies to fine-grained human labels. We introduce a novel
approach that trains diffusion models conditioned on embeddings from SSL. Our
diffusion models successfully project these features back to high-quality
histopathology and remote sensing images. In addition, we construct larger
images by assembling spatially consistent patches inferred from SSL embeddings,
preserving long-range dependencies. Augmenting real data by generating
variations of real images improves downstream classifier accuracy for
patch-level and larger, image-scale classification tasks. Our models are
effective even on datasets not encountered during training, demonstrating their
robustness and generalizability. Generating images from learned embeddings is
agnostic to the source of the embeddings. The SSL embeddings used to generate a
large image can either be extracted from a reference image, or sampled from an
auxiliary model conditioned on any related modality (e.g. class labels, text,
genomic data). As proof of concept, we introduce the text-to-large image
synthesis paradigm where we successfully synthesize large pathology and
satellite images out of text descriptions.",CVPR
"Recent advancements in large-scale pre-trained text-to-image models have led
to remarkable progress in semantic image synthesis. Nevertheless, synthesizing
high-quality images with consistent semantics and layout remains a challenge.
In this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE)
that harnesses pre-trained models to alleviate the aforementioned issues.
Specifically, we first employ the layout control map to faithfully represent
layouts in the feature space. Subsequently, we combine the layout and semantic
features in a timestep-adaptive manner to synthesize images with realistic
details. During fine-tuning, we propose the Semantic Alignment (SA) loss to
further enhance layout alignment. Additionally, we introduce the Layout-Free
Prior Preservation (LFP) loss, which leverages unlabeled data to maintain the
priors of pre-trained models, thereby improving the visual quality and semantic
consistency of synthesized images. Extensive experiments demonstrate that our
approach performs favorably in terms of visual quality, semantic consistency,
and layout alignment. The source code and model are available at
https://github.com/cszy98/PLACE/tree/main.",CVPR
"Crowd counting has achieved significant progress by training regressors to
predict instance positions. In heavily crowded scenarios, however, regressors
are challenged by uncontrollable annotation variance, which causes density map
bias and context information inaccuracy. In this study, we propose mutual
prompt learning (mPrompt), which leverages a regressor and a segmenter as
guidance for each other, solving bias and inaccuracy caused by annotation
variance while distinguishing foreground from background. In specific, mPrompt
leverages point annotations to tune the segmenter and predict pseudo head masks
in a way of point prompt learning. It then uses the predicted segmentation
masks, which serve as spatial constraint, to rectify biased point annotations
as context prompt learning. mPrompt defines a way of mutual information
maximization from prompt learning, mitigating the impact of annotation variance
while improving model accuracy. Experiments show that mPrompt significantly
reduces the Mean Average Error (MAE), demonstrating the potential to be general
framework for down-stream vision tasks.",CVPR
"We present SplattingAvatar, a hybrid 3D representation of photorealistic
human avatars with Gaussian Splatting embedded on a triangle mesh, which
renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We
disentangle the motion and appearance of a virtual human with explicit mesh
geometry and implicit appearance modeling with Gaussian Splatting. The
Gaussians are defined by barycentric coordinates and displacement on a triangle
mesh as Phong surfaces. We extend lifted optimization to simultaneously
optimize the parameters of the Gaussians while walking on the triangle mesh.
SplattingAvatar is a hybrid representation of virtual humans where the mesh
represents low-frequency motion and surface deformation, while the Gaussians
take over the high-frequency geometry and detailed appearance. Unlike existing
deformation methods that rely on an MLP-based linear blend skinning (LBS) field
for motion, we control the rotation and translation of the Gaussians directly
by mesh, which empowers its compatibility with various animation techniques,
e.g., skeletal animation, blend shapes, and mesh editing. Trainable from
monocular videos for both full-body and head avatars, SplattingAvatar shows
state-of-the-art rendering quality across multiple datasets.",CVPR
"3D correspondence, i.e., a pair of 3D points, is a fundamental concept in
computer vision. A set of 3D correspondences, when equipped with compatibility
edges, forms a correspondence graph. This graph is a critical component in
several state-of-the-art 3D point cloud registration approaches, e.g., the one
based on maximal cliques (MAC). However, its properties have not been well
understood. So we present the first study that introduces graph signal
processing into the domain of correspondence graph. We exploit the generalized
degree signal on correspondence graph and pursue sampling strategies that
preserve high-frequency components of this signal. To address time-consuming
singular value decomposition in deterministic sampling, we resort to a
stochastic approximate sampling strategy. As such, the core of our method is
the stochastic spectral sampling of correspondence graph. As an application, we
build a complete 3D registration algorithm termed as FastMAC, that reaches
real-time speed while leading to little to none performance drop. Through
extensive experiments, we validate that FastMAC works for both indoor and
outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while
maintaining high registration success rate on KITTI. Codes are publicly
available at https://github.com/Forrest-110/FastMAC.",CVPR
"In this paper, we introduce Fairy, a minimalist yet robust adaptation of
image-editing diffusion models, enhancing them for video editing applications.
Our approach centers on the concept of anchor-based cross-frame attention, a
mechanism that implicitly propagates diffusion features across frames, ensuring
superior temporal coherence and high-fidelity synthesis. Fairy not only
addresses limitations of previous models, including memory and processing
speed. It also improves temporal consistency through a unique data augmentation
strategy. This strategy renders the model equivariant to affine transformations
in both source and target images. Remarkably efficient, Fairy generates
120-frame 512x384 videos (4-second duration at 30 FPS) in just 14 seconds,
outpacing prior works by at least 44x. A comprehensive user study, involving
1000 generated samples, confirms that our approach delivers superior quality,
decisively outperforming established methods.",CVPR
"To bridge the gap between vision and language modalities, Multimodal Large
Language Models (MLLMs) usually learn an adapter that converts visual inputs to
understandable tokens for Large Language Models (LLMs). However, most adapters
generate consistent visual tokens, regardless of the specific objects of
interest mentioned in the prompt. Since these adapters distribute equal
attention to every detail in the image and focus on the entire scene, they may
increase the cognitive load for LLMs, particularly when processing complex
scenes. To alleviate this problem, we propose prompt-aware adapters. These
adapters are designed with the capability to dynamically embed visual inputs
based on the specific focus of the prompt. Specifically, prompt-aware adapters
utilize both global and local textual features to capture the most relevant
visual clues from the prompt at both coarse and fine granularity levels. This
approach significantly enhances the ability of LLMs to understand and interpret
visual content. Experiments on various visual question answering tasks, such as
counting and position reasoning, demonstrate the effectiveness of prompt-aware
adapters.",CVPR
"Before developing a Document Layout Analysis (DLA) model in real-world
applications, conducting comprehensive robustness testing is essential.
However, the robustness of DLA models remains underexplored in the literature.
To address this, we are the first to introduce a robustness benchmark for DLA
models, which includes 450K document images of three datasets. To cover
realistic corruptions, we propose a perturbation taxonomy with 36 common
document perturbations inspired by real-world document processing.
Additionally, to better understand document perturbation impacts, we propose
two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and
Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we
introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA),
which improves attention mechanisms to boost extraction of robust features.
Experiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and
M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of
115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA
achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.",CVPR
"Camouflaged vision perception is an important vision task with numerous
practical applications. Due to the expensive collection and labeling costs,
this community struggles with a major bottleneck that the species category of
its datasets is limited to a small number of object species. However, the
existing camouflaged generation methods require specifying the background
manually, thus failing to extend the camouflaged sample diversity in a low-cost
manner. In this paper, we propose a Latent Background Knowledge
Retrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To
our knowledge, our contributions mainly include: (1) For the first time, we
propose a camouflaged generation paradigm that does not need to receive any
background inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented
method with interpretability for camouflaged generation, in which we propose an
idea that knowledge retrieval and reasoning enhancement are separated
explicitly, to alleviate the task-specific challenges. Moreover, our method is
not restricted to specific foreground targets or backgrounds, offering a
potential for extending camouflaged vision perception to more diverse domains.
(3) Experimental results demonstrate that our method outperforms the existing
approaches, generating more realistic camouflage images.",CVPR
"Large language models have demonstrated impressive universal capabilities
across a wide range of open-ended tasks and have extended their utility to
encompass multimodal conversations. However, existing methods encounter
challenges in effectively handling both image and video understanding,
particularly with limited visual tokens. In this work, we introduce Chat-UniVi,
a Unified Vision-language model capable of comprehending and engaging in
conversations involving images and videos through a unified visual
representation. Specifically, we employ a set of dynamic visual tokens to
uniformly represent images and videos. This representation framework empowers
the model to efficiently utilize a limited number of visual tokens to
simultaneously capture the spatial details necessary for images and the
comprehensive temporal relationship required for videos. Moreover, we leverage
a multi-scale representation, enabling the model to perceive both high-level
semantic concepts and low-level visual details. Notably, Chat-UniVi is trained
on a mixed dataset containing both images and videos, allowing direct
application to tasks involving both mediums without requiring any
modifications. Extensive experimental results demonstrate that Chat-UniVi
consistently outperforms even existing methods exclusively designed for either
images or videos. Code is available at
https://github.com/PKU-YuanGroup/Chat-UniVi.",CVPR
"As for human avatar reconstruction, contemporary techniques commonly
necessitate the acquisition of costly data and struggle to achieve satisfactory
results from a small number of casual images. In this paper, we investigate
this task from a few-shot unconstrained photo album. The reconstruction of
human avatars from such data sources is challenging because of limited data
amount and dynamic articulated poses. For handling dynamic data, we integrate a
skinning mechanism with deep marching tetrahedra (DMTet) to form a drivable
tetrahedral representation, which drives arbitrary mesh topologies generated by
the DMTet for the adaptation of unconstrained images. To effectively mine
instructive information from few-shot data, we devise a two-phase optimization
method with few-shot reference and few-shot guidance. The former focuses on
aligning avatar identity with reference images, while the latter aims to
generate plausible appearances for unseen regions. Overall, our framework,
called HaveFun, can undertake avatar reconstruction, rendering, and animation.
Extensive experiments on our developed benchmarks demonstrate that HaveFun
exhibits substantially superior performance in reconstructing the human body
and hand. Project website: https://seanchenxy.github.io/HaveFunWeb/.",CVPR
"Contrastive Vision-Language Pre-training, known as CLIP, has shown promising
effectiveness in addressing downstream image recognition tasks. However, recent
works revealed that the CLIP model can be implanted with a downstream-oriented
backdoor. On downstream tasks, one victim model performs well on clean samples
but predicts a specific target class whenever a specific trigger is present.
For injecting a backdoor, existing attacks depend on a large amount of
additional data to maliciously fine-tune the entire pre-trained CLIP model,
which makes them inapplicable to data-limited scenarios. In this work,
motivated by the recent success of learnable prompts, we address this problem
by injecting a backdoor into the CLIP model in the prompt learning stage. Our
method named BadCLIP is built on a novel and effective mechanism in backdoor
attacks on CLIP, i.e., influencing both the image and text encoders with the
trigger. It consists of a learnable trigger applied to images and a
trigger-aware context generator, such that the trigger can change text features
via trigger-aware prompts, resulting in a powerful and generalizable attack.
Extensive experiments conducted on 11 datasets verify that the clean accuracy
of BadCLIP is similar to those of advanced prompt learning methods and the
attack success rate is higher than 99% in most cases. BadCLIP is also
generalizable to unseen classes, and shows a strong generalization capability
under cross-dataset and cross-domain settings.",CVPR
"Prompt learning has emerged as a valuable technique in enhancing
vision-language models (VLMs) such as CLIP for downstream tasks in specific
domains. Existing work mainly focuses on designing various learning forms of
prompts, neglecting the potential of prompts as effective distillers for
learning from larger teacher models. In this paper, we introduce an
unsupervised domain prompt distillation framework, which aims to transfer the
knowledge of a larger teacher model to a lightweight target model through
prompt-driven imitation using unlabeled domain images. Specifically, our
framework consists of two distinct stages. In the initial stage, we pre-train a
large CLIP teacher model using domain (few-shot) labels. After pre-training, we
leverage the unique decoupled-modality characteristics of CLIP by pre-computing
and storing the text features as class vectors only once through the teacher
text encoder. In the subsequent stage, the stored class vectors are shared
across teacher and student image encoders for calculating the predicted logits.
Further, we align the logits of both the teacher and student models via KL
divergence, encouraging the student image encoder to generate similar
probability distributions to the teacher through the learnable prompts. The
proposed prompt distillation process eliminates the reliance on labeled data,
enabling the algorithm to leverage a vast amount of unlabeled images within the
domain. Finally, the well-trained student image encoders and pre-stored text
features (class vectors) are utilized for inference. To our best knowledge, we
are the first to (1) perform unsupervised domain-specific prompt-driven
knowledge distillation for CLIP, and (2) establish a practical pre-storing
mechanism of text features as shared class vectors between teacher and student.
Extensive experiments on 11 datasets demonstrate the effectiveness of our
method.",CVPR
"This paper explores how deep learning techniques can improve visual-based
SLAM performance in challenging environments. By combining deep feature
extraction and deep matching methods, we introduce a versatile hybrid visual
SLAM system designed to enhance adaptability in challenging scenarios, such as
low-light conditions, dynamic lighting, weak-texture areas, and severe jitter.
Our system supports multiple modes, including monocular, stereo,
monocular-inertial, and stereo-inertial configurations. We also perform
analysis how to combine visual SLAM with deep learning methods to enlighten
other researches. Through extensive experiments on both public datasets and
self-sampled data, we demonstrate the superiority of the SL-SLAM system over
traditional approaches. The experimental results show that SL-SLAM outperforms
state-of-the-art SLAM algorithms in terms of localization accuracy and tracking
robustness. For the benefit of community, we make public the source code at
https://github.com/zzzzxxxx111/SLslam.",CVPR
"Advancements in 3D Gaussian Splatting have significantly accelerated 3D
reconstruction and generation. However, it may require a large number of
Gaussians, which creates a substantial memory footprint. This paper introduces
GES (Generalized Exponential Splatting), a novel representation that employs
Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer
particles to represent a scene and thus significantly outperforming Gaussian
Splatting methods in efficiency with a plug-and-play replacement ability for
Gaussian-based utilities. GES is validated theoretically and empirically in
both principled 1D setup and realistic 3D scenes.
  It is shown to represent signals with sharp edges more accurately, which are
typically challenging for Gaussians due to their inherent low-pass
characteristics. Our empirical analysis demonstrates that GEF outperforms
Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and
parabolic signals), thereby reducing the need for extensive splitting
operations that increase the memory footprint of Gaussian Splatting. With the
aid of a frequency-modulated loss, GES achieves competitive performance in
novel-view synthesis benchmarks while requiring less than half the memory
storage of Gaussian Splatting and increasing the rendering speed by up to 39%.
The code is available on the project website https://abdullahamdi.com/ges .",CVPR
"Non-isometric shape correspondence remains a fundamental challenge in
computer vision. Traditional methods using Laplace-Beltrami operator (LBO)
eigenmodes face limitations in characterizing high-frequency extrinsic shape
changes like bending and creases. We propose a novel approach of combining the
non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell
hessian with the intrinsic ones of the LBO, creating a hybrid spectral space in
which we construct functional maps. To this end, we present a theoretical
framework to effectively integrate non-orthogonal basis functions into
descriptor- and learning-based functional map methods. Our approach can be
incorporated easily into existing functional map pipelines across varying
applications and is able to handle complex deformations beyond isometries. We
show extensive evaluations across various supervised and unsupervised settings
and demonstrate significant improvements. Notably, our approach achieves up to
15% better mean geodesic error for non-isometric correspondence settings and up
to 45% improvement in scenarios with topological noise.",CVPR
"Audio-visual saliency prediction can draw support from diverse modality
complements, but further performance enhancement is still challenged by
customized architectures as well as task-specific loss functions. In recent
studies, denoising diffusion models have shown more promising in unifying task
frameworks owing to their inherent ability of generalization. Following this
motivation, a novel Diffusion architecture for generalized audio-visual
Saliency prediction (DiffSal) is proposed in this work, which formulates the
prediction problem as a conditional generative task of the saliency map by
utilizing input audio and video as the conditions. Based on the spatio-temporal
audio-visual features, an extra network Saliency-UNet is designed to perform
multi-modal attention modulation for progressive refinement of the ground-truth
saliency map from the noisy map. Extensive experiments demonstrate that the
proposed DiffSal can achieve excellent performance across six challenging
audio-visual benchmarks, with an average relative improvement of 6.3\% over the
previous state-of-the-art results by six metrics.",CVPR
"Ethical concerns surrounding copyright protection and inappropriate content
generation pose challenges for the practical implementation of diffusion
models. One effective solution involves watermarking the generated images.
However, existing methods often compromise the model performance or require
additional training, which is undesirable for operators and users. To address
this issue, we propose Gaussian Shading, a diffusion model watermarking
technique that is both performance-lossless and training-free, while serving
the dual purpose of copyright protection and tracing of offending content. Our
watermark embedding is free of model parameter modifications and thus is
plug-and-play. We map the watermark to latent representations following a
standard Gaussian distribution, which is indistinguishable from latent
representations obtained from the non-watermarked diffusion model. Therefore we
can achieve watermark embedding with lossless performance, for which we also
provide theoretical proof. Furthermore, since the watermark is intricately
linked with image semantics, it exhibits resilience to lossy processing and
erasure attempts. The watermark can be extracted by Denoising Diffusion
Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian
Shading on multiple versions of Stable Diffusion, and the results demonstrate
that Gaussian Shading not only is performance-lossless but also outperforms
existing methods in terms of robustness.",CVPR
"The audio-visual segmentation (AVS) task aims to segment sounding objects
from a given video. Existing works mainly focus on fusing audio and visual
features of a given video to achieve sounding object masks. However, we
observed that prior arts are prone to segment a certain salient object in a
video regardless of the audio information. This is because sounding objects are
often the most salient ones in the AVS dataset. Thus, current AVS methods might
fail to localize genuine sounding objects due to the dataset bias. In this
work, we present an audio-visual instance-aware segmentation approach to
overcome the dataset bias. In a nutshell, our method first localizes potential
sounding objects in a video by an object segmentation network, and then
associates the sounding object candidates with the given audio. We notice that
an object could be a sounding object in one video but a silent one in another
video. This would bring ambiguity in training our object segmentation network
as only sounding objects have corresponding segmentation masks. We thus propose
a silent object-aware segmentation objective to alleviate the ambiguity.
Moreover, since the category information of audio is unknown, especially for
multiple sounding sources, we propose to explore the audio-visual semantic
correlation and then associate audio with potential objects. Specifically, we
attend predicted audio category scores to potential instance masks and these
scores will highlight corresponding sounding instances while suppressing
inaudible ones. When we enforce the attended instance masks to resemble the
ground-truth mask, we are able to establish audio-visual semantics correlation.
Experimental results on the AVS benchmarks demonstrate that our method can
effectively segment sounding objects without being biased to salient objects.",CVPR
"Blind video quality assessment (BVQA) plays a pivotal role in evaluating and
improving the viewing experience of end-users across a wide range of
video-based platforms and services. Contemporary deep learning-based models
primarily analyze video content in its aggressively subsampled format, while
being blind to the impact of the actual spatial resolution and frame rate on
video quality. In this paper, we propose a modular BVQA model and a method of
training it to improve its modularity. Our model comprises a base quality
predictor, a spatial rectifier, and a temporal rectifier, responding to the
visual content and distortion, spatial resolution, and frame rate changes on
video quality, respectively. During training, spatial and temporal rectifiers
are dropped out with some probabilities to render the base quality predictor a
standalone BVQA model, which should work better with the rectifiers. Extensive
experiments on both professionally-generated content and user-generated content
video databases show that our quality model achieves superior or comparable
performance to current methods. Additionally, the modularity of our model
offers an opportunity to analyze existing video quality databases in terms of
their spatial and temporal complexity.",CVPR
"This paper addresses text-supervised semantic segmentation, aiming to learn a
model capable of segmenting arbitrary visual concepts within images by using
only image-text pairs without dense annotations. Existing methods have
demonstrated that contrastive learning on image-text pairs effectively aligns
visual segments with the meanings of texts. We notice that there is a
discrepancy between text alignment and semantic segmentation: A text often
consists of multiple semantic concepts, whereas semantic segmentation strives
to create semantically homogeneous segments. To address this issue, we propose
a novel framework, Image-Text Co-Decomposition (CoDe), where the paired image
and text are jointly decomposed into a set of image regions and a set of word
segments, respectively, and contrastive learning is developed to enforce
region-word alignment. To work with a vision-language model, we present a
prompt learning mechanism that derives an extra representation to highlight an
image segment or a word segment of interest, with which more effective features
can be extracted from that segment. Comprehensive experimental results
demonstrate that our method performs favorably against existing text-supervised
semantic segmentation methods on six benchmark datasets.",CVPR
"We propose a new structure-from-motion framework to recover accurate camera
poses and point clouds from unordered images. Traditional SfM systems typically
rely on the successful detection of repeatable keypoints across multiple views
as the first step, which is difficult for texture-poor scenes, and poor
keypoint detection may break down the whole SfM system. We propose a new
detector-free SfM framework to draw benefits from the recent success of
detector-free matchers to avoid the early determination of keypoints, while
solving the multi-view inconsistency issue of detector-free matchers.
Specifically, our framework first reconstructs a coarse SfM model from
quantized detector-free matches. Then, it refines the model by a novel
iterative refinement pipeline, which iterates between an attention-based
multi-view matching module to refine feature tracks and a geometry refinement
module to improve the reconstruction accuracy. Experiments demonstrate that the
proposed framework outperforms existing detector-based SfM systems on common
benchmark datasets. We also collect a texture-poor SfM dataset to demonstrate
the capability of our framework to reconstruct texture-poor scenes. Based on
this framework, we take $\textit{first place}$ in Image Matching Challenge
2023.",CVPR
"Learning from a limited amount of data, namely Few-Shot Learning, stands out
as a challenging computer vision task. Several works exploit semantics and
design complicated semantic fusion mechanisms to compensate for rare
representative features within restricted data. However, relying on naive
semantics such as class names introduces biases due to their brevity, while
acquiring extensive semantics from external knowledge takes a huge time and
effort. This limitation severely constrains the potential of semantics in
Few-Shot Learning. In this paper, we design an automatic way called Semantic
Evolution to generate high-quality semantics. The incorporation of high-quality
semantics alleviates the need for complex network structures and learning
algorithms used in previous works. Hence, we employ a simple two-layer network
termed Semantic Alignment Network to transform semantics and visual features
into robust class prototypes with rich discriminative features for few-shot
classification. The experimental results show our framework outperforms all
previous methods on six benchmarks, demonstrating a simple network with
high-quality semantics can beat intricate multi-modal modules on few-shot
classification tasks. Code is available at
https://github.com/zhangdoudou123/SemFew.",CVPR
"Indirect time-of-flight (iToF) imaging allows us to capture dense depth
information at a low cost. However, iToF imaging often suffers from multipath
interference (MPI) artifacts in the presence of scattering media, resulting in
severe depth-accuracy degradation. For instance, iToF cameras cannot measure
depth accurately through fog because ToF active illumination scatters back to
the sensor before reaching the farther target surface. In this work, we propose
a polarimetric iToF imaging method that can capture depth information robustly
through scattering media. Our observations on the principle of indirect ToF
imaging and polarization of light allow us to formulate a novel computational
model of scattering-aware polarimetric phase measurements that enables us to
correct MPI errors. We first devise a scattering-aware polarimetric iToF model
that can estimate the phase of unpolarized backscattered light. We then combine
the optical filtering of polarization and our computational modeling of
unpolarized backscattered light via scattering analysis of phase and amplitude.
This allows us to tackle the MPI problem by estimating the scattering energy
through the participating media. We validate our method on an experimental
setup using a customized off-the-shelf iToF camera. Our method outperforms
baseline methods by a significant margin by means of our scattering model and
polarimetric phase measurements.",CVPR
"Previous methods for Video Frame Interpolation (VFI) have encountered
challenges, notably the manifestation of blur and ghosting effects. These
issues can be traced back to two pivotal factors: unavoidable motion errors and
misalignment in supervision. In practice, motion estimates often prove to be
error-prone, resulting in misaligned features. Furthermore, the reconstruction
loss tends to bring blurry results, particularly in misaligned regions. To
mitigate these challenges, we propose a new paradigm called PerVFI
(Perception-oriented Video Frame Interpolation). Our approach incorporates an
Asymmetric Synergistic Blending module (ASB) that utilizes features from both
sides to synergistically blend intermediate features. One reference frame
emphasizes primary content, while the other contributes complementary
information. To impose a stringent constraint on the blending process, we
introduce a self-learned sparse quasi-binary mask which effectively mitigates
ghosting and blur artifacts in the output. Additionally, we employ a
normalizing flow-based generator and utilize the negative log-likelihood loss
to learn the conditional distribution of the output, which further facilitates
the generation of clear and fine details. Experimental results validate the
superiority of PerVFI, demonstrating significant improvements in perceptual
quality compared to existing methods. Codes are available at
\url{https://github.com/mulns/PerVFI}",CVPR
"We propose a novel self-supervised embedding to learn how actions sound from
narrated in-the-wild egocentric videos. Whereas existing methods rely on
curated data with known audio-visual correspondence, our multimodal
contrastive-consensus coding (MC3) embedding reinforces the associations
between audio, language, and vision when all modality pairs agree, while
diminishing those associations when any one pair does not. We show our approach
can successfully discover how the long tail of human actions sound from
egocentric video, outperforming an array of recent multimodal embedding
techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal
tasks.",CVPR
"We introduce DyNFL, a novel neural field-based approach for high-fidelity
re-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDAR
measurements from dynamic environments, accompanied by bounding boxes of moving
objects, to construct an editable neural field. This field, comprising
separately reconstructed static background and dynamic objects, allows users to
modify viewpoints, adjust object positions, and seamlessly add or remove
objects in the re-simulated scene. A key innovation of our method is the neural
field composition technique, which effectively integrates reconstructed neural
assets from various scenes through a ray drop test, accounting for occlusions
and transparent surfaces. Our evaluation with both synthetic and real-world
environments demonstrates that DyNFL substantially improves dynamic scene LiDAR
simulation, offering a combination of physical fidelity and flexible editing
capabilities.",CVPR
"Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.",CVPR
"With the rapid development of Multi-modal Large Language Models (MLLMs), a
number of diagnostic benchmarks have recently emerged to evaluate the
comprehension capabilities of these models. However, most benchmarks
predominantly assess spatial understanding in the static image tasks, while
overlooking temporal understanding in the dynamic video tasks. To alleviate
this issue, we introduce a comprehensive Multi-modal Video understanding
Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot
be effectively solved with a single frame. Specifically, we first introduce a
novel static-to-dynamic method to define these temporal-related tasks. By
transforming various static tasks into dynamic ones, we enable the systematic
generation of video tasks that require a broad spectrum of temporal skills,
ranging from perception to cognition. Then, guided by the task definition, we
automatically convert public video annotations into multiple-choice QA to
evaluate each task. On one hand, such a distinct paradigm allows us to build
MVBench efficiently, without much manual intervention. On the other hand, it
guarantees evaluation fairness with ground-truth video annotations, avoiding
the biased scoring of LLMs. Moreover, we further develop a robust video MLLM
baseline, i.e., VideoChat2, by progressive multi-modal training with diverse
instruction-tuning data. The extensive results on our MVBench reveal that, the
existing MLLMs are far from satisfactory in temporal understanding, while our
VideoChat2 largely surpasses these leading models by over 15% on MVBench. All
models and data are available at https://github.com/OpenGVLab/Ask-Anything.",CVPR
"Appearance-based gaze estimation has shown great promise in many applications
by using a single general-purpose camera as the input device. However, its
success is highly depending on the availability of large-scale well-annotated
gaze datasets, which are sparse and expensive to collect. To alleviate this
challenge we propose ConGaze, a contrastive learning-based framework that
leverages unlabeled facial images to learn generic gaze-aware representations
across subjects in an unsupervised way. Specifically, we introduce the
gaze-specific data augmentation to preserve the gaze-semantic features and
maintain the gaze consistency, which are proven to be crucial for effective
contrastive gaze representation learning. Moreover, we devise a novel
subject-conditional projection module that encourages a share feature extractor
to learn gaze-aware and generic representations. Our experiments on three
public gaze estimation datasets show that ConGaze outperforms existing
unsupervised learning solutions by 6.7% to 22.5%; and achieves 15.1% to 24.6%
improvement over its supervised learning-based counterpart in cross-dataset
evaluations.",CVPR
"Recent works have shown that objects discovery can largely benefit from the
inherent motion information in video data. However, these methods lack a proper
background processing, resulting in an over-segmentation of the non-object
regions into random segments. This is a critical limitation given the
unsupervised setting, where object segments and noise are not distinguishable.
To address this limitation we propose BMOD, a Background-aware Motion-guided
Objects Discovery method. Concretely, we leverage masks of moving objects
extracted from optical flow and design a learning mechanism to extend them to
the true foreground composed of both moving and static objects. The background,
a complementary concept of the learned foreground class, is then isolated in
the object discovery process. This enables a joint learning of the objects
discovery task and the object/non-object separation. The conducted experiments
on synthetic and real-world datasets show that integrating our background
handling with various cutting-edge methods brings each time a considerable
improvement. Specifically, we improve the objects discovery performance with a
large margin, while establishing a strong baseline for object/non-object
separation.",CVPR
"Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs.",EMNLP
"The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.",EMNLP
"Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method FIZZ (Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document) for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary{'}s factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems. We release the code at https://github.com/plm3332/FIZZ.",EMNLP
"We discover that many natural-language prompts can be replaced by corresponding prompts that are unintelligible to humans but that provably elicit similar behavior in language models. We call these prompts {``}evil twins{''} because they are obfuscated and uninterpretable (evil), but at the same time mimic the functionality of the original natural-language prompts (twins). Remarkably, evil twins transfer between models. We find these prompts by solving a maximum-likelihood problem which has applications of independent interest.",EMNLP
"TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).",EMNLP
"Despite the longstanding adage {''}an image is worth a thousand words,{''} generating accurate hyper-detailed image descriptions remains unsolved. Trained on short web-scraped image-text, vision-language models often generate incomplete descriptions with visual inconsistencies. We address this via a novel data-centric approach with ImageInWords (IIW), a carefully designed human-in-the-loop framework for curating hyper-detailed image descriptions. Human evaluations on IIW data show major gains compared to recent datasets (+66{\%}) and GPT-4V (+48{\%}) across comprehensiveness, specificity, hallucinations, and more. We also show that fine-tuning with IIW data improves these metrics by +31{\%} against models trained with prior work, even with only 9k samples. Lastly, we evaluate IIW models with text-to-image generation and vision-language reasoning tasks. Our generated descriptions result in the highest fidelity images, and boost compositional reasoning by up to 6{\%} on ARO, SVO-Probes, and Winoground datasets. We release the IIW-Eval benchmark with human judgement labels, object and image-level annotations from our framework, and existing image caption datasets enriched via IIW-model.",EMNLP
"This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents{'} social behaviors. Results affirm the framework{'}s effectiveness in creating adaptive agents and suggest LLM-based agents{'} potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field{'}s research and applications.",EMNLP
"Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in healthcare applications. However, the application of LLMs in the identification and analysis of depressive states remains relatively unexplored, presenting an intriguing avenue for future research. In this paper, we present an innovative approach to employ an LLM in the realm of depression detection, integrating acoustic speech information into the LLM framework for this specific application. We investigate an efficient method for automatic depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. This approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. By encoding acoustic landmarks information into LLMs, evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines.",EMNLP
"Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their prolonged training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training{---}a key factor in the costs associated with adding or customizing voices{---}often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement.",EMNLP
"Hate speech detection is a prevalent research field, yet it remains underexplored at the level of word meaning. This is significant, as terms used to convey hate often involve non-standard or novel usages which might be overlooked by commonly leveraged LMs trained on general language use. In this paper, we introduce the Hateful Word in Context Classification (\textbf{HateWiC}) task and present a dataset of {\textasciitilde}4000 WiC-instances, each labeled by three annotators. Our analyses and computational exploration focus on the interplay between the subjective nature (context-dependent connotations) and the descriptive nature (as described in dictionary definitions) of hateful word senses. HateWiC annotations confirm that hatefulness of a word in context does not always derive from the sense definition alone. We explore the prediction of both majority and individual annotator labels, and we experiment with modeling context- and sense-based inputs. Our findings indicate that including definitions proves effective overall, yet not in cases where hateful connotations vary. Conversely, including annotator demographics becomes more important for mitigating performance drop in subjective hate prediction.",EMNLP
"Hate speech is a complex and subjective phenomenon. In this paper, we present a dataset (GAZE4HATE) that provides gaze data collected in a hate speech annotation experiment. We study whether the gaze of an annotator provides predictors of their subjective hatefulness rating, and how gaze features can improve Hate Speech Detection (HSD). We conduct experiments on statistical modeling of subjective hate ratings and gaze and analyze to what extent rationales derived from hate speech models correspond to human gaze and explanations in our data. Finally, we introduce MEANION, a first gaze-integrated HSD model. Our experiments show that particular gaze features like dwell time or fixation counts systematically correlate with annotators{'} subjective hate ratings and improve predictions of text-only hate speech models.",EMNLP
"Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of {``}42{''}, we suggest using {``}2:42{''} as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.",EMNLP
"Existing debiasing techniques are typically training-based or require access to the model{'}s internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine whether structured prompting techniques can offer opportunities for fair text generation. We evaluate a comprehensive end-user-focused iterative framework of debiasing that applies System 2 thinking processes for prompts to induce logical, reflective, and critical text generation, with single, multi-step, instruction, and role-based variants. By systematically evaluating many LLMs across many datasets and different prompting strategies, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques demonstrating lower mean bias in the outputs with competitive performance on the downstream tasks. Our work offers research directions for the design and the potential of end-user-focused evaluative frameworks for LLM use.",EMNLP
"Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its essential role in product recommendation and business user profiling analysis, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as {``}how a customer uses a product{''}, and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph: category-rigidity and property-ambiguity. They limit its ability to strongly align user intents with products having the most desirable property, and to recommend useful products across diverse categories. Following these observations, we introduce a Product Recovery Benchmark featuring a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark. Our code and dataset are available at https://github.com/stayones/Usgae-Centric-Intent-Understanding.",EMNLP
"Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.",EMNLP
"The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs{'} ability to simulate political debates on topics that are important aspects of people{'}s day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model{'}s inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.",EMNLP
"Ensuring that the benefits of sign language technologies are distributed equitably among all community members is crucial. Thus, it is important to address potential biases and inequities that may arise from the design or use of these resources. Crowd-sourced sign language datasets, such as the ASL Citizen dataset, are great resources for improving accessibility and preserving linguistic diversity, but they must be used thoughtfully to avoid reinforcing existing biases.In this work, we utilize the rich information about participant demographics and lexical features present in the ASL Citizen dataset to study and document the biases that may result from models trained on crowd-sourced sign datasets. Further, we apply several bias mitigation techniques during model training, and find that these techniques reduce performance disparities without decreasing accuracy. With the publication of this work, we release the demographic information about the participants in the ASL Citizen dataset to encourage future bias mitigation work in this space.",EMNLP
"Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures (e.g., semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges (e.g., $[0,\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed *Rank-Calibration*, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score (e.g., ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",EMNLP
"Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs{'} capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model{'}s resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.",EMNLP
"Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",EMNLP
"Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",EMNLP
"Political discourse on social media often contains similar language with opposing intended meanings. For example, the phrase thoughts and prayers, is used to express sympathy for mass shooting victims, as well as satirically criticize the lack of legislative action on gun control. Understanding such discourse fully by reading only the text is difficult. However, knowledge of the social context information makes it easier. We characterize the social context required to fully understand such ambiguous discourse, by grounding the text in real-world entities, actions, and attitudes. We propose two datasets that require understanding social context and benchmark them using large pre-trained language models and several novel structured models. We show that structured models, explicitly modeling social context, outperform larger models on both tasks, but still lag significantly behind human performance. Finally, we perform an extensive analysis, to obtain further insights into the language understanding challenges posed by our social grounding tasks.",EMNLP
"The patent citation count is a good indicator of patent quality. This often generates monetary value for the inventors and organizations. However, the factors that influence a patent receiving high citations over the year are still not well understood. With the patents over the past two decades, we study the problem of patent citation prediction and formulate this as a binary classification problem. We create a semantic graph of patents based on their semantic similarities, enabling the use of Graph Neural Network (GNN)-based approaches for predicting citations. Our experimental results demonstrate the effectiveness of our GNN-based methods when applied to the semantic graph, showing that they can accurately predict patent citations using only patent text. More specifically, these methods produce up to 94{\%} recall for patents with high citations and outperform existing baselines. Furthermore, we leverage this constructed graph to gain insights and explanations for the predictions made by the GNNs.",EMNLP
"Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality. In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of these factors. We find that LLMs display strong translation capability after being fine-tuned on as few as 32 parallel sentences and that fine-tuning on a single translation direction enables translation in multiple directions. However, the choice of direction is critical: fine-tuning LLMs with only English on the target side can lead to task misinterpretation, which hinders translation into non-English languages. Problems also arise when noisy synthetic data is placed on the target side, especially when the target language is well-represented in LLM pre-training. Yet interestingly, synthesized data in an under-represented language has a less pronounced effect. Our findings suggest that when adapting LLMs to translation, the requirement on data quantity can be eased but careful considerations are still crucial to prevent an LLM from exploiting unintended data biases.",EMNLP
"The powerful generative abilities of large language models (LLMs) show potential in generating relevance labels for search applications. Previous work has found that directly asking about relevancy, such as ''*How relevant is document A to query Q?*{''}, results in suboptimal ranking. Instead, the pairwise-ranking prompting (PRP) approach produces promising ranking performance through asking about pairwise comparisons, e.g., ''*Is document A more relevant than document B to query Q?*{''}. Thus, while LLMs are effective at their ranking ability, this is not reflected in their relevance label generation.In this work, we propose a post-processing method to consolidate the relevance labels generated by an LLM with its powerful ranking abilities. Our method takes both LLM generated relevance labels and pairwise preferences. The labels are then altered to satisfy the pairwise preferences of the LLM, while staying as close to the original values as possible. Our experimental results indicate that our approach effectively balances label accuracy and ranking performance. Thereby, our work shows it is possible to combine both the ranking and labeling abilities of LLMs through post-processing.",EMNLP
"We investigate non-collaborative dialogue agents, which are expected to engage in strategic conversations with diverse users, for securing a mutual agreement that leans favorably towards the system{'}s objectives. This poses two main challenges for existing dialogue agents: 1) The inability to integrate user-specific characteristics into the strategic planning, and 2) The difficulty of training strategic planners that can be generalized to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.",EMNLP
"While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at measuring the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77{\%}, ii) the drop in correctness caused by these perturbations was affected based on their detectability.",EMNLP
"With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25{\%}). The second step involves preserving dataset diversity through a clustering process. In our experiment, CaR selected a subset containing only 1.96{\%} of Alpaca{'}s IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1{\%} in GPT-4 evaluations. Furthermore, our method utilizes small models (550M parameters) and requires only 11.2{\%} of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios.",EMNLP
"We study the presence of heteronormative biases and prejudice against interracial romantic relationships in large language models by performing controlled name-replacement experiments for the task of relationship prediction. We show that models are less likely to predict romantic relationships for (a) same-gender character pairs than different-gender pairs; and (b) intra/inter-racial character pairs involving Asian names as compared to Black, Hispanic, or White names. We examine the contextualized embeddings of first names and find that gender for Asian names is less discernible than non-Asian names. We discuss the social implications of our findings, underlining the need to prioritize the development of inclusive and equitable technology.",EMNLP
"We introduce EmphAssess, a prosodic benchmark designed to evaluate the capability of speech-to-speech models to encode and reproduce prosodic emphasis. We apply this to two tasks: speech resynthesis and speech-to-speech translation. In both cases, the benchmark evaluates the ability of the model to encode emphasis in the speech input and accurately reproduce it in the output, potentially across a change of speaker and language. As part of the evaluation pipeline, we introduce EmphaClass, a new model that classifies emphasis at the frame or word level.",EMNLP
"Large language models (LLMs) have emerged as valuable tools for enhancing textual features in various text-related tasks. Despite their superiority in capturing the lexical semantics between tokens for text analysis, our preliminary study on two popular LLMs, i.e., ChatGPT and Llama2, showcases that simply applying the news embeddings from LLMs is ineffective for fake news detection. Such embeddings only encapsulate the language styles between tokens. Meanwhile, the high-level semantics among named entities and topics, which reveal the deviating patterns of fake news, have been ignored. Therefore, we propose a topic model together with a set of specially designed prompts to extract topics and real entities from LLMs and model the relations among news, entities, and topics as a heterogeneous graph to facilitate investigating news semantics. We then propose a Generalized Page-Rank model and a consistent learning criteria for mining the local and global semantics centered on each news piece through the adaptive propagation of features across the graph. Our model shows superior performance on five benchmark datasets over seven baseline methods and the efficacy of the key ingredients has been thoroughly validated.",EMNLP
"While learning with limited labelled data can effectively deal with a lack of labels, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (i.e., non-deterministic decisions such as choice or order of samples). We propose and formalise a method to systematically investigate the effects of individual randomness factors while taking the interactions (dependence) between them into consideration. To this end, our method mitigates the effects of other factors while observing how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works led to inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format.",EMNLP
"Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs{'} instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future.",EMNLP
"In this paper, we apply a method to quantify biases associated with named entities from various countries. We create counterfactual examples with small perturbations on target-domain data instead of relying on templates or specific datasets for bias detection. On widely used classifiers for subjectivity analysis, including sentiment, emotion, hate speech, and offensive text using Twitter data, our results demonstrate positive biases related to the language spoken in a country across all classifiers studied. Notably, the presence of certain country names in a sentence can strongly influence predictions, up to a 23{\%} change in hate speech detection and up to a 60{\%} change in the prediction of negative emotions such as anger. We hypothesize that these biases stem from the training data of pre-trained language models (PLMs) and find correlations between affect predictions and PLMs likelihood in English and unknown languages like Basque and Maori, revealing distinct patterns with exacerbate correlations. Further, we followed these correlations in-between counterfactual examples from a same sentence to remove the syntactical component, uncovering interesting results suggesting the impact of the pre-training data was more important for English-speaking-country names.",EMNLP
"LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA{'}s performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here.",EMNLP
"With the growing popularity of general-purpose Large Language Models (LLMs), comes a need for more global explanations of model behaviors. Concept-based explanations arise as a promising avenue for explaining high-level patterns learned by LLMs. Yet their evaluation poses unique challenges, especially due to their non-local nature and high dimensional representation in a model{'}s hidden space. Current methods approach concepts from different perspectives, lacking a unified formalization. This makes evaluating the core measures of concepts, namely faithfulness or readability, challenging. To bridge the gap, we introduce a formal definition of concepts generalizing to diverse concept-based explanations{'} settings. Based on this, we quantify the faithfulness of a concept explanation via perturbation. We ensure adequate perturbation in the high-dimensional space for different concepts via an optimization problem. Readability is approximated via an automatic and deterministic measure, quantifying the coherence of patterns that maximally activate a concept while aligning with human understanding. Finally, based on measurement theory, we apply a meta-evaluation method for evaluating these measures, generalizable to other types of explanations or tasks as well. Extensive experimental analysis has been conducted to inform the selection of explanation evaluation measures.",EMNLP
"Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student{'}s persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher{'}s adaptive scaffolding strategies.",EMNLP
"Insight gradually becomes a crucial form of long-term memory for an agent. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce **M**ulti-**S**cale **I**nsight Agent (MSI-Agent), an embodied agent designed to improve LLMs{'} planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios.",EMNLP
"Detecting logical fallacies in texts can help users spot argument flaws, but automating this detection is not easy. Manually annotating fallacies in large-scale, real-world text data to create datasets for developing and validating detection models is costly. This paper introduces CoCoLoFa, the largest known logical fallacy dataset, containing 7,706 comments for 648 news articles, with each comment labeled for fallacy presence and type. We recruited 143 crowd workers to write comments embodying specific fallacy types (e.g., slippery slope) in response to news articles. Recognizing the complexity of this writing task, we built an LLM-powered assistant into the workers{'} interface to aid in drafting and refining their comments. Experts rated the writing quality and labeling validity of CoCoLoFa as high and reliable. BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy detection (F1=0.86) and classification (F1=0.87) performance on its test set, outperforming the state-of-the-art LLMs. Our work shows that combining crowdsourcing and LLMs enables us to more effectively construct datasets for complex linguistic phenomena that crowd workers find challenging to produce on their own.",EMNLP
"Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document{'}s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.",EMNLP
"Warning: this paper contains content that may be inappropriate or offensive.As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. In this work, we propose an automatic red teaming framework that evaluates a given black-box model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. In particular, taking text-to-image models as target models, we explore different feedback mechanisms to automatically learn effective and diverse adversarial prompts. Our experiments demonstrate that even with enhanced safety features, Stable Diffusion (SD) models are vulnerable to our adversarial prompts, raising concerns on their robustness in practical uses. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models.",EMNLP
"Language models will inevitably err in situations with which they are unfamiliar. However, by effectively communicating uncertainties, they can still guide humans toward making sound decisions in those contexts. We demonstrate this idea by developing HEAR, a system that can successfully guide humans in simulated residential environments despite generating potentially inaccurate instructions. Diverging from systems that provide users with only the instructions they generate, HEAR warns users of potential errors in its instructions and suggests corrections. This rich uncertainty information effectively prevents misguidance and reduces the search space for users. Evaluation with 80 users shows that HEAR achieves a 13{\%} increase in success rate and a 29{\%} reduction in final location error distance compared to only presenting instructions to users. Interestingly, we find that offering users possibilities to explore, HEAR motivates them to make more attempts at the task, ultimately leading to a higher success rate. To our best knowledge, this work is the first to show the practical benefits of uncertainty communication in a long-horizon sequential decision-making problem.",EMNLP
"Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5.Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.",EMNLP
"Large language models have seen widespread adoption in math problem-solving, yet for geometry problems, which often necessitate visual aids even for humans, the most advanced multi-modal models still struggle to effectively utilize image information. High-quality data is crucial for enhancing the geometric capabilities of multi-modal models, yet existing open-source datasets and related efforts are either too challenging for direct model learning or suffer from misalignment between text and images. To overcome this issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to generate relatively basic geometry problems with aligned text and images, facilitating model learning. We have produced a dataset of 4.9K geometry problems and combined it with 19K open-source data to form our GeoGPT4V dataset. Experimental results demonstrate that the GeoGPT4V dataset significantly improves the geometry performance of various models on the MathVista and MathVision benchmarks. The code is available at https://anonymous.4open.science/r/GeoGPT4V-08B2.",EMNLP
"Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained transformers, which often split entities into nonsensical fragments. Splitting entities diminishes retrieval accuracy and limits the model{'}s ability to incorporate up-to-date world knowledge not included in the training data. In this work, we enhance the LSR vocabulary with Wikipedia concepts and entities, enabling the model to resolve ambiguities more effectively and stay current with evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo) head, which leverages existing entity embeddings and an entity retrieval component that identifies entities relevant to a query or document. We use the DyVo head to generate entity weights, which are then merged with word piece weights to create joint representations for efficient indexing and retrieval using an inverted index. In experiments across three entity-rich document ranking datasets, the resulting DyVo model substantially outperforms several state-of-the-art baselines.",EMNLP
"Parameter-efficient fine-tuning (\textbf{PEFT}) is crucial for customizing Large Language Models (LLMs) with constrained resource. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for specific task tend to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose the expert-specialized fine-tuning method, which tunes the experts most relevant to downstream tasks while freezing the other experts; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing the both the training efficiency and effectiveness.",EMNLP
"Embedding models play a pivotal role in modern NLP applications such as document retrieval. However, existing embedding models are limited to encoding short documents of typically 512 tokens, restrained from application scenarios requiring long inputs. This paper explores context window extension of existing embedding models, pushing their input length to a maximum of 32,768. We begin by evaluating the performance of existing embedding models using our newly constructed LongEmbed benchmark, which includes two synthetic and four real-world tasks, featuring documents of varying lengths and dispersed target information. The benchmarking results highlight huge opportunities for enhancement in current models. Via comprehensive experiments, we demonstrate that training-free context window extension strategies can effectively increase the input length of these models by several folds. Moreover, comparison of models using Absolute Position Encoding (APE) and Rotary Position Encoding (RoPE) reveals the superiority of RoPE-based embedding models in context window extension, offering empirical guidance for future models. Our benchmark, code and trained models will be released to advance the research in long context embedding models.",EMNLP
"Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps using chain-of-thought prompting under zero-shot or few-shot settings. However, zero-shot prompting always encounters low performance, and the superior performance of few-shot prompting hinges on the manual-crafting of task-specific demonstrations one by one. In this paper, we present **RoSE** (**R**easoning with **O**rchestrated **S**treaming **E**xperiences), a general framework for solving reasoning tasks that can self-improve as it answers various reasoning questions. To enable RoSE, we describe an architecture that extends an LLM to store all answered reasoning questions and their reasoning steps in a streaming experience pool and orchestrate helpful questions from the pool to assist itself in answering new questions. To set up a question-aware orchestration mechanism, RoSE first calculates the similarity of each question in the pool with the question to be answered. Since the solution to each question in the experience pool is not always correct, RoSE will sort the questions according to their similarity with the question to be answered, and then uniformly divide them into multiple buckets. It finally extracts one question from each bucket to make the extracted questions more diverse. To make the extracted questions help RoSE answer new questions as much as possible, we introduce two other attributes of uncertainty and complexity for each question. RoSE will preferentially select the questions with low uncertainty and high complexity from each bucket. We evaluate the versatility of RoSE in various complex reasoning tasks and LLMs, such as arithmetic and commonsense reasoning, and find that it can achieve excellent performance without any labeled data and pre-set unlabeled data.",EMNLP
"Dialogue Aspect-based Sentiment Quadruple analysis (DiaASQ) extends ABSA to more complex real-world scenarios (i.e., dialogues), which makes existing generation methods encounter heightened noise and order bias challenges, leading to decreased robustness and accuracy.To address these, we propose the Segmentation-Aided multi-grained Denoising and Debiasing (SADD) method. For noise, we propose the Multi-Granularity Denoising Generation model (MGDG), achieving word-level denoising via sequence labeling and utterance-level denoising via topic-aware dialogue segmentation. Denoised Attention in MGDG integrates multi-grained denoising information to help generate denoised output.For order bias, we first theoretically analyze its direct cause as the gap between ideal and actual training objectives and propose a distribution-based solution. Since this solution introduces a one-to-many learning challenge, our proposed Segmentation-aided Order Bias Mitigation (SOBM) method utilizes dialogue segmentation to supplement order diversity, concurrently mitigating this challenge and order bias.Experiments demonstrate SADD{'}s effectiveness, achieving state-of-the-art results with a 6.52{\%} F1 improvement.",EMNLP
"Emotion significantly influences human behavior and decision-making processes. We propose a labeling methodology grounded in Plutchik{'}s Wheel of Emotions theory for emotion classification. Furthermore, we employ a Mixture of Experts (MoE) architecture to evaluate the efficacy of this labeling approach, by identifying the specific emotions that each expert learns to classify. Experimental results reveal that our methodology improves the performance of emotion classification.",EMNLP
"Event Causality Identification (ECI) aims at determining the existence of a causal relation between two events. Although recent prompt learning-based approaches have shown promising improvements on the ECI task, their performance are often subject to the delicate design of multiple prompts and the positive correlations between the main task and derivate tasks. The in-context learning paradigm provides explicit guidance for label prediction in the prompt learning paradigm, alleviating its reliance on complex prompts and derivative tasks. However, it does not distinguish between positive and negative demonstrations for analogy learning. Motivated from such considerations, this paper proposes an **I**n-**C**ontext **C**ontrastive **L**earning (ICCL) model that utilizes contrastive learning to enhance the effectiveness of both positive and negative demonstrations. Additionally, we apply contrastive learning to event pairs to better facilitate event causality identification. Our ICCL is evaluated on the widely used corpora, including the EventStoryLine and Causal-TimeBank, and results show significant performance improvements over the state-of-the-art algorithms.",EMNLP
"Best practices for high conflict conversations like counseling or customer support almost always include recommendations to paraphrase the previous speaker. Although paraphrase classification has received widespread attention in NLP, paraphrases are usually considered independent from context, and common models and datasets are not applicable to dialog settings. In this work, we investigate paraphrases across turns in dialog (e.g., Speaker 1: {``}That book is mine.{''} becomes Speaker 2: {``}That book is yours.{''}). We provide an operationalization of context-dependent paraphrases, and develop a training for crowd-workers to classify paraphrases in dialog. We introduce ContextDeP, a dataset with utterance pairs from NPR and CNN news interviews annotated for context-dependent paraphrases. To enable analyses on label variation, the dataset contains 5,581 annotations on 600 utterance pairs. We present promising results with in-context learning and with token classification models for automatic paraphrase detection in dialog.",EMNLP
"Language models learn rare syntactic phenomena, but the extent to which this is attributable to generalization vs. memorization is a major open question. To that end, we iteratively trained transformer language models on systematically manipulated corpora which were human-scale in size, and then evaluated their learning of a rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction ({``}a beautiful five days{''}). We compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which AANN sentences were removed. We found that AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., {``}a few days{''}). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena. Data and code: https://github.com/kanishkamisra/aannalysis.",EMNLP
"Data annotation and synthesis generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation and synthesis. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation and synthesis. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.",EMNLP
"Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even if not being trained explicitly for translation. Yet, they still struggle with translating low-resource languages. As supported by our experiments, a bilingual dictionary between the source and the target language could help. Motivated by the fact that multilingual training effectively improves cross-lingual performance, we show that a chained multilingual dictionary with words expressed in more languages can provide more information to better enhance the LLM translation. To this end, we present a novel framework, CoD, Chain-of-Dictionary Prompting, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Experiments indicate that ChatGPT and InstructGPT still have room for improvement in translating many language pairs. And CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot in-context learning for low-resource languages. Using CoD helps ChatGPT to obviously surpass the SOTA translator NLLB 3.3B.",EMNLP
"Fine-tuning large language models (LLMs) has achieved remarkable performance across various natural language processing tasks, yet it demands more and more memory as model sizes keep growing. To address this issue, the recently proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs using only forward passes, thereby avoiding the need for a backpropagation graph. However, significant performance drops and a high risk of divergence have limited their widespread adoption. In this paper, we propose the Adaptive Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed to improve the performance and convergence of the ZO methods. To enhance dimension-dependent ZO estimation accuracy, we introduce a fast-forward, low-parameter tensorized adapter. To tackle the frequently observed divergence issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number schedule that guarantees convergence. Detailed theoretical analysis and extensive experimental results on Roberta-Large and Llama-2-7B models substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory efficiency, and convergence speed.",EMNLP
"Pre-trained language models, trained on large-scale corpora, demonstrate strong generalizability across various NLP tasks. Fine-tuning these models for specific tasks typically involves updating all parameters, which is resource-intensive. Parameter-efficient fine-tuning (PEFT) methods, such as the popular LoRA family, introduce low-rank matrices to learn only a few parameters efficiently. However, during inference, the product of these matrices updates all pre-trained parameters, complicating tasks like knowledge editing that require selective updates. We propose a novel PEFT method, which conducts \textbf{r}ow and c\textbf{o}lumn-wise spar\textbf{se} \textbf{lo}w-\textbf{r}ank \textbf{a}daptation (RoseLoRA), to address this challenge. RoseLoRA identifies and updates only the most important parameters for a specific task, maintaining efficiency while preserving other model knowledge. By adding a sparsity constraint on the product of low-rank matrices and converting it to row and column-wise sparsity, we ensure efficient and precise model updates. Our theoretical analysis guarantees the lower bound of the sparsity with respective to the matrix product. Extensive experiments on five benchmarks across twenty datasets demonstrate that RoseLoRA outperforms baselines in both general fine-tuning and knowledge editing tasks.",EMNLP
"Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often struggle with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly.",EMNLP
"Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with $N=2,624$ participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights.",EMNLP
"Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex Reinforcement Learning from Human Feedback (RLHF). Despite its promising efficacy, DPO faces a notable drawback: {``}verbosity{''}, a common over-optimization phenomenon also observed in RLHF. While previous studies mainly attributed verbosity to biased labels within the data, we propose that the issue also stems from an inherent algorithmic length reliance in DPO. Specifically, we suggest that the discrepancy between sequence-level Kullback{--}Leibler (KL) divergences between chosen and rejected sequences, used in DPO, results in overestimated or underestimated rewards due to varying token lengths. Empirically, we utilize datasets with different label lengths to demonstrate the presence of biased rewards. We then introduce an effective downsampling approach, named SamPO, to eliminate potential length reliance. Our experimental evaluations, conducted across three LLMs of varying scales and a diverse array of conditional and open-ended benchmarks, highlight the efficacy of SamPO in mitigating verbosity, achieving improvements of 5{\%} to 12{\%} over DPO through debaised rewards. Our code can be accessed at: https://github.com/LuJunru/SamPO/.",EMNLP
"The cross-cultural adaptation of recipes is an important application of identifying and bridging cultural differences in language. The challenge lies in retaining the essence of the original recipe while also aligning with the writing and dietary habits of the target culture. Information Retrieval (IR) offers a way to address the challenge because it retrieves results from the culinary practices of the target culture while maintaining relevance to the original recipe. We introduce a novel task about cross-cultural recipe retrieval and present a unique Chinese-English cross-cultural recipe retrieval benchmark. Our benchmark is manually annotated under limited resource, utilizing various retrieval models to generate a pool of candidate results for manual annotation. The dataset provides retrieval samples that are culturally adapted but textually diverse, presenting greater challenges. We propose CARROT, a plug-and-play cultural-aware recipe information retrieval framework that incorporates cultural-aware query rewriting and re-ranking methods and evaluate it both on our benchmark and intuitive human judgments. The results show that our framework significantly enhances the preservation of the original recipe and its cultural appropriateness for the target culture. We believe these insights will significantly contribute to future research on cultural adaptation.",EMNLP
"The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model{'}s generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RAFE on three medical VQA datasets, achieving an average improvement of 20.8{\%} in factual accuracy.",EMNLP
"The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data{'}s transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to time-series baselines, but not compared to traditional trading signals, across various cryptocurrencies and market conditions. Our code and data are available at \url{https://github.com/Xtra-Computing/CryptoTrade}",EMNLP
"With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",EMNLP
"Parsing documents from pixels, such as pictures and scanned PDFs, into hierarchical structures is extensively demanded in the daily routines of data storage, retrieval and understanding. However, previously the research on this topic has been largely hindered since most existing datasets are small-scale, or contain documents of only a single type, which are characterized by a lack of document diversity. Moreover, there is a significant discrepancy in the annotation standards across datasets. In this paper, we introduce a large and diverse document hierarchy parsing (DHP) dataset to compensate for the data scarcity and inconsistency problem. We aim to set a new standard as a more practical, long-standing benchmark. Meanwhile, we present a new DHP framework designed to grasp both fine-grained text content and coarse-grained pattern at layout element level, enhancing the capacity of pre-trained text-layout models in handling the multi-page and multi-level challenges in DHP. Through exhaustive experiments, we validate the effectiveness of our proposed dataset and method.",EMNLP
"The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks{---}HumanEval, MBPP, and EvalPlus{---}attests to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at https://github.com/ChiYeungLaw/AMR-Evol.",EMNLP
"Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.",EMNLP
"This work suggests fundamentally rethinking the current practice of pruning large language models (LLMs). The way it is done is by divide and conquer: split the model into submodels, sequentially prune them, and reconstruct predictions of the dense counterparts on small calibration data one at a time; the final model is obtained simply by putting the resulting sparse submodels together. While this approach enables pruning under memory constraints, it generates high reconstruction errors. In this work, we first present an array of reconstruction techniques that can significantly reduce this error by more than 90{\%}. Unwittingly, however, we discover that minimizing reconstruction error is not always ideal and can overfit the given calibration data, resulting in rather increased language perplexity and poor performance at downstream tasks. We find out that a strategy of self-generating calibration data can mitigate this trade-off between reconstruction and generalization, suggesting new directions in the presence of both benefits and pitfalls of reconstruction for pruning LLMs.",EMNLP
"The advent of transformers has fueled progress in machine translation. More recently large language models (LLMs) have come to the spotlight thanks to their generality and strong performance in a wide range of language tasks, including translation. Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot. We also demonstrate that injection of minimal background information, which is easy with an LLM, brings further performance gains, especially on challenging technical subject-matter. This highlights LLMs{'} potential for building next generation of massively multilingual, context-aware and terminologically accurate SiMT systems that require no resource-intensive training or fine-tuning.",EMNLP
"Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1{\%} variation in paper decisions due to reviewers{'} biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms.",EMNLP
"Conversational search requires accurate interpretation of user intent from complex multi-turn contexts. This paper presents ChatRetriever, which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval. To achieve this, we propose a simple and effective dual-learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high-quality conversational instruction tuning data. Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever significantly outperforms existing conversational dense retrievers, achieving state-of-the-art performance on par with LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits superior robustness in handling diverse conversational contexts. Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction.",EMNLP
"Large language models (LLMs) have shown promising abilities as cost-effective and reference-free evaluators for assessing language generation quality. In particular, pairwise LLM evaluators, which compare two generated texts and determine the preferred one, have been employed in a wide range of applications. However, LLMs exhibit preference biases and worrying sensitivity to prompt designs. In this work, we first reveal that the predictive preference of LLMs can be highly brittle and skewed, even with semantically equivalent instructions. We find that fairer predictive preferences from LLMs consistently lead to judgments that are better aligned with humans. Motivated by this phenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt Optimization framework, ZEPO, which aims to produce fairer preference decisions and improve the alignment of LLM evaluators with human judgments. To this end, we propose a zero-shot learning objective based on the preference decision fairness. ZEPO demonstrates substantial performance improvements over state-of-the-art LLM evaluators, without requiring labeled data, on representative meta-evaluation benchmarks. Our findings underscore the critical correlation between preference fairness and human alignment, positioning ZEPO as an efficient prompt optimizer for bridging the gap between LLM evaluators and human judgments.",EMNLP
"Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods.",EMNLP
"In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language Models (LLMs), existing works are highly dependent on large-scale labeled support sets, not always feasible in practical scenarios. To refine this approach, we focus primarily on an innovative selective annotation mechanism, which precedes the standard demonstration retrieval. We introduce the Language Model-based Determinant Point Process (LM-DPP) that simultaneously considers the uncertainty and diversity of unlabeled instances for optimal selection. Consequently, this yields a subset for annotation that strikes a trade-off between the two factors. We apply LM-DPP to various language models, including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2 Generation datasets demonstrate that LM-DPP can effectively select canonical examples. Further analysis reveals that LLMs benefit most significantly from subsets that are both low uncertainty and high diversity.",EMNLP
"Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help. We provide a two-fold explanation by analyzing tokens from each modality. First, we demonstrate that image tokens possess significantly different semantics compared to text tokens, rendering pre-trained language models no more effective in modeling them than randomly initialized ones. Second, the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation of language models{'} capability.",EMNLP
"Question Under Discussion (QUD) is a discourse framework that uses implicit questions to reveal discourse relationships between sentences. In QUD parsing, each sentence is viewed as an answer to a question triggered by an anchor sentence in prior context. The resulting QUD structure is required to conform to several theoretical criteria like answer compatibility(how well the question is answered), making QUD parsing a challenging task. Previous works construct QUD parsers in a pipelined manner (i.e. detect the trigger sentence in context and then generate the question). However, these parsers lack a holistic view of the task and can hardly satisfy all the criteria. In this work, we introduce QUDSELECT, a joint-training framework that selectively decodes the QUD dependency structures considering the QUD criteria criteria. Using instruction-tuning, we train models to simultaneously predict the anchor sentence and generate the associated question. To explicitly incorporate the criteria, we adopt a selective decoding strategy of sampling multiple QUD candidates during inference, followed by selecting the best one with criteria scorers. Our method outperforms the state-of-the-art baseline models by 9{\%} in human evaluation and 4{\%} in automatic evaluation, demonstrating the effectiveness of our framework. Code and data are in https://github.com/asuvarna31/qudselect.",EMNLP
No abstract found,EMNLP
"Recent advances in foundation models have emphasized the need to align pre-trained models with specialized domains using small, curated datasets. Studies on these foundation models underscore the importance of low-data training and fine-tuning. This topic, well-known in natural language processing (NLP), has also gained increasing attention in the emerging field of scientific machine learning (SciML). To address the limitations of low-data training and fine-tuning, we draw inspiration from Heavy-Tailed Self-Regularization (HT-SR) theory, analyzing the shape of empirical spectral densities (ESDs) and revealing an imbalance in training quality across different model layers. To mitigate this issue, we adapt a recently proposed layer-wise learning rate scheduler, TempBalance, which effectively balances training quality across layers and enhances low-data training and fine-tuning for both NLP and SciML tasks. Notably, TempBalance demonstrates increasing performance gains as the amount of available tuning data decreases. Comparative analyses further highlight the effectiveness of TempBalance and its adaptability as an {``}add-on{''} method for improving model performance.",EMNLP
"Aligning language models (LMs) based on human-annotated preference data is a crucial step in obtaining practical and performant LM-based systems. However, multilingual human preference data are difficult to obtain at scale, making it challenging to extend this framework to diverse languages. In this work, we evaluate a simple approach for zero-shot cross-lingual alignment, where a reward model is trained on preference data in one source language and directly applied to other target languages. On summarization and open-ended dialog generation, we show that this method is consistently successful under comprehensive evaluation settings, including human evaluation: cross-lingually aligned models are preferred by humans over unaligned models on up to {\textgreater}70{\%} of evaluation instances. We moreover find that a different-language reward model sometimes yields better aligned models than a same-language reward model. We also identify best practices when there is no language-specific data for even supervised finetuning, another component in alignment.",EMNLP
"Pre-trained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in-domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving state-of-the-art performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurations{---}such as parameter sizes, pre-training duration, and alignment processes{---}on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. We evaluate over 15 different backbone LLMs and non-LLMs. Our findings reveal that larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field.",EMNLP
"Conventional Knowledge Graph Reasoning (KGR) models learn the embeddings of KG components over the structure of KGs, but their performances are limited when the KGs are severely incomplete. Recent LLM-enhanced KGR models input KG structural information into LLMs. However, they require fine-tuning on open-source LLMs and are not applicable to closed-source LLMs. Therefore, in this paper, to leverage the knowledge in LLMs without fine-tuning to assist and enhance conventional KGR models, we propose a new three-stage pipeline, including knowledge alignment, KG reasoning and entity reranking. Specifically, in the alignment stage, we propose three strategies to align the knowledge in LLMs to the KG schema by explicitly associating unconnected nodes with semantic relations. Based on the enriched KGs, we train structure-aware KGR models to integrate aligned knowledge to original knowledge existing in KGs. In the reranking stage, after obtaining the results of KGR models, we rerank the top-scored entities with LLMs to recall correct answers further. Experiments show our pipeline can enhance the KGR performance in both incomplete and general situations. Code and datasets are available.",EMNLP
"Recently, tool use with LLMs has become one of the primary research topics as it can help LLM generate truthful and helpful responses. Existing studies on tool use with LLMs primarily focus on enhancing the tool-calling ability of LLMs. In practice, like chat assistants, LLMs are also required to align with human values in the context of tool use. Specifically, LLMs should refuse to answer unsafe tool use relevant instructions and insecure tool responses to ensure their reliability and harmlessness. At the same time, LLMs should demonstrate autonomy in tool use to reduce the costs associated with tool calling. To tackle this issue, we first introduce the principle that LLMs should follow in tool use scenarios: H2A. The goal of H2A is to align LLMs with **helpfulness**, **harmlessness**, and **autonomy**. In addition, we propose ToolAlign, a dataset comprising instruction-tuning data and preference data to align LLMs with the H2A principle for tool use. Based on ToolAlign, we develop LLMs by supervised fine-tuning and preference learning, and experimental results demonstrate that the LLMs exhibit remarkable tool-calling capabilities, while also refusing to engage with harmful content, and displaying a high degree of autonomy in tool utilization. The code and datasets are available at: https://github.com/zhiyuanc2001/ToolAlign.",EMNLP
"The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the quality of this data is challenging due to its sheer volume and the absence of sample-level quality annotations and enhancements. In this paper, we introduce DecorateLM, a data engineering method designed to refine the pretraining corpus through data rating, tagging and editing. Specifically, DecorateLM rates texts against quality criteria, tags texts with hierarchical labels, and edits texts into a more formalized format. Due to the massive size of the pretraining corpus, adopting an LLM for decorating the entire corpus is less efficient. Therefore, to balance performance with efficiency, we curate a meticulously annotated training corpus for DecorateLM using a large language model and distill data engineering expertise into a compact 1.2 billion parameter small language model (SLM). We then apply DecorateLM to enhance 100 billion tokens of the training corpus, selecting 45 billion tokens that exemplify high quality and diversity for the further training of another 1.2 billion parameter LLM. Our results demonstrate that employing such high-quality data can significantly boost model performance, showcasing a powerful approach to enhance the quality of the pretraining corpus.",EMNLP
"When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these {\_}lookback ratio{\_} features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector{---}**Lookback Lens**{---}is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6{\%} in the XSum summarization task.",EMNLP
"Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the {''}alignment tax{''}{--}a compromise where enhancements in alignment within one objective (e.g., harmlessness) can diminish performance in others (e.g., helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the {''}3H{''} (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.",EMNLP
"The Matthew effect is a big challenge in Recommender Systems (RSs), where popular items tend to receive increasing attention, while less popular ones are often overlooked, perpetuating existing disparities. Although many existing methods attempt to mitigate Matthew effect in the static or quasi-static recommendation scenarios, such issue will be more pronounced as users engage with the system over time. To this end, we propose a novel framework, Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation (HiCore), aiming to address Matthew effect in the Conversational Recommender System (CRS) involving the dynamic user-system feedback loop. It devotes to learn multi-level user interests by building a set of hypergraphs (i.e., item-, entity-, word-oriented multiple-channel hypergraphs) to alleviate the Matthew effec. Extensive experiments on four CRS-based datasets showcase that HiCore attains a new state-of-the-art performance, underscoring its superiority in mitigating the Matthew effect effectively. Our code is available at https://github.com/zysensmile/HiCore.",EMNLP
No abstract found,EMNLP
"Multiple-choice visual question answering (VQA) is to automatically choose a correct answer from a set of choices after reading an image. Existing efforts have been devoted to a separate generation of an image-related question, a correct answer, or challenge distractors. By contrast, we turn to a holistic generation and optimization of questions, answers, and distractors (QADs) in this study. This integrated generation strategy eliminates the need for human curation and guarantees information consistency. Furthermore, we first propose to put the spotlight on different image regions to diversify QADs. Accordingly, a novel framework ReBo is formulated in this paper. ReBo cyclically generates each QAD based on a recurrent multimodal encoder, and each generation is focusing on a different area of the image compared to those already concerned by the previously generated QADs. In addition to traditional VQA comparisons with state-of-the-art approaches, we also validate the capability of ReBo in generating augmented data to benefit VQA models.",EMNLP
"The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks that use embeddings, such as image-to-text or text-to-image retrieval, have been largely ignored from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain.",EMNLP
"Large language models (LLMs) are capable of producing high quality information at unprecedented rates. As these models continue to entrench themselves in society, the content they produce will become increasingly pervasive in databases that are, in turn, incorporated into the pre-training data, fine-tuning data, retrieval data, etc. of other language models. In this paper we formalize the idea of a communication network of LLMs and introduce a method for representing the perspective of individual models within a collection of LLMs. Given these tools we systematically study information diffusion in the communication network of LLMs in various simulated settings.",EMNLP
"A multimodal large language model MLLMs may struggle with answering visual-based (personal) entity questions (VEQA), such as {''}who is A?{''} or {''}who is A that B is talking to?{''} for various reasons, e.g., the absence of the name of A in the caption or the inability of MLLMs to recognize A, particularly for less common entities. Furthermore, even if the MLLMs can identify A, it may refrain from answering due to privacy concerns. In this paper, we introduce a novel method called Matching-Augmented Reasoning (MAR) to enhance VEQA. Given a collection of visual objects with captions, MAR preprocesses each object individually, identifying faces, names, and their alignments within the object. It encodes this information and stores their vector representations in vector databases. When handling VEQA, MAR retrieves matching faces and names and organizes these entities into a matching graph. MAR then derives the answer to the query by reasoning over this matching graph. Extensive experiments show that MAR significantly improves VEQA compared with the state-of-the-art methods using MLLMs.",EMNLP
"Large language models (LLMs) have demonstrated impressive capabilities, but still suffer from inconsistency issues (e.g. LLMs can react differently to disturbances like rephrasing or inconsequential order change). In addition to these inconsistencies, we also observe that LLMs, while capable of solving hard problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy inconsistency, we develop the ConsisEval benchmark, where each entry comprises a pair of questions with a strict order of difficulty. Furthermore, we introduce the concept of consistency score to quantitatively measure this inconsistency and analyze the potential for improvement in consistency by relative consistency score. Based on comprehensive experiments across a variety of existing models, we find: (1) GPT-4 achieves the highest consistency score of 92.2{\%} but is still inconsistent to specific questions due to distraction by redundant information, misinterpretation of questions, etc.; (2) models with stronger capabilities typically exhibit higher consistency, but exceptions also exist; (3) hard data enhances consistency for both fine-tuning and in-context learning. Our data and code will be publicly available on GitHub.",EMNLP
"Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the **I**terative step-level **P**rocess **R**efinement **(IPR)** framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical finds highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.",EMNLP
"Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children{'}s reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 45{\%} to 100{\%} increase in precise accuracy across open and commercial LLMs evaluated, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content.",EMNLP
"Cross-domain Named Entity Recognition (CDNER) is crucial for Knowledge Graph (KG) construction and natural language processing (NLP), enabling learning from source to target domains with limited data. Previous studies often rely on manually collected entity-relevant sentences from the web or attempt to bridge the gap between tokens and entity labels across domains. These approaches are time-consuming and inefficient, as these data are often weakly correlated with the target task and require extensive pre-training.To address these issues, we propose automatically generating task-oriented knowledge (GTOK) using large language models (LLMs), focusing on the reasoning process of entity extraction. Then, we employ task-oriented pre-training (TOPT) to facilitate domain adaptation. Additionally, current cross-domain NER methods often lack explicit explanations for their effectiveness. Therefore, we introduce the concept of information density to better evaluate the model{'}s effectiveness before performing entity recognition.We conduct systematic experiments and analyses to demonstrate the effectiveness of our proposed approach and the validity of using information density for model evaluation.",EMNLP
"Retrieval-Augmented Generative (RAG) models enhance Large Language Models (LLMs) by integrating external knowledge bases, improving their performance in applications like fact-checking and information searching. In this paper, we demonstrate a security threat where adversaries can exploit the openness of these knowledge bases by injecting deceptive content into the retrieval database, intentionally changing the model{'}s behavior. This threat is critical as it mirrors real-world usage scenarios where RAG systems interact with publicly accessible knowledge bases, such as web scrapings and user-contributed data pools. To be more realistic, we target a realistic setting where the adversary has no knowledge of users{'} queries, knowledge base data, and the LLM parameters. We demonstrate that it is possible to exploit the model successfully through crafted content uploads with access to the retriever. Our findings emphasize an urgent need for security measures in the design and deployment of RAG systems to prevent potential manipulation and ensure the integrity of machine-generated content.",EMNLP
"Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG{'}s representation, and significantly improve the performance.",EMNLP
"Large Language Models (LLMs) have transformed machine learning but raised significant legal concerns due to their potential to produce text that infringes on copyrights, resulting in several high-profile lawsuits. The legal landscape is struggling to keep pace with these rapid advancements, with ongoing debates about whether generated text might plagiarize copyrighted materials. Current LLMs may infringe on copyrights or overly restrict non-copyrighted texts, leading to these challenges: (i) the need for a comprehensive evaluation benchmark to assess copyright compliance from multiple aspects; (ii) evaluating robustness against safeguard bypassing attacks; and (iii) developing effective defenses targeted against the generation of copyrighted text.To tackle these challenges, we introduce a curated dataset to evaluate methods, test attack strategies, and propose a lightweight, real-time defense mechanism to prevent the generation of copyrighted text, ensuring the safe and lawful use of LLMs. Our experiments demonstrate that current LLMs frequently output copyrighted text, and that jailbreaking attacks can significantly increase the volume of copyrighted output. Our proposed defense mechanism substantially reduces the volume of copyrighted text generated by LLMs by effectively refusing malicious requests.",EMNLP
"Soccer is a globally popular sport with a vast audience, in this paper, we consider constructing an automatic soccer game commentary model to improve the audiences{'} viewing experience. In general, we make the following contributions: *First*, observing the prevalent video-text misalignment in existing datasets, we manually annotate timestamps for 49 matches, establishing a more robust benchmark for soccer game commentary generation, termed as *SN-Caption-test-align*; *Second*, we propose a multi-modal temporal alignment pipeline to automatically correct and filter the existing dataset at scale, creating a higher-quality soccer game commentary dataset for training, denoted as *MatchTime*; *Third*, based on our curated dataset, we train an automatic commentary generation model, named **MatchVoice**. Extensive experiments and ablation studies have demonstrated the effectiveness of our alignment pipeline, and training model on the curated datasets achieves state-of-the-art performance for commentary generation, showcasing that better alignment can lead to significant performance improvements in downstream tasks.",EMNLP
"Recent advancements in State Space Models (SSMs) have attracted significant interest, particularly in models optimized for parallel training and handling long-range dependencies. Architectures like Mamba have scaled to billions of parameters with selective SSM. To facilitate broader applications using Mamba, exploring its efficiency is crucial. While token reduction techniques offer a straightforward post-training strategy, we find that applying existing methods directly to SSMs leads to substantial performance drops. Through insightful analysis, we identify the reasons for this failure and the limitations of current techniques. In response, we propose a tailored, unified post-training token reduction method for SSMs. Our approach integrates token importance and similarity, thus taking advantage of both pruning and merging, to devise a fine-grained intra-layer token reduction strategy. Extensive experiments show that our method improves the average accuracy by 5.7{\%} to 13.1{\%} on six benchmarks with Mamba-2 compared to existing methods, while significantly reducing computational demands and memory requirements.",EMNLP
"Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with multiple roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent{'}s multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8{\%} and 20.7{\%}, respectively.",EMNLP
"The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose \textbf{M}odel \textbf{E}xclusive \textbf{T}ask \textbf{A}rithmetic for merging \textbf{GPT}-scale models (MetaGPT) which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs{'} local linearity and task vectors{'} orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs. Extensive experiments demonstrate that MetaGPT leads to improvement of task arithmetic and achieves state-of-the-art performance on multiple tasks.",EMNLP
"Event causality identification (ECI), a process that extracts causal relations between events from text, is crucial for distinguishing causation from correlation. Traditional approaches to ECI have primarily utilized linguistic patterns and multi-hop relational inference, risking false causality identification due to informal usage of causality and specious graphical inference. In this paper, we adopt the Rubin Causal Model to identify event causality: given two temporally ordered events, we see the first event as the treatment and the second one as the observed outcome. Determining their causality involves manipulating the treatment and estimating the resultant change in the likelihood of the outcome. Given that it is only possible to implement manipulation conceptually in the text domain, as a work-around, we try to find a twin for the protagonist from existing corpora. This twin should have identical life experiences with the protagonist before the treatment but undergoes an intervention of treatment. However, the practical difficulty of locating such a match limits its feasibility. Addressing this issue, we use the synthetic control method to generate such a twin{'} from relevant historical data, leveraging text embedding synthesis and inversion techniques. This approach allows us to identify causal relations more robustly than previous methods, including GPT-4, which is demonstrated on a causality benchmark, COPES-hard.",EMNLP
"Protein Language Models traditionally depend on Multiple Sequence Alignments (MSA) to incorporate evolutionary knowledge. However, MSA-based approaches suffer from substantial computational overhead and generally underperform in generalizing to de novo proteins. This study reevaluates the role of MSA, proposing it as a retrieval augmentation method and questioning the necessity of sequence alignment. We show that a simple alternative, Retrieved Sequence Augmentation (RSA), can enhance protein representation learning without the need for alignment and cumbersome preprocessing. RSA surpasses MSA Transformer by an average of 5{\%} in both structural and property prediction tasks while being 373 times faster. Additionally, RSA demonstrates enhanced transferability for predicting de novo proteins. This methodology addresses a critical need for efficiency in protein prediction and can be rapidly employed to identify homologous sequences, improve representation learning, and enhance the capacity of Large Language Models to interpret protein structures.",EMNLP
"Large Vision-Language Models (LVLMs) have shown remarkable performance on many visual-language tasks. However, these models still suffer from \textit{multimodal hallucination}, which means the generation of objects or content that violates the images. Many existing work detects hallucination by directly judging whether an object exists in an image, overlooking the association between the object and semantics. To address this issue, we propose Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding (HELPD). This framework incorporates hallucination feedback at both object and sentence semantic levels. Remarkably, even with a marginal degree of training, this approach can alleviate over 15{\%} of hallucination. Simultaneously, HELPD penalizes the output logits according to the image attention window to avoid being overly affected by generated text. HELPD can be seamlessly integrated with any LVLMs. Our experiments demonstrate that the proposed framework yields favorable results across multiple hallucination benchmarks. It effectively mitigates hallucination for different LVLMs and concurrently improves their text generation quality.",EMNLP
"Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of {`}non-human{'} agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs in this setup remain unattested and underexplored. In this work, we study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50{\%} compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82{\%} on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.",EMNLP
"Language models can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we{'}ve observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware Adversarial Attack (DA$^3$) method. DA$^3$ considers the distribution shifts of adversarial examples to improve attacks{'} effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for the attack task. We conduct experiments on four widely used datasets to validate the attack effectiveness and transferability of adversarial examples generated by DA$^3$ against both the white-box BERT-base and RoBERTa-base models and the black-box LLaMA2-7b model.",EMNLP
"In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.",EMNLP
"Sentiment classification (SC) often suffers from low-resource challenges such as domain-specific contexts, imbalanced label distributions, and few-shot scenarios. The potential of the diffusion language model (LM) for textual data augmentation (DA) remains unexplored, moreover, textual DA methods struggle to balance the diversity and consistency of new samples. Most DA methods either perform logical modifications or rephrase less important tokens in the original sequence with the language model. In the context of SC, strong emotional tokens could act critically on the sentiment of the whole sequence. Therefore, contrary to rephrasing less important context, we propose DiffusionCLS to leverage a diffusion LM to capture in-domain knowledge and generate pseudo samples by reconstructing strong label-related tokens. This approach ensures a balance between consistency and diversity, avoiding the introduction of noise and augmenting crucial features of datasets. DiffusionCLS also comprises a Noise-Resistant Training objective to help the model generalize. Experiments demonstrate the effectiveness of our method in various low-resource scenarios including domain-specific and domain-general problems. Ablation studies confirm the effectiveness of our framework{'}s modules, and visualization studies highlight optimal deployment conditions, reinforcing our conclusions.",EMNLP
"While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated by the research of retrieval-augmented generation in the field of natural language processing, we use Dense Passage Retrieval (DPR) to retrieve related knowledge to help the model answer questions. However, DPR conduct retrieving in natural language space, which may not ensure comprehensive acquisition of image information. Thus, the retrieved knowledge is not truly conducive to helping answer the question, affecting the performance of the overall system. To address this issue, we propose a novel framework that leverages the visual-language model to select the key knowledge retrieved by DPR and answer questions. The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned by self-bootstrapping: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83{\%}.",EMNLP
"Simultaneous Machine Translation (SiMT) requires target tokens to be generated in real-time as streaming source tokens are consumed. Traditional approaches to SiMT typically require sophisticated architectures and extensive parameter configurations for training adaptive read/write policies, which in turn demand considerable computational power and memory. We propose PsFuture, the first zero-shot adaptive read/write policy for SiMT, enabling the translation model to independently determine read/write actions without the necessity for additional training. Furthermore, we introduce a novel training strategy, Prefix-to-Full (P2F), specifically tailored to adjust offline translation models for SiMT applications, exploiting the advantages of the bidirectional attention mechanism inherent in offline models. Experiments across multiple benchmarks demonstrate that our zero-shot policy attains performance on par with strong baselines and the P2F method can further enhance performance, achieving an outstanding trade-off between translation quality and latency.",EMNLP
"Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in chart understanding. However, the sheer size of these models limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through Program-of-Thoughts (PoT) learning, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences through Vision Token Merging, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on various chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart-understanding MLLMs with up to 13B parameters, and close-sourced MLLM GPT-4V on ChartQA, with higher throughput during inference due to a smaller model scale and more efficient vision encoding.",EMNLP
"This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese using CHEF dataset. To better reflect real-world fact-checking, we first develop a novel Chinese document-level evidence retriever, achieving state-of-the-art performance. We then demonstrate the limitations of translation-based methods and multilingual language models, highlighting the need for language-specific systems. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has a large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual language models and is more robust toward biases, emphasizing the importance of language-specific fact-checking systems.",EMNLP
"Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models{'} advanced reasoning ability. Traditional Vision-Language models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose **C**omplex **V**isual **R**easoning **L**arge **L**anguage **M**odels (**CVR-LLM**), capitalizing on VLMs{'} visual perception proficiency and LLMs{'} extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs{'} text knowledge for accurate predictions without extra training. We also introduce a novel multi-modal in-context learning (ICL) methodology to enhance LLMs{'} contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all.",EMNLP
"Text detoxification aims to minimize the risk of language models producing toxic content. Existing detoxification methods of directly constraining the model output or further training the model on the non-toxic corpus fail to achieve a decent balance between detoxification effectiveness and generation quality. This issue stems from the neglect of constrain imposed by the context since language models are designed to generate output that closely matches the context while detoxification methods endeavor to ensure the safety of the output even if it semantically deviates from the context. In view of this, we introduce a Context-aware Model self-Detoxification (CMD) framework that pays attention to both the context and the detoxification process, i.e., first detoxifying the context and then making the language model generate along the safe context. Specifically, CMD framework involves two phases: utilizing language models to synthesize data and applying these data for training. We also introduce a toxic contrastive loss that encourages the model generation away from the negative toxic samples. Experiments on various LLMs have verified the effectiveness of our MSD framework, which can yield the best performance compared to baselines.",EMNLP
"In recent years, large language models (LLMs) have achieved remarkable success in the field of natural language generation. Compared to previous small-scale models, they are capable of generating fluent output based on the provided prefix or prompt. However, one critical challenge {---} the *hallucination* problem {---} remains to be resolved. Generally, the community refers to the undetected hallucination scenario where the LLMs generate text unrelated to the input text or facts. In this study, we intend to model the distributional distance between the regular conditional output and the unconditional output, which is generated without a given input text. Based upon Taylor Expansion for this distance at the output probability space, our approach manages to leverage the embedding and first-order gradient information. The resulting approach is plug-and-play that can be easily adapted to any autoregressive LLM. On the hallucination benchmarks HADES and other datasets, our approach achieves state-of-the-art performance.",EMNLP
"Zero-shot singing voice synthesis (SVS) with style transfer and style control aims to generate high-quality singing voices with unseen timbres and styles (including singing method, emotion, rhythm, technique, and pronunciation) from audio and text prompts. However, the multifaceted nature of singing styles poses a significant challenge for effective modeling, transfer, and control. Furthermore, current SVS models often fail to generate singing voices rich in stylistic nuances for unseen singers. To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control. Specifically, TCSinger proposes three primary modules: 1) the clustering style encoder employs a clustering vector quantization model to stably condense style information into a compact latent space; 2) the Style and Duration Language Model (S{\&}D-LM) concurrently predicts style information and phoneme duration, which benefits both; 3) the style adaptive decoder uses a novel mel-style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TCSinger outperforms all baseline models in synthesis quality, singer similarity, and style controllability across various tasks, including zero-shot style transfer, multi-level style control, cross-lingual style transfer, and speech-to-singing style transfer.",EMNLP
"For a conversation to help and support, speakers should maintain an {``}effect-effort{''} tradeoff. As outlined in the gist of {``}Cognitive Relevance Principle{''}, helpful speakers should optimize the {``}cognitive relevance{''} through maximizing the {``}cognitive effects{''} and minimizing the {``}processing effort{''} imposed on listeners. Although preference learning methods have given rise a boon of studies in pursuit of{``}effect-optimization{''}, none have delved into the critical {``}effort-optimiazation{''} to fully cultivate the awareness of {``}optimal relevance{''} into thecognition of conversation agents. To address this gap, we integrate the {``}Cognitive Relevance Principle{''} into emotional support agents in the environment of multi-turn conversation. The results demonstrate a significant and robust improvement against the baseline systems with respect to response quality, human-likedness and supportivenss. This study offers compelling evidence for the effectiveness of the {``}Relevance Principle{''} in generating human-like, helpful, and harmless emotional support conversations. The source code will be available at https://github.com/CN-Eyetk/VLESA-ORL.git",EMNLP
"In interactions between users and language model agents, user utterances frequently exhibit ellipsis (omission of words or phrases) or imprecision (lack of exactness) to prioritize efficiency. This can lead to varying interpretations of the same input based on different assumptions or background knowledge. It is thus crucial for agents to adeptly handle the inherent ambiguity in queries to ensure reliability. However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge. To address these issues, we propose Alignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to manage ambiguous queries by leveraging their own assessment of ambiguity (i.e., perceived ambiguity). Experimental results on question-answering datasets demonstrate that APA empowers LLMs to explicitly detect and manage ambiguous queries while retaining the ability to answer clear questions. Furthermore, our finding proves that APA excels beyond training with gold-standard labels, especially in out-of-distribution scenarios. The data and code are available at https://github.com/heyjoonkim/APA.",EMNLP
"Despite recent advances in the general visual instruction-following ability of Multimodal Large Language Models (MLLMs), they still struggle with critical problems when required to provide a precise and detailed response to a visual instruction: (1) failure to identify novel objects or entities, (2) mention of non-existent objects, and (3) neglect of object{'}s attributed details. Intuitive solutions include improving the size and quality of data or using larger foundation models. They show effectiveness in mitigating these issues, but at an expensive cost of collecting a vast amount of new data and introducing a significantly larger model. Standing at the intersection of these approaches, we examine the three object-oriented problems from the perspective of the image-to-text mapping process by the multimodal connector. In this paper, we first identify the limitations of multimodal connectors stemming from insufficient training data. Driven by this, we propose to enhance the mapping with retrieval-augmented tag tokens, which contain rich object-aware information such as object names and attributes. With our Tag-grounded visual instruction tuning with retrieval Augmentation (TUNA), we outperform baselines that share the same language model and training data on 12 benchmarks. Furthermore, we show the zero-shot capability of TUNA when provided with specific datastores.",EMNLP
"Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders its generality. To overcome the limitation, this work proposes GLaPE, a gold label-agnostic prompt evaluation method to alleviate dependence on gold labels. GLaPE is composed of two critical aspects: self-consistency evaluation of a single prompt and mutual-consistency refinement across multiple prompts. Experimental results on 8 widely-recognized reasoning tasks demonstrate that GLaPE can produce more effective prompts, achieving performance comparable to those derived from manually annotated gold labels. Analysis shows that GLaPE provides reliable evaluations aligned with accuracy, even in the absence of gold labels. Code is publicly available at **Anonymous**.",EMNLP
No abstract found,EMNLP
"Code retrieval aims to identify code from extensive codebases that semantically aligns with a given query code snippet. Collecting a broad and high-quality set of query and code pairs is crucial to the success of this task. However, existing data collection methods struggle to effectively balance scalability and annotation quality. In this paper, we first analyze the factors influencing the quality of function annotations generated by Large Language Models (LLMs). We find that the invocation of intra-repository functions and third-party APIs plays a significant role. Building on this insight, we propose a novel annotation method that enhances the annotation context by incorporating the content of functions called within the repository and information on third-party API functionalities. Additionally, we integrate LLMs with a novel sorting method to address the multi-level function call relationships within repositories. Furthermore, by applying our proposed method across a range of repositories, we have developed the Query4Code dataset. The quality of this synthesized dataset is validated through both model training and human evaluation, demonstrating high-quality annotations. Moreover, cost analysis confirms the scalability of our annotation method.",EMNLP
"Vision-language models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning (ETL) has gained significant attention for effectively adapting to downstream tasks. However, previous studies have overlooked the challenge of varying transfer difficulty of downstream tasks. In this paper, we empirically analyze how each ETL method behaves with respect to transfer difficulty. Our observations indicate that utilizing vision prompts and text adapters is crucial for adaptability and generalizability in domains with high difficulty. Also, by applying an adaptive ensemble approach that integrates task-adapted VLMs with pre-trained VLMs and strategically leverages more general knowledge in low-difficulty and less in high-difficulty domains, we consistently enhance performance across both types of domains. Based on these observations, we propose an adaptive ensemble method that combines visual prompts and text adapters with pre-trained VLMs, tailored by transfer difficulty, to achieve optimal performance for any target domain. Upon experimenting with extensive benchmarks, our method consistently outperforms all baselines, particularly on unseen tasks, demonstrating its effectiveness.",EMNLP
"Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55{\%} Ã¢Â†Â’ 82.79{\%}), MATH (17.00{\%} Ã¢Â†Â’ 26.80{\%}), CSQA (68.14{\%} Ã¢Â†Â’ 72.97{\%}), and StrategyQA (82.86{\%} Ã¢Â†Â’ 83.25{\%}). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.",EMNLP
"With the rapidly-growing deployment of large language model (LLM) inference services, privacy concerns have arisen regarding to the user input data. Recent studies are exploring transforming user inputs to obfuscated embedded vectors, so that the data will not be eavesdropped by service provides. However, in this paper we show that again, without a solid and deliberate security design and analysis, such embedded vector obfuscation failed to protect users{'} privacy. We demonstrate the conclusion via conducting a novel inversion attack called Element-wise Differential Nearest Neighbor (EDNN) on the glide-reflection proposed in (CITATION), and the result showed that the original user input text can be 100{\%} recovered from the obfuscated embedded vectors. We further analyze security requirements on embedding obfuscation and present several remedies to our proposed attack.",EMNLP
"The recent years have witnessed great advances in video generation. However, the development of automatic video metrics is lagging significantly behind. None of the existing metric is able to provide reliable scores over generated videos. The main barrier is the lack of large-scale human-annotated dataset. In this paper, we release VideoFeedback, the first large-scale dataset containing human-provided multi-aspect score over 37.6K synthesized videos from 11 existing video generative models. We train VideoScore (initialized from Mantis)based on VideoFeedback to enable automatic video quality assessment. Experiments show that the Spearman{'}s correlation betweenVideoScore and humans can reach 77.1 on VideoFeedback-test, beating the prior best metrics by about 50 points. Further result onother held-out EvalCrafter, GenAI-Bench, and VBench show that VideoScore has consistently much higher correlation with humanjudges than other metrics. Due to these results, we believe VideoScore can serve as a great proxy for human raters to (1) rate different video models to track progress (2) simulate fine-grained human feedback in Reinforcement Learning with Human Feedback (RLHF) to improve current video generation models.",EMNLP
"We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs{'} prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs{'} learning of logical rules, with identified reasoning failures ranging from 29{\%} to 90{\%} across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5{\%}. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs{'} formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings.",EMNLP
"Information Extraction (IE), aiming to extract structured information from unstructured natural language texts, can significantly benefit from pre-trained language models. However, existing pre-training methods solely focus on exploiting the textual knowledge, relying extensively on annotated large-scale datasets, which is labor-intensive and thus limits the scalability and versatility of the resulting models. To address these issues, we propose SKIE, a novel pre-training framework tailored for IE that integrates structural semantic knowledge via contrastive learning, effectively alleviating the annotation burden. Specifically, SKIE utilizes Abstract Meaning Representation (AMR) as a low-cost supervision source to boost model performance without human intervention. By enhancing the topology of AMR graphs, SKIE derives high-quality cohesive subgraphs as additional training samples, providing diverse multi-level structural semantic knowledge. Furthermore, SKIE refines the graph encoder to better capture cohesive information and edge relation information, thereby improving the pre-training efficacy. Extensive experimental results demonstrate that SKIE outperforms state-of-the-art baselines across multiple IE tasks and showcases exceptional performance in few-shot and zero-shot settings.",EMNLP
"Data-generation based zero-shot learning, although effective in training Small Task-specific Models (STMs) via synthetic datasets generated by Pre-trained Language Models (PLMs), is often limited by the low quality of such synthetic datasets. Previous solutions have primarily focused on single PLM settings, where synthetic datasets are typically restricted to specific sub-spaces and often deviate from real-world distributions, leading to severe distribution bias. To mitigate such bias, we propose FuseGen, a novel data-generation based zero-shot learning framework that introduces a new criteria for subset selection from synthetic datasets via utilizing multiple PLMs and trained STMs. The chosen subset provides in-context feedback to each PLM, enhancing dataset quality through iterative data generation. Trained STMs are then used for sample re-weighting as well, further improving data quality. Extensive experiments across diverse tasks demonstrate that FuseGen substantially outperforms existing methods, highly effective in boosting STM performance in a PLM-agnostic way. The code is available at https://github.com/LindaLydia/FuseGen.",EMNLP
"This study explores the proactive ability of LLMs to seek user support. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability. Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support. The findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies. Source code: https://github.com/appier-research/i-need-help",EMNLP
We address the task of detecting abusive sentences in which identity groups are depicted as deviating from the norm (e.g. Gays sprinkle flour over their gardens for good luck). These abusive utterances need not be stereotypes or negative in sentiment. We introduce the first dataset for this task. It is created via crowdsourcing and includes 7 identity groups. We also report on classification experiments.,EMNLP
"Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. In this paper, we propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). Specifically, we design a visual prompt that directs MLLMs to utilize visualized sensor data alongside descriptions of the target sensory task. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10{\%} higher accuracy compared to text-based prompts and reducing token costs by 15.8 times. Our findings highlight the effectiveness and cost-efficiency of using visual prompts with MLLMs for various sensory tasks. The source code is available at https://github.com/diamond264/ByMyEyes.",EMNLP
"Despite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to find a set of key-value cache, coined {\_}CushionCache{\_}, which mitigates outliers in subsequent tokens when inserted as a prefix. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods. We thoroughly evaluate our method over a wide range of models and benchmarks and find that it significantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method.",EMNLP
"In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries. We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting. This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history. We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs. Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs. Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ.",EMNLP
"The proliferation of Internet memes in the age of social media necessitates effective identification of harmful ones. Due to the dynamic nature of memes, existing data-driven models may struggle in low-resource scenarios where only a few labeled examples are available. In this paper, we propose an agency-driven framework for low-resource harmful meme detection, employing both outward and inward analysis with few-shot annotated samples. Inspired by the powerful capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first retrieve relative memes with annotations to leverage label information as auxiliary signals for the LMM agent. Then, we elicit knowledge-revising behavior within the LMM agent to derive well-generalized insights into meme harmfulness. By combining these strategies, our approach enables dialectical reasoning over intricate and implicit harm-indicative patterns. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the low-resource harmful meme detection task.",EMNLP
"This paper introduces VIVA, a benchmark for VIsion-grounded decision-making driven by human VA. While most large vision-language models (VLMs) focus on physical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under a vision-depicted situation. VIVA contains 1,062 images depicting diverse real-world situations and the manually annotated decisions grounded in them. Given an image there, the model should select the most appropriate action to address the situation and provide the relevant human values and reason underlying the decision. Extensive experiments based on VIVA show the limitation of VLMs in using human values to make multimodal decisions. Further analyses indicate the potential benefits of exploiting action consequences and predicted human values.",EMNLP
"Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.",EMNLP
"The alignment of reasoning abilities between smaller and larger Language Models are largely conducted via supervised fine-tuning using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-improve their abilities.Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on synthetic demonstrations provided by LLMs, and then the instructed models self-improve their abilities through preference optimization strategies.In particular, the second phase operates refinement heuristics based on Direct Preference Optimization, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs.Results obtained on commonsense and math reasoning tasks show that this approach consistently outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger language models.",EMNLP
"To effectively use large language models (LLMs) for real-world queries, it is imperative that they generalize to the long-tail distribution, i.e. rare examples where models exhibit low confidence. In this work, we take the first step towards evaluating LLMs in the long-tail distribution of inferential knowledge. We exemplify long-tail evaluation on the Natural Language Inference task. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic long-tail data generation framework, to obtain factually-correct yet long-tail inferential statements. LINK uses variable-wise prompting grounded on symbolic rules to seek low-confidence statements while ensuring factual correctness. We then use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail inferential knowledge dataset that contains 108K statements spanning four domains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs show significant performance drop (21{\%} relative drop for GPT4) on long-tail data as compared to on head distribution data, and smaller models show even more generalization weakness. These results further underscore the necessity of long-tail evaluation in developing generalizable LLMs.",EMNLP
"Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Our work is now open-source.",EMNLP
"Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models{'} vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs{'} backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes{'} inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs{'} neurons.",EMNLP
"Visual arguments, often used in advertising or social causes, rely on images to persuade viewers to do or believe something. Understanding these arguments requires selective vision: only specific visual stimuli within an image are relevant to the argument, and relevance can only be understood within the context of a broader argumentative structure. While visual arguments are readily appreciated by human audiences, we ask: are today{'}s AI capable of similar understanding?We present VisArgs, a dataset of 1,611 images annotated with 5,112 visual premises (with regions), 5,574 commonsense premises, and reasoning trees connecting them into structured arguments. We propose three tasks for evaluating visual argument understanding: premise localization, premise identification, and conclusion deduction.Experiments show that 1) machines struggle to capture visual cues: GPT-4-O achieved 78.5{\%} accuracy, while humans reached 98.0{\%}. Models also performed 19.5{\%} worse when distinguishing between irrelevant objects within the image compared to external objects. 2) Providing relevant visual premises improved model performance significantly.",EMNLP
"Humans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?In response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.",EMNLP
"Text style transfer (TST) is crucial in natural language processing, aiming to endow text with a new style without altering its meaning. In real-world scenarios, not all styles have abundant resources. This work introduces TWIST (reusing Transferable Weight Increments for Style Text generation), a novel framework to mitigate data scarcity by utilizing style features in weight increments to transfer low-resource styles effectively. During target style learning, we derive knowledge via a specially designed weight pool and initialize the parameters for the unseen style. To enhance the effectiveness of merging, the target style weight increments are often merged from multiple source style weight increments through singular vectors. Considering the diversity of styles, we also designed a multi-key memory network that simultaneously focuses on task- and instance-level information to derive the most relevant weight increments. Results from multiple style transfer datasets show that TWIST demonstrates remarkable performance across different backbones, achieving particularly effective results in low-resource scenarios.",EMNLP
"Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be effectively applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with over 1000 students. Based on student responses, we found that LLM-based assignment evaluators are generally acceptable to students when they have free access to these tools. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions, resulting in unreasonable assessments. Additionally, we observed that students can easily manipulate the LLM to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we offer several recommendations for effectively integrating LLMs into future classroom evaluations. Our observation also highlights potential directions for improving LLM-based evaluators, including their instruction-following ability and vulnerability to prompt hacking.",EMNLP
"State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on multi-hop reasoning{---}the ability to identify and integrate information from multiple textual sources.Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. We propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs and show that their multi-hop reasoning performance is affected, as indicated by up to 45{\%} relative decrease in F1 score when presented with such seemingly plausible alternatives. We also find that{---}while LLMs tend to ignore misleading lexical cues{---}misleading reasoning paths indeed present a significant challenge. The code and data are made available at https://github.com/zawedcvg/Are-Large-Language-Models-Attentive-Readers",EMNLP
"Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-training. In pre-training from scratch, Instruction Pre-training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps.",EMNLP
"Large language models (LLMs) require continual knowledge updates to stay abreast of the ever-changing world facts, prompting the formulation of lifelong model editing task. While recent years have witnessed the development of various techniques for single and batch editing, these methods either fail to apply or perform sub-optimally when faced with lifelong editing. In this paper, we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong model editing. We first analyze the factors influencing the effectiveness of conventional MoE adaptor in lifelong editing, including catastrophic forgetting, inconsistent routing and order sensitivity. Based on these insights, we propose a tailored module insertion method to achieve lifelong editing, incorporating a novel KV anchor routing to enhance routing consistency between training and inference stage, along with a concise yet effective clustering-based editing order planning. Experimental results demonstrate the effectiveness of our method in lifelong editing, surpassing previous model editing techniques while maintaining outstanding performance in batch editing task. Our code will be available.",EMNLP
"Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked.",EMNLP
"In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via Transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chinese historical psychology corpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to demonstrate its superior performance compared with other approaches. The CCR method outperforms word-embedding-based approaches across all of our tasks and exceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline against objective, external data to further verify its validity.",EMNLP
"While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at \url{https://github.com/fanqiwan/KCA}.",EMNLP
"Reasoning is key to many decision making processes. It requires consolidating a set of rule-like premises that are often associated with degrees of uncertainty and observations to draw conclusions. In this work, we address both the case where premises are specified as numeric probabilistic rules and situations in which humans state their estimates using words expressing degrees of certainty. Existing probabilistic reasoning datasets simplify the task, e.g., by requiring the model to only rank textual alternatives, by including only binary random variables, or by making use of a limited set of templates that result in less varied text.In this work, we present QUITE, a question answering dataset of real-world Bayesian reasoning scenarios with categorical random variables and complex relationships. QUITE provides high-quality natural language verbalizations of premises together with evidence statements and expects the answer to a question in the form of an estimated probability. We conduct an extensive set of experiments, finding that logic-based models outperform out-of-the-box large language models on all reasoning types (causal, evidential, and explaining-away). Our results provide evidence that neuro-symbolic models are a promising direction for improving complex reasoning. We release QUITE and code for training and experiments on Github.",EMNLP
"Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (e.g., distinction between \textit{animal species}), however, has been probed insufficiently, despite its downstream importance. We fill this evaluation gap by creating FOCI (\textbf{F}ine-grained \textbf{O}bject \textbf{C}lass\textbf{I}fication), a difficult multiple-choice benchmark for fine-grained object classification, from existing object classification datasets: (1) multiple-choice avoids ambiguous answers associated with casting classification as open-ended QA task; (2) we retain classification difficulty by mining negative labels with a CLIP model. FOCI complements five popular classification datasets with four domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on and show that it tests for a \textit{complementary skill} to established image understanding and reasoning benchmarks. Crucially, CLIP models exhibit dramatically better performance than LVLMs. Since the image encoders of LVLMs come from these CLIP models, this points to inadequate alignment for fine-grained object distinction between the encoder and the LLM and warrants (pre)training data with more fine-grained annotation. We release our code at \url{ANONYMIZED}.",EMNLP
"Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \textbf{FAITH} (\textbf{F}alse premise \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating \textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1{\%} of the attention heads in the model yields a notable increase of nearly 20{\%} of model performance.",EMNLP
"Polysemy and synonymy are two crucial interrelated facets of lexicalambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP,leading to dedicated systems, they are often being consideredindependently in practictal problems. While many tasks dealing with polysemy (e.g. Word SenseDisambiguiation or Induction) highlight the role of word{'}s senses,the study of synonymy is rooted in the study of concepts, i.e. meaningsshared across the lexicon. In this paper, we introduce ConceptInduction, the unsupervised task of learning a soft clustering amongwords that defines a set of concepts directly from data. This taskgeneralizes Word Sense Induction. We propose a bi-levelapproach to Concept Induction that leverages both a locallemma-centric view and a global cross-lexicon view to induceconcepts. We evaluate the obtained clustering on SemCor{'}s annotateddata and obtain good performance (BCubed F1 above0.60). We find that the local and the global levels are mutuallybeneficial to induce concepts and also senses in our setting. Finally,we create static embeddings representing our induced concepts and usethem on the Word-in-Context task, obtaining competitive performancewith the State-of-the-Art.",EMNLP
"The safety defense methods of Large language models (LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. However, similar to traditional text adversarial attacks, this approach, while effective, is limited by the challenge of the discrete tokens. This gradient based discrete optimization attack requires over 100,000 LLM calls, and due to the unreadable of adversarial suffixes, it can be relatively easily penetrated by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffix Embedding Translation Framework (ASETF), aimed at transforming continuous adversarial suffix embeddings into coherent and understandable text. This method greatly reduces the computational overhead during the attack process and helps to automatically generate multiple adversarial samples, which can be used as data to strengthen LLM{'}s security defense. Experimental evaluations were conducted on Llama2, Vicuna, and other prominent LLMs, employing harmful directives sourced from the Advbench dataset.The results indicate that our method significantly reduces the computation time of adversarial suffixes and achieves a much better attack success rate than existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini.",EMNLP
"Modern large language models (LLMs) have exhibited cooperative synergy on complex task-solving, and collective decision-making (CDM) is a pivotal component in LLM-based multi-agent collaboration frameworks. Our survey on 52 recent such systems uncovers a severe lack of diversity, with a heavy reliance on dictatorial and plurality voting for CDM. Through the lens of social choice theory, we scrutinize widely-adopted CDM methods and identify their limitations. To enrich current landscape of LLM-based CDM, we present GEDI, an electoral CDM module that incorporates various ordinal preferential voting mechanisms. Our empirical case study across three benchmarks shows that the integration of certain CDM methods can markedly improve the reasoning capabilities and robustness of some leading LLMs, all without requiring intricate system designs. Additionally, we find that some CDM mechanisms generate positive synergies even with as few as three agents. The voting-based methods also demonstrate robustness against single points of failure, as well as diversity in terms of hit-rate@k and subject-wise impacts.",EMNLP
"Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (e.g., visual question answering). LVLMs, however, often \textit{hallucinate} and produce captions that mention concepts that cannot be found in the image. These hallucinations erode the trustworthiness of LVLMs and are arguably among the main obstacles to their ubiquitous adoption. Recent work suggests that addition of grounding objectives{---}those that explicitly align image regions or objects to text spans{---}reduces the amount of LVLM hallucination. Although intuitive, this claim is not empirically justified as the reduction effects have been established, we argue, with flawed evaluation protocols that (i) rely on data (i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure hallucination via question answering rather than open-ended caption generation.In this work, in contrast, we offer the first systematic analysis of the effect of fine-grained object grounding on LVLM hallucination under an evaluation protocol that more realistically captures LVLM hallucination in open generation. Our extensive experiments over three backbone LLMs reveal that grounding objectives have little to no effect on object hallucination in open caption generation.",EMNLP
"Recent studies have explored the working mechanisms of In-Context Learning (ICL). However, they mainly focus on classification and simple generation tasks, limiting their broader application to more complex generation tasks in practice. To address this gap, we investigate the impact of demonstrations on token representations within the practical alignment tasks. We find that the transformer embeds the task function learned from demonstrations into the separator token representation, which plays an important role in the generation of prior response tokens. Once the prior response tokens are determined, the demonstrations become redundant. Motivated by this finding, we propose an efficient Progressive In-Context Alignment (PICA) method consisting of two stages. In the first few-shot stage, the model generates several prior response tokens via standard ICL while concurrently extracting the ICL vector that stores the task function from the separator token representation. In the following zero-shot stage, this ICL vector guides the model to generate responses without further demonstrations. Extensive experiments demonstrate that our PICA not only surpasses vanilla ICL but also achieves comparable performance to other alignment tuning methods. The proposed training-free method reduces the time cost (e.g., 5.45{\mbox{$\times$}}) with improved alignment performance (e.g., 6.57+). Consequently, our work highlights the application of ICL for alignment and calls for a deeper understanding of ICL for complex generations. The code will be available at https://github.com/HITsz-TMG/PICA.",EMNLP
"The growing demand for larger-scale models in the development of Large Language Models (LLMs) poses challenges for efficient training within limited computational resources. Traditional fine-tuning methods often exhibit instability in multi-task learning and rely heavily on extensive training resources. Here, we propose MoDULA (Mixture of Domain-Specific and Universal LoRA), a novel Parameter Efficient Fine-Tuning (PEFT) Mixture-of-Expert (MoE) paradigm for improved fine-tuning and parameter efficiency in multi-task learning. The paradigm effectively improves the multi-task capability of the model by training universal experts, domain-specific experts, and routers separately. MoDULA-Res is a new method within the MoDULA paradigm, which maintains the model{'}s general capability by connecting universal and task-specific experts through residual connections. The experimental results demonstrate that the overall performance of the MoDULA-Flan and MoDULA-Res methods surpasses that of existing fine-tuning methods on various LLMs. Notably, MoDULA-Res achieves more significant performance improvements in multiple tasks while reducing training costs by over 80{\%} without losing general capability. Moreover, MoDULA displays flexible pluggability, allowing for the efficient addition of new tasks without retraining existing experts from scratch. This progressive training paradigm circumvents data balancing issues, enhancing training efficiency and model stability. Overall, MoDULA provides a scalable, cost-effective solution for fine-tuning LLMs with enhanced parameter efficiency and generalization capability.",EMNLP
"Emotion classification has wide applications in education, robotics, virtual reality, etc. However, identifying subtle differences between fine-grained emotion categories remains challenging. Current methods typically aggregate numerous token embeddings of a sentence into a single vector, which, while being an efficient compressor, may not fully capture complex semantic and temporal distributions. To solve this problem, we propose SEmantic ANchor Graph Neural Networks (SEAN-GNN) for fine-grained emotion classification. It learns a group of representative, multi-faceted semantic anchors in the token embedding space: using these anchors as a global reference, any sentence can be projected onto them to form a {``}semantic-anchor graph{''}, with node attributes and edge weights quantifying the semantic and temporal information respectively. The graph structure is well aligned across sentences and, importantly, allows for generating comprehensive emotion representations regarding $K$ different anchors. Message passing on this graph can further integrate and refine the learned features. Empirically, SEAN-GNN can generate meaningful semantic anchors and discriminative graph patterns for different emotion, with promising classification results on 6 popular benchmark datasets against state-of-the-arts.",EMNLP
"Philology, the study of ancient manuscripts, demands years of professional training in ex-tensive knowledge memorization and manual textual retrieval. Despite these requirements align closely with strengths of recent successful Large Language Models (LLMs), the scarcity of high-quality, specialized training data has hindered direct applications. To bridge this gap, we curated the PhiloCorpus-ZH, a rich collec-tion of ancient Chinese texts spanning a millen-nium with 30 diverse topics, including firsthand folk copies. This corpus facilitated the develop-ment of PhiloGPT, the first LLM tailored for discovering ancient Chinese manuscripts. To effectively tackle complex philological tasks like restoration, attribution, and linguistic anal-ysis, we introduced the PhiloCoP framework. Modeled on the analytical patterns of philol-ogists, PhiloCoP enhances LLM{'}s handling of historical linguistic peculiarities such as phonetic loans, polysemy, and syntactic inver-sions. We further integrated these tasks into the PhiloBenchmark, establishing a new standard for evaluating ancient Chinese LLMs address-ing philology tasks. Deploying PhiloGPT in practical scenarios has enabled Dunhuang spe-cialists to resolve philology tasks, such as iden-tifying duplication of copied text and assisting archaeologists with text completion, demon-strating its potential in real-world applications.",EMNLP
"Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines Competitive Index and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach.",EMNLP
"Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment triplets in a given corpus. Existing approaches within the pretraining-finetuning paradigm tend to either meticulously craft complex tagging schemes and classification heads, or incorporate external semantic augmentation to enhance performance. In this study, we, for the first time, re-evaluate the redundancy in tagging schemes and the internal enhancement in pretrained representations. We propose a method to improve and utilize pretrained representations by integrating a minimalist tagging scheme and a novel token-level contrastive learning strategy. The proposed approach demonstrates comparable or superior performance compared to state-of-the-art techniques while featuring a more compact design and reduced computational overhead. Additionally, we are the first to formally evaluate GPT-4{'}s performance in few-shot learning and Chain-of-Thought scenarios for this task. The results demonstrate that the pretraining-finetuning paradigm remains highly effective even in the era of large language models.",EMNLP
"Large Language Models (LLMs) undergo extensive evaluation against various benchmarks collected in established leaderboards to assess their performance across multiple tasks. However, to the best of our knowledge, there is a lack of comprehensive studies evaluating these models{'} linguistic abilities independent of specific tasks. In this paper, we introduce a novel evaluation methodology designed to test LLMs{'} sentence generation abilities under specific linguistic constraints. Drawing on the {`}linguistic profiling{'} approach, we rigorously investigate the extent to which five LLMs of varying sizes, tested in both zero- and few-shot scenarios, effectively adhere to (morpho)syntactic constraints. Our findings shed light on the linguistic proficiency of LLMs, revealing both their capabilities and limitations in generating linguistically-constrained sentences.",EMNLP
"Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to {``}hear{''} via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for \textit{in silico} experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.",EMNLP
"Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of a given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize the information to induce programs over this KB. Experiments show that KB-Plugin outperforms SoTA low-resourced PI methods with 25x smaller backbone LLM on both large-scale and domain-specific KBs, and even approaches the performance of supervised methods.",EMNLP
"Independent Component Analysis (ICA) offers interpretable semantic components of embeddings.While ICA theory assumes that embeddings can be linearly decomposed into independent components, real-world data often do not satisfy this assumption. Consequently, non-independencies remain between the estimated components, which ICA cannot eliminate. We quantified these non-independencies using higher-order correlations and demonstrated that when the higher-order correlation between two components is large, it indicates a strong semantic association between them, along with many words sharing common meanings with both components. The entire structure of non-independencies was visualized using a maximum spanning tree of semantic components. These findings provide deeper insights into embeddings through ICA.",EMNLP
No abstract found,EMNLP
No abstract found,EMNLP
"Natural language explanations represent a proxy for evaluating explanation-based and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains.",EMNLP
"Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \textit{Uncertainty} about the question and the \textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method, \textit{UF Calibration}, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \textit{Truly Well-Calibrated Confidence} for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",EMNLP
"Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training. This study explores whether stronger reward models invariably lead to better language models. In this paper, through experiments on relevance, factuality, and completeness tasks using the QA-FEEDBACK dataset and reward models based on Longformer, we uncover a surprising paradox: language models trained with moderately accurate reward models outperform those guided by highly accurate ones. This challenges the widely held belief that stronger reward models always lead to better language models, and opens up new avenues for future research into the key factors driving model performance and how to choose the most suitable reward models.",EMNLP
"Natural Language Inference (NLI) evaluation is crucial for assessing language understanding models; however, popular datasets suffer from systematic spurious correlations that artificially inflate actual model performance. To address this, we propose a method for the automated creation of a challenging test set without relying on the manual construction of artificial and unrealistic examples. We categorize the test set of popular NLI datasets into three difficulty levels by leveraging methods that exploit training dynamics. This categorization significantly reduces spurious correlation measures, with examples labeled as having the highest difficulty showing markedly decreased performance and encompassing more realistic and diverse linguistic phenomena. When our characterization method is applied to the training set, models trained with only a fraction of the data achieve comparable performance to those trained on the full dataset, surpassing other dataset characterization techniques. Our research addresses limitations in NLI dataset construction, providing a more authentic evaluation of model performance with implications for diverse NLU applications.",EMNLP
"Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora. However, these annotations are unavailable in many low-resource languages. In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages. This approach outperforms current state-of-the-art annotation-free GED methods. We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors.",EMNLP
"Large Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.",EMNLP
"Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER.",EMNLP
"We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.",EMNLP
"Despite the remarkable progress made by large language models in mathematical reasoning, interactive theorem proving in formal logic still remains a prominent challenge. Previous methods resort to neural models for proofstep generation and search. However, they suffer from exploring possible proofsteps empirically in a large search space. Moreover, they directly use a less rigorous informal proof for proofstep generation, neglecting the incomplete reasoning within. In this paper, we propose BC-Prover, a backward chaining framework guided by pseudo steps. Specifically, BC-Prover prioritizes pseudo steps to proofstep generation. The pseudo steps boost the proof construction in two aspects: (1) Backward Chaining that decomposes the proof into sub-goals for goal-oriented exploration. (2) Step Planning that makes a fine-grained planning to bridge the gap between informal and formal proofs. Experiments on the miniF2F benchmark show significant performance gains by our framework over the state-of-the-art approaches. Our framework is also compatible with existing provers and further improves their performance with the backward chaining technique.",EMNLP
"Interpretability and analysis (IA) research is a growing subfield within NLP with the goal of developing a deeper understanding of the behavior or inner workings of NLP systems and methods. Despite growing interest in the subfield, a criticism of this work is that it lacks actionable insights and therefore has little impact on NLP. In this paper, we seek to quantify the impact of IA research on the broader field of NLP. We approach this with a mixed-methods analysis of: (1) a citation graph of 185K+ papers built from all papers published at ACL and EMNLP conferences from 2018 to 2023, and their references and citations, and (2) a survey of 138 members of the NLP community. Our quantitative results show that IA work is well-cited outside of IA, and central in the NLP citation graph. Through qualitative analysis of survey responses and manual annotation of 556 papers, we find that NLP researchers build on findings from IA work and perceive it as important for progress in NLP, multiple subfields, and rely on its findings and terminology for their own work. Many novel methods are proposed based on IA findings and highly influenced by them, but highly influential non-IA work cites IA findings without being driven by them. We end by summarizing what is missing in IA work today and provide a call to action, to pave the way for a more impactful future of IA research.",EMNLP
"The integration of visual and textual information represents a promising direction in the advancement of language models. In this paper, we explore the dual modality of language{---}both visual and textual{---}within an autoregressive framework, pre-trained on both document images and texts. Our method employs a multimodal training strategy, utilizing visual data through next patch prediction with a regression head and/or textual data through next token prediction with a classification head. We focus on understanding the interaction between these two modalities and their combined impact on model performance. Our extensive evaluation across a wide range of benchmarks shows that incorporating both visual and textual data significantly improves the performance of pixel-based language models. Remarkably, we find that a unidirectional pixel-based model trained solely on visual data can achieve comparable results to state-of-the-art bidirectional models on several language understanding tasks. This work uncovers the untapped potential of integrating visual and textual modalities for more effective language modeling. We release our code, data, and model checkpoints at https://github.com/ernie-research/pixelgpt.",EMNLP
"Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We make our code and data publicly available at https://github.com/ernie-research/gptfluence.",EMNLP
"Recent improvements in natural language processing (NLP) and machine learning (ML) and increased mainstream adoption have led to researchers frequently discussing the {``}democratization{''} of artificial intelligence. In this paper, we seek to clarify how democratization is understood in NLP and ML publications, through large-scale mixed-methods analyses of papers using the keyword {``}democra*{''} published in NLP and adjacent venues. We find that democratization is most frequently used to convey (ease of) access to or use of technologies, without meaningfully engaging with theories of democratization, while research using other invocations of {``}democra*{''} tends to be grounded in theories of deliberation and debate. Based on our findings, we call for researchers to enrich their use of the term democratization with appropriate theory, towards democratic technologies beyond superficial access.",EMNLP
"Visual document understanding (VDU) is a challenging task that involves understanding documents across various modalities (text and image) and layouts (forms, tables, etc.). This study aims to enhance generalizability of small VDU models by distilling knowledge from LLMs. We identify that directly prompting LLMs often fails to generate informative and useful data. In response, we present a new framework (called DocKD) that enriches the data generation process by integrating external document knowledge. Specifically, we provide an LLM with various document elements like key-value pairs, layouts, and descriptions, to elicit open-ended answers. Our experiments show that DocKD produces high-quality document annotations and surpasses the direct knowledge distillation approach that does not leverage external document knowledge. Moreover, student VDU models trained with solely DocKD-generated data is not only comparable to those trained with human-annotated data on in-domain tasks but also significantly excel them on out-of-domain tasks.",EMNLP
"Automatic question generation (QG) serves a wide range of purposes, such as augmenting question-answering (QA) corpora, enhancing chatbot systems, and developing educational materials. Despite its importance, most existing datasets predominantly focus on English, resulting in a considerable gap in data availability for other languages. Cross-lingual transfer for QG (XLT-QG) addresses this limitation by allowing models trained on high-resource language datasets to generate questions in low-resource languages. In this paper, we propose a simple and efficient XLT-QG method that operates without the need for monolingual, parallel, or labeled data in the target language, utilizing a small language model. Our model, trained solely on English QA datasets, learns interrogative structures from a limited set of question exemplars, which are then applied to generate questions in the target language. Experimental results show that our method outperforms several XLT-QG baselines and achieves performance comparable to GPT-3.5-turbo across different languages. Additionally, the synthetic data generated by our model proves beneficial for training multilingual QA models. With significantly fewer parameters than large language models and without requiring additional training for target languages, our approach offers an effective solution for QG and QA tasks across various languages.",EMNLP
"High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.",EMNLP
"The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission. On the other hand, although it shows promise in mitigating hallucination and omission, the overall performance of MT in different language directions remains mixed, with slight increases in BLEU and decreases in COMET.",EMNLP
"Previous studies on multi-party dialogue generation predominantly concentrated on modeling the reply-to structure of dialogue histories, always overlooking the coherence between generated responses and target utterances. To address this issue, we propose a Reinforcement Learning approach emphasizing both Topic and Rhetorical Coherence (RL-TRC). In particular, the topic- and rhetorical-coherence tasks are designed to enhance the model{'}s perception of coherence with the target utterance. Subsequently, an agent is employed to learn a coherence policy, which guides the generation of responses that are topically and rhetorically aligned with the target utterance. Furthermore, three discourse-aware rewards are developed to assess the coherence between the generated response and the target utterance, with the objective of optimizing the policy. The experimental results and in-depth analyses on two popular datasets demonstrate that our RL-TRC significantly outperforms the state-of-the-art baselines, particularly in generating responses that are more coherent with the target utterances.",EMNLP
"Continual learning (CL) is crucial for language models to dynamically adapt to the evolving real-world demands. To mitigate the catastrophic forgetting problem in CL, data replay has been proven a simple and effective strategy, and the subsequent data-replay-based distillation can further enhance the performance. However, existing methods fail to fully exploit the knowledge embedded in models from previous tasks, resulting in the need for a relatively large number of replay samples to achieve good results. In this work, we first explore and emphasize the importance of attention weights in knowledge retention, and then propose a SElective attEntion-guided Knowledge Retention method (SEEKR) for data-efficient replay-based continual learning of large language models (LLMs). Specifically, SEEKR performs attention distillation on the selected attention heads for finer-grained knowledge retention, where the proposed forgettability-based and task-sensitivity-based measures are used to identify the most valuable attention heads. Experimental results on two continual learning benchmarks for LLMs demonstrate the superiority of SEEKR over the existing methods on both performance and efficiency. Explicitly, SEEKR achieves comparable or even better performance with only 1/10 of the replayed data used by other methods, and reduces the proportion of replayed data to 1{\%}. The code is available at https://github.com/jinghan1he/SEEKR.",EMNLP
"Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify {``}value neurons{''} directly contributing to the final prediction, we propose a method for identifying {``}query neurons{''} which activate these {``}value neurons{''}. Finally, we apply our methods to analyze six types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. The code is available on https://github.com/zepingyu0512/neuron-attribution.",EMNLP
"We investigate the mechanism of in-context learning (ICL) on sentence classification tasks with semantically-unrelated labels ({``}foo{''}/{``}bar{''}). We find intervening in only 1{\%} heads (named {``}in-context heads{''}) significantly affects ICL accuracy from 87.6{\%} to 24.4{\%}. To understand this phenomenon, we analyze the value-output vectors in these heads and discover that the vectors at each label position contain substantial information about the corresponding labels. Furthermore, we observe that the prediction shift from {``}foo{''} to {``}bar{''} is due to the respective reduction and increase in these heads{'} attention scores at {``}foo{''} and {``}bar{''} positions. Therefore, we propose a hypothesis for ICL: in in-context heads, the value-output matrices extract label features, while the query-key matrices compute the similarity between the features at the last position and those at each label position. The query and key matrices can be considered as two towers that learn the similarity metric between the last position{'}s features and each demonstration at label positions. Using this hypothesis, we explain the majority label bias and recency bias in ICL and propose two methods to reduce these biases by 22{\%} and 17{\%}, respectively.",EMNLP
"We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations. To delve into the reason, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons. Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages. These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions. Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism.",EMNLP
"Pixel-based language models have emerged as a compelling alternative to subword-based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has been pre-trained on rendered text. While PIXEL has shown promising cross-script transfer abilities and robustness to orthographic perturbations, it falls short of outperforming monolingual subword counterparts like BERT in most other contexts. This discrepancy raises questions about the amount of linguistic knowledge learnt by these models and whether their performance in language tasks stems more from their visual capabilities than their linguistic ones. To explore this, we probe PIXEL using a variety of linguistic and visual tasks to assess its position on the vision-to-language spectrum. Our findings reveal a substantial gap between the model{'}s visual and linguistic understanding. The lower layers of PIXEL predominantly capture superficial visual features, whereas the higher layers gradually learn more syntactic and semantic abstractions. Additionally, we examine variants of PIXEL trained with different text rendering strategies, discovering that introducing certain orthographic constraints at the input level can facilitate earlier learning of surface-level features. With this study, we hope to provide insights that aid the further development of pixel-based language models.",EMNLP
"Privacy issues arise prominently during the inappropriate transmission of information between entities. Existing research primarily studies privacy by exploring various privacy attacks, defenses, and evaluations within narrowly predefined patterns, while neglecting that privacy is not an isolated, context-free concept limited to traditionally sensitive data (e.g., social security numbers), but intertwined with intricate social contexts that complicate the identification and analysis of potential privacy violations. The advent of Large Language Models (LLMs) offers unprecedented opportunities for incorporating the nuanced scenarios outlined in privacy laws to tackle these complex privacy issues. However, the scarcity of open-source relevant case studies restricts the efficiency of LLMs in aligning with specific legal statutes. To address this challenge, we introduce a novel framework, GoldCoin, designed to efficiently ground LLMs in privacy laws for judicial assessing privacy violations. Our framework leverages the theory of contextual integrity as a bridge, creating numerous synthetic scenarios grounded in relevant privacy statutes (e.g., HIPAA), to assist LLMs in comprehending the complex contexts for identifying privacy risks in the real world. Extensive experimental results demonstrate that GoldCoin markedly enhances LLMs{'} capabilities in recognizing privacy risks across real court cases, surpassing the baselines on different judicial tasks.",EMNLP
"We present a framework for detecting and categorizing noise in literary texts, demonstrated through its application to Danish and Norwegian literature from the late 19-th century. Noise, understood as {``}aberrant sonic behaviour,{''} is not only an auditory phenomenon but also a cultural construct tied to the processes of civilization and urbanization.We begin by utilizing topic modeling techniques to identify noise-related documents, followed by fine-tuning BERT-based language models trained on Danish and Norwegian texts to analyze a corpus of over 800 novels.We identify and track the prevalence of noise in these texts, offering insights into the literary perceptions of noise during the Scandinavian {``}Modern Breakthrough{''} period (1870-1899). Our contributions include the development of a comprehensive dataset annotated for noise-related segments and their categorization into human-made, non-human-made, and musical noises. This study illustrates the framework{'}s potential for enhancing the understanding of the relationship between noise and its literary representations, providing a deeper appreciation of the auditory elements in literary works, including as sources for cultural history.",EMNLP
"Large Language Models (LLMs) from the GPT family have become extremely popular, leading to a race towards reducing their inference costs to allow for efficient local computation. However, the vast majority of existing work focuses on weight-only quantization, which can reduce runtime costs in the memory-bound one-token-at-a-time generative setting, but does not address costs in compute-bound scenarios, such as batched inference or prompt processing.In this paper, we address the general quantization problem, where \textit{both weights and activations} should be quantized, which leads to computational improvements in general. We show that the majority of inference computations for large generative models can be performed with both weights and activations being cast to 4 bits, while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK that compresses most of the weights and activations to 4-bit, while keeping a small fraction of {``}outlier{''} weights and activations in higher-precision. QUIK is that it is designed with computational efficiency in mind: we provide GPU kernels matching the QUIK format with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.4x relative to FP16 execution. We provide detailed studies for models from the OPT, LLaMA-2 and Falcon families, as well as a first instance of accurate inference using quantization plus 2:4 sparsity.Anonymized code is available.",EMNLP
"Can human reading comprehension be assessed from eye movements in reading? In this work, we address this longstanding question using large-scale eyetracking data. We focus on a cardinal and largely unaddressed variant of this question: predicting reading comprehension of a single participant for a single question from their eye movements over a single paragraph. We tackle this task using a battery of recent models from the literature, and three new multimodal language models. We evaluate the models in two different reading regimes: ordinary reading and information seeking, and examine their generalization to new textual items, new participants, and the combination of both. The evaluations suggest that the task is highly challenging, and highlight the importance of benchmarking against a strong text-only baseline. While in some cases eye movements provide improvements over such a baseline, they tend to be small. This could be due to limitations of current modelling approaches, limitations of the data, or because eye movement behavior does not sufficiently pertain to fine-grained aspects of reading comprehension processes. Our study provides an infrastructure for making further progress on this question.",EMNLP
"Retrieval-augmented generation (RAG) methods encounter difficulties when addressing complex questions like multi-hop queries.While iterative retrieval methods improve performance by gathering additional information, current approaches often rely on multiple calls of large language models (LLMs).In this paper, we introduce EfficientRAG, an efficient retriever for multi-hop question answering.EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information.Experimental results demonstrate that EfficientRAG surpasses existing RAG methods on three open-domain multi-hop question-answering datasets.The code is available in [aka.ms/efficientrag](https://github.com/NIL-zhuang/EfficientRAG-official).",EMNLP
"Large language models demonstrate impressive reasoning abilities but struggle to provide personalized content due to their lack of individual user preference information. Existing methods, such as in-context learning and parameter-efficient fine-tuning, fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. In this paper, we propose a novel approach utilizing small parameter models as preference agents to generate natural language rules that guide a larger, pre-trained model, enabling efficient personalization. Our method involves a small, local {``}steering wheel{''} model that directs the outputs of a much larger foundation model, producing content tailored to an individual{'}s preferences while leveraging the extensive knowledge and capabilities of the large model. Importantly, this personalization is achieved without the need to fine-tune the large model. Experimental results on email and article datasets, demonstrate that our technique significantly outperforms baseline personalization methods. By allowing foundation models to adapt to individual preferences in a data and compute-efficient manner, our approach paves the way for highly personalized language model applications.",EMNLP
"The potential effectiveness of counterspeech as a hate speech mitigation strategy is attracting increasing interest in the NLG research community, particularly towards the task of automatically producing it. However, automatically generated responses often lack the argumentative richness which characterises expert-produced counterspeech. In this work, we focus on two aspects of counterspeech generation to produce more cogent responses. First, by investigating the tension between helpfulness and harmlessness of LLMs, we test whether the presence of safety guardrails hinders the quality of the generations. Secondly, we assess whether attacking a specific component of the hate speech results in a more effective argumentative strategy to fight online hate. By conducting an extensive human and automatic evaluation, we show how the presence of safety guardrails can be detrimental also to a task that inherently aims at fostering positive social interactions. Moreover, our results show that attacking a specific component of the hate speech, and in particular its implicit negative stereotype and its hateful parts, leads to higher-quality generations.",EMNLP
No abstract found,EMNLP
"Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100{\%} in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2{\%} improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.",EMNLP
"With the proliferation of large language models, Parameter Efficient Fine-Tuning (PEFT) method, which freeze pre-trained parameters and only fine-tune a few task-specific parameters, are playing an increasingly important role. However, previous work primarily applied uniform operations across all layers of the model, overlooking the fact that different layers in a transformer store different information. In the process of exploration, We find that there is a significant differences in fine-tuning strategies between different layers, and fine-tuning only a subset of layers can even achieve comparable performance. Based on this, we propose the Hybrid LoRA-Prefix Tuning(HLPT) method, which uses enhanced LoRA and Prefix-tuning methods with learnable adaptive mechanism separately for the bottom and top layers, and the Half Hybrid LoRA-Prefix Tuning($H^2$LPT) method, which goes a step further, reducing the parameter count to nearly half by omitting fine-tuning in the middle layers. Extensive experiments with large language models on various downstream tasks provide strong evidence for the potential of PEFT focusing on different layers{'} interactions and the effectiveness of our methods. Furthermore, we validate the robustness of these methods and their advantages in speeding up training convergence, reducing inference time requirements.",EMNLP
"Recent studies have explored the use of Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA). They typically require rewriting retrieved subgraphs into natural language formats comprehensible to LLMs. However, when tackling complex questions, the knowledge rewritten by existing methods may include irrelevant information, omit crucial details, or fail to align with the question{'}s semantics. To address them, we propose a novel rewriting method CoTKR, Chain- of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces and corresponding knowledge in an interleaved manner, thereby mitigating the limitations of single-step knowledge rewriting. Additionally, to bridge the preference gap between the knowledge rewriter and the question answering (QA) model, we propose a training strategy PAQAF, Preference Alignment from Question Answering Feedback, for leveraging feedback from the QA model to further optimize the knowledge rewriter. We conduct experiments using various LLMs across several KGQA benchmarks. Experimental results demonstrate that, compared with previous knowledge rewriting methods, CoTKR generates the most beneficial knowledge representation for QA models, which significantly improves the performance of LLMs in KGQA.",EMNLP
"In linguistics, all languages can be considered as symbolic systems, with each language relying on symbolic processes to associate specific symbols with meanings. In the same language, there is a fixed correspondence between linguistic symbol and meaning. In different languages, universal meanings follow varying rules of symbolization in one-to-one correspondence with symbols. Most work overlooks the properties of languages as symbol systems. In this paper, we shift the focus to the symbolic properties and introduce MTLS: a pre-training method to improve the multilingual capability of models by Making Texts into Linguistic Symbols. Initially, we replace the vocabulary in pre-trained language models by mapping relations between linguistic symbols and semantics. Subsequently, universal semantics within the symbolic system serve as bridges, linking symbols from different languages to the embedding space of the model, thereby enabling the model to process linguistic symbols. To evaluate the effectiveness of MTLS, we conducted experiments on multilingual tasks using BERT and RoBERTa, respectively, as the backbone. The results indicate that despite having just over 12,000 pieces of English data in pre-training, the improvement that MTLS brings to multilingual capabilities is remarkably significant.",EMNLP
No abstract found,EMNLP
"Fine-grained category discovery using only coarse-grained supervision is a cost-effective yet challenging task. Previous training methods focus on aligning query samples with positive samples and distancing them from negatives. They often neglect intra-category and inter-category semantic similarities of fine-grained categories when navigating sample distributions in the embedding space. Furthermore, some evaluation techniques that rely on pre-collected test samples are inadequate for real-time applications. To address these shortcomings, we introduce a method that successfully detects fine-grained clusters of semantically similar texts guided by a novel objective function. The method uses semantic similarities in a logarithmic space to guide sample distributions in the Euclidean space and to form distinct clusters that represent fine-grained categories. We also propose a centroid inference mechanism to support real-time applications. The efficacy of the method is both theoretically justified and empirically confirmed on three benchmark tasks. The proposed objective function is integrated in multiple contrastive learning based neural models. Its results surpass existing state-of-the-art approaches in terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of the detected fine-grained categories. Code and data are publicly available at https://github.com/changtianluckyforever/F-grained-STAR.",EMNLP
"Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators{'} needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey study with volunteer moderators to gain insight into their perspectives on useful moderation models. Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules. Moderators{'} reports provide guides for future work on developing moderation assistant models.",EMNLP
"Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset, code, and process data are publicly available at https://github.com/Alice1998/URS.",EMNLP
"Despite tremendous advancements, current state-of-the-art Vision-Language Models (VLMs) are still far from perfect. They tend to hallucinate and may generate biased responses. In such circumstances, having a way to assess the reliability of a given response generated by a VLM is quite useful. Existing methods, such as estimating uncertainty using answer likelihoods or prompt-based confidence generation, often suffer from overconfidence. Other methods use self-consistency comparison but are affected by confirmation biases. To alleviate these, we propose Decompose and Compare Consistency (DeCC) for reliability measurement. By comparing the consistency between the direct answer generated using the VLM{'}s internal reasoning process, and the indirect answers obtained by decomposing the question into sub-questions and reasoning over the sub-answers produced by the VLM, DeCC measures the reliability of VLM{'}s direct answer. Experiments across six vision-language tasks with three VLMs show DeCC{'}s reliability estimation achieves better correlation with task accuracy compared to the existing methods.",EMNLP
"Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM{'}s understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty. It can be filled with validated knowledge and progressively expanded. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs.",EMNLP
"In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.",EMNLP
"Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility.",EMNLP
"Large Language Models (LLMs) face significant challenges at inference time due to their high computational demands. To address this, we present Performance-Guided Knowledge Distillation (PGKD), a cost-effective and high-throughput solution for production text classification applications. PGKD utilizes teacher-student Knowledge Distillation to distill the knowledge of LLMs into smaller, task-specific models. PGKD establishes an active learning routine between the student model and the LLM; the LLM continuously generates new training data leveraging hard-negative mining, student model validation performance, and early-stopping protocols to inform the data generation. By employing a cyclical, performance-aware approach tailored for highly multi-class, sparsely annotated datasets prevalent in industrial text classification, PGKD effectively addresses training challenges and outperforms traditional BERT-base models and other knowledge distillation methods on several multi-class classification datasets. Additionally, cost and latency benchmarking reveals that models fine-tuned with PGKD are up to 130X faster and 25X less expensive than LLMs for inference on the same classification task. While PGKD is showcased for text classification tasks, its versatile framework can be extended to any LLM distillation task, including language generation, making it a powerful tool for optimizing performance across a wide range of AI applications.",EMNLP
"Argument mining (AM) involves the identification of argument relations (AR) between Argumentative Discourse Units (ADUs). The essence of ARs among ADUs is context-dependent and lies in maintaining a coherent flow of ideas, often centered around the relations between discussed entities, topics, themes or concepts. However, these relations are not always explicitly stated; rather, inferred from implicit chains of reasoning connecting the concepts addressed in the ADUs. While humans can infer such background knowledge, machines face challenges when the contextual cues are not explicitly provided. This paper leverages external resources, including WordNet, ConceptNet, and Wikipedia to identify semantic paths (knowledge paths) connecting the concepts discussed in the ADUs to obtain the implicit chains of reasoning. To effectively leverage these paths for AR prediction, we propose attention-based Multi-Network architectures. Various architecture are evaluated on the external resources, and the Wikipedia based configuration attains F-scores of 0.85, 0.84, 0.70, and 0.87, respectively, on four diverse datasets, showing strong performance over the baselines.",EMNLP
"The development of tools and techniques to analyze and extract organizations{'} data habits from privacy policies are critical for scalable regulatory compliance audits. Unfortunately, these tools are becoming increasingly limited in their ability to identify compliance issues and fixes. After all, most were developed using regulation-agnostic datasets of annotated privacy policies obtained from a time before the introduction of landmark privacy regulations such as EU{'}s GDPR and California{'}s CCPA. In this paper, we describe the first open regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA Privacy Policy Provision Annotations), aimed to address this challenge. C3PA contains over 48K expert-labeled privacy policy text segments associated with responses to CCPA-specific disclosure mandates from 411 unique organizations. We demonstrate that the C3PA dataset is uniquely suited for aiding automated audits of compliance with CCPA-related disclosure mandates.",EMNLP
"Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient instruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.",EMNLP
"For extremely weak-supervised text classification, pioneer research generates pseudo labels by mining texts similar to the class names from the raw corpus, which may end up with very limited or even no samples for the minority classes. Recent works have started to generate the relevant texts by prompting LLMs using the class names or definitions; however, there is a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus where the text classifier will be applied) data, leading to ungeneralizable classifiers. In this paper, we combine the advantages of these two approaches and propose to bridge the gap via a novel framework, \textit{text grafting}, which aims to obtain clean and near-distribution weak supervision for minority classes. Specifically, we first use LLM-based logits to mine masked templates from the raw corpus, which have a high potential for data synthesis into the target minority class. Then, the templates are filled by state-of-the-art LLMs to synthesize near-distribution texts falling into minority classes. Text grafting shows significant improvement over direct mining or synthesis on minority classes. We also use analysis and case studies to comprehend the property of text grafting.",EMNLP
"In this paper, we aim to generate text classification data given arbitrary class definitions (i.e., user instruction), so one can train a text classifier without any human annotation or raw corpus. Recent advances in large language models (LLMs) lead to pioneer attempts to individually generate texts for each class via prompting. In this paper, we propose Incubator, the first framework that can handle complicated and even mutually dependent classes (e.g., ''\textit{TED Talk given by Educator}'' and ''\textit{Other}''). Specifically, our Incubator is a fine-tuned LLM that takes the instruction of all class definitions as input, and in each inference, it can jointly generate one sample for every class. First, we tune Incubator on the instruction-to-data mappings that we obtained from classification datasets and descriptions on Hugging Face together with in-context augmentation by GPT-4. To emphasize the uniformity and diversity in generations, we refine Incubator by fine-tuning with the cluster centers of semantic textual embeddings of the generated samples. We compare Incubator on various classification tasks with strong baselines such as direct LLM-based inference and training data generation by prompt engineering. Experiments show Incubator is able to (1) outperform previous methods on traditional benchmarks, (2) take label interdependency and user preference into consideration, and (3) enable logical text mining by incubating multiple classifiers",EMNLP
"Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.",EMNLP
"The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., '*If* Ann has a queen, *then* Bob has a jack{'}) and epistemic modals (e.g., {`}Ann *might* have an ace{'}, {`}Bob *must* have a king{'}). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental human ability to reason about distal possibilities. Assessing LLMs on these inferences is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. All the LLMs we tested make some basic mistakes with conditionals or modals, though zero-shot chain-of-thought prompting helps them make fewer mistakes. Even the best performing LLMs make basic errors in modal reasoning, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals, and give answers about complex conditional inferences that do not match reported human judgments. These results highlight gaps in basic logical reasoning in today{'}s LLMs.",EMNLP
"Teaching large language models (LLMs) to generate text with citations to evidence sources can mitigate hallucinations and enhance verifiability in information-seeking systems. However, improving this capability requires high-quality attribution data, which is costly and labor-intensive. Inspired by recent advances in self-improvement that enhance LLMs without manual annotation, we present START, a Self-Taught AttRibuTion framework for iteratively improving the attribution capability of LLMs. First, to prevent models from stagnating due to initially insufficient supervision signals, START leverages the model to self-construct synthetic training data for warming up. To further self-improve the model{'}s attribution ability, START iteratively utilizes fine-grained preference supervision signals constructed from its sampled responses to encourage robust, comprehensive, and attributable generation. Experiments on three open-domain question-answering datasets, covering long-form QA and multi-step reasoning, demonstrate significant performance gains of 25.13{\%} on average without relying on human annotations and more advanced models. Further analysis reveals that START excels in aggregating information across multiple sources.",EMNLP
"Speech Emotion Captioning (SEC) has gradually become an active research task. The emotional content conveyed through human speech are often complex, and classifying them into fixed categories may not be enough to fully capture speech emotions. Describing speech emotions through natural language may be a more effective approach. However, existing SEC methods often produce hallucinations and lose generalization on unseen speech. To overcome these problems, we propose AlignCap, which Aligning Speech Emotion Captioning to Human Preferences based on large language model (LLM) with two properties: 1) Speech-Text Alignment, which minimizing the divergence between the LLM{'}s response prediction distributions for speech and text inputs using knowledge distillation (KD) Regularization. 2) Human Preference Alignment, where we design Preference Optimization (PO) Regularization to eliminate factuality and faithfulness hallucinations. We also extract emotional clues as a prompt for enriching fine-grained information under KD-Regularization. Experiments demonstrate that AlignCap presents stronger performance to other state-of-the-art methods on Zero-shot SEC task.",EMNLP
"Language models recognized as a new form of knowledge bases, face challenges of outdated, erroneous, and privacy-sensitive information, necessitating knowledge editing to rectify errors without costly retraining. Existing methods, spanning model{'}s parameters modification, external knowledge integration, and in-context learning, lack in-depth analysis from a model interpretability perspective. Our work explores the instability in in-context learning outcomes, providing insights into its reasons and distinctions from other methods. Leveraging findings on the critical role of feed-forward MLPs in decoder-only models, we propose a tailored knowledge editing method, TailoredKE, that considers the unique information flow of each sample. Model interpretability reveals diverse attribute recall across transformer layers, guiding edits to specific features at different depths and mitigating over-editing issues.",EMNLP
"Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PROMST that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6{\%}-29.3{\%} improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks.",EMNLP
"In recent years, the rapid increase in online video content has underscored the limitations of static Video Question Answering (VideoQA) models trained on fixed datasets, as they struggle to adapt to new questions or tasks posed by newly available content. In this paper, we explore the novel challenge of VideoQA within a continual learning framework, and empirically identify a critical issue: fine-tuning a large language model (LLM) for a sequence of tasks often results in catastrophic forgetting. To address this, we propose Collaborative Prompting (ColPro), which integrates specific question constraint prompting, knowledge acquisition prompting, and visual temporal awareness prompting. These prompts aim to capture textual question context, visual content, and video temporal dynamics in VideoQA, a perspective underexplored in prior research. Experimental results on the NExT-QA and DramaQA datasets show that ColPro achieves superior performance compared to existing approaches, achieving 55.14{\%} accuracy on NExT-QA and 71.24{\%} accuracy on DramaQA, highlighting its practical relevance and effectiveness.",EMNLP
"Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model{'}s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.",EMNLP
"Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (i.e., follow open-ended instructions) and faithfulness (i.e., ground responses in given context) when training LMs with these objectives. For instance, fine-tuning LLaMA-7B on instruction following datasets renders it less faithful. Conversely, instruction-tuned Vicuna-7B shows degraded performance at following instructions when further optimized on tasks that require contextual grounding. One common remedy is multi-task learning (MTL) with data mixing, yet it remains far from achieving a synergic outcome. We propose a simple yet effective method that relies on Reject-sampling by Self-instruct with Continued Fine-tuning (ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find that less is more, as training ReSet with high-quality, yet substantially smaller data (three-fold less) yields superior results. Our findings offer a better understanding of objective discrepancies in alignment training of LMs.",EMNLP
"Large Language Models (LLMs) are typically shipped with tokenizers that *deterministically* encode text into so-called *canonical* token sequences, to which the LLMs assign probability values.One common assumption is that the probability of a piece of text is the probability of its canonical token sequence.However, the tokenization of a string is not unique: e.g., the Llama2 tokenizer encodes {`}Tokens{`} as {`}[Tok,ens]{`}, but {`}[Tok,en,s]{`} also represents the same text.In this paper, we study non-canonical tokenizations.We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations.We then show how the marginal is, in most cases, indistinguishable from the canonical probability.Surprisingly, we then empirically demonstrate the existence of a significant amount of signal hidden within tokenization space.Notably, by simply aggregating the probabilities of non-canonical tokenizations, we achieve improvements across a range of LLM evaluation benchmarks for a variety of architectures, including transformers and state space models.",EMNLP
"Recently it has been shown that deep learning models for NLP tasks are prone to attacks that can even reconstruct the verbatim training texts. To prevent privacy leakage, researchers have investigated word-level perturbations, relying on the formal guarantees of differential privacy (DP) in the embedding space. However, many existing approaches either achieve unsatisfactory performance in the high privacy regime when using the Laplacian or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior to the canonical DP in terms of privacy strength. This raises the question of whether a new method for private word embedding can be designed to overcome these limitations. In this paper, we propose a novel private embedding method called the high dimensional truncated Laplacian mechanism. Specifically, we introduce a non-trivial extension of the truncated Laplacian mechanism, which was previously only investigated in one-dimensional space cases. Theoretically, we show that our method has a lower variance compared to the previous private word embedding methods. To further validate its effectiveness, we conduct comprehensive experiments on private embedding and downstream tasks using three datasets. Remarkably, even in the high privacy regime, our approach only incurs a slight decrease in utility compared to the non-private scenario.",EMNLP
No abstract found,EMNLP
"Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types.",EMNLP
"Large language models (LLMs) have demonstrated remarkable progress in leveraging diverse knowledge sources. This study investigates how nine widely used LLMs allocate knowledge between local context and global parameters when answering open-ended questions in knowledge-consistent scenarios. We introduce a novel dataset, WikiAtomic, and systematically vary context sizes to analyze how LLMs prioritize and utilize the provided information and their parametric knowledge in knowledge-consistent scenarios. Additionally, we also study their tendency to hallucinate under varying context sizes. Our findings reveal consistent patterns across models, including a consistent reliance on both contextual (around 70{\%}) and parametric (around 30{\%}) knowledge, and a decrease in hallucinations with increasing context. These insights highlight the importance of more effective context organization and developing models that use input more deterministically for robust performance.",EMNLP
"Neural networks without hierarchical biases often struggle to learn linguistic rules that come naturally to humans. However, neural networks are trained primarily on form alone, while children acquiring language additionally receive data about meaning. Would neural networks generalize more like humans when trained on both form and meaning? We investigate this by examining if Transformers{---}neural networks without a hierarchical bias{---}better achieve hierarchical generalization when trained on both form and meaning compared to when trained on form alone. Our results show that Transformers trained on form and meaning do favor the hierarchical generalization more than those trained on form alone, suggesting that statistical learners without hierarchical biases can leverage semantic training signals to bootstrap hierarchical syntactic generalization.",EMNLP
"Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33{\%}. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the {``}curse of multilinguality{''}). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.",EMNLP
"In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. We expect human language to be informative (i.e., providing feedback on agents{'} past behaviors and offering guidance on achieving their future goals) and diverse (i.e., encompassing a wide range of expressions and style nuances). To enable flexibility of language use in teaching agents tasks, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness and diversity impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world.",EMNLP
"Translation systems, including foundation models capable of translation, can produce errors that result in gender mistranslation, and such errors can be especially harmful. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts, including several traditionally under-represented in digital resources. The dataset is constructed with handcrafted passages that target known failure patterns, longer synthetically generated passages, and natural passages sourced from multiple domains. We demonstrate the usefulness of the dataset by evaluating both neural machine translation systems and foundation models, and show that all systems exhibit gender mistranslation and potential harm, even in high resource languages.",EMNLP
"Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5{\%} performance gaps between high and low-resource languages, potentially due to LLMs{'} drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2{\%} improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling.",EMNLP
"While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it {``}plugs into{''} a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.",EMNLP
"Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is important yet challenging. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall.To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite inputs along various stylistic axes (e.g., formality, length) while maintaining low computational costs. StyleRemix outperforms state-of-the-art baselines and much larger LLMs on an array of domains on both automatic and human evaluation.Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions.",EMNLP
"When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26{\%} and 12{\%} of the time, respectively. Error analysis shows that 62{\%} of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark and the code to reproduce the experiments.",EMNLP
"Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3{\%} to 69.8{\%}. Furthermore, we demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191{\%}, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.",EMNLP
"Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs{'} political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20{\%} of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users{'} political views is required, as their use becomes more widespread.",EMNLP
"Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning.",EMNLP
"Reasoning is most powerful when an LLM accurately aggregates relevant information. We examine the critical role of information aggregation in reasoning by requiring the LLM to analyze sports narratives. To succeed at this task, an LLM must infer points from actions, identify related entities, attribute points accurately to players and teams, and compile key statistics to draw conclusions. We conduct comprehensive experiments with real NBA basketball data and present SportsGen, a new method to synthesize game narratives. By synthesizing data, we can rigorously evaluate LLMs{'} reasoning capabilities under complex scenarios with varying narrative lengths and density of information. Our findings show that most models, including GPT-4o, often fail to accurately aggregate basketball scores due to frequent scoring patterns. Open-source models like Llama-3 further suffer from significant score hallucinations. Finally, the effectiveness of reasoning is influenced by narrative complexity, information density, and domain-specific terms, highlighting the challenges in analytical reasoning tasks.",EMNLP
"FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages of varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate 3 mitigations to our knowledge source that ultimately improve FActScore estimation across all languages.",EMNLP
"Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available.",EMNLP
"Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA{'}s answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3{\%} of the most competitive LLM{'}s answers are preferred to LFRQA{'}s answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research.",EMNLP
"Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training.In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token{'}s hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM.Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.",EMNLP
"Yoruba{---}an African language with roughly 47 million speakers{---}encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus; YORULECT across three domains and four regional yoruba dialects. To develop this corpus, we engaged native speakers, traveling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard yoruba and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yoruba and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We will release YORULECT dataset and models publicly under an open license.",EMNLP
"Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we ask the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct wrong reasoning after the RL stage. The RL procedure requires substantial hyperparameter tuning and often generates errors such as repetitive words and incomplete sentences. With correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on the multi-modal datasets ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. The ARES rationale achieves around 70{\%} win rate compared to baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5{\%} increase in inference answer accuracy on average for the multi-modal datasets.",EMNLP
"Large Language Models (LLMs) have the promise to revolutionize computing broadly, but their complexity and extensive training data also expose significant privacy vulnerabilities. One of the simplest privacy risks associated with LLMs is their susceptibility to membership inference attacks (MIAs), wherein an adversary aims to determine whether a specific data point was part of the model{'}s training set. Although this is a known risk, state of the art methodologies for MIAs rely on training multiple computationally costly {`}shadow models{'}, making risk evaluation prohibitive for large models. Here we adapt a recent line of work which uses quantile regression to mount membership inference attacks; we extend this work by proposing a low-cost MIA that leverages an ensemble of small quantile regression models to determine if a document belongs to the model{'}s training set or not. We demonstrate the effectiveness of this approach on fine-tuned LLMs of varying families (OPT, Pythia, Llama) and across multiple datasets. Across all scenarios we obtain comparable or improved accuracy compared to state of the art {`}shadow model{'} approaches, with as little as 6{\%} of their computation budget. We demonstrate increased effectiveness across multi-epoch trained target models, and architecture miss-specification robustness, that is, we can mount an effective attack against a model using a different tokenizer and architecture, without requiring knowledge on the target model.",EMNLP
"Existing text-to-video diffusion models rely solely on text-only encoders for their pretraining. This limitation stems from the absence of large-scale multimodal prompt video datasets, resulting in a lack of visual grounding and restricting their versatility and application in multimodal integration. To address this, we construct a large-scale multimodal prompt dataset by employing retrieval methods to pair in-context examples with the given text prompts and then utilize a two-stage training strategy to enable diverse video generation tasks within a model. In the first stage, we propose a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. Secondly, we fine-tune the model from the first stage on various video generation tasks, incorporating multimodal instructions. This process further refines the model{'}s ability to handle diverse inputs and tasks, ensuring seamless integration of multimodal information. After this two-stage training process, VIMI demonstrates multimodal understanding capabilities, producing contextually rich and personalized videos grounded in the provided inputs, as shown in Figure1. Compared to previous subject-driven video generation methods, our generator can synthesize consistent and temporally coherent videos with large motion while retaining the semantic control. Our generator also achieves state-of-the-art text-to-video generation results on UCF101 benchmark.",EMNLP
"Hate speech (HS) on social media exacerbates misinformation and baseless prejudices. Evidence-supported counterspeech (CS) is crucial for correcting misinformation and reducing prejudices through facts. Existing methods for generating evidence-supported CS often lack clear guidance with a core claim for organizing evidence and do not adequately address factuality and faithfulness hallucinations in CS within anti-hate contexts. In this paper, to mitigate the aforementioned, we propose F$^2$RL, a Factuality and Faithfulness Reinforcement Learning framework for generating claim-guided and evidence-supported CS. Firstly, we generate counter-claims based on hate speech and design a self-evaluation mechanism to select the most appropriate one. Secondly, we propose a coarse-to-fine evidence retrieval method. This method initially generates broad queries to ensure the diversity of evidence, followed by carefully reranking the retrieved evidence to ensure its relevance to the claim. Finally, we design a reinforcement learning method with a triplet-based factuality reward model and a multi-aspect faithfulness reward model. The method rewards the generator to encourage greater factuality, more accurate refutation of hate speech, consistency with the claim, and better utilization of evidence. Extensive experiments on three benchmark datasets demonstrate that the proposed framework achieves excellent performance in CS generation, with strong factuality and faithfulness.",EMNLP
"Social networks are rife with noise and misleading information, presenting multifaceted challenges for rumor detection. In this paper, from the perspective of human cognitive subjectivity, we introduce the mining of individual latent intentions and propose a novel multi-task learning framework, the Intent-Aware Rumor Detection Network (IRDNet). IRDNet is designed to discern multi-level rumor semantic features and latent user intentions, addressing the challenges of robustness and key feature mining and alignment that plague existing models. In IRDNet, the multi-level semantic extraction module captures sequential and hierarchical features to generate robust semantic representations. The hierarchical contrastive learning module incorporates two complementary strategies, event-level and intent-level, to establish cognitive anchors that uncover the latent intentions of information disseminators. Event-level contrastive learning employs high-quality data augmentation and adversarial perturbations to enhance model robustness. Intent-level contrastive learning leverages the intent encoder to capture latent intent features and optimize consistency within the same intent while ensuring heterogeneity between different intents to clearly distinguish key features from irrelevant elements. Experimental results demonstrate that IRDNet significantly improves the effectiveness of rumor detection and effectively addresses the challenges present in the field of rumor detection.",EMNLP
"Vision Large Language Models (VLLMs) are transforming the intersection of computer vision and natural language processing; however, the potential of using visual prompts for emotion recognition in these models remains largely unexplored and untapped. Traditional methods in VLLMs struggle with spatial localization and often discard valuable global context. We propose a novel Set-of-Vision prompting (SoV) approach that enhances zero-shot emotion recognition by using spatial information, such as bounding boxes and facial landmarks, to mark targets precisely. SoV improves accuracy in face count and emotion categorization while preserving the enriched image context. Through comprehensive experimentation and analysis of recent commercial or open-source VLLMs, we evaluate the SoV model{'}s ability to comprehend facial expressions in natural environments. Our findings demonstrate the effectiveness of integrating spatial visual prompts into VLLMs for improving emotion recognition performance.",EMNLP
"The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio water- marking, has not been adequately studied. In this paper, we design a dual-embedding wa- termarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods.",EMNLP
"In multi-person communications, conflicts often arise. Each individual may have their own perspective, which can differ. Additionally, commonly referenced offensive datasets frequently neglect contextual information and are primarily constructed with a focus on intended offenses. This study suggests that conflicts are pivotal in revealing a broader range of human interactions, including instances of unintended offensive language. This paper proposes a conflict-based data collection method to utilize inter-conflict cues in multi-person communications. By focusing on specific cue posts within conversation threads, our proposed approach effectively identifies relevant instances for analysis. Detailed analyses are provided to showcase the proposed approach efficiently gathers data on subtly offensive content. The experimental results indicate that incorporating elements of conflict into data collection significantly enhances the comprehensiveness and accuracy of detecting offensive language but also enriches our understanding of conflict dynamics in digital communication.",EMNLP
"Automatic counterspeech generation methods have been developed to assist efforts in combating hate speech. Existing research focuses on generating counterspeech with linguistic attributes such as being polite, informative, and intent-driven. However, the real impact of counterspeech in online environments is seldom considered. This study aims to develop methods for generating counterspeech constrained by conversation outcomes and evaluate their effectiveness. We experiment with large language models (LLMs) to incorporate into the text generation process two desired conversation outcomes: low conversation incivility and non-hateful hater reentry. Specifically, we experiment with instruction prompts, LLM finetuning, and LLM reinforcement learning (RL). Evaluation results show that our methods effectively steer the generation of counterspeech toward the desired outcomes. Our analyses, however, show that there are differences in the quality and style depending on the model.",EMNLP
"In this paper, we address the data scarcity problem in automatic data-driven glossing for low-resource languages by coordinating multiple sources of linguistic expertise. We enhance models by incorporating both token-level and sentence-level translations, utilizing the extensive linguistic capabilities of modern LLMs, and incorporating available dictionary resources. Our enhancements lead to an average absolute improvement of 5{\%}-points in word-level accuracy over the previous state of the art on a typologically diverse dataset spanning six low-resource languages. The improvements are particularly noticeable for the lowest-resourced language Gitksan, where we achieve a 10{\%}-point improvement. Furthermore, in a simulated ultra-low resource setting for the same six languages, training on fewer than 100 glossed sentences, we establish an average 10{\%}-point improvement in word-level accuracy over the previous state-of-the-art system.",EMNLP
"Adversarial textual examples reveal the vulnerability of natural language processing (NLP) models. Most existing text attack methods are designed for English text, while the robust implementation of the second popular language, i.e., Chinese with 1 billion users, is greatly underestimated. Although several Chinese attack methods have been presented, they either directly transfer from English attacks or adopt simple greedy search to optimize the attack priority, usually leading to unnatural sentences. To address these issues, we propose an adaptive Immune-based Sound-Shape Code (ISSC) algorithm for adversarial Chinese text attacks. Firstly, we leverage the Sound-Shape code to generate natural substitutions, which comprehensively integrate multiple Chinese features. Secondly, we employ adaptive immune algorithm (IA) to determine the replacement order, which can reduce the duplication of population to improve the search ability. Extensive experimental results validate the superiority of our ISSC in producing high-quality Chinese adversarial texts. Our code and data can be found in https://github.com/nohuma/chinese-attack-issc.",EMNLP
"Reinforcement learning shows promise in optimizing dialogue policies, but addressing the challenge of reward sparsity remains crucial. While curriculum learning offers a practical solution by strategically training policies from simple to complex, it hinges on the assumption of a gradual increase in goal difficulty to ensure a smooth knowledge transition across varied complexities. In complex dialogue environments without intermediate goals, achieving seamless knowledge transitions becomes tricky. This paper proposes a novel Bootstrapped Policy Learning (BPL) framework, which adaptively tailors progressively challenging subgoal curriculum for each complex goal through goal shaping, ensuring a smooth knowledge transition. Goal shaping involves goal decomposition and evolution, decomposing complex goals into subgoals with solvable maximum difficulty and progressively increasing difficulty as the policy improves. Moreover, to enhance BPL{'}s adaptability across various environments, we explore various combinations of goal decomposition and evolution within BPL, and identify two universal curriculum patterns that remain effective across different dialogue environments, independent of specific environmental constraints. By integrating the summarized curriculum patterns, our BPL has exhibited efficacy and versatility across four publicly available datasets with different difficulty levels.",EMNLP
"As awareness of mental health issues grows, online counseling support services are becoming increasingly prevalent worldwide. Detecting whether users express suicidal ideation in text-based counseling services is crucial for identifying and prioritizing at-risk individuals. However, the lack of domain-specific systems to facilitate fine-grained suicide detection and corresponding risk assessment in online counseling poses a significant challenge for automated crisis intervention aimed at suicide prevention. In this paper, we propose PsyGUARD, an automated system for detecting suicide ideation and assessing risk in psychological counseling. To achieve this, we first develop a detailed taxonomy for detecting suicide ideation based on foundational theories. We then curate a large-scale, high-quality dataset called PsySUICIDE for suicide detection. To evaluate the capabilities of automated systems in fine-grained suicide detection, we establish a range of baselines. Subsequently, to assist automated services in providing safe, helpful, and tailored responses for further assessment, we propose to build a suite of risk assessment frameworks. Our study not only provides an insightful analysis of the effectiveness of automated risk assessment systems based on fine-grained suicide detection but also highlights their potential to improve mental health services on online counseling platforms. Code, data, and models are available at https://github.com/qiuhuachuan/PsyGUARD.",EMNLP
"Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. The conventional norm in VLM data construction uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation.In this paper, we present World to Code ($W2C$), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. The pipeline leverages the VLM itself to extract cross-modal information via different prompts and filter the generated outputs again via a consistency filtering strategy. Experiments have demonstrated the high quality of $W2C$ by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code is available at https://github.com/foundation-multimodal-models/World2Code.",EMNLP
"Large language models (LLMs) are widely used in question-answering (QA) systems but often generate information with hallucinations. Retrieval-augmented generation (RAG) offers a potential remedy, yet the uneven retrieval quality and irrelevant contents may distract LLMs.In this work, we address these issues at the generation phase by treating RAG as a multi-document QA task.We propose a novel decoding strategy, Dynamic Contrastive Decoding, which dynamically amplifies knowledge from selected documents during the generation phase. involves constructing inputs batchwise, designing new selection criteria to identify documents worth amplifying, and applying contrastive decoding with a specialized weight calculation to adjust the final logits used for sampling answer tokens. Zero-shot experimental results on ALCE-ASQA, NQ, TQA and PopQA benchmarks show that our method outperforms other decoding strategies. Additionally, we conduct experiments to validate the effectiveness of our selection criteria, weight calculation, and general multi-document scenarios. Our method requires no training and can be integrated with other methods to improve the RAG performance. Our codes will be publicly available at https://github.com/JulieJin-km/Dynamic{\_}Contrastive{\_}Decoding.",EMNLP
"Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5{\%} on the Llama-Base model and 4.3{\%} on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model{'}s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",EMNLP
"Large language models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM{'}s context. Instead, it combines the LLM{'}s action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline {``}retrospection{''} process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong baselines.",EMNLP
"Numerous recent works target to extend effective context length for language models and various methods, tasks and benchmarks exist to measure model{'}s effective memory length. However, through thorough investigations, we find limitations for currently existing evaluations on model{'}s memory. We provide an extensive survey for limitations in this work and propose a new method called forgetting curve to measure the memorization capability of long-context models. We show that forgetting curve has the advantage of being robust to the tested corpus and the experimental settings, of not relying on prompt and can be applied to any model size. We apply our forgetting curve to a large variety of models involving both transformer and RNN/SSM based architectures. Our measurement provides empirical evidence for the effectiveness of transformer extension techniques while raises questions for the effective length of RNN/SSM based models. We also examine the difference between our measurement and existing benchmarks as well as popular metrics for various models.",EMNLP
"Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.",EMNLP
"In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves.In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses for instructions. To effectively refine the responses, we develop an iterative framework following a {\_}debate-advise-edit-judge{\_} paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs.",EMNLP
"This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities.",EMNLP
"Recent advances in large language models (LLMs) show the potential of using LLMs as evaluators for assessing the quality of text generations from LLMs. However, applying LLM evaluators naively to compare different systems can lead to unreliable results due to the inaccuracy and intrinsic bias of LLM evaluators. In order to mitigate this problem, we propose two calibration methods, Bayesian Win-Rate Sampling (BWRS) and Bayesian Dawid-Skene, both of which leverage Bayesian inference to more accurately infer the true win rate of generative language models. We empirically validate our methods on six datasets covering story generation, summarization, and instruction following tasks. We show that both our methods are effective in improving the accuracy of win rate estimation using LLMs as evaluators, offering a promising direction for reliable automatic text quality evaluation.",EMNLP
"The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via **mu**lti-perspective data augmenting methods and then synthesize **code**-nested solutions to them. The open LLMs (e.g., Llama-2) are finetuned on the augmented dataset to get the resulting models, **MuMath-Code** ($\mu$-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code.Our MuMath-Code-7B achieves 83.8{\%} on GSM8K and 52.4{\%} on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods{---}achieving 90.7{\%} on GSM8K and 55.1{\%} on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy.We release the proposed dataset along with the associated code for public use: https://github.com/youweihao-tal/MuMath-Code.",EMNLP
"Recent studies have shown that distributed machine learning is vulnerable to gradient inversion attacks, where private training data can be reconstructed by analyzing the gradients of the models shared in training. Previous attacks established that such reconstructions are possible using gradients from all parameters in the entire models. However, we hypothesize that most of the involved modules, or even their sub-modules, are at risk of training data leakage, and we validate such vulnerabilities in various intermediate layers of language models. Our extensive experiments reveal that gradients from a single Transformer layer, or even a single linear component with 0.54{\%} parameters, are susceptible to training data leakage. Additionally, we show that applying differential privacy on gradients during training offers limited protection against the novel vulnerability of data disclosure.",EMNLP
"Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from the web. This paper further explores CLIP from the perspectives of data and model architecture. To mitigate the impact of the noise data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to combine and refine information from web-based image-text pairs, synthetic captions, and detection tags. Additionally, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Extensive experiments across different model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust vision-language representation learner and it achieves state-of-the-art performance across multiple downstream tasks, including linear probing, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP.",EMNLP
"Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children{'}s unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.",EMNLP
"In translation, a concept represented by a single word in a source language can have multiple variations in a target language. The task of lexical selection requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when translating from English. We evaluate recent LLMs and neural machine translation systems on DTAiLS, with the best-performing model, GPT-4, achieving from 67 to 85{\%} accuracy across languages. Finally, we use language models to generate English rules describing target-language concept variations. Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4.",EMNLP
"Recent advances in generative AI technologies like large language models have boosted the incorporation of AI assistance in writing workflows, leading to the rise of a new paradigm of human-AI co-creation in writing. To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people{'}s perceptions of the writing on various aspects, including their evaluation on the quality of the writing, and their ranking of different writings. Our results suggest that disclosing the AI assistance in the writing process, especially if AI has provided assistance in generating new content, decreases the average quality ratings for both argumentative essays and creative stories. This decrease in the average quality ratings often comes with an increased level of variations in different individuals{'} quality evaluations of the same writing. Indeed, factors such as an individual{'}s writing confidence and familiarity with AI writing assistants are shown to moderate the impact of AI assistance disclosure on their writing quality evaluations. We also find that disclosing the use of AI assistance may significantly reduce the proportion of writings produced with AI{'}s content generation assistance among the top-ranked writings.",EMNLP
"Electronic healthcare records are vital for patient safety as they document conditions, plans, and procedures in both free text and medical codes. Language models have significantly enhanced the processing of such records, streamlining workflows and reducing manual data entry, thereby saving healthcare providers significant resources. However, the black-box nature of these models often leaves healthcare professionals hesitant to trust them. State-of-the-art explainability methods increase model transparency but rely on human-annotated evidence spans, which are costly. In this study, we propose an approach to produce plausible and faithful explanations without needing such annotations. We demonstrate on the automated medical coding task that adversarial robustness training improves explanation plausibility and introduce AttInGrad, a new explanation method superior to previous ones. By combining both contributions in a fully unsupervised setup, we produce explanations of comparable quality, or better, to that of a supervised approach. We release our code and model weights.",EMNLP
"In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user{'}s smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10{\%} over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability.",EMNLP
"The dynamic nature of real-world information necessitates knowledge editing (KE) in large language models (LLMs). The edited knowledge should propagate and facilitate the deduction of new information based on existing model knowledge. We term the existing related knowledge in LLM serving as the origination of knowledge propagation as {''}deduction anchors{''}. However, current KE approaches, which only operate on (subject, relation, object) triple. We both theoretically and empirically observe that this simplified setting often leads to uncertainty when determining the deduction anchors, causing low confidence in their answers. To mitigate this issue, we propose a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor and enabling LLMs to propagate knowledge confidently. We curate a new benchmark dataset Evedit derived from the CounterFact dataset and validate its superiority in improving model confidence. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6{\%} consistency improvement while maintaining the naturalness of generation.",EMNLP
"We study LMs pretrained sequentially on two languages ({``}L2LMs{''}) for modeling nonnative sentence processing. In particular, we pretrain GPT2 on 6 different first languages (L1s), followed by English as the second language (L2). We examine the effect of the choice of pretraining L1 on the model{'}s ability to predict human reading times, evaluating on English readers from a range of L1 backgrounds. Experimental results show that, while all of the LMs{'} word surprisals improve prediction of L2 reading times, especially for human L1s distant from English, there is no reliable effect of the choice of L2LM{'}s L1. We also evaluate the learning trajectory of a monolingual English LM: for predicting L2 as opposed to L1 reading, it peaks much earlier and immediately falls off, possibly mirroring the difference in proficiency between the native and nonnative populations. Lastly, we provide examples of L2LMs{'} surprisals, which could potentially generate hypotheses about human L2 reading.",EMNLP
"We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct 50k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks.",EMNLP
"Training large language models (LLMs) for external tool usage is a rapidly expanding field, with recent research focusing on generating synthetic data to address the shortage of available data. However, the absence of systematic data quality checks poses complications for properly training and testing models. To that end, we propose two approaches for assessing the reliability of data for training LLMs to use external tools. The first approach uses intuitive, human-defined correctness criteria. The second approach uses a model-driven assessment with in-context evaluation. We conduct a thorough evaluation of data quality on two popular benchmarks, followed by an extrinsic evaluation that showcases the impact of data quality on model performance. Our results demonstrate that models trained on high-quality data outperform those trained on unvalidated data, even when trained with a smaller quantity of data. These findings empirically support the significance of assessing and ensuring the reliability of training data for tool-using LLMs.",EMNLP
"Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1{\%} and 6.5{\%} respectively. Additionally, we demonstrate our models{'} outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research. Our dataset is publicly available (https://github.com/leolya/CD-ADD).",EMNLP
"Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by a aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41{\%} tunable backbone parameters.",EMNLP
"Despite the advances in large language models (LLMs), how they use their knowledge for reasoning is not yet well understood.In this study, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with predecessors of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, a discrepancy in LLM performance on simpler sub-problems versus complex questions. We also measure backward discrepancy where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models exhibit more discrepancies than larger models. Distinct patterns of discrepancies are observed across model capacity and possibility of training data memorization. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.",EMNLP
"Large Language models (LLMs) have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance. However, this study reveals the misalignment between the translation-specific understanding and the general understanding inside LLMs. This understanding misalignment leads to LLMs mistakenly or literally translating some complicated concepts that they accurately comprehend in the general scenarios (e.g., QA). To align the translation-specific understanding to the general one, we propose a novel translation process, DUAT (Difficult words Understanding Aligned Translation), explicitly incorporating the general understanding on the complicated content incurring inconsistent understandings to guide the translation. Specifically, DUAT performs cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools to improve DUAT in detecting difficult words and generating helpful interpretations. We conduct experiments on the self-constructed benchmark Challenge-WMT, consisting of samples that are prone to mistranslation. Human evaluation results on high-resource and low-resource language pairs indicate that DUAT significantly facilitates the understanding alignment, which improves the translation quality (up to +3.85 COMET) and reduces translation literalness by -25{\%} Ã¢ÂˆÂ¼ -51{\%}.",EMNLP
"Word sense disambiguation (WSD) is a key task in natural language processing and lexical semantics. Pre-trained language models with contextualized word embeddings have significantly improved performance in regular WSD tasks. However, these models still struggle with recognizing semantic boundaries and often misclassify homonyms in adversarial context. Therefore, we propose FOOL: FOur-fold Obscure Lexical, a new coarse-grained WSD dataset, which includes four different test sets designed to assess the robustness of language models in WSD tasks. Two sets feature typical WSD scenarios, while the other two include sentences with opposing contexts to challenge the models further.We tested two types of models on the proposed dataset: models with encoders, such as the BERT and T5 series of varying sizes by probing their embeddings, and state-of-the-art large decoder models like GPT-4o and the LlaMA3 family, using zero shot prompting. Across different state-of-the-art language models, we observed a decrease in performance in the latter two sets compared to the first two, with some models being affected more than others. We show interesting findings where small models like T5-large and BERT-large performed better than GPT-4o on Set 3 of the dataset. This indicates that, despite excelling in regular WSD tasks, these models still struggle to correctly disambiguate homonyms in artificial (Set 3) or realistic adversarial contexts (Set 4).",EMNLP
"Instruction tuning, or supervised finetuning on extensive task-specific data, is necessary for Large Vision-Language Models (LVLMs) to generalize well across a broad range of vision-language (VL) tasks. However, training on large VL datasets can become prohibitively expensive. In this work, we introduce COINCIDE, an effective and scalable data selection technique that uses a small model as a reference model to select visual instruction tuning data for efficient finetuning of a target LVLM, focusing on diversity and transferability. Specifically, we cluster the training data using internal activations from a small model, which identifies VL concept-skill compositions needed by a target LVLM. We then sample data from these diverse clusters by considering their density and transferability, or the ability to transfer well to other concept-skill compositions. This approach ensures the diversity of these compositions, which is vital for LVLM generalization. Extensive experiments demonstrate that COINCIDE achieves superior performance and data selection efficiency against 8 strong baselines on two distinct datasets: LLaVA-1.5 and Vision-Flan. Using only 20{\%} of the LLaVA-1.5 dataset, COINCIDE achieves performance comparable to the LVLM finetuned on the whole dataset, with 70{\%} reduction of the wall-clock running time. On the Vision-Flan dataset, our method achieves superior results with only 16.7{\%} of the training data.",EMNLP
"Claim: This work is not advocating the use of LLMs for paper (meta-)reviewing. Instead, wepresent a comparative analysis to identify and distinguish LLM activities from human activities. Two research goals: i) Enable better recognition of instances when someone implicitly uses LLMs for reviewing activities; ii) Increase community awareness that LLMs, and AI in general, are currently inadequate for performing tasks that require a high level of expertise and nuanced judgment.This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload?This study focuses on the topic of LLMs as NLP Researchers, particularly examining the effectiveness of LLMs in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with {``}deficiency{''} labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) {``}LLMs as Reviewers{''}, how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) {``}LLMs as Metareviewers{''}, how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis.",EMNLP
"Commercially available models dominate academic leaderboards. While impressive, this has concentrated research on creating and adapting general-purpose models to improve NLP leaderboard standings for large language models. However, leaderboards collect many individual tasks and general-purpose models often underperform in specialized domains; domain-specific or adapted models yield superior results. This focus on large general-purpose models excludes many academics and draws attention away from areas where they can make important contributions. We advocate for a renewed focus on developing and evaluating domain- and task-specific models, and highlight the unique role of academics in this endeavor.",EMNLP
"In Machine Translation (MT) evaluations, the conventional approach is to compare a translated sentence against its human-created reference sentence. MT metrics provide an absolute score (e.g., from 0 to 1) to a candidate sentence based on the similarity with the reference sentence. Thus, existing MT metrics give the maximum score to the reference sentence. However, this approach overlooks the potential for a candidate sentence to exceed the reference sentence in terms of quality. In particular, recent advancements in Large Language Models (LLMs) have highlighted this issue, as LLM-generated sentences often exceed the quality of human-written sentences. To address the problem, we introduce the Residual score Metric (ResuMe), which evaluates the relative quality between reference and candidate sentences. ResuMe assigns a positive score to candidate sentences that outperform their reference sentences, and a negative score when they fall short. By adding the residual scores from ResuMe to the absolute scores from MT metrics, it can be possible to allocate higher scores to candidate sentences than what reference sentences are received from MT metrics. Experimental results demonstrate that ResuMe enhances the alignments between MT metrics and human judgments both at the segment-level and the system-level.",EMNLP
"Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.",EMNLP
"Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of English training data, raising concerns about potential cultural misrepresentation. To address these challenges, through a collaborative movement, we introduce SEACrowd, a comprehensive resource center that fills the resource gap by providing standardized corpora in nearly 1,000 SEA languages across three modalities. Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA. Furthermore, we propose strategies to facilitate greater AI advancements, maximizing potential utility and resource equity for the future of AI in Southeast Asia.",EMNLP
"Large Language Models (LLMs) have demonstrated capability in {``}instruction induction,{''} generating instructions from demonstrations (input-output pairs). However, existing methods often rely on large datasets or numerous examples, which is impractical and costly in real-world scenarios. In this work, we propose a low-cost, task-level framework called Induct-Learn. It induces pseudo instructions from a few demonstrations and a short phrase, adding a CoT process into existing demonstrations. When encountering new problems, the learned pseudo instructions and demonstrations with the pseudo CoT process can be combined into a prompt to guide the LLM{'}s problem-solving process. We validate our approach on the BBH-Induct and Evals-Induct datasets, and the results show that the Induct-Learn framework outperforms state-of-the-art methods. We also exhibit cross-model adaptability and achieve superior performance at a lower cost compared to existing methods.",EMNLP
No abstract found,EMNLP
"Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. However, LLMs are also prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model{'}s confidence on its generation, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce Luq and its two variations, a series of novel sampling-based UQ approaches specifically designed for long text. Our findings reveal that Luq outperforms existing baseline methods in correlating with the model{'}s factuality scores (negative coefficient of -0.85 observed for Gemini Pro). To further improve the factuality of LLM responses, we propose Luq-Ensemble, a method that ensembles responses from multiple models and selects the response with the lowest uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.",EMNLP
"As the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM{'}s training data through black-box access, have been explored. The Min-K{\%} Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at \url{https://github.com/zhang-wei-chao/DC-PDD}.",EMNLP
"Logical reasoning remains a challenge for natural language processing, but it can be improved by training language models to mimic theorem provers on procedurally generated problems. Previous work used domain-specific proof generation algorithms, which biases reasoning toward specific proof traces and limits auditability and extensibility. We present a simpler and more general declarative framework with flexible context-sensitive rules binding multiple languages (specifically, simplified English and the TPTP theorem-proving language). We construct first-order logic problems by selecting up to 32 premises and one hypothesis. We demonstrate that using semantic constraints during generation and careful English verbalization of predicates enhances logical reasoning without hurting natural English tasks. Using relatively small DeBERTa-v3 models, we achieve state-of-the-art accuracy on the FOLIO human-authored logic dataset, surpassing GPT-4 in accuracy with or without an external solver by 12{\%}.",EMNLP
"Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",EMNLP
"Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server{'}s limitation of handling only one client{'}s training request at a time hinders parallel training, severely impacting training efficiency. In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improving training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server. Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework.",EMNLP
"Having been trained on massive pretraining data, large language models have shown excellent performance on many knowledge-intensive tasks. However, pretraining data tends to contain misleading and even conflicting information, and it is intriguing to understand how LLMs handle these noisy data during training. In this study, we systematically analyze LLMs{'} learning preferences for data with conflicting knowledge. We find that pretrained LLMs establish learning preferences similar to humans, i.e., preferences towards formal texts and texts with fewer spelling errors, resulting in faster learning and more favorable treatment of knowledge in data with such features when facing conflicts. This finding is generalizable across models and languages and is more evident in larger models. An in-depth analysis reveals that LLMs tend to trust data with features that signify consistency with the majority of data, and it is possible to instill new preferences and erase old ones by manipulating the degree of consistency with the majority data.",EMNLP
"The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly multimodal in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. However, this effectiveness hinges on the appropriate selection of in-context examples, a process currently biased towards visual data, overlooking textual information. More importantly, the area of supervised retrievers for retrieval of multimodal in-context learning, crucial for optimal in-context example selection, continues to be investigated. Our study provides an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Based on the above finding, we introduce a novel supervised MLLM prompt retriever MSIER that leverages a trained retriever based on MLLM{'}s confidence to select examples, which enhances multimodal in-context learning efficiency. This approach is validated through extensive testing across three different tasks, demonstrating the method{'}s effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method{'}s training and explore the transferability of the supervised prompt retriever. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data. The public code is available at https://github.com/NUS-HPC-AI-Lab/Multimodal-ICL-Retriever.",EMNLP
"Collecting diverse human opinions is costly and challenging. This leads to a recent trend in exploiting large language models (LLMs) for generating diverse data for potential scalable and efficient solutions. However, the extent to which LLMs can generate diverse perspectives on subjective topics is still unclear. In this study, we explore LLMs{'} capacity of generating diverse perspectives and rationales on subjective topics such as social norms and argumentative texts. We introduce the problem of extracting maximum diversity from LLMs. Motivated by how humans form opinions based on values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting to generate more outputs from the model iteratively. Our methods, applied to various tasks, show that LLMs can indeed produce diverse opinions according to the degree of task subjectivity. We also find that LLMs performance of extracting maximum diversity is on par with human.",EMNLP
"Answering reasoning-based complex questions over text and hybrid sources, including tables, is a challenging task. Recent advances in large language models (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire proficiency in a specific task using only a few demonstration samples (exemplars). A critical challenge in ICL is the selection of optimal exemplars, which can be either task-specific (static) or test-example-specific (dynamic). Static exemplars provide faster inference times and increased robustness across a distribution of test examples. In this paper, we propose an algorithm for static exemplar subset selection for complex reasoning tasks. We introduce EXPLORA, a novel exploration method designed to estimate the parameters of the scoring function, which evaluates exemplar subsets without incorporating confidence information. EXPLORA significantly reduces the number of LLM calls to {\textasciitilde}11{\%} of those required by state-of-the-art methods and achieves a substantial performance improvement of 12.24{\%}. We open-source our code and data (https://github.com/kiranpurohit/EXPLORA).",EMNLP
"Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructiveness outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). In this paper we propose an LLM feature-based framework for dialogue constructiveness assessment that combines the strengths of feature-based and neural approaches, while mitigating their downsides. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature-based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models outperform or performs at least as well as standard feature-based models and neural models. We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models.",EMNLP
"Retrieving accurate domain knowledge and providing helpful information are crucial in developing an effective end-to-end task-oriented dialogue system (E2ETOD). The field has witnessed numerous methods following the retrieve-then-generate paradigm and training their systems on one specific domain. However, existing approaches still suffer from the Distractive Attributes Problem (DAP): struggling to deal with false but similar knowledge (hard negative entities), which is even more intractable when countless pieces of knowledge from different domains are blended in a real-world scenario. To alleviate DAP, we propose the Relevance-aware Adaptive Learning (ReAL) method, a two-stage training framework that eliminates hard negatives step-by-step and aligns retrieval with generation. In the first stage, we introduce a top-k adaptive contrastive loss and utilize the divergence-driven feedback from the frozen generator to pre-train the retriever. In the second stage, we propose using the metric score distribution as an anchor to align retrieval with generation. Thorough experiments on three benchmark datasets demonstrate ReAL{'}s superiority over existing methods, with extensive analysis validating its strong capabilities of overcoming in- and cross-domain distractions.",EMNLP
"Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability.In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow.To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss.Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains.",EMNLP
"Diffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied. In this paper, we examine how prompts affect image variability in black-box diffusion-based models. We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances. Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation. Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78{\%} of the time. Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10-50 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50-200 times. Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt{'}s length, CLIP embedding norm, concreteness, and word senses influence variability most. As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective. Our project page is at http://w1kp.com.",EMNLP
"In light of the recent 2024 European Parliament elections, we are investigating if LLMs can be used as Voting Advice Applications (VAAs). We audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the stance of political parties based on the latest {``}EU and I{''} voting assistance questionnaire. Furthermore, we explore alternatives to improve models{'} performance by augmenting the input context via Retrieval-Augmented Generation (RAG) relying on web search, and Self-Reflection using staged conversations that aim to re-collect relevant content from the model{'}s internal memory. We find that MIXTRAL is highly accurate with an 82{\%} accuracy on average with a significant performance disparity across different political groups (50-95{\%}). Augmenting the input context with expert-curated information can lead to a significant boost of approx. 9{\%}, which remains an open challenge for automated RAG approaches, even considering curated content.",EMNLP
"Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit sufficient knowledge from LLMs to answer a hard question. Meanwhile, a sophisticated one will force the LLM to generate redundant or even inaccurate intermediate steps toward a simple question. Consequently, the performance of existing methods fluctuates among various questions.In this work, we propose Adaption-of-Thought (AdoT), an adaptive method to improve LLMs for the reasoning problem, which first measures the question difficulty and then tailors demonstration set construction and difficulty-adapted retrieval strategies for the adaptive demonstration construction. Experimental results on three reasoning tasks prove the superiority of our proposed method, showing an absolute improvement of up to 5.5{\%} on arithmetic reasoning, 7.4{\%} on symbolic reasoning, and 2.3{\%} on commonsense reasoning. Our codes and implementation details are available at: https://github.com/NLPGM/AdoT",EMNLP
"Document-level relation extraction (DocRE) aims to identify relationships between entities within a document. Due to the vast number of entity pairs, fully annotating all fact triplets is challenging, resulting in datasets with numerous false negative samples. Recently, self-training-based methods have been introduced to address this issue. However, these methods are purely black-box and sub-symbolic, making them difficult to interpret and prone to overlooking symbolic interdependencies between relations.To remedy this deficiency, our insight is that symbolic knowledge, such as logical rules, can be used as diagnostic tools to identify conflicts between pseudo-labels. By resolving these conflicts through logical diagnoses, we can correct erroneous pseudo-labels, thus enhancing the training of neural models.To achieve this, we propose **LogicST**, a neural-logic self-training framework that iteratively resolves conflicts and constructs the minimal diagnostic set for updating models. Extensive experiments demonstrate that LogicST significantly improves performance and outperforms previous state-of-the-art methods. For instance, LogicST achieves an increase of **7.94{\%}** in F1 score compared to CAST (Tan et al., 2023a) on the DocRED benchmark (Yao et al., 2019). Additionally, LogicST is more time-efficient than its self-training counterparts, requiring only **10{\%}** of the training time of CAST.",EMNLP
"Multilingual large language models (LLMs) seem to generalize somewhat across languages. We hypothesize this is a result of implicit vector space alignment. Evaluating such alignment, we see that larger models exhibit very high-quality linear alignments between corresponding concepts in different languages. Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts. For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear {--} an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods.",EMNLP
"Transformer-based large language models (LLMs) exhibit limitations such as generating unsafe responses, unreliable reasoning, etc. Existing inference intervention approaches attempt to mitigate these issues by finetuning additional models to produce calibration signals (such as rewards) that guide the LLM{'}s decoding process. However, this solution introduces substantial time and space overhead due to the separate models required. This work proposes Non-disruptive parameters insertion (Otter), inserting extra parameters into the transformer architecture to predict calibration signals along with the original LLM output. Otter offers state-of-the-art performance on multiple demanding tasks while saving up to 86.5{\%} extra space and 98.5{\%} extra time. Furthermore, Otter seamlessly integrates with existing inference engines, requiring only a one-line code change, and the original model response remains accessible after the parameter insertion.",EMNLP
"Norwegian, spoken by only 5 million population, is under-representative within the most impressive breakthroughs in NLP tasks. To the best of our knowledge, there has not yet been a comprehensive evaluation of the existing language models (LMs) on Norwegian generation tasks during the article writing process. To fill this gap, we 1) compiled the existing Norwegian dataset and pre-trained 4 Norwegian Open Language Models varied from parameter scales and architectures, collectively called NorGLM; 2) introduced a comprehensive benchmark, NLEBench, for evaluating natural language generation capabilities in Norwegian, encompassing translation and human annotation. Based on the investigation, we find that: 1) the mainstream, English-dominated LM GPT-3.5 has limited capability in understanding the Norwegian context; 2) the increase in model parameter scales demonstrates limited impact on the performance of downstream tasks when the pre-training dataset is constrained in size; 3) smaller models also demonstrate the reasoning capability through Chain-of-Thought; 4) a multi-task dataset that includes synergy tasks can be used to verify the generalizability of LLMs on natural language understanding and, meanwhile, test the interconnectedness of these NLP tasks. We share our resources and code for reproducibility under a CC BY-NC 4.0 license.",EMNLP
"Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency. Our code is available at https://github.com/Ewanwong/RSA-Control.",EMNLP
"The scaling of large language models (LLMs) is a critical research area for the efficiency and effectiveness of model training and deployment. Our work investigates the transferability and discrepancies of scaling laws between Dense Models and Mixture of Experts (MoE) models. Through a combination of theoretical analysis and extensive experiments, including consistent loss scaling, optimal batch size/learning rate scaling, and resource allocation strategies scaling, our findings reveal that the power-law scaling framework also applies to MoE Models, indicating that the fundamental principles governing the scaling behavior of these models are preserved, even though the architecture differs. Additionally, MoE Models demonstrate superior generalization, resulting in lower testing losses with the same training compute budget compared to Dense Models. These findings indicate the scaling consistency and transfer generalization capabilities of MoE Models, providing new insights for optimizing MoE Model training and deployment strategies.",EMNLP
"End-to-end Task-Oriented Dialog (TOD) systems typically require extensive training datasets to perform well. In contrast, large language model (LLM) based TOD systems can excel even with limited data due to their ability to learn tasks through in-context exemplars. However, these models lack alignment with the style of responses in training data and often generate comprehensive responses, making it difficult for users to grasp the information quickly. In response, we propose SyncTOD that synergizes LLMs with task-specific hints to improve alignment in low-data settings. SyncTOD employs small auxiliary models to provide hints and select exemplars for in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings.",EMNLP
"Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM based RAG system, by incorporating a specially designed assessnent module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR.",EMNLP
"Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong{'}s test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model{'}s long-context modeling capabilities.",EMNLP
"How far have we come in mitigating performance disparities across genders in multilingual speech recognition? We compare the impact on gender disparity of different fine-tuning algorithms for automated speech recognition across model sizes, languages and gender. We look at both performance-focused and fairness-promoting algorithms. Across languages, we see slightly better performance for female speakers for larger models regardless of the fine-tuning algorithm. The best trade-off between performance and parity is found using adapter fusion. Fairness-promoting fine-tuning algorithms (Group-DRO and Spectral Decoupling) hurt performance compared to adapter fusion with only slightly better performance parity. LoRA increases disparities slightly. Fairness-mitigating fine-tuning techniques led to slightly higher variance in performance across languages, with the exception of adapter fusion.",EMNLP
"The field of privacy-preserving Natural Language Processing has risen in popularity, particularly at a time when concerns about privacy grow with the proliferation of large language models. One solution consistently appearing in recent literature has been the integration of Differential Privacy (DP) into NLP techniques. In this paper, we take these approaches into critical view, discussing the restrictions that DP integration imposes, as well as bring to light the challenges that such restrictions entail. To accomplish this, we focus on **DP-Prompt**, a recent method for text privatization leveraging language models to rewrite texts. In particular, we explore this rewriting task in multiple scenarios, both with DP and without DP. To drive the discussion on the merits of DP in NLP, we conduct empirical utility and privacy experiments. Our results demonstrate the need for more discussion on the usability of DP in NLP and its benefits over non-DP approaches.",EMNLP
"In recent years, multimodal large language models (MLLMs) have attracted widespread attention from both industry and academia. Based on the integration position, MLLMs can be categorized into external and internal fusion architectures, with the former being more predominant. However, there remains considerable debate on how to construct the optimal external fusion MLLM architecture, especially regarding the performance of different connectors on tasks with varying granularities. This paper systematically investigates the impact of connectors on MLLM performance. Specifically, we classify connectors into feature-preserving and feature-compressing types. Utilizing a unified classification standard, we categorize sub-tasks from three comprehensive benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained perception, fine-grained perception, and reasoning, and evaluate the performance from this perspective. Our findings reveal significant performance differences between different types of connectors across various tasks, offering essential guidance for MLLM architecture design and advancing the understanding of MLLM architecture optimization.",EMNLP
"The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world{'}s languages. An increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being typologically diverse. In this meta-analysis, we systematically investigate NLP research that includes claims regarding typological diversity. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of resulting language samples along several axes and find that the results vary considerably across papers. Crucially, we show that skewed language selection can lead to overestimated multilingual performance. We recommend future work to include an operationalization of typological diversity that empirically justifies the diversity of language samples. To help facilitate this, we release the code for our diversity measures.",EMNLP
"The ability for individuals to constructively engage with one another across lines of difference is a critical feature of a healthy pluralistic society. This is also true in online discussion spaces like social media platforms. To date, much social media research has focused on preventing ills{---}like political polarization and the spread of misinformation. While this is important, enhancing the quality of online public discourse requires not just reducing ills, but also, promoting foundational human virtues. In this study, we focus on one particular virtue: {``}intellectual humility{''} (IH), or acknowledging the potential limitations in one{'}s own beliefs. Specifically, we explore the development of computational methods for measuring IH at scale. We manually curate and validate an IH codebook on 350 posts about religion drawn from subreddits and use them to develop LLM-based models for automating this measurement. Our best model achieves a Macro-F1 score of 0.64 across labels (and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an expected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human annotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results both highlight the challenging nature of detecting IH online{---}opening the door to new directions in NLP research{---}and also lay a foundation for computational social science researchers interested in analyzing and fostering more IH in online public discourse.",EMNLP
"The inability to utilise future contexts and the pre-determined left-to-right generation order are major limitations of unidirectional language models. Bidirectionality has been introduced to address those deficiencies. However, a crucial shortcoming of bidirectional language models is the potential inconsistency of their conditional distributions. This fundamental flaw greatly diminishes their applicability and hinders their capability of tractable sampling and likelihood computation. In this work, we introduce a class of bidirectional language models, called latent language models, that are consistent by definition and can be efficiently used both for generation and scoring of sequences. We define latent language models based on the well-understood formalism of bisequential decompositions from automata theory. This formal correspondence allows us to precisely charaterise the abilities and limitations of a subclass of latent language models, called rational language models. As a result, we obtain that latent language models are exponentially more concise and significantly more expressive than unidirectional language models.",EMNLP
"Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM{'}s geo-diverse cultural understanding. We curate a diverse collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.",EMNLP
"Code-switching (CS) is the process of speakers interchanging between two or more languages which in the modern world becomes increasingly common. In order to better describe CS speech the Matrix Language Frame (MLF) theory introduces the concept of a Matrix Language, which is the language that provides the grammatical structure for a CS utterance. In this work the MLF theory was used to develop systems for Matrix Language Identity (MLID) determination. The MLID of English/Mandarin and English/Spanish CS text and speech was compared to acoustic language identity (LID), which is a typical way to identify a language in monolingual utterances. MLID predictors from audio show higher correlation with the textual principles than LID in all cases while also outperforming LID in an MLID recognition task based on F1 macro (60{\%}) and correlation score (0.38). This novel approach has identified that non-English languages (Mandarin and Spanish) are preferred over the English language as the ML contrary to the monolingual choice of LID.",EMNLP
"Emotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance.",EMNLP
"Despite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever growing sizes only increasing the barrier for use. One particular issue stems from the high latency associated with auto-regressive generation in LLMs, rendering the largest LLMs difficult to use without advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger expert model{'}s generation, has helped alleviate this concern, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain of interest relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains as long as an individual draft model is effective. We observe these results hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play.",EMNLP
"With the rise of large language models (LLMs), many studies are interested in transferring the reasoning capabilities of LLMs to small language models (SLMs). Previous distillation methods usually utilize the capabilities of LLMs to generate chain-of-thought (CoT) samples and teach SLMs via fine-tuning. However, such a standard distillation approach performs poorly when applied to out-of-distribution (OOD) examples, and the diversity of the generated CoT samples is insufficient. In this work, we propose a novel counterfactual distillation framework. Firstly, we leverage LLMs to automatically generate high-quality counterfactual data. Given an input text example, our method generates a counterfactual example that is very similar to the original input, but its task label has been changed to the desired one. Then, we utilize multi-view CoT to enhance the diversity of reasoning samples. Experiments on four NLP benchmarks show that our approach enhances the reasoning capabilities of SLMs and is more robust to OOD data. We also conduct extensive ablations and sample studies to understand the reasoning capabilities of SLMs.",EMNLP
"In this paper, we explore the utility of Translationese as synthetic data created using machine translation for pre-training language models (LMs) for low-resource languages (LRLs). Our simple methodology consists of translating large amounts of web-crawled monolingual documents (clean) into the LRLs, followed by filtering the translated documents using tiny LMs trained on small but clean LRL data. Taking the case of Indian languages, we pre-train LMs from scratch with 28M and 85M parameters, and then fine-tune them for 5 downstream natural language understanding (NLU) and 4 generative (NLG) tasks. We observe that pre-training on filtered synthetic data leads to relative performance drops of only 0.87{\%} for NLU and 2.35{\%} for NLG, compared to pre-training on clean data, and this gap further diminishes upon the inclusion of a small amount of clean data. We also study the impact of synthetic data filtering and the choice of source language for synthetic data generation. Furthermore, evaluating continually pre-trained larger models like Gemma-2B and Llama-3-8B in few-shot settings, we observe that using synthetic data is competitive with using clean data. Our findings suggest that synthetic data shows promise for bridging the pre-training gap between English and LRLs.",EMNLP
"There is a scarcity of multilingual vision-language models that properly account for the perceptual differences that are reflected in image captions across languages and cultures. In this work, through a multimodal, multilingual retrieval case study, we quantify the existing lack of model flexibility. We empirically show performance gaps between training on captions that come from native German perception and captions that have been either machine-translated or human-translated from English into German. To address these gaps, we further propose and evaluate caption augmentation strategies. While we achieve mean recall improvements (+1.3), gaps still remain, indicating an open area of future work for the community.",EMNLP
"Although Dense Passage Retrieval (DPR) models have achieved significantly enhanced performance, their widespread application is still hindered by the demanding inference efficiency and high deployment costs. Knowledge distillation is an efficient method to compress models, which transfers knowledge from strong teacher models to weak student models. Previous studies have proved the effectiveness of knowledge distillation in DPR. However, there often remains a significant performance gap between the teacher and the distilled student. To narrow this performance gap, we propose MTA4DPR, a Multi-Teaching-Assistants based iterative knowledge distillation method for Dense Passage Retrieval, which transfers knowledge from the teacher to the student with the help of multiple assistants in an iterative manner; with each iteration, the student learns from more performant assistants and more difficult data. The experimental results show that our 66M student model achieves the state-of-the-art performance among models with same parameters on multiple datasets, and is very competitive when compared with larger, even LLM-based, DPR models.",EMNLP
"Solidarity is a crucial concept to understand social relations in societies. In this study, we investigate the frequency of (anti-)solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets, we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other models, approaching human annotation accuracy. Using GPT-4, we automatically annotate 18,300 further instances and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. This study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion.",EMNLP
"Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) withoutaffecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity. The code and data have been released at https://github.com/ybai-nlp/CItruS.",EMNLP
"We present a novel approach to modeling fictional narratives. The proposed model creates embeddings that represent a story such that similar narratives, that is, reformulations of the same story, will result in similar embeddings. We showcase the prowess of our narrative-focused embeddings on various datasets, exhibiting state-of-the-art performance on multiple retrieval tasks. The embeddings also show promising results on a narrative understanding task. Additionally, we perform an annotation-based evaluation to validate that our introduced computational notion of narrative similarity aligns with human perception. The approach can help to explore vast datasets of stories, with potential applications in recommender systems and in the computational analysis of literature.",EMNLP
"Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveals that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character. Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves a 2.1{\%} enhancement in general scenarios and a significant 12{\%} improvement in vertical domain scenarios compared to existing methods, establishing state-of-the-art performance.",EMNLP
"Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks.",EMNLP
"Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8{\%}, 9.9{\%}, 18.6{\%}, and 10.1{\%} on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.",EMNLP
"Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",EMNLP
"Language models strongly rely on frequency information because they maximize the likelihood of tokens during pre-training. As a consequence, language models tend to not generalize well to tokens that are seldom seen during training. Moreover, maximum likelihood training has been discovered to give rise to anisotropy: representations of tokens in a model tend to cluster tightly in a high-dimensional cone, rather than spreading out over their representational capacity.Our work introduces a method for quantifying the frequency bias of a language model by assessing sentence-level perplexity with respect to token-level frequency. We then present a method for reducing the frequency bias of a language model by inducing a syntactic prior over token representations during pre-training. Our Syntactic Smoothing method adjusts the maximum likelihood objective function to distribute the learning signal to syntactically similar tokens. This approach results in better performance on infrequent English tokens and a decrease in anisotropy. We empirically show that the degree of anisotropy in a model correlates with its frequency bias.",EMNLP
"Detecting hate speech and offensive language is essential for maintaining a safe and respectful digital environment. This study examines the limitations of state-of-the-art large language models (LLMs) in identifying offensive content within systematically perturbed data, with a focus on Chinese, a language particularly susceptible to such perturbations. We introduce ToxiCloakCN, an enhanced dataset derived from ToxiCN, augmented with homophonic substitutions and emoji transformations, to test the robustness of LLMs against these cloaking perturbations. Our findings reveal that existing models significantly underperform in detecting offensive content when these perturbations are applied. We provide an in-depth analysis of how different types of offensive content are affected by these perturbations and explore the alignment between human and model explanations of offensiveness. Our work highlights the urgent need for more advanced techniques in offensive language detection to combat the evolving tactics used to evade detection mechanisms.",EMNLP
"Analogical reasoning plays a critical role in human cognition, enabling us to understand new concepts by associating them with familiar ones. Previous research in the AI community has mainly focused on identifying and generating analogies and then examining their quality under human evaluation, which overlooks the practical application of these analogies in real-world settings. Inspired by the human education process, in this paper, we propose to investigate how analogies created by teacher language models (LMs) can assist student LMs in understanding scientific concepts, thereby aligning more closely with practical scenarios. Our results suggest that free-form analogies can indeed aid LMs in understanding concepts. Additionally, analogies generated by student LMs can improve their own performance on scientific question answering, demonstrating their capability to use analogies for self-learning new knowledge. Resources are available athttps://github.com/siyuyuan/SCUA.",EMNLP
"Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs{'} context usage throughout the generation. In this work, we present MIRAGE {--} Model Internals-based RAG Explanations {--} a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE{'}s attributions and underscores the promising application of model internals for RAG answer attribution. Code and data released at https://github.com/Betswish/MIRAGE.",EMNLP
"Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. Nevertheless, the rapid advancement in their deployment trails a comprehensive understanding of their internal mechanisms, as well as a delineation of their capabilities and limitations. A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this attribute, we develop a benchmark that challenges these models to enumerate all information they possess on specific topics. This benchmark assesses whether the models recall excessive, insufficient, or the precise amount of required information, thereby indicating their awareness of how much they know about the given topic. Our findings reveal that the emergence of this property varies across different architectures and manifests at diverse rates. However, with sufficient scaling, all tested models are ultimately capable of performing this task. The insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics.",EMNLP
"Eliciting chain of thought (CoT) rationales - sequences of token that convey a {``}reasoning{''} process has been shown to consistently improve LLM performance on tasks like question answering. More recent efforts have shown that such rationales can also be used for model distillation: Including CoT sequences (elicited from a large {``}teacher{''} model) in addition to target labels when fine-tuning a small student model yields (often substantial) improvements. In this work we ask: Why and how does this additional training signal help in model distillation? We perform ablations to interrogate this, and report some potentially surprising results. Specifically: (1) Placing CoT sequences after labels (rather than before) realizes consistently better downstream performance {--} this means that no student {``}reasoning{''} is necessary at test time to realize gains. (2) When rationales are appended in this way, they need not be coherent reasoning sequences to yield improvements; performance increases are robust to permutations of CoT tokens, for example. In fact, (3) a small number of key tokens are sufficient to achieve improvements equivalent to those observed when full rationales are used in model distillation.",EMNLP
"Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics.",EMNLP
"Image-text contrastive models like CLIP have wide applications in zero-shot classification, image-text retrieval, and transfer learning. However, they often struggle on compositional visio-linguistic tasks (e.g., attribute-binding or object-relationships) where their performance is no better than random chance. To address this, we introduce SDS-CLIP, a lightweight and sample-efficient distillation method to enhance CLIP{'}s compositional visio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation objective borrowed from large text-to-image generative models like Stable-Diffusion, which are known for their strong visio-linguistic reasoning abilities. On the challenging Winoground benchmark, SDS-CLIP improves the visio-linguistic performance of various CLIP models by up to 7{\%}, while on the ARO dataset, it boosts performance by up to 3{\%}. This work underscores the potential of well-designed distillation objectives from generative models to enhance contrastive image-text models with improved visio-linguistic reasoning capabilities.",EMNLP
"Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks.As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to {``}distill{''} LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85{\%} F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness.",EMNLP
"Language models (LMs) are known to suffer from hallucinations and misinformation. Retrieval augmented generation (RAG) that retrieves verifiable information from an external knowledge corpus to complement the parametric knowledge in LMs provides a tangible solution to these problems. However, the generation quality of RAG is highly dependent on the relevance between a user{'}s query and the retrieved documents. Inaccurate responses may be generated when the query is outside of the scope of knowledge represented in the external knowledge corpus or if the information in the corpus is out-of-date. In this work, we establish a statistical framework that assesses how well a query can be answered by an RAG system by capturing the relevance of knowledge. We introduce an online testing procedure that employs goodness-of-fit (GoF) tests to inspect the relevance of each user query to detect out-of-knowledge queries with low knowledge relevance. Additionally, we develop an offline testing framework that examines a collection of user queries, aiming to detect significant shifts in the query distribution which indicates the knowledge corpus is no longer sufficiently capable of supporting the interests of the users. We demonstrate the capabilities of these strategies through a systematic evaluation on eight question-answering (QA) datasets, the results of which indicate that the new testing framework is an efficient solution to enhance the reliability of existing RAG systems.",EMNLP
"Recent research has focused on examining Large Language Models{'} (LLMs) characteristics from a psychological standpoint, acknowledging the necessity of understanding their behavioral characteristics. The administration of personality tests to LLMs has emerged as a noteworthy area in this context. However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate. Our study aims to determine the reliability of applying personality assessments to LLMs, explicitly investigating whether LLMs demonstrate consistent personality traits. Analysis of 2,500 settings per model, including GPT-3.5, GPT-4, Gemini-Pro, and LLaMA-3.1, reveals that various LLMs show consistency in responses to the Big Five Inventory, indicating a satisfactory level of reliability. Furthermore, our research explores the potential of GPT-3.5 to emulate diverse personalities and represent various groups{---}a capability increasingly sought after in social sciences for substituting human participants with LLMs to reduce costs. Our findings reveal that LLMs have the potential to represent different personalities with specific prompt instructions.",EMNLP
"Massive-scale historical document collections are crucial for social science research. Despite increasing digitization, these documents typically lack unique cross-document identifiers for individuals mentioned within the texts, as well as individual identifiers from external knowledge bases like Wikipedia/Wikidata. Existing entity disambiguation methods often fall short in accuracy for historical documents, which are replete with individuals not remembered in contemporary knowledge bases. This study makes three key contributions to improve cross-document coreference resolution and disambiguation in historical texts: a massive-scale training dataset replete with hard negatives - that sources over 190 million entity pairs from Wikipedia contexts and disambiguation pages - high-quality evaluation data from hand-labeled historical newswire articles, and trained models evaluated on this historical benchmark. We contrastively train bi-encoder models for coreferencing and disambiguating individuals in historical texts, achieving accurate, scalable performance that identifies out-of-knowledge base individuals. Our approach significantly surpasses other entity disambiguation models on our historical newswire benchmark. Our models also demonstrate competitive performance on modern entity disambiguation benchmarks, particularly on certain news disambiguation datasets.",EMNLP
"Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs such as LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate descriptive visual attributes based on a concept that appears within an input image despite their prominent zero-shot image captioning ability. In-depth analyses show that instruction-tuned LVLMs suffer from modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept. In an effort to further the community{'}s endeavor in this direction, we propose a multiple granularity attribute-centric benchmark and training mixture, Finer, which aims to establish a ground to evaluate LVLMs{'} fine-grained visual comprehension ability and provide significantly improved explainability.",EMNLP
"One useful application of NLP models is to support people in reading complex text from unfamiliar domains (e.g., scientific articles). Simplifying the entire text makes it understandable but sometimes removes important details. On the contrary, helping adult readers understand difficult concepts in context can enhance their vocabulary and knowledge. In a preliminary human study, we first identify that lack of context and unfamiliarity with difficult concepts is a major reason for adult readers{'} difficulty with domain-specific text. We then introduce targeted concept simplification, a simplification task for rewriting text to help readers comprehend text containing unfamiliar concepts. We also introduce WikiDomains, a new dataset of 22k definitions from 13 academic domains paired with a difficult concept within each definition. We benchmark the performance of open-source and commercial LLMs and a simple dictionary baseline on this task across human judgments of ease of understanding and meaning preservation. Interestingly, our human judges preferred explanations about the difficult concept more than simplifications of the concept phrase. Further, no single model achieved superior performance across all quality dimensions, and automated metrics also show low correlations with human evaluations of concept simplification ({\textasciitilde}0.2), opening up rich avenues for research on personalized human reading comprehension support.",EMNLP
"As large vision-language models (LVLMs) evolve rapidly, the demand for high-quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time-intensive. In this paper, we investigate the efficacy of AI feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the first large-scale vision-language feedback dataset, comprising over 82K multi-modal instructions and comprehensive rationales generated by off-the-shelf models without human annotations. To evaluate the effectiveness of AI feedback for vision-language alignment, we train Silkie, an LVLM fine-tuned via direct preference optimization on VLFeedback. Silkie showcases exceptional performance regarding helpfulness, visual faithfulness, and safety metrics. It outperforms its base model by 6.9{\%} and 9.5{\%} in perception and cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits enhanced resilience against red-teaming attacks. Furthermore, our analysis underscores the advantage of AI feedback, particularly in fostering preference diversity to deliver more comprehensive improvements. Our dataset, training code and models are available at \url{https://vlf-silkie.github.io}.",EMNLP
"In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We hypothesize that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content, which we validate both theoretically and experimentally. Inspired by how humans learn from examples, we propose a training-free method FocusICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FocusICL based on model perplexity of demonstrations. Comprehensive experiments validate that FocusICL achieves an average performance improvement of 5.2{\%} over vanilla ICL and scales well with many-shot demonstrations.",EMNLP
"Historically, sign language machine translation has been posed as a sentence-level task: datasets consisting of continuous narratives are chopped up and presented to the model as isolated clips. In this work, we explore the limitations of this task framing. First, we survey a number of linguistic phenomena in sign languages that depend on discourse-level context. Then as a case study, we perform the first human baseline for sign language translation that actually substitutes a human into the machine learning task framing, rather than provide the human with the entire document as context. This human baseline{---}for ASL to English translation on the How2Sign dataset{---}shows that for 33{\%} of sentences in our sample, our fluent Deaf signer annotators were only able to understand key parts of the clip in light of additional discourse-level context. These results underscore the importance of understanding and sanity checking examples when adapting machine learning to new domains.",EMNLP
"Perceiving and understanding non-speech sounds and non-verbal speech is essential to making decisions that help us interact with our surroundings. In this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model (LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We build GAMA by integrating an LLM with multiple types of audio representations, including features from a custom Audio Q-Former, a multi-layer aggregator that aggregates features from multiple layers of an audio encoder. We fine-tune GAMA on a large-scale audio-language dataset, which augments it with audio understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for Complex Audio Reasoning), a synthetically generated instruction-tuning (IT) dataset with instructions that require the model to perform complex reasoning on the input audio. We instruction-tune GAMA with CompA-R to endow it with complex reasoning abilities, where we further add a soft prompt as input with high-level semantic evidence by leveraging event tags of the input audio. Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for evaluating the capabilities of LALMs on open-ended audio question-answering that requires complex reasoning. Through automated and expert human evaluations, we show that GAMA outperforms all other LALMs in literature on diverse audio understanding tasks by margins of 1{\%}-84{\%} and demonstrates state-of-the-art performance on deductive reasoning and hallucination evaluation benchmarks. Further, GAMA IT-ed on CompA-R proves to be superior in its complex reasoning capabilities.",EMNLP
"We introduce a new database of cognate words and etymons for the five main Romance languages, the most comprehensive one to date. We propose a strong benchmark for the automatic reconstruction of protowords for Romance languages, by applying a set of machine learning models and features on these data. The best results reach 90{\%} accuracy in predicting the protoword of a given cognate set, surpassing existing state-of-the-art results for this task and showing that computational methods can be very useful in assisting linguists with protoword reconstruction.",EMNLP
"While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.",EMNLP
"Large language models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. While unintuitive from a classic view of LMs, recent work has shown that the truth value of a statement can be elicited from the model{'}s representations. This paper presents an explanation for why LMs appear to know the truth despite not being trained with truth labels. We hypothesize that the pretraining data is generated by groups of (un)truthful agents whose outputs share common features, and they form a (un)truthful persona. By training on this data, LMs can infer and represent the persona in its activation space. This allows the model to separate truth from falsehoods and controls the truthfulness of its generation. We show evidence for the persona hypothesis via two observations: (1) we can probe whether a model{'}s answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that structures of the pretraining data are crucial for the model to infer the truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.",EMNLP
"Large language models (LLMs) are capable of producing documents, and retrieval augmented generation (RAG) has shown itself to be a powerful method for improving accuracy without sacrificing fluency. However, not all information can be retrieved from text. We propose an approach that uses the analysis of structured data to generate fact sets that are used to guide generation in much the same way that retrieved documents are used in RAG. This analytics augmented generation (AAG) approach supports the ability to utilize standard analytic techniques to generate facts that are then converted to text and passed to an LLM. We present a neurosymbolic platform, Satyrn, that leverages AAG to produce accurate, fluent, and coherent reports grounded in large scale databases. In our experiments, we find that Satyrn generates reports in which over 86{\%} of claims are accurate while maintaining high levels of fluency and coherence, even when using smaller language models such as Mistral-7B, as compared to GPT-4 Code Interpreter in which just 57{\%} of claims are accurate.",EMNLP
"In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised learning approach for speech representation learning. In contrast to the prior methods that use random masking schemes for Masked Acoustic Modeling (MAM), we introduce a novel selective and adaptive masking strategy. Specifically, during SSL training, we progressively introduce harder regions to the model for reconstruction. Our approach automatically selects hard regions and is built on the observation that the reconstruction loss of individual frames in MAM can provide natural signals to judge the difficulty of solving the MAM pre-text task for that frame. To identify these hard regions, we employ a teacher model that first predicts the frame-wise losses and then decides which frames to mask. By learning to create challenging problems, such as identifying harder frames and solving them simultaneously, the model is able to learn more effective representations and thereby acquire a more comprehensive understanding of the speech. Quantitatively, EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks by 5{\%}-10{\%}. Additionally, we conduct a thorough analysis to show that the regions masked by EH-MAM effectively capture useful context across speech frames.",EMNLP
"Long-horizon decision-making tasks present significant challenges for LLM-based agents due to the need for extensive planning over multiple steps. In this paper, we propose a hierarchical framework that decomposes complex tasks into manageable subgoals, utilizing separate LLMs for subgoal prediction and low-level action generation. To address the challenge of creating training signals for unannotated datasets, we develop a reward model that leverages multimodal environment feedback to automatically generate reward signals. We introduce Environment Preference Optimization (EPO), a novel method that generates preference signals from the environment{'}s feedback and uses them to train LLM-based agents. Extensive experiments on ALFRED demonstrate the state-of-the-art performance of our framework, achieving first place on the ALFRED public leaderboard and showcasing its potential to improve long-horizon decision-making in diverse environments.",EMNLP
"The diversity of text can be measured beyond word-level features, however existing diversity evaluation focuses primarily on word-level features. Here we propose a method for evaluating diversity over syntactic features to characterize general repetition in models, beyond frequent $n$-grams. Specifically, we define \textit{syntactic templates} (e.g., strings comprising parts-of-speech) and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference textsWe find that most (76{\%}) templates in model-generated text can be found in pre-training data (compared to only 35{\%} of human-authored text), and are not overwritten during fine-tuning or alignment processes such as RLHF. The connection between templates in generated text and the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data.We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions.Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs.",EMNLP
"Smaller-scale Vision-Language Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the {``}Uncontextualized Uncommon Objects{''} (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs. Code and project details for UOUO can be found at https://zoezheng126.github.io/UOUO-Website/.",EMNLP
"In this work, we optimize speculative sampling for parallel hardware accelerators to improve sampling speed. We notice that substantial portions of the intermediate matrices necessary for speculative sampling can be computed concurrently. This allows us to distribute the workload across multiple GPU threads, enabling simultaneous operations on matrix segments within thread blocks. This results in profiling time improvements ranging from 6{\%} to 13{\%} relative to the baseline implementation, without compromising accuracy. To further accelerate speculative sampling, probability distributions parameterized by softmax are approximated by sigmoid. This approximation approach results in significantly greater relative improvements in profiling time, ranging from 37{\%} to 94{\%}, with a minor decline in accuracy. We conduct extensive experiments on both automatic speech recognition and summarization tasks to validate the effectiveness of our optimization methods.",EMNLP
"Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce Personalized Pieces (Per-Pcs), a framework that allows users to safely share and assemble personalized PEFT efficiently with collaborative efforts. Per-Pcs involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental results show Per-Pcs outperforms non-personalized and PEFT retrieval baselines, offering performance comparable to OPPU with significantly lower resource use across six tasks. Further analysis highlights Per-Pcs{'}s robustness concerning sharer count and selection strategy, pieces sharing ratio, and scalability in computation time and storage space. Per-Pcs{'}s modularity promotes safe sharing, making LLM personalization more efficient, effective, and widely accessible through collaborative efforts.",EMNLP
"Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs{'} interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU{'}s enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.",EMNLP
"In the real world, documents are organized in different formats and varied modalities. Traditional retrieval pipelines require tailored document parsing techniques and content extraction modules to prepare input for indexing. This process is tedious, prone to errors, and has information loss. To this end, we propose Document Screenshot Embedding (DSE), a novel retrieval paradigm that regards document screenshots as a unified input format, which does not require any content extraction preprocess and preserves all the information in a document (e.g., text, image and layout). DSE leverages a large vision-language model to directly encode document screenshots into dense representations for retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a 1.3M Wikipedia web page screenshots as the corpus to answer the questions from the Natural Questions dataset. In such a text-intensive document retrieval setting, DSE shows competitive effectiveness compared to other text retrieval methods relying on parsing. For example, DSE outperforms BM25 by 17 points in top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide retrieval, DSE significantly outperforms OCR text retrieval methods by over 15 points in nDCG@10. These experiments show that DSE is an effective document retrieval paradigm for diverse types of documents. Model checkpoints, code, and Wiki-SS collection will be released.",EMNLP
"Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer.",EMNLP
"We audit how hallucination in large language models (LLMs) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through the examination of the literature, we identify a lack of agreement with the term {`}hallucination{'} in the field of NLP. Additionally, to compliment our audit, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis calls for the necessity of explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.",EMNLP
"Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various *knowledge-critical* subnetworks: particular sparse computational subgraphs that can, if removed, precisely suppress specific knowledge the model has memorized. We propose a multi-objective differentiable masking scheme that can be applied to both weights and neurons to discover such subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98{\%}+ sparsity) that are critical for expressing specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial abilities but struggles to represent the suppressed knowledge.",EMNLP
"Significant advancements have recently been made in large language models, represented by GPT models.Users frequently have multi-round private conversations with cloud-hosted GPT models for task optimization.Yet, this operational paradigm introduces additional attack surfaces, particularly in custom GPTs and hijacked chat sessions.In this paper, we introduce a straightforward yet potent Conversation Reconstruction Attack.This attack targets the contents of previous conversations between GPT models and benign users, i.e., the benign users{'} input contents during their interaction with GPT models.The adversary could induce GPT models to leak such contents by querying them with designed malicious prompts.Our comprehensive examination of privacy risks during the interactions with GPT models under this attack reveals GPT-4{'}s considerable resilience.We present two advanced attacks targeting improved reconstruction of past conversations, demonstrating significant privacy leakage across all models under these advanced techniques.Evaluating various defense mechanisms, we find them ineffective against these attacks.Our findings highlight the ease with which privacy can be compromised in interactions with GPT models, urging the community to safeguard against potential abuses of these models{'} capabilities.",EMNLP
"Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *{``}In which city was Silvio Berlusconi{'}s first wife born?{''}*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *{``}Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?{''}* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons ($R^3$), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks{---}question answering, claim verification, and preference matching{---}our findings showcase $R^3$ as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",EMNLP
"Recent advances in Large Language Models (LLM) have led to substantial interest in their application to commonsense reasoning tasks. Despite their potential, LLMs are susceptible to reasoning errors and hallucinations that may be harmful in use cases where accurate reasoning is critical. This challenge underscores the need for verifiable, debuggable, and repairable LLM reasoning. Recent works have made progress toward verifiable reasoning with LLMs by using them as either (i) a reasoner over an axiomatic knowledge base, or (ii) a semantic parser for use in existing logical inference systems. However, both settings are unable to extract commonsense axioms from the LLM that are not already formalized in the knowledge base, and also lack a reliable method to repair missed commonsense inferences. In this work, we present LLM-TRes, a logical reasoning framework based on the notion of {``}theory resolution{''} that allows for seamless integration of the commonsense knowledge from LLMs with a verifiable logical reasoning framework that mitigates hallucinations and facilitates debugging of the reasoning procedure as well as repair. We crucially prove that repaired axioms are theoretically guaranteed to be given precedence over flawed ones in our theory resolution inference process. We conclude by evaluating on three diverse language-based reasoning tasks{---}preference reasoning, deductive reasoning, and causal commonsense reasoning{---}and demonstrate the superior performance of LLM-TRes vs. state-of-the-art LLM-based reasoning methods in terms of both accuracy and reasoning correctness.",EMNLP
"We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user{'}s desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation.",EMNLP
"Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting a lack of generalisation ability. By contrast, systems such as causal models, that learn abstract variables and causal relationships, can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks. We also investigate the level of independence and domain specialisation and show that LLMs rely on pre-trained partially domain-invariant mechanisms resilient to fine-tuning.",EMNLP
"This study explores the effectiveness of Large Language Models (LLMs) in creating personalized {``}mirror stories{''} that reflect and resonate with individual readers{'} identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories.",EMNLP
"Large language models (LLMs) have demonstrated the potential to mimic human social intelligence. However, most studies focus on simplistic and static self-report or performance-based tests, which limits the depth and validity of the analysis. In this paper, we developed a novel framework, InterIntent, to assess LLMs{'} social intelligence by mapping their ability to understand and manage intentions in a game setting. We focus on four dimensions of social intelligence: situational awareness, self-regulation, self-awareness, and theory of mind. Each dimension is linked to a specific game task: intention selection, intention following, intention summarization, and intention guessing. Our findings indicate that while LLMs exhibit high proficiency in selecting intentions, achieving an accuracy of 88{\%}, their ability to infer the intentions of others is significantly weaker, trailing human performance by 20{\%}. Additionally, game performance correlates with intention understanding, highlighting the importance of the four components towards success in this game. These findings underline the crucial role of intention understanding in evaluating LLMs{'} social intelligence and highlight the potential of using social deduction games as a complex testbed to enhance LLM evaluation. InterIntent contributes a structured approach to bridging the evaluation gap in social intelligence within multiplayer LLM-based games.",EMNLP
"To explain social phenomena and identify systematic biases, much research in computational social science focuses on comparative text analyses. These studies often rely on coarse corpus-level statistics or local word-level analyses, mainly in English. We introduce the InfoGap method{---}an efficient and reliable approach to locating information gaps and inconsistencies in articles at the fact level, across languages. We evaluate InfoGap by analyzing LGBT people{'}s portrayals, across 2.7K biography pages on English, Russian, and French Wikipedias. We find large discrepancies in factual coverage across the languages. Moreover, our analysis reveals that biographical facts carrying negative connotations are more likely to be highlighted in Russian Wikipedia. Crucially, InfoGap both facilitates large scale analyses, and pinpoints local document- and fact-level information gaps, laying a new foundation for targeted and nuanced comparative language analysis at scale.",EMNLP
"Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models{'} cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures {--} underscoring the necessity for enhancing multicultural understanding in vision-language models.",EMNLP
"Textual style expresses a diverse set of information, including interpersonal dynamics (e.g., formality) and the author{'}s emotions or attitudes (e.g., disgust). An open question is how language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. One approach to such controlled generation is multi-objective reinforcement learning (RL), but how to best combine multiple objectives in a reward function is an open question. In this paper, we investigate various formulations of multi-style reward formulations, including calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that our proposed dynamic weighting outperforms static weighting approaches with respect style control while maintaining linguistic quality, and we explore its effectiveness in 2- and 3-style control.",EMNLP
"Projecting visual features into word embedding space has become a significant fusion strategy adopted by Multimodal Large Language Models (MLLMs). However, its internal mechanisms have yet to be explored. Inspired by multilingual research, we identify domain-specific neurons in multimodal large language models. Specifically, we investigate the distribution of domain-specific neurons and the mechanism of how MLLMs process features from diverse domains. Furthermore, we propose a three-stage framework for language model modules in MLLMs when handling projected image features, and verify this hypothesis using logit lens. Extensive experiments indicate that while current MLLMs exhibit Visual Question Answering (VQA) capability, they may not fully utilize domain-specific information. Manipulating domain-specific neurons properly will result in a 10{\%} change of accuracy at most, shedding light on the development of cross-domain, all-encompassing MLLMs in the future. The source code is available at https://anonymous.4open.science/r/MMNeuron.",EMNLP
"Recent advances in machine learning have significantly impacted the field of information extraction, with Language Models (LMs) playing a pivotal role in extracting structured information from unstructured text. Prior works typically represent information extraction as triplet-centric and use classical metrics such as precision and recall for evaluation. We reformulate the task to be entity-centric, enabling the use of diverse metrics that can provide more insights from various perspectives. We contribute to the field by introducing Structured Entity Extraction and proposing the Approximate Entity Set OverlaP (AESOP) metric, designed to appropriately assess model performance. Later, we introduce a new Multistage Structured Entity Extraction (MuSEE) model that harnesses the power of LMs for enhanced effectiveness and efficiency by decomposing the extraction task into multiple stages. Quantitative and human side-by-side evaluations confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction. Our source code is available at https://github.com/microsoft/Structured-Entity-Extraction.",EMNLP
"LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks. However, when using pairwise comparisons to rank a set of candidates, the computational cost scales quadratically with the number of candidates, which has practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair{'}s score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate well with human judgements. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. With many candidate texts, using as few as 2{\%} of comparisons the PoE solution can achieve similar performance to when all comparisons are used.",EMNLP
"In the ten years since the development of the Abstract Meaning Representation (AMR) formalism, substantial progress has been made on AMR-related tasks such as parsing and alignment. Still, the engineering applications of AMR are not fully understood. In this survey, we categorize and characterize more than 100 papers which use AMR for downstream tasks{---} the first survey of this kind for AMR. Specifically, we highlight (1) the range of applications for which AMR has been harnessed, and (2) the techniques for incorporating AMR into those applications. We also detect broader AMR engineering patterns and outline areas of future work that seem ripe for AMR incorporation. We hope that this survey will be useful to those interested in using AMR and that it sparks discussion on the role of symbolic representations in the age of neural-focused NLP research.",EMNLP
"Visual representation learning has been a cornerstone in computer vision, involving typical forms such as visual embeddings, structural symbols, and text-based representations. Despite the success of CLIP-type visual embeddings, they often lack access to world knowledge critical for visual reasoning. In this work, we propose Visual Table, a novel form of visual representation tailored for visual reasoning. Visual tables are constructed as hierarchical descriptions of visual scenes, featuring a scene description and multiple object-centric descriptions covering categories, attributes, and knowledge. Thanks to the structural and textual formats, visual tables offer unique properties over mere visual embeddings, such as explainability and controllable editing. Furthermore, they deliver instance-level world knowledge and detailed attributes that are essential for visual reasoning. To create visual tables, we develop a generator trained on the dataset with collected, small-scale annotations. Extensive results on 11 visual reasoning benchmarks demonstrate that the generated visual tables significantly outperform previous structural and text-based representations. Moreover, they consistently enhance state-of-the-art multi-modal large language models across diverse benchmarks, showcasing their potential for advancing visual reasoning tasks. Our code is available at https://github.com/LaVi-Lab/Visual-Table.",EMNLP
"Caregiver strategy classification in pediatric rehabilitation contexts is strongly motivated by real-world clinical constraints but highly under-resourced and seldom studied in natural language processing settings. We introduce a large dataset of 4,037 caregiver strategies in this setting, a five-fold increase over the nearest contemporary dataset. These strategies are manually categorized into clinically established constructs with high agreement ($\kappa$=0.68-0.89). We also propose two techniques to further address identified data constraints. First, we manually supplement target task data with publicly relevant data from online child health forums. Next, we propose a novel data augmentation technique to generate synthetic caregiver strategies with high downstream task utility. Extensive experiments showcase the quality of our dataset. They also establish evidence that both the publicly available data and the synthetic strategies result in large performance gains, with relative F$_1$ increases of 22.6{\%} and 50.9{\%}, respectively.",EMNLP
"Ensuring the security of released large language models (LLMs) poses a significant dilemma, as existing mechanisms either compromise ownership rights or raise data privacy concerns. To address this dilemma, we introduce TaylorMLP to protect the ownership of released LLMs and prevent their abuse. Specifically, TaylorMLP preserves the ownership of LLMs by transforming the weights of LLMs into parameters of Taylor-series. Instead of releasing the original weights, developers can release the Taylor-series parameters with users, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent abuse of LLMs by adjusting the generation speed. It can induce low-speed token generation for the protected LLMs by increasing the terms in the Taylor-series. This intentional delay helps LLM developers prevent potential large-scale unauthorized uses of their models. Empirical experiments across five datasets and three LLM architectures demonstrate that TaylorMLP induces over increase in latency, producing the tokens precisely matched with original LLMs. Subsequent defensive experiments further confirm that TaylorMLP effectively prevents users from reconstructing the weight values based on downstream datasets.",EMNLP
"Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMs{'} temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR$^4$.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8{\%} and 22.5{\%} on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",EMNLP
"Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 18.2 to 27.2 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments.",EMNLP
"Large language models (LLMs) can handle multilingual and cross-lingual text within a single input; however, previous works leveraging multilingualism in LLMs primarily focus on using English as the pivot language to enhance language understanding and reasoning. Given that multiple languages are a compensation for the losses caused by a single language{'}s limitations, it{'}s a natural next step to enrich the model{'}s learning context through the integration of the original input with its multiple translations. In this paper, we start by revealing that LLMs learn from parallel multilingual input (PMI). Our comprehensive evaluation shows that PMI enhances the model{'}s comprehension of the input, achieving superior performance than conventional in-context learning (ICL). Furthermore, to explore how multilingual processing affects prediction, we examine the activated neurons in LLMs. Surprisingly, involving more languages in the input activates fewer neurons, leading to more focused and effective neural activation patterns. Also, this neural reaction coincidently mirrors the neuroscience insight about synaptic pruning, highlighting a similarity between artificial and biological {`}brains{'}.",EMNLP
"Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval.",EMNLP
"The era of Large Language Models (LLMs) raises new demands for automatic evaluation metrics, which should be adaptable to various application scenarios while maintaining low cost and effectiveness. Traditional metrics for automatic text evaluation are often tailored to specific scenarios, while LLM-based evaluation metrics are costly, requiring fine-tuning or rely heavily on the generation capabilities of LLMs. Besides, previous LLM-based metrics ignore the fact that, within the space of LLM representations, there exist direction vectors that indicate the estimation of text quality. To this end, we introduce RepEval, a metric that leverages the projection of LLM representations for evaluation. Through simple prompt modifications, RepEval can easily transition to various tasks, requiring only minimal sample pairs for direction vector construction. Results on fourteen datasets across two evaluation tasks demonstrate the high effectiveness of our method, which exhibits a higher correlation with human judgments than previous methods, even in complex evaluation scenarios involving pair-wise selection under nuanced aspects. Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics.",EMNLP
"Medical decision rules play a key role in many clinical decision support systems (CDSS). However, these rules are conventionally constructed by medical experts, which is expensive and hard to scale up. In this study, we explore the automatic extraction of medical decision rules from text, leading to a solution to construct large-scale medical decision rules. We adopt a formulation of medical decision rules as binary trees consisting of condition/decision nodes. Such trees are referred to as medical decision trees and we introduce several generative models to extract them from text. The proposed models inherit the merit of two categories of successful natural language generation frameworks, i.e., sequence-to-sequence generation and autoregressive generation. To unleash the potential of pretrained language models, we design three styles of linearization (natural language, augmented natural language and JSON code), acting as the target sequence for our models. Our final system achieves 67{\%} tree accuracy on a comprehensive Chinese benchmark, outperforming state-of-the-art baseline by 12{\%}. The result demonstrates the effectiveness of generative models on explicitly modeling structural decision-making roadmaps, and shows great potential to boost the development of CDSS and explainable AI. Our code will be open-source upon acceptance.",EMNLP
"Seeking answers effectively for long videos is essential to build video question answering (videoQA) systems. Previous methods adaptively select frames and regions from long videos to save computations. However, this fails to reason over the whole sequence of video, leading to sub-optimal performance. To address this problem, we introduce a state space layer (SSL) into multi-modal Transformer to efficiently integrate global semantics of the video, which mitigates the video information loss caused by frame and region selection modules. Our SSL includes a gating unit to enable controllability over the flow of global semantics into visual representations. To further enhance the controllability, we introduce a cross-modal compositional congruence objective to encourage global semantics aligned with the question. To rigorously evaluate long-form videoQA capacity, we construct two new benchmarks Ego-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5 minutes and 1.9 hours, respectively. Extensive experiments demonstrate the superiority of our framework on these new as well as existing datasets.",EMNLP
"Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM{'}s representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.",EMNLP
"Legal case retrieval (LCR) aims to provide similar cases as references for a given fact description. This task is crucial for promoting consistent judgments in similar cases, effectively enhancing judicial fairness and improving work efficiency for judges. However, existing works face two main challenges for real-world applications: existing works mainly focus on case-to-case retrieval using lengthy queries, which does not match real-world scenarios; and the limited data scale, with current datasets containing only hundreds of queries, is insufficient to satisfy the training requirements of existing data-hungry neural models. To address these issues, we introduce an automated method to construct synthetic query-candidate pairs and build the largest LCR dataset to date, LEAD, which is hundreds of times larger than existing datasets. This data construction method can provide ample training signals for LCR models. Experimental results demonstrate that model training with our constructed data can achieve state-of-the-art results on two widely-used LCR benchmarks. Besides, the construction method can also be applied to civil cases and achieve promising results. The data and codes can be found in https://github.com/thunlp/LEAD.",EMNLP
"Large language models (LLMs) have demonstrated remarkable capabilities in comprehensively handling various types of natural language processing (NLP) tasks. However, there are significant differences in the knowledge and abilities required for different tasks. Therefore, it is important to understand whether the same LLM processes different tasks in the same way. Are there specific neurons in a LLM for different tasks? Inspired by neuroscience, this paper pioneers the exploration of whether distinct neurons are activated when a LLM handles different tasks. Compared with current research exploring the neurons of language and knowledge, task-specific neurons present a greater challenge due to their abstractness, diversity, and complexity. To address these challenges, this paper proposes a method for task-specific neuron localization based on Causal Gradient Variation with Special Tokens (CGVST). CGVST identifies task-specific neurons by concentrating on the most significant tokens during task processing, thereby eliminating redundant tokens and minimizing interference from non-essential neurons. Compared to traditional neuron localization methods, our approach can more effectively identify task-specific neurons. We conduct experiments across eight different public tasks. Experiments involving the inhibition and amplification of identified neurons demonstrate that our method can accurately locate task-specific neurons.",EMNLP
No abstract found,EMNLP
"Acoustic foundation models, fine-tuned for Automatic Speech Recognition (ASR), suffer from performance degradation in wild acoustic test settings when deployed in real-world scenarios. Stabilizing online Test-Time Adaptation (TTA) under these conditions remains an open and unexplored question. Existing wild vision TTA methods often fail to handle speech data effectively due to the unique characteristics of high-entropy speech frames, which are unreliably filtered out even when containing crucial semantic content. Furthermore, unlike static vision data, speech signals follow short-term consistency, requiring specialized adaptation strategies. In this work, we propose a novel wild acoustic TTA method tailored for ASR fine-tuned acoustic foundation models. Our method, Confidence-Enhanced Adaptation, performs frame-level adaptation using a confidence-aware weight scheme to avoid filtering out essential information in high-entropy frames. Additionally, we apply consistency regularization during test-time optimization to leverage the inherent short-term consistency of speech signals. Our experiments on both synthetic and real-world datasets demonstrate that our approach outperforms existing baselines under various wild acoustic test settings, including Gaussian noise, environmental sounds, accent variations, and sung speech.",EMNLP
"We introduce iterative retrieval, a novel framework that empowers retrievers to make iterative decisions through policy optimization. Finding an optimal portfolio of retrieved items is a combinatorial optimization problem, generally considered NP-hard. This approach provides a learned approximation to such a solution, meeting specific task requirements under a given family of large language models (LLMs). We propose a training procedure based on reinforcement learning, incorporating feedback from LLMs. We instantiate an iterative retriever for composing in-context learning (ICL) exemplars and apply it to various semantic parsing tasks that demand synthesized programs as outputs. By adding only 4M additional parameters for state encoding, we convert an off-the-shelf dense retriever into a stateful iterative retriever, outperforming previous methods in selecting ICL exemplars on semantic parsing datasets such as CalFlow, TreeDST, and MTOP. Additionally, the trained iterative retriever generalizes across different inference LLMs beyond the one used during training.",EMNLP
"Academic paper search is an essential task for efficient literature discovery and scientific advancement. While dense retrieval has advanced various ad-hoc searches, it often struggles to match the underlying academic concepts between queries and documents, which is critical for paper search. To enable effective academic concept matching for paper search, we propose Taxonomy-guided Semantic Indexing (TaxoIndex) framework. TaxoIndex extracts key concepts from papers and organizes them as a semantic index guided by an academic taxonomy, and then leverages this index as foundational knowledge to identify academic concepts and link queries and documents. As a plug-and-play framework, TaxoIndex can be flexibly employed to enhance existing dense retrievers. Extensive experiments show that TaxoIndex brings significant improvements, even with highly limited training data, and greatly enhances interpretability.",EMNLP
"Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the logical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models. In particular, MultiPoT achieves more than 4.6{\%} improvement on average on ChatGPT (gpt-3.5-turbo-0701).",EMNLP
"Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy Coordinate Gradient (GCG). However, GCG struggles with computational inefficiency, limiting further investigations regarding suffix transferability and scalability across models and data. In this work, we bridge the connection between search efficiency and suffix transferability. We propose a two-stage transfer learning framework, DeGCG, which decouples the search process into behavior-agnostic pre-searching and behavior-relevant post-searching. Specifically, we employ direct first target token optimization in pre-searching to facilitate the search process. We apply our approach to cross-model, cross-data, and self-transfer scenarios. Furthermore, we introduce an interleaved variant of our approach, i-DeGCG, which iteratively leverages self-transferability to accelerate the search process. Experiments on HarmBench demonstrate the efficiency of our approach across various models and domains. Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of 43.9 ($+ 22.2$) and 39.0 ($+19.5$) on valid and test sets, respectively. Further analysis on cross-model transfer indicates the pivotal role of first target token optimization in leveraging suffix transferability for efficient searching.",EMNLP
"Although existing fashionable generation methods on Incomplete Utterance Rewriting (IUR) can generate coherent utterances, they often result in the inclusion of irrelevant and redundant tokens in rewritten utterances due to their inability to focus on critical tokens in dialogue context. Furthermore, the limited size of the training datasets also contributes to the insufficient training of the IUR model. To address the first issue, we propose a multi-task learning framework EO-IUR (Editing Operation-guided Incomplete Utterance Rewriting) that introduces the editing operation labels generated by sequence labeling module to guide generation model to focus on critical tokens. Furthermore, we introduce a token-level heterogeneous graph to represent dialogues. To address the second issue, we propose a two-dimensional utterance augmentation strategy, namely editing operation-based incomplete utterance augmentation and LLM-based historical utterance augmentation. The experimental results on three datasets demonstrate that our EO-IUR outperforms previous state-of-the-art (SOTA) baselines in both open-domain and task-oriented dialogue.",EMNLP
"Fuzzy reasoning is vital due to the frequent use of imprecise information in daily contexts. However, the ability of current large language models (LLMs) to handle such reasoning remains largely uncharted. In this paper, we introduce a new benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical word problems that incorporate generalized quantifiers. Our experimental findings reveal that fuzzy reasoning continues to pose significant challenges for LLMs. Moreover, we find that existing methods designed to enhance reasoning do not consistently improve performance in tasks involving fuzzy logic. Additionally, our results show an inverse scaling effect in the performance of LLMs on FRoG. Interestingly, we also demonstrate that strong mathematical reasoning skills are not necessarily indicative of success on our benchmark.",EMNLP
"Large language models such as ChatGPT exhibit striking political biases. If users query them about political information, they often take a normative stance. To overcome this, we align LLMs with diverse political viewpoints from 100,000 comments written by candidates running for national parliament in Switzerland. Models aligned with this data can generate more accurate political viewpoints from Swiss parties, compared to commercial models such as ChatGPT. We also propose a procedure to generate balanced overviews summarizing multiple viewpoints using such models. The replication package contains all code and data.",EMNLP
"Social science research has shown that candidates with names indicative of certain races or genders often face discrimination in employment practices. Similarly, Large Language Models (LLMs) have demonstrated racial and gender biases in various applications. In this study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate hiring decisions and salary recommendations for candidates with 320 first names that strongly signal their race and gender, across over 750,000 prompts. Our empirical results indicate a preference among these models for hiring candidates with White female-sounding names over other demographic groups across 40 occupations. Additionally, even among candidates with identical qualifications, salary recommendations vary by as much as 5{\%} between different subgroups. A comparison with real-world labor data reveals inconsistent alignment with U.S. labor market characteristics, underscoring the necessity of risk investigation of LLM-powered systems.",EMNLP
"Scaling the rotary position embedding (RoPE) has become a common method for extending the context window of RoPE-based large language models (LLMs). However, existing scaling methods often rely on empirical approaches and lack a profound understanding of the internal distribution within RoPE, resulting in suboptimal performance in extending the context window length. In this paper, we propose to optimize the context window extending task from the view of rotary angle distribution. Specifically, we first estimate the distribution of the rotary angles within the model and analyze the extent to which length extension perturbs this distribution. Then, we present a novel extension strategy that minimizes the disturbance between rotary angle distributions to maintain consistency with the pre-training phase, enhancing the model{'}s capability to generalize to longer sequences. Experimental results compared to the strong baseline methods demonstrate that our approach reduces by up to 72{\%} of the distributional disturbance when extending LLaMA2{'}s context window to 8k, and reduces by up to 32{\%} when extending to 16k. On the LongBench-E benchmark, our method achieves an average improvement of up to 4.33{\%} over existing state-of-the-art methods. Furthermore, Our method maintains the model{'}s performance on the Hugging Face Open LLM benchmark after context window extension, with only an average performance fluctuation ranging from -0.12 to +0.22.",EMNLP
"This study evaluates the effectiveness of pre-trained language models in identifying argument structure constructions, important for modeling both first and second language learning. We examine three methodologies: (1) supervised training with RoBERTa using a gold-standard ASC treebank, including by-tag accuracy evaluation for sentences from both native and non-native English speakers, (2) prompt-guided annotation with GPT-4, and (3) generating training data through prompts with GPT-4, followed by RoBERTa training. Our findings indicate that RoBERTa trained on gold-standard data shows the best performance. While data generated through GPT-4 enhances training, it does not exceed the benchmarks set by gold-standard data.",EMNLP
"Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs{'} reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs{'} capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37{\%}. Our data and code can be found here https://github.com/cathyxl/MAgIC.",EMNLP
"The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.",EMNLP
"The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed{'}s large-scale, de-identified medical image-text pairs to address these limitations, they often fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an {`}unblinded{'} capacity to denoise and reformat the data, resulting in the creation of the **PubMedVision** dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of MLLMs, showing significant improvement in benchmarks including the MMMU Health {\&} Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM **HuatuoGPT-Vision**, which shows superior performance in medical multimodal scenarios among open-source MLLMs. Our code and data are available at https://github.com/FreedomIntelligence/HuatuoGPT-Vision.",EMNLP
No abstract found,EMNLP
"In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs{'} internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.",EMNLP
"We present three innovations in tokenization and subword segmentation. First, we propose to use unsupervised morphological analysis with Morfessor as pre-tokenization. Second, we present an algebraic method for obtaining subword embeddings grounded in a word embedding space. Based on that, we design a novel subword segmentation algorithm that uses the embeddings, ensuring that the procedure considers lexical meaning. Third, we introduce an efficient segmentation algorithm based on a subword bigram model that can be initialized with the lexically aware segmentation method to avoid using Morfessor and large embedding tables at inference time. We evaluate the proposed approaches using two intrinsic metrics and measure their performance on two downstream tasks: part-of-speech tagging and machine translation. Our experiments show significant improvements in the morphological plausibility of the segmentation when evaluated using segmentation precision on morpheme boundaries and improved R{\'e}nyi efficiency in 8 languages. Although the proposed tokenization methods do not have a large impact on automatic translation quality, we observe consistent performance gains in the arguably more morphological task of part-of-speech tagging.",EMNLP
"Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios of up to **5x**, which is 1.3x that of EAGLE. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a **lossless** acceleration algorithm.",EMNLP
"Large language models are able to generate code for visualisations in response to simple user requests.This is a useful application and an appealing one for NLP research because plots of data provide grounding for language.However, there are relatively few benchmarks, and those that exist may not be representative of what users do in practice.This paper investigates whether benchmarks reflect real-world use through an empirical study comparing benchmark datasets with code from public repositories.Our findings reveal a substantial gap, with evaluations not testing the same distribution of chart types, attributes, and actions as real-world examples.One dataset is representative, but requires extensive modification to become a practical end-to-end benchmark. This shows that new benchmarks are needed to support the development of systems that truly address users{'} visualisation needs.These observations will guide future data creation, highlighting which features hold genuine significance for users.",EMNLP
"While Large Language Models (LLMs) demonstrate impressive generation abilities, they frequently struggle when it comes to specialized domains due to their limited domain-specific knowledge. Studies on domain-specific LLMs resort to expanding the vocabulary before fine-tuning on domain-specific corpus, aiming to decrease the sequence length and enhance efficiency during decoding, without thoroughly investigating the results of vocabulary expansion to LLMs over different domains. Our pilot study reveals that expansion with only a subset of the entire vocabulary may lead to superior performance. Guided by the discovery, this paper explores how to identify a vocabulary subset to achieve the optimal results. We introduce VEGAD, an adaptive method that automatically identifies valuable words from a given domain vocabulary. Our method has been validated through experiments on three Chinese datasets, demonstrating its effectiveness. Additionally, we have undertaken comprehensive analyses of the method. The selection of a optimal subset for expansion has shown to enhance performance on both domain-specific tasks and general tasks, showcasing the potential of VEGAD.",EMNLP
"Recent studies highlight the effectiveness of using in-context learning (ICL) to steer large language models (LLMs) in processing tabular data, a challenging task given the structured nature of such data. Despite advancements in performance, the fairness implications of these methods are less understood. This study investigates how varying demonstrations within ICL prompts influence the fairness outcomes of LLMs. Our findings reveal that deliberately including minority group samples in prompts significantly boosts fairness without sacrificing predictive accuracy. Further experiments demonstrate that the proportion of minority to majority samples in demonstrations affects the trade-off between fairness and prediction accuracy. Based on these insights, we introduce a mitigation technique that employs clustering and evolutionary strategies to curate a diverse and representative sample set from the training data. This approach aims to enhance both predictive performance and fairness in ICL applications. Experimental results validate that our proposed method dramatically improves fairness across various metrics, showing its efficacy in real-world scenarios.",EMNLP
"Vietnamese, a low-resource language, is typically categorized into three primary dialect groups that belong to Northern, Central, and Southern Vietnam. However, each province within these regions exhibits its own distinct pronunciation variations. Despite the existence of various speech recognition datasets, none of them has provided a fine-grained classification of the 63 dialects specific to individual provinces of Vietnam. To address this gap, we introduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive dataset capturing the rich diversity of 63 provincial dialects spoken across Vietnam. Our dataset comprises 102.56 hours of audio, consisting of approximately 19,000 utterances, and the associated transcripts contain over 1.2 million words. To provide benchmarks and simultaneously demonstrate the challenges of our dataset, we fine-tune state-of-the-art pre-trained models for two downstream tasks: (1) Dialect identification and (2) Speech recognition. The empirical results suggest two implications including the influence of geographical factors on dialects, and the constraints of current approaches in speech recognition tasks involving multi-dialect speech data. Our dataset is available for research purposes.",EMNLP
"Large Language Models (LLMs) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-LLMs to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment LLMs, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores. Since adversaries may not know or have access to the judge-LLMs, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-LLMs. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of LLM-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.",EMNLP
"Large Language Models (LLMs) have exhibited exceptional performance across diverse domains. However, recent studies reveal that LLMs are plagued by the {``}reversal curse{''}. Most existing methods rely on aggressive sample permutation and pay little attention to delving into the underlying reasons for this issue, resulting in only partial mitigation. In this paper, inspired by human knowledge reversal, we investigate and quantify the individual influence of three potential reasons on the reversal curse: 1) knowledge clarity, 2) entity correlation modeling, and 3) pairwise relationship reasoning capability. Motivated by the analysis of these reasons, we propose a novel **P**airwise entity **O**rder- and **R**elationship-**E**nhanced (**PORE**) data strategy, which facilitates bidirectional entity correlation modeling and pairwise relationship reasoning to overcome the reversal curse. Specifically, PORE augments the samples with entity order-reversal and semantically preserved question-answer pairs, enhancing the encoding of entity correlations in both directions. PORE also employs entity-interleaved pairwise relationship data, which elevates the model{'}s capability for relationship reasoning. Additionally, to improve the recall of reverse relationships, we leverage knowledge clarity to construct high-clarity data for PORE. Extensive experimental results on available and two newly assembled datasets demonstrate the effectiveness and generalization of our method in both data-sufficient and -constrained situations.",EMNLP
"The performance on general tasks decreases after Large Language Models (LLMs) are fine-tuned on domain-specific tasks, the phenomenon is known as Catastrophic Forgetting (CF). However, this paper presents a further challenge for real application of domain-specific LLMs beyond CF, called General Capabilities Integration (GCI), which necessitates the integration of both the general capabilities and domain knowledge within a single instance. The objective of GCI is not merely to retain previously acquired general capabilities alongside new domain knowledge, but to harmonize and utilize both sets of skills in a cohesive manner to enhance performance on domain-specific tasks. Taking legal domain as an example, we carefully design three groups of training and testing tasks without lacking practicability, and construct the corresponding datasets. To better incorporate general capabilities across domain-specific scenarios, we introduce ALoRA, which utilizes a multi-head attention module upon LoRA, facilitating direct information transfer from preceding tokens to the current one. This enhancement permits the representation to dynamically switch between domain-specific knowledge and general competencies according to the attention. Extensive experiments are conducted on the proposed tasks. The results exhibit the significance of our setting, and the effectiveness of our method.",EMNLP
No abstract found,EMNLP
"Large Language Models (LLMs) have been shown to effectively perform zero-shot document retrieval, a process that typically consists of two steps: i) retrieving relevant documents, and ii) re-ranking them based on their relevance to the query. This paper presents GENRA, a new approach to zero-shot document retrieval that incorporates rank aggregation to improve retrieval effectiveness. Given a query, GENRA first utilizes LLMs to generate informative passages that capture the query{'}s intent. These passages are then employed to guide the retrieval process, selecting similar documents from the corpus. Next, we use LLMs again for a second refinement step. This step can be configured for either direct relevance assessment of each retrieved document or for re-ranking the retrieved documents. Ultimately, both approaches ensure that only the most relevant documents are kept. Upon this filtered set of documents, we perform multi-document retrieval, generating individual rankings for each document. As a final step, GENRA leverages rank aggregation, combining the individual rankings to produce a single refined ranking. Extensive experiments on benchmark datasets demonstrate that GENRA improves existing approaches, highlighting the effectiveness of the proposed methodology in zero-shot retrieval.",EMNLP
"Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM{'}s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the \textit{debugger-scores} for multidimensional quality analysis. Our explanations include \textit{why-choose} and \textit{why-not-choose} components, \textit{reason-elements}, and \textit{debugger-scores} that collectively illuminate the LLM{'}s reasoning behavior. Our evaluations demonstrate XplainLLM{'}s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available.",EMNLP
"The automation of radiology report generation (RRG) holds immense potential to alleviate radiologists{'} workloads and improve diagnostic accuracy. Despite advancements in image captioning and vision-language pretraining, RRG remains challenging due to the lengthy and complex nature of radiology reports. In this work, we proposes the Divide and Conquer Radiology Report Generation (DCRRG) model, which breaks down full-text radiology reports into concise observation descriptions. This approach enables the model to capture fine-grained representations from each observation through a two-stage process: an encoding stage focusing on observation prediction tasks to learn fine-grained representations, and a decoding stage for integrating these descriptions into cohesive and comprehensive radiology reports. Experimental results on two benchmark datasets demonstrate that DCRRG achieves significant improvements across all evaluated metrics, underscoring its capability to generate semantically coherent and clinically accurate radiology reports.",EMNLP
"Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs{'} Retrieval-Augmented Generation (RAG) capabilities remains underutilized. Existing works either focus solely on the text modality or are limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize retrieved information and are sensitive to irrelevant or misleading references. To address these challenges, we propose a self-refinement framework designed to teach LVLMs to \textbf{S}electively \textbf{U}tilize \textbf{R}etrieved In\textbf{f}ormation (SURf). Specifically, when given questions that are incorrectly answered by the LVLM backbone, we obtain references that help correct the answers (positive references) and those that do not (negative references). We then fine-tune the LVLM backbone using a combination of these positive and negative references. Our experiments across three tasks and seven datasets demonstrate that our framework significantly enhances LVLMs{'} ability to effectively utilize retrieved multimodal references and improves their robustness against irrelevant or misleading information. The source code is available at https://anonymous.4open.science/r/SURf-6433.",EMNLP
"Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions. With large language models (LLMs) demonstrating powerful capabilities between tasks, we can{'}t help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions with the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player.",EMNLP
"The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist agents capable of operating within complex environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, we seek to investigate the intriguing potential of tools to augment LLMs in handling such complexity by introducing a novel class of tools, termed *middleware*, to aid in the proactive exploration within these massive environments. Such specialized tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments{---}knowledge bases (KBs) and databases{---}we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with the middleware, GPT-4 achieves **2.8**X the performance of the best baseline in tasks requiring access to database content and **2.2**X in KB tasks. Our findings illuminate the path for advancing language agents in real-world applications.",EMNLP
"Personalized Dialogue Generation (PDG) aims to create coherent responses according to roles or personas. Traditional PDG relies on external role data, which can be scarce and raise privacy concerns. Approaches address these issues by extracting role information from dialogue history, which often fail to generically model roles in continuous space. To overcome these limitations, we introduce a novel framework Models Roles from Personalized Dialogue History by Exploring and Utilizing Latent Space (MORPHEUS) through a three-stage training process. Specifically, we create a persona codebook to represent roles in latent space compactly, and this codebook is used to construct a posterior distribution of role information. This method enables the model to generalize across roles, allowing the generation of personalized dialogues even for unseen roles. Experiments on both Chinese and English datasets demonstrate that MORPHEUS enhances the extraction of role information, and improves response generation without external role data. Additionally, MORPHEUS can be considered an efficient fine-tuning for large language models.",EMNLP
"The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for substitution, struggle to simultaneously improve performance and preserve privacy.They either rely on a local model for generation, resulting in a performance decline, or take advantage of APIs, directly exposing the data to API servers. To address this issue, we propose \textit{KnowledgeSG}, a novel client-server framework which enhances synthetic data quality and improves model performance while ensuring privacy. We achieve this by learning local knowledge from the private data with differential privacy (DP) and distilling professional knowledge from the server. Additionally, inspired by federated learning, we transmit models rather than data between the client and server to prevent privacy leakage.Extensive experiments in medical and financial domains demonstrate the effectiveness of *KnowledgeSG*. Our code is now publicly available at https://github.com/wwh0411/KnowledgeSG.",EMNLP
"Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination. As we know, both the visual encoder and the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing the model to extract visual information and generate text outputs via attention mechanisms. We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects in the image. We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination. To address the issue, we propose DAMRO, a novel training-free strategy that **D**ive into **A**ttention **M**echanism of LVLM to **R**educe **O**bject Hallucination. Specifically, our approach employs classification token (CLS) of ViT to filter out high-attention tokens scattered in the background and then eliminate their influence during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5, LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME and GPT-4V Aided Evaluation. The results demonstrate that our approach significantly reduces the impact of these outlier tokens, thus effectively alleviating the hallucination of LVLMs.",EMNLP
"Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities. However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it. Specifically, we explore whether future decisions have been considered in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.",EMNLP
"In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this paper, we explores an alternative approach to constructing a LLM for a new language by continually pre-training (CPT) from existing pre-trained LLMs, instead of using randomly initialized parameters. Based on parallel experiments on 40 model sizes ranging from 40M to 5B parameters, we find that 1) CPT converges faster and saves significant resources in a scalable manner. 2) CPT adheres to an extended scaling law derived from with a joint data-parameter scaling term. 3) The compute-optimal data-parameter allocation for CPT markedly differs based on our estimated scaling factors. 4) The effectiveness of transfer scale is influenced by training duration and linguistic properties, while robust to data replaying, a method that effectively mitigates catastrophic forgetting in CPT. We hope our findings provide deeper insights into the transferability of LLMs at scale for the research community.",EMNLP
"Reasoning is one crucial capability in Large Language Models (LLMs), allowing them to perform complex tasks such as solving math problems and multi-step planning. While reasoning capability can emerge in larger models, smaller ones usually have to rely on distillation to transfer this capability from a larger model. However, recent efforts to distill reasoning capabilities have focused mainly on English, leaving multilingual distillation underexplored. To address this gap, this paper examines existing English reasoning distillation methods that utilize a variety of positive rationales in multilingual settings and proposes d-CoT-nR, a novel approach that incorporates incorrect rationales as additional guidance. Empirical results from multilingual high-school examinations show that d-CoT-nR significantly surpasses the baseline, improving accuracy in unseen languages and correctness in step-by-step reasoning.",EMNLP
"We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response should reflect this uncertainty by hedging its answer (e.g., {``}I{'}m not sure, but I think...{''}). We formalize faithful response uncertainty based on the gap between the model{'}s intrinsic confidence in the assertions it makes and the decisiveness by which they are conveyed. This example-level metric reliably indicates whether the model reflects its uncertainty, as it penalizes both excessive and insufficient hedging. We evaluate a variety of aligned LLMs at faithfully conveying uncertainty on several knowledge-intensive question answering tasks. Our results provide strong evidence that modern LLMs are poor at faithfully conveying their uncertainty, and that better alignment is necessary to improve their trustworthiness.",EMNLP
"When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model{'}s knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model{'}s tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.",EMNLP
"The widespread presence of hate speech on the internet, including formats such as text-based tweets and multimodal memes, poses a significant challenge to digital platform safety. Recent research has developed detection models tailored to specific modalities; however, there is a notable gap in transferring detection capabilities across different formats. This study conducts extensive experiments using few-shot in-context learning with large language models to explore the transferability of hate speech detection between modalities. Our findings demonstrate that text-based hate speech examples can significantly enhance the classification accuracy of vision-language hate speech. Moreover, text-based demonstrations outperform vision-language demonstrations in few-shot learning settings. These results highlight the effectiveness of cross-modality knowledge transfer and offer valuable insights for improving hate speech detection systems.",EMNLP
"Improving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product images, and incurs high costs for scalability. To address these issues, we introduce MIND, a multimodal framework that allows Large Vision-Language Models (LVLMs) to infer purchase intentions from multimodal product metadata and prioritize human-centric ones. Using Amazon Review data, we apply MIND and create a multimodal intention knowledge base, which contains 1,264,441 intentions derived from 126,142 co-buy shopping records across 107,215 products. Extensive human evaluations demonstrate the high plausibility and typicality of our obtained intentions and validate the effectiveness of our distillation framework and filtering mechanism. Further experiments reveal the positive downstream benefits that MIND brings to intention comprehension tasks and highlight the importance of multimodal generation and role-aware filtering. Additionally, MIND shows robustness to different prompts and superior generation quality compared to previous methods.",EMNLP
"The rise of large language models (LLMs) has significantly influenced the quality of information in decision-making systems, leading to the prevalence of AI-generated content and challenges in detecting misinformation and managing conflicting information, or {``}inter-evidence conflicts.{''} This study introduces a method for generating diverse, validated evidence conflicts to simulate real-world misinformation scenarios. We evaluate conflict detection methods, including Natural Language Inference (NLI) models, factual consistency (FC) models, and LLMs, on these conflicts (RQ1) and analyze LLMs{'} conflict resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models exhibit high precision in detecting answer conflicts, though weaker models suffer from low recall; (2) FC models struggle with lexically similar answer conflicts, while NLI and LLM models handle these better; and (3) stronger models like GPT-4 show robust performance, especially with nuanced conflicts. For conflict resolution, LLMs often favor one piece of conflicting evidence without justification and rely on internal knowledge if they have prior beliefs.",EMNLP
"To assist human fact-checkers, researchers have developed automated approaches for visual misinformation detection. These methods assign veracity scores by identifying inconsistencies between the image and its caption, or by detecting forgeries in the image. However, they neglect a crucial point of the human fact-checking process: identifying the original meta-context of the image. By explaining what is actually true about the image, fact-checkers can better detect misinformation, focus their efforts on check-worthy visual content, engage in counter-messaging before misinformation spreads widely, and make their explanation more convincing. Here, we fill this gap by introducing the task of automated image contextualization. We create 5Pils, a dataset of 1,676 fact-checked images with question-answer pairs about their original meta-context. Annotations are based on the 5 Pillars fact-checking framework. We implement a first baseline that grounds the image in its original meta-context using the content of the image and textual evidence retrieved from the open web. Our experiments show promising results while highlighting several open challenges in retrieval and reasoning.",EMNLP
No abstract found,EMNLP
"In this paper, we introduce a subspace-inspired Low-Rank Adaptation (LoRA) method, which is computationally efficient, easy to implement, and readily applicable to large language, multimodal, and diffusion models. Initially, we equivalently decompose the weights of LoRA into two subspaces, and find that simply mixing them can enhance performance. To study such a phenomenon, we revisit it through a fine-grained subspace lens, showing that such modification is equivalent to employing a fixed mixer to fuse the subspaces. To be more flexible, we jointly learn the mixer with the original LoRA weights, and term the method as Mixture-of-Subspaces LoRA (MoSLoRA). MoSLoRA consistently outperforms LoRA on tasks in different modalities, including commonsense reasoning, visual instruction tuning, and subject-driven text-to-image generation, demonstrating its effectiveness and robustness.",EMNLP
"Evaluation of multilingual Large Language Models (LLMs) is challenging due to a variety of factors {--} the lack of benchmarks with sufficient linguistic diversity, contamination of popular benchmarks into LLM pre-training data and the lack of local, cultural nuances in translated benchmarks. In this work, we study human and LLM-based evaluation in a multilingual, multi-cultural setting. We evaluate 30 models across 10 Indic languages by conducting 90K human evaluations and 30K LLM-based evaluations and find that models such as GPT-4o and Llama-3 70B consistently perform best for most Indic languages. We build leaderboards for two evaluation settings - pairwise comparison and direct assessment and analyse the agreement between humans and LLMs. We find that humans and LLMs agree fairly well in the pairwise setting but the agreement drops for direct assessment evaluation especially for languages such as Bengali and Odia. We also check for various biases in human and LLM-based evaluation and find evidence of self-bias in the GPT-based evaluator. Our work presents a significant step towards scaling up multilingual evaluation of LLMs.",EMNLP
"We present LawBench, the first evaluation benchmark composed of 20 tasks aimed to assess the ability of Large Language Models (LLMs) to perform Chinese legal-related tasks. LawBench is meticulously crafted to enable precise assessment of LLMs{'} legal capabilities from three cognitive levels that correspond to the widely accepted Bloom{'}s cognitive taxonomy. Using LawBench, we present a comprehensive evaluation of 21 popular LLMs and the first comparative analysis of the empirical results in order to reveal their relative strengths and weaknesses. All data, model predictions and evaluation code are accessible from https://github.com/open-compass/LawBench.",EMNLP
"Scientific leaderboards are standardized ranking systems that facilitate evaluating and comparing competitive methods. Typically, a leaderboard is defined by a task, dataset, and evaluation metric (TDM) triple, allowing objective performance assessment and fostering innovation through benchmarking. However, the exponential increase in publications has made it infeasible to construct and maintain these leaderboards manually. Automatic leaderboard construction has emerged as a solution to reduce manual labor. Existing datasets for this task are based on the community-contributed leaderboards without additional curation. Our analysis shows that a large portion of these leaderboards are incomplete, and some of them contain incorrect information. In this work, we present SciLead, a manually-curated Scientific Leaderboard dataset that overcomes the aforementioned problems. Building on this dataset, we propose three experimental settings that simulate real-world scenarios where TDM triples are fully defined, partially defined, or undefined during leaderboard construction. While previous research has only explored the first setting, the latter two are more representative of real-world applications. To address these diverse settings, we develop a comprehensive LLM-based framework for constructing leaderboards. Our experiments and analysis reveal that various LLMs often correctly identify TDM triples while struggling to extract result values from publications. We make our code and data publicly available.",EMNLP
"Current Vision-Language (VL) models owe their success to large-scale pre-training on web-collected data, which in turn requires high-capacity architectures and large compute resources for training. We posit that when the downstream tasks are known in advance, which is in practice common, the pretraining process can be aligned to the downstream domain, leading to more efficient and accurate models, while shortening the pretraining step. To this end, we introduce a domain-aligned pretraining strategy that, without additional data collection, improves the accuracy on a domain of interest, herein, that of human activities, while largely preserving the generalist knowledge. At the core of our approach stands a new LLM-based method that, provided with a simple set of concept seeds, produces a concept hierarchy with high coverage of the target domain.The concept hierarchy is used to filter a large-scale web-crawled dataset and, then, enhance the resulting instances with targeted synthetic labels. We study in depth how to train such approaches and their resulting behavior. We further show generalization to video-based data by introducing a fast adaptation approach for transitioning from a static (image) model to a dynamic one (i.e. with temporal modeling). On the domain of interest, our approach significantly outperforms models trained on up to $60\times$ more samples and between $10-100\times$ shorter training schedules for image retrieval, video retrieval and action recognition. Code will be released.",EMNLP
"Diffusion-based text-to-image models have demonstrated impressive achievements in diversity and aesthetics but struggle to generate images with legible visual texts. Existing backbone models have limitations such as misspelling, failing to generate texts, and lack of support for Chinese texts, but their development shows promising potential. In this paper, we propose a series of methods, aiming to empower backbone models to generate visual texts in English and Chinese. We first conduct a preliminary study revealing that BPE tokenization and insufficient learning of cross-attention modules restrict the performance of the backbone models. Based on these observations, we make the following improvements: (1) We design a mixed granularity input strategy to provide more suitable text representations; (2) We propose to augment the conventional training objective with three glyph-aware training losses, which enhance the learning of cross-attention modules and encourage the model to focus on visual texts. Through experiments, we demonstrate that our methods can effectively empower backbone models to generate semantic relevant, aesthetically appealing, and accurate visual text images, while maintaining their fundamental image generation quality.",EMNLP
"Large language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs{'} character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CROSS dataset from literature experts and assess the generated profiles by comparing them with ground truth references and evaluating their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs. Resources are available at https://github.com/Joanna0123/character{\_}profiling.",EMNLP
"Recently, Large Language Models (LLMs) have shown impressive language capabilities, while most of them have very unbalanced performance across different languages. Multilingual alignment based on the translation parallel data is an effective method to enhance LLMs{'} multilingual capabilities. In this work, we first discover and comprehensively investigate the spontaneous multilingual alignment of LLMs. Firstly, we find that LLMs instruction-tuned on the question translation data (i.e. without annotated answers) are able to encourage the alignment between English and a wide range of languages, even including those unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to analyze the LLM{'}s performance in the multilingual scenario comprehensively. Our work suggests that LLMs have enormous potential for improving multilingual alignment efficiently with great language generalization and task generalization.",EMNLP
"Recent advancements in large language models (LLMs) have been remarkable. Users face a choice between using cloud-based LLMs for generation quality and deploying local-based LLMs for lower computational cost. The former option is typically costly and inefficient, while the latter usually fails to deliver satisfactory performance for reasoning steps requiring deliberate thought processes. In this work, we propose a novel LLM utilization paradigm that facilitates the collaborative operation of large cloud-based LLMs and smaller local-deployed LLMs. Our framework comprises two primary modules: the local agent instantiated with a relatively smaller LLM, handling less complex reasoning steps, and the cloud agent equipped with a larger LLM, managing more intricate reasoning steps. This collaborative processing is enabled through an adaptive mechanism where the local agent introspectively identifies errors and proactively seeks assistance from the cloud agent, thereby effectively integrating the strengths of both locally-deployed and cloud-based LLMs, resulting in significant enhancements in task completion performance and efficiency. We evaluate AdaSwitch across 7 benchmarks, ranging from mathematical reasoning and complex question answering, using various types of LLMs to instantiate the local and cloud agents. The empirical results show that AdaSwitch effectively improves the performance of the local agent, and sometimes achieves competitive results compared to the cloud agent while utilizing much less computational overhead.",EMNLP
"Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate models for each task. Yet, existing MTL strategies for LLMs often fall short by either being computationally intensive or failing to ensure simultaneous task convergence. This paper presents CoBa, a new MTL approach designed to effectively manage task convergence balance with minimal computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically adjusts task weights during the training process, ensuring that the validation loss of all tasks progress towards convergence at an even pace while mitigating the issue of individual task divergence. The results of our experiments involving three disparate datasets underscore that this approach not only fosters equilibrium in task improvement but enhances the LLMs{'} performance by up to 13{\%} relative to the second-best baselines. Code is open-sourced at https://github.com/codefuse-ai/MFTCoder.",EMNLP
"Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent improvement. Through a comparative experiment, we identify the unconditional preference problem in multimodal preference optimization, where the model overlooks the image condition. To address this problem, we propose mDPO, a multimodal DPO objective that prevents the over-prioritization of language-only preferences by also optimizing image preference. Moreover, we introduce a reward anchor that forces the reward to be positive for chosen responses, thereby avoiding the decrease in their likelihood{---}an intrinsic problem of relative preference optimization. Experiments on two multimodal LLMs of different sizes and three widely used benchmarks demonstrate that mDPO effectively addresses the unconditional preference problem in multimodal preference optimization and significantly improves model performance, particularly in reducing hallucination.",EMNLP
"Data are crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.",EMNLP
"Tools for translating natural language into code promise natural, open-ended interaction with databases, web APIs, and other software systems. However, this promise is complicated by the diversity and continual development of these systems, each with its own interface and distinct set of features. Building a new language-to-code translator, even starting with a large language model (LM), typically requires annotating a large set of natural language commands with their associated programs. In this paper, we describe ICIP (In-Context Inverse Programming), a method for bootstrapping a language-to-code system using mostly (or entirely) unlabeled programs written using a potentially unfamiliar (but human-readable) library or API. ICIP uses a pre-trained LM to assign candidate natural language descriptions to these programs, then iteratively refines the descriptions to ensure global consistency. Across nine different application domains from the Overnight and Spider benchmarks and text-davinci-003 and CodeLlama-7b-Instruct models, ICIP outperforms a number of prompting baselines. Indeed, in a {``}nearly unsupervised{''} setting with only a single annotated program and 100 unlabeled examples, it achieves up to 85{\%} of the performance of a fully supervised system.",EMNLP
"LLMs can help humans working with long documents, but are known to hallucinate. *Attribution* can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiments with different approaches to attribution on 5 LLMs of different sizes. We find that *citation*, i.e. response generation and evidence extraction in one step, performs best for large and fine-tuned models, while additional retrieval can help for small, prompted models. We investigate whether the {``}Lost in the Middle{''} phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation. [Link](https://github.com/UKPLab/arxiv2024-attribute-or-abstain)",EMNLP
"Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M$^3$OE) module. This method not only preserves privacy but also enhances the model{'}s ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data. Source codes are available at https://github.com/XiaochenWang-PSU/FedKIM.",EMNLP
"In-context learning (ICL) has been instrumental in adapting large language models (LLMs) to downstream tasks using correct input-output examples. Recent advances have attempted to improve model performance through principles derived from mistakes, yet these approaches suffer from lack of customization and inadequate error coverage. To address these limitations, we propose Retrieved In-Context Principles (RICP), a novel teacher-student framework. In RICP, the teacher model analyzes mistakes from the student model to generate reasons and insights for preventing similar mistakes. These mistakes are clustered based on their underlying reasons for developing task-level principles, enhancing the error coverage of principles. During inference, the most relevant mistakes for each question are retrieved to create question-level principles, improving the customization of the provided guidance. RICP is orthogonal to existing prompting methods and does not require intervention from the teacher model during inference. Experimental results across seven reasoning benchmarks reveal that RICP effectively enhances performance when applied to various prompting strategies.",EMNLP
"While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services.",EMNLP
"Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit.Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables. In this paper, we introduce **Vector Post-Training Quantization (VPTQ)** for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization.We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ.In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model.Our experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5{\%} on LLaMA-2, 1{\%} on Mistral-7B, 11-22{\%} on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6{\%} of the quantization algorithm execution time, resulting in a 1.6-$1.8\times$ increase in inference throughput compared to SOTA.",EMNLP
"Extracting finite state automata (FSAs) fromblack-box models offers a powerful approachto gaining interpretable insights into complexmodel behaviors. To support this pursuit, wepresent a weighted variant of Angluin{'}s (1987)L* algorithm for learning FSAs. We stay faithful to the original formulation, devising a wayto exactly learn deterministic weighted FSAswhose weights support division. Furthermore,we formulate the learning process in a mannerthat highlights the connection with FSA minimization, showing how L* directly learns aminimal automaton for the target language.",EMNLP
"Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse query generation are utilized to enhance both the precision and breadth of the retrieved documents. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms baselines.",EMNLP
"Large Visual Language Models (LVLMs) struggle with hallucinations in visual instruction following task(s). These issues hinder their trustworthiness and real-world applicability. We propose Pelican {--} a novel framework designed to detect and mitigate hallucinations through claim verification. Pelican first decomposes the visual claim into a chain of sub-claims based on first-order predicates. These sub-claims consists of (predicate, question) pairs and can be conceptualized as nodes of a computational graph. We then use use Program-of-Thought prompting to generate Python code for answering these questions through flexible composition of external tools. Pelican improves over prior work by introducing (1) intermediate variables for precise grounding of object instances, and (2) shared computation for answering the sub-question to enable adaptive corrections and inconsistency identification. We finally use reasoning abilities of LLM to verify the correctness of the the claim by considering the consistency and confidence of the (question, answer) pairs from each sub-claim. Our experiments demonstrate consistent performance improvements over various baseline LVLMs and existing hallucination mitigation approaches across several benchmarks.",EMNLP
"We tackle societal bias in image-text datasets by removing spurious correlations between protected groups and image attributes. Traditional methods only target labeled attributes, ignoring biases from unlabeled ones. Using text-guided inpainting models, our approach ensures protected group independence from all attributes and mitigates inpainting biases through data filtering. Evaluations on multi-label image classification and image captioning tasks show our method effectively reduces bias without compromising performance across various models. Specifically, we achieve an average societal bias reduction of 46.1{\%} in leakage-based bias metrics for multi-label classification and 74.8{\%} for image captioning.",EMNLP
"The latest advancements in large language models (LLMs) have sparked interest in their potential for software vulnerability detection. However, there is currently a lack of research specifically focused on vulnerabilities in the PHP language, and challenges in data sampling and processing persist, hindering the model{'}s ability to effectively capture the characteristics of specific vulnerabilities. In this paper, we present RealVul, the first LLM-based framework designed for PHP vulnerability detection, addressing these issues. By improving code sampling methods and employing normalization techniques, we can isolate potential vulnerability triggers while streamlining the code and eliminating unnecessary semantic information, enabling the model to better understand and learn from the generated vulnerability samples. We also address the issue of insufficient PHP vulnerability samples by improving data synthesis methods. To evaluate RealVul{'}s performance, we conduct an extensive analysis using five distinct code LLMs on vulnerability data from 180 PHP projects. The results demonstrate a significant improvement in both effectiveness and generalization compared to existing methods, effectively boosting the vulnerability detection capabilities of these models.",EMNLP
"Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step. These annotations can be costly to produce, error-prone, and require both domain and annotation expertise. With advances in LLMs, we hypothesize that unlabeled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised. We consider a novel unsupervised setting of only (1) a well-defined API schema (2) a set of unlabeled dialogues between a user and agent. We propose an innovative approach using expectation-maximization (EM) that infers turn-level annotations as latent variables using a noisy channel model to build an end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline.",EMNLP
"Adopting human and large language models (LLM) as judges (*a.k.a* human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating **Misinformation Oversight Bias**, **Gender Bias**, **Authority Bias** and **Beauty Bias** on LLM and human judges. We curate a dataset referring to the revised Bloom{'}s Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems.",EMNLP
"Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6{\%} on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 76.7{\%} based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/WPO.",EMNLP
"The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training, which is impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles, we propose a novel strategy named perspective-taking prompting (PeT) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to 89{\%}) and bias (up to 73{\%}) in LLMs{'} responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing PeT{'}s superiority in producing less harmful responses, outperforming five strong baselines.",EMNLP
"The popularity of Large Language Models (LLMs) have unleashed a new age of Language Agents for solving a diverse range of tasks. While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform sub-optimally. To address this, recent works have explored techniques to improve their performance using self reflection and prompt optimization techniques. While techniques like self reflection work well in an online setup, contemporary prompt optimization techniques are designed to work on simpler tasks. To address this, we introduce METAREFLECTION, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials. We demonstrate the efficacy of METAREFLECTION by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, with different agent design. METAREFLECTION boosts Language agents{'} performance by 4 {\%} to 16.82 {\%} over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.",EMNLP
"Large language models (LLMs) offer many opportunities to scale high-quality personalized tutoring. A promising approach is to build dialog tutoring models to scaffold students{'} problem-solving. However, even though existing models perform well in solving reasoning questions, they can struggle to precisely detect student{'}s errors and tailor their feedback to these errors. Inspired by real-world teaching practice where teachers identify student errors and customize their response based on them, we focus on verifying student solutions and show how grounding to such verification improves the overall quality of tutor response generation. We collect a dataset of 1,002 stepwise math reasoning chains with the first error step annotated by teachers. We show empirically that finding the mistake in a student solution is challenging for current models. We propose and evaluate several verifiers for detecting these errors. Using both automatic and human evaluation we show that the student solution verifiers steer the generation model towards highly targeted responses to student error which are more often correct with less hallucinations compared to existing baselines. The benchmark dataset and code will be released openly.",EMNLP
"Unsupervised parsing, also known as grammar induction, aims to infer syntactic structure from raw text. Recently, binary representation has exhibited remarkable information-preserving capabilities at both lexicon and syntax levels. In this paper, we explore the possibility of leveraging this capability to deduce parsing trees from raw text, relying solely on the implicitly induced grammars within models. To achieve this, we upgrade the bit-level CKY from zero-order to first-order to encode the lexicon and syntax in a unified binary representation space, switch training from supervised to unsupervised under the contrastive hashing framework, and introduce a novel loss function to impose stronger yet balanced alignment signals. Our model shows competitive performance on various datasets, therefore, we claim that our method is effective and efficient enough to acquire high-quality parsing trees from pre-trained language models at a low cost.",EMNLP
"With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.",EMNLP
"Recent studies have revealed the vulnerability of pre-trained language models to adversarial attacks. Adversarial defense techniques have been proposed to reconstruct adversarial examples within feature or text spaces. However, these methods struggle to effectively repair the semantics in adversarial examples, resulting in unsatisfactory defense performance. To repair the semantics in adversarial examples, we introduce a novel approach named Reactive Perturbation Defocusing (Rapid), which employs an adversarial detector to identify the fake labels of adversarial examples and leverages adversarial attackers to repair the semantics in adversarial examples. Our extensive experimental results, conducted on four public datasets, demonstrate the consistent effectiveness of Rapid in various adversarial attack scenarios. For easy evaluation, we provide a click-to-run demo of Rapid at https://tinyurl.com/22ercuf8.",EMNLP
"Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages? In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline.",EMNLP
"*Uncertainty expressions* such as {`}probably{'} or {`}highly unlikely{'} are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans quantitatively interpret these expressions, there has been little inquiry into the abilities of language models in the same context. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model{'}s own certainty about that statement. We find that 7 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI and AI-AI communication.",EMNLP
"Contrastive decoding (CD) (Li et al., 2022) improves the next-token distribution of a large expert language model (LM) using a small amateur LM. Although CD is applied to various LMs and domains to enhance open-ended text generation, it is still unclear why CD often works well, when it could fail, and how we can make it better. To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM. We also highlight that the linear extrapolation could make CD unable to output the most obvious answers that have already been assigned high probabilities by the amateur LM.To overcome CD{'}s limitation, we propose a new unsupervised decoding method called Asymptotic Probability Decoding (APD). APD explicitly extrapolates the probability curves from the LMs of different sizes to infer the asymptotic probabilities from an infinitely large LM without inducing more inference costs than CD. In FactualityPrompts, an open-ended text generation benchmark, sampling using APD significantly boosts factuality in comparison to the CD sampling and its variants, and achieves state-of-the-art results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA datasets, APD is often significantly better than CD and achieves a similar effect of using a larger LLM. For example, the perplexity of APD on top of Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA and LAMBADA.",EMNLP
"Zero-shot cross-domain dialogue state tracking (DST) enables us to manage task-oriented dialogues in new, unseen domains without the cost of collecting in-domain data. Previous studies have implemented slot-based input improvements, such as schema-driven descriptions and question-answering formats, but still suffer from negative transfer for seen slots and inefficient transfer for unseen slots due to the significant source-target domain gap. To address these issues, we introduce a novel framework called Context-aware Auto-prompting and Instruction-following Contrastive Decoding (CAPID). This framework generates dynamic, context-aware slot queries, effectively improving the model{'}s transferability. Our context-aware auto-prompting approach tailors slot queries to the current dialogue context, increasing flexibility and reducing ambiguities. Additionally, an instruction-following contrastive decoding strategy helps reduce errors related to off-topic slots by penalizing deviations from the provided instructions. Extensive experiments on two datasets, with varying model sizes (from 60M to 7B), demonstrate the superior performance of CAPID. The source code is provided for reproducibility.",EMNLP
"This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.",EMNLP
"The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users{'} critical thinking through access to facts. Such efforts are often hampered by challenges with scalability, and by platform users{'} personal biases. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we introduce a framework (MisinfoEval) for generating and comprehensively evaluating large language model (LLM) based misinformation interventions. We present (1) an experiment with a simulated social media environment to measure effectiveness of misinformation interventions, and (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of countering misinformation by appealing to their pre-existing values. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 41.72{\%}). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability and users shown personalized interventions have significantly higher accuracy at identifying misinformation.",EMNLP
"The stock market provides a rich well of information that can be split across modalities, making it an ideal candidate for multimodal evaluation. Multimodal data plays an increasingly important role in the development of machine learning and has shown to positively impact performance. But information can do more than exist across modes{---} it can exist across time. How should we attend to temporal data that consists of multiple information types? This work introduces (i) the MEANT model, a Multimodal Encoder for Antecedent information and (ii) a new dataset called TempStock, which consists of price, Tweets, and graphical data with over a million Tweets from all of the companies in the S{\&}P 500 Index. We find that MEANT improves performance on existing baselines by over 15{\%}, and that the textual information affects performance far more than the visual information on our time-dependent task from our ablation study. The code and dataset will be made available upon publication.",EMNLP
"Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.",EMNLP
"Ranking is a fundamental problem in search, however, existing ranking algorithms usually restrict the granularity of ranking to full passages or require a specific dense index for each desired level of granularity. Such lack of flexibility in granularity negatively affects many applications that can benefit from more granular ranking, such as sentence-level ranking for open-domain QA, or proposition-level ranking for attribution. In this work, we introduce the idea of any-granularity ranking which leverages multi-vector embeddings to rank at varying levels of granularity while maintaining encoding at a single (coarser) level of granularity. We propose a multi-granular contrastive loss for training multi-vector approaches and validate its utility with both sentences and propositions as ranking units. Finally, we demonstrate the application of proposition-level ranking to post-hoc citation addition in retrieval-augmented generation, surpassing the performance of prompt-driven citation generation.",EMNLP
"Large Language Models (LLMs) have significantly advanced the field of information retrieval, particularly for reranking. Listwise LLM rerankers have showcased superior performance and generalizability compared to existing supervised approaches. However, conventional listwise LLM reranking methods lack efficiency as they provide ranking output in the form of a generated ordered sequence of candidate passage identifiers. Further, they are trained with the typical language modeling objective, which treats all ranking errors uniformly{--}potentially at the cost of misranking highly relevant passages. Addressing these limitations, we introduce FIRST, a novel listwise LLM reranking approach leveraging the output logits of the first generated identifier to directly obtain a ranked ordering of the candidates. Further, we incorporate a learning-to-rank loss during training, prioritizing ranking accuracy for the more relevant passages. Empirical results demonstrate that FIRST accelerates inference by 50{\%} while maintaining a robust ranking performance with gains across the BEIR benchmark. Finally, to illustrate the practical effectiveness of listwise LLM rerankers, we investigate their application in providing relevance feedback for retrievers during inference. Our results show that LLM rerankers can provide a stronger distillation signal compared to cross-encoders, yielding substantial improvements in retriever recall after relevance feedback.",EMNLP
"Nested Named Entity Recognition (NER) poses a significant challenge in Natural Language Processing (NLP), demanding sophisticated techniques to identify entities within entities. This research investigates the application of Large Language Models (LLMs) to nested NER, exploring methodologies from prior work and introducing specific reasoning techniques and instructions to improve LLM efficacy. Through experiments conducted on the ACE 2004, ACE 2005, and GENIA datasets, we evaluate the impact of these approaches on nested NER performance. Results indicate that output format critically influences nested NER performance, methodologies from previous works are less effective, and our nested NER-tailored instructions significantly enhance performance. Additionally, we find that label information and descriptions of nested cases are crucial in eliciting the capabilities of LLMs for nested NER, especially in specific domains (i.e., the GENIA dataset). However, these methods still do not outperform BERT-based models, highlighting the ongoing need for innovative approaches in nested NER with LLMs.",EMNLP
"The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the data used in their pretraining. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs{'} pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs{'} behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.",EMNLP
"Weight-based model editing methods update the parametric knowledge of language models post-training. However, these methods can unintentionally alter unrelated parametric knowledge representations, potentially increasing the risk of harm. In this work, we investigate how weight editing methods unexpectedly amplify model biases after edits. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias amplification of model editing methods for demographic traits such as race, geographic origin, and gender. We use Seesaw-CF to examine the impact of model editing on bias in five large language models. Our results demonstrate that edited models exhibit, to various degrees, more biased behavior for certain demographic groups than before they were edited, specifically becoming less confident in properties for Asian and African subjects. Additionally, editing facts about place of birth, country of citizenship, or gender has particularly negative effects on the model{'}s knowledge about unrelated properties, such as field of work, a pattern observed across multiple models.",EMNLP
"This paper investigates Who{'}s Harry Potter (WHP), a pioneering yet insufficiently understood method for LLM unlearning. We explore it in two steps. First, we introduce a new task of LLM targeted unlearning, where given an unlearning target (e.g., a person) and some unlearning documents, we aim to unlearn only the information about the target, rather than everything in the unlearning documents. We further argue that a successful unlearning should satisfy criteria such as not outputting gibberish, not fabricating facts about the unlearning target, and not releasing factual information under jailbreak attacks. Second, we construct a causal intervention framework for targeted unlearning, where the knowledge of the unlearning target is modeled as a confounder between LLM input and output, and the unlearning process as a deconfounding process. This framework justifies and extends WHP, deriving a simple unlearning algorithm that includes WHP as a special case. Experiments on existing and new datasets show that our approach, without explicitly optimizing for the aforementioned criteria, achieves competitive performance in all of them.",EMNLP
"Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent works proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring the impact of various design choices throughout the whole training process. We first conduct a rigorous analysis over a three-stage training pipeline consisting of supervised fine-tuning, offline preference learning, and online preference learning. We have found that using techniques like sequence packing, loss masking in SFT, increasing the preference dataset size in DPO, and online DPO training can significantly improve the performance of language models. We then train from Gemma-2b-base and LLama-3-8b-base, and find that our best models exceed the performance of the official instruct models tuned with closed-source data and algorithms. Our code and models can be found at https://github.com/Columbia-NLP-Lab/LionAlignment.",EMNLP
"This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format. We instruction-tune local LLMs as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models{'} abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5. Our models are available at: https://huggingface.co/NECOUDBFM/JellyfishOur instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct",EMNLP
"In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",EMNLP
"Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.",EMNLP
"Medical coding, the translation of unstructured clinical text into standardized medical codes, is a crucial but time-consuming healthcare practice. Though large language models (LLM) could automate the coding process and improve the efficiency of such tasks, interpretability remains paramount for maintaining patient trust. Current efforts in interpretability of medical coding applications rely heavily on label attention mechanisms, which often leads to the highlighting of extraneous tokens irrelevant to the ICD code. To facilitate accurate interpretability in medical language models, this paper leverages dictionary learning that can efficiently extract sparsely activated representations from dense language model embeddings in superposition. Compared with common label attention mechanisms, our model goes beyond token-level representations by building an interpretable dictionary which enhances the mechanistic-based explanations for each ICD code prediction, even when the highlighted tokens are medically irrelevant. We show that dictionary features are human interpretable, can elucidate the hidden meanings of upwards of 90{\%} of medically irrelevant tokens, and steer model behavior.",EMNLP
"The talk will present evidence that today's large language models (LLMs) display somewhat deeper ""understanding'' than one would naively expect.1. When asked to solve a task by combining a set of k simpler skills (""test of compositional capability""), they are able to do so despite not having seen the same combination of skills during their training.2. They demonstrate ability to reason about of their own learning processes, which is analogous to ""metacognitive knowledge""[Flavel'76] in humans. For instance, given examples of an evaluation task, they can produce a catalog of suitably named skills that are relevant for solving each example of that task. Furthermore, this catalog of skills is meaningful, in the sense that incorporating it into training pipelines improves performance (including of other unrelated LLMs) on that task.We discuss mechanisms by which such complex understanding could arise (including a theory by [Arora,Goyal'23] that tries to explain (a)) and also give examples of how to leverage LLM meta knowledge to improve LLM training pipelines as well as evaluations.
1. When asked to solve a task by combining a set of k simpler skills (""test of compositional capability""), they are able to do so despite not having seen the same combination of skills during their training.
2. They demonstrate ability to reason about of their own learning processes, which is analogous to ""metacognitive knowledge""[Flavel'76] in humans. For instance, given examples of an evaluation task, they can produce a catalog of suitably named skills that are relevant for solving each example of that task. Furthermore, this catalog of skills is meaningful, in the sense that incorporating it into training pipelines improves performance (including of other unrelated LLMs) on that task.",KDD
"Computation has fundamentally changed the way we study nature. New data collection technologies, such as GPS, high-definition cameras, autonomous vehicles under water, on the ground, and in the air, genotyping, acoustic sensors, and crowdsourcing, are generating data about life on the planet that are orders of magnitude richer than any previously collected. Yet, our ability to extract insight from this data lags substantially behind our ability to collect it.
The need for understanding is more urgent than ever and the challenges are great. We are in the middle of the 6th extinction, losing the planet's biodiversity at an unprecedented rate and scale. In many cases, we do not even have the basic numbers of what species we are losing, which impacts our ability to understand biodiversity loss drivers, predict the impact on ecosystems, and implement policy. From the basic science perspective, the new data opens the possibility of understanding function of traits of organisms and ecosystems, which is critical for biologists to predict effects of environmental change or genetic manipulation and to understand the significance of patterns in the four-billion-year evolutionary history of life.
The key to unlocking the potential of this data are machine learning (ML) and artificial intelligence (AI) methods, which are already beginning to have significant impacts on research across ecology and conservation. AI can turn data into high resolution information source about living organisms, enabling scientific inquiry, conservation, and policy decisions.
The talk introduces a new field of science, imageomics, and presents a vision and examples of AI as a trustworthy partner both in science and biodiversity conservation, discussing opportunities and challenges.",KDD
"The data science ecosystem encompasses data fairness, statistical, ML and AI methods and tools, interpretable data analysis and results, and trustworthy decision-making. Rapid advancements in AI have revolutionized data utilization and enabled machines to learn from data more effectively. Statistics, as the science of learning from data while accounting for uncertainty, plays a pivotal role in addressing complex real-world problems and facilitating trustworthy decision-making. In this talk, I will discuss the challenges and opportunities involved in building an end-to-end scalable and interpretable data science ecosystem using the analysis of whole genome sequencing studies and biobanks that integrates statistics, ML/AI, and genomic and health science as an example. Biobanks collect whole genome data, electronic health records and epidemiological data. I will illustrate key points using the analysis of multi-ancestry whole genome sequencing studies and biobanks by discussing a few scalable and interpretable statistical and ML/AI methods, tools and data science resources.
Specifically, first, data fairness and diversity is a critical pillar of a trustworthy data science ecosystem. About 85+% of genome wide association study samples in the last 15 years are European, resulting in disparity in genetic research. I will discuss the community effort on improving diversity in genetic studies in the last 10 years. I will present trans-ancestry polygenic risk scores (PRS) using millions of common genetic variants across the genome by leveraging large GWAS sample sizes of European and smaller sample sizes of under-represented populations for predicting disease risk using transfer learning and genetic association summary statistics. The performance of deep learning methods for PRS will also be discussed. Second, scalability in cloud platforms is critical for large scale affordable analysis for multi-ancestry biobanks and whole genome studies. I will discuss improving scalability in cloud-computing using interpretable sparsity via FastSparseGRM.
To build an interpretable and powerful end-to-end ecosystem of rare variant analysis of large scale whole genome sequencing studies and biobanks, I will first introduce FAVOR, a multi-faceted variant functional annotation database and portal of all possible 9 billions of variants across the whole genome. I will discuss FAVOR-GPT, a LLM interface of the FAVOR functional annotation database to improve user experience for navigating FAVOR and performing variant functional annotation query and variant functional summary statistics calculations. I will also discuss FAVORannotator which can be used to functionally annotate any whole genome sequencing studies. I will also discuss STAAR and STAAR and STAARpipeline, the WGS rare variant analysis pipeline that boosts the power of WGS rare variant association analysis by dynamically incorporating multi-faceted variant functional annotations. Extension of incorporating single-cell data in WGS analysis will also be discussed. I will also discuss ensemble methods that improve the power of rare variant association tests.
Cloud-deployment of these resources and tools in several ecosystems will be presented, such as RAP for the UK biobank, AnVIL for the NHGRI Genome Sequencing Program and All of Us, and BioData Catalyst for the NHLBI Trans-omics Precision Medine Program (TOPMed). This talk aims to ignite proactive and thought-provoking discussions, foster collaboration, and cultivate open-minded approaches to advance scientific discovery.",KDD
"The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improvesuser utility and generative search engine traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to 40% in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators.",KDD
"The sustainable training of modern neural network models represents an open challenge. Several existing methods approach this issue by identifying a subset of relevant data samples from the full training data to be used in model optimization with the goal of matching the performance of the full data training with that of the subset data training. Our work explores using memorization scores to find representative and atypical samples. We demonstrate that memorization-aware dataset summarization improves the subset construction performance. However, computing memorization scores is notably resource-intensive. To this end, we propose a novel method that leverages the discrepancy between sharpness-aware minimization and stochastic gradient descent to capture data points atypicality. We evaluate our metric over several efficient approximation functions for memorization scores - namely proxies -, empirically showing superior correlation and effectiveness. We explore the causes behind our approximation quality, highlighting how typical data points trigger a flatter loss landscape compared to atypical ones. Extensive experiments confirm the effectiveness of our proxy for dataset pruning and summarization tasks, surpassing state-of-the-art approaches both on canonical setups - where atypical data points benefit performance - and few-shot learning scenarios-where atypical data points can be detrimental.",KDD
"We study the problem of resilient clustering in the metric setting where one is interested in designing algorithms that return high quality solutions that preserve the clustering structure under perturbations of the input points. Our first contribution is to introduce a formal notion of algorithmic resiliency for clustering problems that, roughly speaking, requires an algorithm to have similar outputs on close inputs. Then, we notice that classic algorithms have weak resiliency guarantees and develop new algorithms for fundamental clustering problems such as k-center, k-median, and k-means. Finally, we complement our results with an experimental analysis showing the effectiveness of our techniques on real-world instances.",KDD
"In many contexts involving ranked preferences, agents submit partial orders over available alternatives. Statistical models often treat these as marginal in the space of total orders, but this approach overlooks information contained in the list length itself. In this work, we introduce and taxonomize approaches for jointly modeling distributions over top-k partial orders and list lengths k, considering two classes of approaches: composite models that view a partial order as a truncation of a total order, and augmented ranking models that model the construction of the list as a sequence of choice decisions, including the decision to stop. For composite models, we consider three dependency structures for joint modeling of order and truncation length. For augmented ranking models, we consider different assumptions on how the stop-token choice is modeled. Using data consisting of partial rankings from San Francisco school choice and San Francisco ranked choice elections, we evaluate how well the models predict observed data and generate realistic synthetic datasets. We find that composite models, explicitly modeling length as a categorical variable, produce synthetic datasets with accurate length distributions, and an augmented model with position-dependent item utilities jointly models length and preferences in the training data best, as measured by negative log loss. Methods from this work have significant implications on the simulation and evaluation of real-world social systems that solicit ranked preferences.",KDD
"Geodesic distances on manifolds have numerous applications in image processing, computer graphics and computer vision. In this work, we introduce an approach called 'LGGD' (Learned Generalized Geodesic Distances). This method involves generating node features by learning a generalized geodesic distance function through a training pipeline that incorporates training data, graph topology and the node content features. The strength of this method lies in the proven robustness of the generalized geodesic distances to noise and outliers. Our contributions encompass improved performance in node classification tasks, competitive results with state-of-the-art methods on real-world graph datasets, the demonstration of the learnability of parameters within the generalized geodesic equation on graph, and dynamic inclusion of new labels.",KDD
"Although time-series classification has many applications in healthcare and manufacturing, the high cost of data collection and labeling hinders its widespread use. To reduce data collection and labeling costs while maintaining high classification accuracy, we propose a novel problem setting, called semi-supervised learning with low-sampling-rate time series, in which the majority of time series are collected at a low sampling rate and are unlabeled whereas the minority of time series are collected at a high sampling rate and are labeled. For this novel problem scenario, we develop the SemiTSR framework equipped with the super-resolution module and the semi-supervised learning module. Here, low-sampling-rate time series are upsampled precisely, taking periodicity and trend at each timestamp into account, and both labeled and unlabeled high-sampling-rate time series are utilized for training. In particular, consistency regularization between artificially downsampled time series derived from an original high-sampling-rate time series is effective at overcoming limited sampling rates. We demonstrate that SemiTSR significantly outperforms conventional semi-supervised learning techniques by assuring high classification accuracy with low-sampling-rate time series.",KDD
"Understanding user intentions is essential for improving product recommendations, navigation suggestions, and query reformulations. However, user intentions can be intricate, involving multiple sessions and attribute requirements connected by logical operators such as And, Or, and Not. For instance, a user may search for Nike or Adidas running shoes across various sessions, with a preference for purple. In another example, a user may have purchased a mattress in a previous session and is now looking for a matching bed frame without intending to buy another mattress. Existing research on session understanding has not adequately addressed making product or attribute recommendations for such complex intentions. In this paper, we present the task of logical session complex query answering (LS-CQA), where sessions are treated as hyperedges of items, and we frame the problem of complex intention understanding as an LS-CQA task on an aggregated hypergraph of sessions, items, and attributes. This is a unique complex query answering task with sessions as ordered hyperedges. We also introduce a new model, the Logical Session Graph Transformer (LSGT), which captures interactions among items across different sessions and their logical connections using a transformer structure. We analyze the expressiveness of LSGT and prove the permutation invariance of the inputs for the logical operators. By evaluating LSGT on three datasets, we demonstrate that it achieves state-of-the-art results.",KDD
"Information extraction (IE) aims to extract meaningful structured tuples from unstructured text. Existing studies usually utilize a pre-trained generative language model that rephrases the original sentence into a target sequence, which can be easily decoded as tuples. However, traditional evaluation metrics treat a slight error within the tuple as an entire prediction failure, which is unable to perceive the correctness extent of a tuple. For this reason, we first propose a novel IE evaluation metric called Matching Score to evaluate the correctness of the predicted tuples in more detail. Moreover, previous works have ignored the effects of semantic uncertainty when focusing on the generation of the target sequence. We argue that leveraging the built-in semantic uncertainty of language models is beneficial for improving its robustness. In this work, we propose <u>B</u>inomial distribution guided <u>c</u>ounterpart <u>s</u>equence (BCS) method, which is a model-agnostic approach. Specifically, we propose to quantify the built-in semantic uncertainty of the language model by bridging all local uncertainties with the whole sequence. Subsequently, with the semantic uncertainty and Matching Score, we formulate a unique binomial distribution for each local decoding step. By sampling from this distribution, a counterpart sequence is obtained, which can be regarded as a semantic complement to the target sequence. Finally, we employ the Kullback-Leibler divergence to align the semantics of the target sequence and its counterpart. Extensive experiments on 14 public datasets over 5 information extraction tasks demonstrate the effectiveness of our approach on various methods. Our code and dataset are available at https://github.com/byinhao/BCS.",KDD
"The contextual bandit has been identified as a powerful framework to formulate the recommendation process as a sequential decision-making process, where each item is regarded as an arm and the objective is to minimize the regret of T rounds. In this paper, we study a new problem, Clustering of Neural Bandits, by extending previous work to the arbitrary reward function, to strike a balance between user heterogeneity and user correlations in the recommender system. To solve this problem, we propose a novel algorithm called M-CNB, which utilizes a meta-learner to represent and rapidly adapt to dynamic clusters, along with an informative Upper Confidence Bound (UCB)-based exploration strategy. We provide an instance-dependent performance guarantee for the proposed algorithm that withstands the adversarial context, and we further prove the guarantee is at least as good as state-of-the-art (SOTA) approaches under the same assumptions. In extensive experiments conducted in both recommendation and online classification scenarios, M-CNB outperforms SOTA baselines. This shows the effectiveness of the proposed approach in improving online recommendation and online classification performance.",KDD
"In this work, we study active covering, a variant of the active-learning problem that involves labeling (or identifying) all of the examples with a positive label. We propose a couple of algorithms, namely Density-Adjusted Non-Adaptive (DANA) learner and Density-Adjusted Adaptive (DAA) learner, that query the labels according to a distance function that is adjusted by the density function. Under mild assumptions, we prove that our algorithms discover all of the positive labels while querying only a sublinear number of examples from the support of negative labels for constant-dimensional spaces (see Theorems 5 and 6). Our experiments show that our champion algorithm DAA consistently improves over the prior work on some standard benchmark datasets, including those used by the previous work, as well as a couple of data sets on credit card fraud. For instance, when measuring performance using AUC, our algorithm is the best in 25 out of 27 experiments over 7 different datasets.",KDD
"Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional Encodings (PE). In this paper, we show that while Transformers, complex message-passing, and PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), we present Graph Mamba Networks (GMNs), a framework for a new class of GNNs based on selective SSMs. We discuss the new challenges when adapting SSMs to graph-structured data, and present four required steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of SSM Encoder, and (4) Local Encoding. We provide theoretical justification for the power of GMNs, and experimentally show that GMNs attain an outstanding performance in various benchmark datasets. The code is available in this link.",KDD
"Community detection techniques are useful for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to preserve their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations, without leaving the platform. In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. Extensive experiments demonstrate that our method outperforms existing baselines, striking the best balance between accuracy and cost.",KDD
"Operating and maintaining hyperscale data centers involving millions of service hosts has been an extremely intricate task to tackle for top Internet companies. Incessant system failures cost operators countless hours of browsing through performance metrics to diagnose the underlying root cause to prevent the recurrence. Although many state-of-the-art (SOTA) methods have used time-series causal discovery to construct causal relationships among anomalous metrics, they only focus on homogeneous service-level performance metrics and fail to yield useful insights on heterogeneous host-level metrics. To address the challenge, this study presents FaultInsight, a highly interpretable deep causal host fault diagnosing framework that offers diagnostic insights from various perspectives to reduce human effort in troubleshooting. We evaluate FaultInsight using dozens of incidents collected from our production environment. FaultInsight provides markedly better root cause identification accuracy than SOTA baselines in our incident dataset. It also shows outstanding advantages in terms of deployability in real production systems. Our engineers are deeply impressed by FaultInsight's ability to interpret incidents from multiple perspectives, helping them quickly understand the mechanism behind the faults.",KDD
"When primary objectives are insensitive or delayed, experimenters may instead focus on proxy metrics derived from secondary outcomes. For example, technology companies often infer the long-term impacts of product interventions from their effects on short-term user engagement signals. We consider the meta-analysis of many historical experiments to learn the covariance of treatment effects on these outcomes, which can support the construction of such proxies. Even when experiments are plentiful, if treatment effects are weak, the covariance of estimated treatment effects across experiments can be highly biased. We overcome this with techniques inspired by weak instrumental variable analysis. We show that Limited Information Maximum Likelihood (LIML) learns a parameter equivalent to fitting total least squares to a transformation of the scatterplot of treatment effects, and that Jackknife Instrumental Variables Estimation (JIVE) learns another parameter computable from the average of Jackknifed covariance matrices across experiments. We also present a total covariance estimator for the latter estimand under homoskedasticity, which is equivalent to a k-class estimator. We show how these parameters can be used to construct unbiased proxy metrics under various structural models. Lastly, we discuss the real-world application of our methods at Netflix.",KDD
"Buss et al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n3T2) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps. A new algorithm for temporal betweenness computation is introduced in this paper. In the case of shortest and shortest foremost paths, it requires O(n + M) space and runs in time O(nM)=O(n3T), where M is the number of temporal edges, thus significantly improving the algorithm of Buss et al in terms of time complexity (note that T is usually large). Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Buss et al. Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges. For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022]. Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM)) for computing the temporal betweenness in the case of several different optimality criteria. Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n2MT2). We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria. Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris. Overall we find a general consistency between the different variants of betweenness centrality. However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks.",KDD
"As location-based services (LBS) have grown in popularity, more human mobility data has been collected. The collected data can be used to build machine learning (ML) models for LBS to enhance their performance and improve overall experience for users. However, the convenience comes with the risk of privacy leakage since this type of data might contain sensitive information related to user identities, such as home/work locations. Prior work focuses on protecting mobility data privacy during transmission or prior to release, lacking the privacy risk evaluation of mobility data-based ML models. To better understand and quantify the privacy leakage in mobility data-based ML models, we design a privacy attack suite containing data extraction and membership inference attacks tailored for point-of-interest (POI) recommendation models, one of the most widely used mobility data-based ML models. These attacks in our attack suite assume different adversary knowledge and aim to extract different types of sensitive information from mobility data, providing a holistic privacy risk assessment for POI recommendation models. Our experimental evaluation using two real-world mobility datasets demonstrates that current POI recommendation models are vulnerable to our attacks. We also present unique findings to understand what types of mobility data are more susceptible to privacy attacks. Finally, we evaluate defenses against these attacks and highlight future directions and challenges.",KDD
"Collaborative Filtering~(CF) typically suffers from the significant challenge of popularity bias due to the uneven distribution of items in real-world datasets. This bias leads to a significant accuracy gap between popular and unpopular items. It not only hinders accurate user preference understanding but also exacerbates the Matthew effect in recommendation systems. To alleviate popularity bias, existing efforts focus on emphasizing unpopular items or separating the correlation between item representations and their popularity. Despite the effectiveness, existing works still face two persistent challenges: (1) how to extract common supervision signals from popular items to improve the unpopular item representations, and (2) how to alleviate the representation separation caused by popularity bias. In this work, we conduct an empirical analysis of popularity bias and propose <u>P</u>opularity-<u>A</u>ware <u>A</u>lignment and <u>C</u>ontrast (PAAC) to address two challenges. Specifically, we use the common supervisory signals modeled in popular item representations and propose a novel popularity-aware supervised alignment module to learn unpopular item representations. Additionally, we suggest re-weighting the contrastive learning loss to mitigate the representation separation from a popularity-centric perspective. Finally, we validate the effectiveness and rationale of PAAC in mitigating popularity bias through extensive experiments on three real-world datasets. Our code is available at https://github.com/miaomiao-cai2/KDD2024-PAAC.",KDD
"Retrieval Augmented Generation (RAG) has become prevalent in question-answering (QA) tasks due to its ability of utilizing search engine to enhance the quality of long-form question-answering (LFQA). Despite the emergence of various open source methods and web-enhanced commercial systems such as Bing Chat, two critical problems remain unsolved, i.e., the lack of factuality and clear logic in the generated long-form answers. In this paper, we remedy these issues via a systematic study on answer generation in web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced generator to achieve clear logic in the generation of multifaceted answers and construct two datasets accordingly. Then we propose a factuality optimization method based on a carefully designed doubly fine-grained RLHF framework, which contains automatic evaluation and reward modeling in different levels of granularity. Our generic framework comprises conventional fine-grained RLHF methods as special cases. Extensive experiments verify the superiority of our proposed Factuality-optimized RAG (FoRAG) method on both English and Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat, the derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly used metrics (i.e., coherence, helpfulness, and factuality), while the number of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets and models are made publicly available for better reproducibility.https://huggingface.co/forag Å‚abelfootnote_dataset_url",KDD
"In label-noise learning, accurately identifying the transition matrix is crucial for developing statistically consistent classifiers. This task is complicated by instance-dependent noise, which introduces identifiability challenges in the absence of stringent assumptions. Existing methods use neural networks to estimate the transition matrix by initially extracting confident clean instances. However, this extraction process is hindered by severe inter-class imbalance and a bias toward selecting unambiguous intra-class instances, leading to a distorted understanding of noise patterns. To tackle these challenges, our paper introduces a Class Rebalance and Geometric Regularization-based Framework (CRGR). CRGR employs a smoothed, noise-tolerant reweighting mechanism to equilibrate inter-class representation, thereby mitigating the risk of model overfitting to dominant classes. Additionally, recognizing that instances with similar characteristics often exhibit parallel noise patterns, we propose that the transition matrix should mirror the similarity of the feature space. This insight promotes the inclusion of ambiguous instances in training, serving as a form of geometric regularization. Such a strategy enhances the model's ability to navigate diverse noise patterns and strengthens its generalization capabilities. By addressing both inter-class and intra-class biases, CRGR offers a more balanced and robust classification model. Extensive experiments on both synthetic and real-world datasets demonstrate CRGR's superiority over existing state-of-the-art methods, significantly boosting classification accuracy and showcasing its effectiveness in handling instance-dependent noise.",KDD
"Graph Neural Networks (GNNs) have demonstrated powerful capabilities in reasoning within Knowledge Graphs (KGs), gathering increasing attention. Our idea stems from the observation that the prior work typically employs hand-designed or sample-designed paradigms in the process of message propagation, engaging a set of adjacent entities at each step of propagation. As a result, such methods struggle with the increasing number of entities involved as propagation steps extend. Moreover, they neglect the message interactions between adjacent entities and propagation relations in KG reasoning, leading to semantic inconsistency during the message aggregation phase. To address these issues, we introduce a novel knowledge graph embedding method through a diffusion process, termed DiffusionE. Specifically, we reformulate the message propagation in knowledge reasoning as a diffusion process, regarding the message semantics as the diffusion signal. In this sense, guided by semantic information, messages can be transmitted between nodes effectively and adaptively. Furthermore, the theoretical analysis suggests our method can leverage an optimal diffusivity for message propagation in the semantic interactions of KGs. It shows that DiffusionE effectively leverages message interactions between entities and propagation relations, ensuring semantic consistency in KG reasoning. Comprehensive experiments reveal that our method attains state-of-the-art performance compared to prior work on several well-established benchmarks.",KDD
"Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation. Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability. The code is available at https://github.com/OUTHIM/power-link",KDD
"Scientific papers of a large scale on the Internet encompass a wealth of data and knowledge, attracting the attention of numerous researchers. To fully utilize these knowledge, Retrieval-Augmented Large Language Models (LLMs) usually leverage large-scale scientific corpus to train and then retrieve relevant passages from external memory to improve generation, which have demonstrated outstanding performance. However, existing methods can only capture one-dimension fragmented textual information without incorporating hierarchical structural knowledge, eg. the deduction relationship of abstract and main body, which makes it difficult to grasp the central thought of papers. To tackle this problem, we propose a hierarchical context augmentation method, which helps Retrieval-Augmented LLMs to autoregressively learn the structure knowledge of scientific papers. Specifically, we utilize the document tree to represent the hierarchical relationship of a paper and enhance the structure information of scientific context from three aspects: scale, format and global information. First, we think each top-bottom path of document tree is a logical independent context, which can be used to largely increase the scale of extracted structural corpus. Second, we propose a novel label-based format to represent the structure of context in textual sequences, unified between training and inference. Third, we introduce the global information of retrieved passages to further enhance the structure of context. Extensive experiments on three scientific tasks show that the proposed method significantly improves the performance of Retrieval-Augmented LLMs on all tasks. Besides, our method achieves start-of-art performance in Question Answer task and outperforms ChatGPT. Moreover, it also brings considerate gains with irrelevant retrieval passages, illustrating its effectiveness on practical application scenarios.",KDD
"We consider a recommender system that takes into account the interplay between recommendations, the evolution of user interests, and harmful content. We model the impact of recommendations on user behavior, particularly the tendency to consume harmful content. We seek recommendation policies that establish a tradeoff between maximizing click-through rate (CTR) and mitigating harm. We establish conditions under which the user profile dynamics have a stationary point, and propose algorithms for finding an optimal recommendation policy at stationarity. We experiment on a semi-synthetic movie recommendation setting initialized with real data and observe that our policies outperform baselines at simultaneously maximizing CTR and mitigating harm.",KDD
"Slow task detection is a critical problem in cloud operation and maintenance since it is highly related to user experience and can bring substantial liquidated damages. Most anomaly detection methods detect it from a single-task aspect. However, considering millions of concurrent tasks in large-scale cloud computing clusters, it becomes impractical and inefficient. Moreover, single-task slowdowns are very common and do not necessarily indicate a malfunction of a cluster due to its violent fluctuation nature in a virtual environment. Thus, we shift our attention to cluster-wide task slowdowns by utilizing the duration time distribution of tasks across a cluster, so that the computation complexity is not relevant to the number of tasks. The task duration time distribution often exhibits compound periodicity and local exceptional fluctuations over time. Though transformer-based methods are one of the most powerful methods to capture these time series normal variation patterns, we empirically find and theoretically explain the flaw of the standard attention mechanism in reconstructing subperiods with low amplitude when dealing with compound periodicity. To tackle these challenges, we propose SORN (i.e., <u>S</u>kimming <u>O</u>ff subperiods in descending amplitude order and <u>R</u>econstructing <u>N</u>on-slowing fluctuation), which consists of a Skimming Attention mechanism to reconstruct the compound periodicity and a Neural Optimal Transport module to distinguish cluster-wide slowdowns from other exceptional fluctuations. Furthermore, since anomalies in the training set are inevitable in a practical scenario, we propose a picky loss function, which adaptively assigns higher weights to reliable time slots in the training set. Extensive experiments demonstrate that SORN outperforms state-of-the-art methods on multiple real-world industrial datasets.",KDD
"Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs. Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance. Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data. Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for imbalance. Our proposed region-based heuristic algorithm, tailored for this NP -hard problem, strikes a balance between low time complexity and high-quality outcomes. Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100Ã— speedup) in both traditional and generalized contexts.",KDD
"Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a sure bet solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs' rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a sure bet solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning. The codes are available at https://github.com/whatashot/excelformer.",KDD
"Clustering is one of the most commonly used techniques for unsupervised data analysis. As real data sets are usually composed of numerical and categorical features that are heterogeneous in nature, the heterogeneity in the distance metric and feature coupling prevents deep representation learning from achieving satisfactory clustering accuracy. Currently, supervised Quaternion Representation Learning (QRL) has achieved remarkable success in efficiently learning informative representations of coupled features from multiple views derived endogenously from the original data. To inherit the advantages of QRL for unsupervised heterogeneous feature representation learning, we propose a deep QRL model that works in an encoder-decoder manner. To ensure that the implicit couplings of heterogeneous feature data can be well characterized by representation learning, a hierarchical coupling encoding strategy is designed to convert the data set into an attributed graph to be the input of QRL. We also integrate the clustering objective into the model training to facilitate a joint optimization of the representation and clustering. Extensive experimental evaluations illustrate the superiority of the proposed Quaternion Graph Representation Learning (QGRL) method in terms of clustering accuracy and robustness to various data sets composed of arbitrary combinations of numerical and categorical features. The source code is opened at https://github.com/Juny-Chen/QGRL.git.",KDD
"Heterogeneous information networks (HIN) have gained increasing popularity in recent years for capturing complex relations between diverse types of nodes. Meta-structures are proposed as a useful tool to identify the important patterns in HINs, but hand-crafted meta-structures pose significant challenges for scaling up, drawing wide research attention towards developing automatic search algorithms. Previous efforts primarily focused on searching for meta-structures with good empirical performance, overlooking the importance of human comprehensibility and generalizability. To address this challenge, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose ReStruct, a meta-structure search framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode the meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate their semantic feasibility. Besides, ReStruct also employs performance-oriented evolutionary operations. These two competing forces allow ReStruct to jointly optimize the semantic explainability and empirical performance of meta-structures. Furthermore, ReStruct contains a differential LLM explainer to generate and refine natural language explanations for the discovered meta-structures by reasoning through the search history. Experiments on eight representative HIN datasets demonstrate that ReStruct achieves state-of-the-art performance in both recommendation and node classification tasks. Moreover, a survey study involving 73 graduate students shows that the discovered meta-structures and generated explanations by ReStruct are substantially more comprehensible. Our code and questionnaire are available at https://github.com/LinChen-65/ReStruct.",KDD
"With the expansion and growth of cities, profiling urban areas with the advent of multi-modal urban datasets (e.g., points-of-interest and street view imagery) has become increasingly important in urban planing and management. Particularly, street view images have gained popularity for understanding the characteristics of urban areas due to its abundant visual information and inherent correlations with human activities. In this study, we define a street segment represented by multiple street view images as the minimum spatial unit for analysis and predict its functional and socioeconomic indicators, which presents several challenges in modeling spatial distributions of images on a street and the spatial topology (adjacency) of streets. Meanwhile, Large Language Models are capable of understanding imagery data based on its extraordinary knowledge base and unveil a remarkable opportunity for profiling streets with images. In view of the challenges and opportunity, we present a semi-supervised Urban Street Profiling Model (USPM) based on street view imagery and spatial adjacency of urban streets. Specifically, given a street with multiple images, we first employ a newly designed spatial context-based contrastive learning method to generate feature vectors of images and then apply the LSTM-based fusion method to encode multiple images on a street to yield the street visual representation; we then create the descriptions of street scenes for street view images based on the SPHINX (a large language model) and produce the street textual representation; finally, we build an urban street graph based on spatial topology (adjacency) and employ a semi-supervised graph learning algorithm to further encode the street representations for prediction. We conduct thorough experiments with real-world datasets to assess the proposed USPM. The experimental results demonstrate that USPM considerably outperforms baseline methods in two urban prediction tasks.",KDD
"Sequential recommendation (SR) and multi-behavior sequential recommendation (MBSR) both come from real-world scenarios. Compared with SR, MBSR takes into account the dependencies of different behaviors. We find that most existing works on MBSR are studied in the context of e-commerce scenarios. In terms of the data format of the behavior types, we observe that the conventional label-formatted data carries limited information and is inadequate for scenarios like social media. With this observation, we introducebehavior set and extend MBSR to behavior set-informed sequential recommendation (BSSR). In BSSR, behavior dependencies become more complex and personalized, and user interest arousal may lack explicit contextual associations. To delve into the dynamics inhered within a behavior set and adaptively tailor recommendation lists upon its variability, we propose a novel solution called Explicit and Implicit modeling via Dual-Path Transformer (EIDP) for BSSR. Our EIDP adopts a dual-path architecture, distinguishing between explicit modeling path (EMP) and implicit modeling path (IMP) based on whether to directly incorporate the behavior representations. EMP features the personalized behavior set-wise transition pattern extractor (PBS-TPE) as its core component. It couples behavioral representations with both the items and positions to explore intra-behavior dynamics within a behavior set at a fine granularity. IMP utilizes light multi-head self-attention blocks (L-MSAB) as encoders under specific behavior types. The obtained multi-view representations are then aggregated by cross-behavior attention fusion (CBAF), using the behavior set of the next time step as a guidance to extract collaborative semantics at the behavioral level. Extensive experiments on two real-world datasets demonstrate the effectiveness of our EIDP. We release the implementation code at: https://github.com/OshiNoCSMA/EIDP.",KDD
"Recent years have witnessed the success of introducing deep learning models to time series forecasting. From a data generation perspective, we illustrate that existing models are susceptible to distribution shifts driven by temporal contexts, whether observed or unobserved. Such context-driven distribution shift (CDS) introduces biases in predictions within specific contexts and poses challenges for conventional training paradigms. In this paper, we introduce a universal calibration methodology for the detection and adaptation of CDS with a trained model. To this end, we propose a novel CDS detector, termed the ""residual-based CDS detector"" or ""Reconditionor"", which quantifies the model's vulnerability to CDS by evaluating the mutual information between prediction residuals and their corresponding contexts. A high Reconditionor score indicates a severe susceptibility, thereby necessitating model adaptation. In this circumstance, we put forth a straightforward yet potent adapter framework for model calibration, termed the ""sample-level contextualized adapter"" or ""SOLID"". This framework involves the curation of a contextually similar dataset to the provided test sample and the subsequent fine-tuning of the model's prediction layer with a limited number of steps. Our theoretical analysis demonstrates that this adaptation strategy can achieve an optimal bias-variance trade-off. Notably, our proposed Reconditionor and SOLID are model-agnostic and readily adaptable to a wide range of models. Extensive experiments show that SOLID consistently enhances the performance of current forecasting models on real-world datasets, especially on cases with substantial CDS detected by the proposed Reconditionor, thus validating the effectiveness of the calibration approach.",KDD
"Large language models (LLMs) have achieved impressive success across various domains, but their capability in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel instruction-tuning dataset aimed at enabling language models to tackle a broad spectrum of graph problems through explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of solving various graph computational problems while generating clear reasoning processes. To further enhance the model's performance and reliability, we integrate the Direct Preference Optimization (DPO) framework within the graph problem-solving context. The improved model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Our study also investigates the relationship between training data volume and model performance, emphasizing the risk of overfitting as data volume increases. Additionally, we explore the transferability of the proposed model across different tasks and datasets, demonstrating its robust zero-shot generalization capability. GraphWiz offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving.",KDD
"To counter the side effect brought by the proliferation of social media platforms, hate speech detection (HSD) plays a vital role in halting the dissemination of toxic online posts at an early stage. However, given the ubiquitous topical communities on social media, a trained HSD classifier can easily become biased towards specific targeted groups (e.g.,female andblack people), where a high rate of either false positive or false negative results can significantly impair public trust in the fairness of content moderation mechanisms, and eventually harm the diversity of online society. Although existing fairness-aware HSD methods can smooth out some discrepancies across targeted groups, they are mostly specific to a narrow selection of targets that are assumed to be known and fixed. This inevitably prevents those methods from generalizing to real-world use cases where new targeted groups constantly emerge (e.g., new forums created on Reddit) over time. To tackle the defects of existing HSD practices, we propose <u>Ge</u>neralizable <u>t</u>arget-aware <u>Fair</u>ness (GetFair), a new method for fairly classifying each post that contains diverse and even unseen targets during inference. To remove the HSD classifier's spurious dependence on target-related features, GetFair trains a series of filter functions in an adversarial pipeline, so as to deceive the discriminator that recovers the targeted group from filtered post embeddings. To maintain scalability and generalizability, we innovatively parameterize all filter functions via a hypernetwork. Taking a target's pretrained word embedding as input, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters. In addition, a novel semantic gap alignment scheme is imposed on the generation process, such that the produced filter function for an unseen target is rectified by its semantic affinity with existing targets used for training. Finally, experiments are conducted on two benchmark HSD datasets, showing advantageous performance of GetFair on out-of-sample targets among baselines.",KDD
"Reinforcement learning-based recommender systems have recently gained popularity. However, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in all domains. To counter these challenges, recent advancements have leveraged offline reinforcement learning methods, notable for their data-driven approach utilizing offline datasets. A prominent example of this is the Decision Transformer. Despite its popularity, the Decision Transformer approach has inherent drawbacks, particularly evident in recommendation methods based on it. This paper identifies two key shortcomings in existing Decision Transformer-based methods: a lack of stitching capability and limited effectiveness in online adoption. In response, we introduce a novel methodology named Max-Entropy enhanced Decision Transformer with Reward Relabeling for Offline RLRS (EDT4Rec). Our approach begins with a max entropy perspective, leading to the development of a max-entropy enhanced exploration strategy. This strategy is designed to facilitate more effective exploration in online environments. Additionally, to augment the model's capability to stitch sub-optimal trajectories, we incorporate a unique reward relabeling technique. To validate the effectiveness and superiority of EDT4Rec, we have conducted comprehensive experiments across six real-world offline datasets and in an online simulator.",KDD
"Understanding customer behavior is crucial for improving service quality in large-scale E-commerce. This paper proposes C-STAR, a new framework that learns compact representations from customer shopping journeys, with good versatility to fuel multiple downstream customer-centric tasks. We define the notion of shopping trajectory that encompasses customer interactions at the level of product categories, capturing the overall flow of their browsing and purchase activities. C-STAR excels at modeling both inter-trajectory distribution similarity-the structural similarities between different trajectories, and intra-trajectory semantic correlation-the semantic relationships within individual ones. This coarse-to-fine approach ensures informative trajectory embeddings for representing customers. To enhance embedding quality, we introduce a pre-training strategy that captures two intrinsic properties within the pre-training data. Extensive evaluation on large-scale industrial and public datasets demonstrates the effectiveness of C-STAR across three diverse customer-centric tasks. These tasks empower customer profiling and recommendation services for enhancing personalized shopping experiences on our E-commerce platform.",KDD
"Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios. Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference. Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability that completely ignores hidden confounders, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions. In this paper, to overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding. With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution. Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data. Since transductive conformal prediction is notoriously costly, we propose wSCP-DR, a two-stage variant of wTCP-DR, based on split conformal prediction with same marginal coverage guarantees but at a significantly lower computational cost. After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs). We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency. Our code can be found at https://github.com/rguo12/KDD24-Conformal.",KDD
"Knowledge Tracing aims to assess student learning states by predicting their performance in answering questions. Different from the existing research which utilizes fixed-length learning sequence to obtain the student states and regards KT as a static problem, this work is motivated by three dynamical characteristics: 1) The scales of students answering records are constantly growing; 2) The semantics of time intervals between the records vary; 3) The relationships between students, questions and concepts are evolving. The three dynamical characteristics above contain the great potential to revolutionize the existing knowledge tracing methods. Along this line, we propose a Dynamic Graph-based Knowledge Tracing model, namely DyGKT. In particular, a continuous-time dynamic question-answering graph for knowledge tracing is constructed to deal with the infinitely growing answering behaviors, and it is worth mentioning that it is the first time dynamic graph learning technology is used in this field. Then, a dual time encoder is proposed to capture long-term and short-term semantics among the different time intervals. Finally, a multiset indicator is utilized to model the evolving relationships between students, questions, and concepts via the graph structural feature. Numerous experiments are conducted on five real-world datasets, and the results demonstrate the superiority of our model. All the used resources are publicly available at https://github.com/PengLinzhi/DyGKT.",KDD
"Structure encoding has proven to be the key feature to distinguishing links in a graph. However, Structure encoding in the temporal graph keeps changing as the graph evolves, repeatedly computing such features can be time-consuming due to the high-order subgraph construction. We develop the Co-Neighbor Encoding Schema (CNES) to address this issue. Instead of recomputing the feature by the link, CNES stores information in the memory to avoid redundant calculations. Besides, unlike the existing memory-based dynamic graph learning method that stores node hidden states, we introduce a hashtable-based memory to compress the adjacency matrix for efficient structure feature construction and updating with vector computation in parallel. Furthermore, CNES introduces a Temporal-Diverse Memory to generate long-term and short-term structure encoding for neighbors with different structural information. A dynamic graph learning framework, Co-Neighbor Encoding Network (CNE-N), is proposed using the aforementioned techniques. Extensive experiments on thirteen public datasets verify the effectiveness and efficiency of the proposed method.",KDD
"Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One significant limitation is that they operate under the assumption that the graph exhibits homophily and that the labels are distributed smoothly. However, real-world graphs can exhibit varying degrees of heterophily, or even be dominated by heterophily, which results in the inadequacy of the current methods.
In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following observations, we propose a efficient algorithm, denoted as R2LP. Specifically, R2LP is an iterative algorithm with three steps: (1) reconstruct the graph to recover the homophily property, (2) utilize label propagation to rectify the noisy labels, (3) select high-confidence labels to retain for the next iteration. By iterating these steps, we obtain a set of ''correct'' labels, ultimately achieving high accuracy in the node classification task. The theoretical analysis is also provided to demonstrate its remarkable denoising effect. Finally, we perform experiments on ten benchmark datasets with different levels of graph heterophily and various types of noise. In these experiments, we compare the performance of R2LP against ten typical baseline methods. Our results illustrate the superior performance of the proposed Ã¸urs. The code and data of this paper can be accessed at: https://github.com/cy623/R2LP.git.",KDD
"Accurately predicting the popularity of multimodal user-generated content (UGC) is fundamental for many real-world applications such as online advertising and recommendation. Existing approaches generally focus on limited contextual information within individual UGCs, yet overlook the potential benefit of exploiting meaningful knowledge in relevant UGCs. In this work, we propose RAGTrans, an aspect-aware retrieval-augmented multi-modal hypergraph transformer that retrieves pertinent knowledge from a multi-modal memory bank and enhances UGC representations via neighborhood knowledge aggregation on multi-model hypergraphs. In particular, we initially retrieve relevant multimedia instances from a large corpus of UGCs via the aspect information and construct a knowledge-enhanced hypergraph based on retrieved relevant instances. This allows capturing meaningful contextual information across the data. We then design a novel bootstrapping hypergraph transformer on multimodal hypergraphs to strengthen UGC representations across modalities via customizing a propagation algorithm to effectively diffuse information across nodes and edges. Additionally, we propose a user-aware attention-based fusion module to comprise the enriched UGC representations for popularity prediction. Extensive experiments on real-world social media datasets demonstrate that RAGTrans outperforms state-of-the-art popularity prediction models across settings.",KDD
"Graph Neural Networks (GNN) have proven successful for graph-related tasks. However, many GNNs methods require labeled data, which is challenging to obtain. To tackle this, graph contrastive learning (GCL) have gained attention. GCL learns by contrasting similar nodes (positives) and dissimilar nodes (negatives). Current GCL methods, using data augmentation for positive samples and random selection for negative samples, can be sub-optimal due to limited positive samples and the possibility of false-negative samples. In this study, we propose an enhanced objective addressing these issues. We first introduce an ideal objective with all positive and no false-negative samples, then transform it probabilistically based on sampling distributions. We next model these distributions with node similarity and derive an enhanced objective. Comprehensive experiments have shown the effectiveness of the proposed enhanced objective for a broad set of GCL models.",KDD
"We propose an efficient boosting algorithm for multiclass classification, called AdaBoost.Iter, that extends SAMME and AdaBoost. The algorithm iteratively applies the weak learnability condition of SAMME to eliminate classes to find the correct classificiation. The iterative weak learnability is a sufficient and necessary condition for boostability, but it is also easier to validate than the EOR criterion of AdaBoost.MM \citeMukherjeeSchapire2013. We show that the training error of AdaBoost.Iter vanishes at the exponential rate, while the generalization error converges to zero at the same rate as AdaBoost. AdaBoost.Iter numerically outperforms SAMME and achieves performance comparable to AdaBoost.MM on benchmark datasets.",KDD
"Today, as increasingly complex predictive models are developed, simple rule sets remain a crucial tool to obtain interpretable predictions and drive high-stakes decision making. However, a single rule set provides a partial representation of a learning task. An emerging paradigm in interpretable machine learning aims at exploring the Rashomon set of all models exhibiting near-optimal performance. Existing work on Rashomon-set exploration focuses on exhaustive search of the Rashomon set for particular classes of models, which can be a computationally challenging task. On the other hand, exhaustive enumeration leads to redundancy that often is not necessary, and a representative sample or an estimate of the size of the Rashomon set is sufficient for many applications. In this work, we propose, for the first time, efficient methods to explore the Rashomon set of rule-set models with or without exhaustive search. Extensive experiments demonstrate the effectiveness of the proposed methods in a variety of scenarios.",KDD
"Providing recommendations that are both relevant and diverse is a key consideration of modern recommender systems. Optimizing both of these measures presents a fundamental trade-off, as higher diversity typically comes at the cost of relevance, resulting in lower user engagement. Existing recommendation algorithms try to resolve this trade-off by combining the two measures, relevance and diversity, into one aim and then seeking recommendations that optimize the combined objective, for a given number of items. Traditional approaches, however, do not consider the user interaction with the suggested items. In this paper, we put the user at the central stage, and build on the interplay between relevance, diversity, and user behavior. In contrast to applications where the goal is solely to maximize engagement, we focus on scenarios aiming at maximizing the total amount of knowledge encountered by the user. We use diversity as a surrogate for the amount of knowledge obtained by the user while interacting with the system, and we seek to maximize diversity. We propose a probabilistic user-behavior model in which users keep interacting with the recommender system as long as they receive relevant suggestions, but they may stop if the relevance of the recommended items drops. Thus, for a recommender system to achieve a high-diversity measure, it will need to produce recommendations that are both relevant and diverse. Finally, we propose a novel recommendation strategy that combines relevance and diversity by a copula function. We conduct an extensive evaluation of the proposed methodology over multiple datasets, and we show that our strategy outperforms several state-of-the-art competitors. Our implementation is publicly available at https://github.com/EricaCoppolillo/EXPLORE.",KDD
"Knowledge tracing (KT) is a crucial task in intelligent education, focusing on predicting students' performance on given questions to trace their evolving knowledge. The advancement of deep learning in this field has led to deep-learning knowledge tracing (DLKT) models that prioritize high predictive accuracy. However, many existing DLKT methods overlook the fundamental goal of tracking students' dynamical knowledge mastery. These models do not explicitly model knowledge mastery tracing processes or yield unreasonable results that educators find difficulty to comprehend and apply in real teaching scenarios. In response, our research conducts a preliminary analysis of mainstream KT approaches to highlight and explain such unreasonableness. We introduce GRKT, a graph-based reasonable knowledge tracing method to address these issues. By leveraging graph neural networks, our approach delves into the mutual influences of knowledge concepts, offering a more accurate representation of how the knowledge mastery evolves throughout the learning process. Additionally, we propose a fine-grained and psychological three-stage modeling process as knowledge retrieval, memory strengthening, and knowledge learning/forgetting, to conduct a more reasonable knowledge tracing process. Comprehensive experiments demonstrate that GRKT outperforms eleven baselines across three datasets, not only enhancing predictive accuracy but also generating more reasonable knowledge tracing results. This makes our model a promising advancement for practical implementation in educational settings. The source code is available at https://github.com/JJCui96/GRKT.",KDD
"Submodular optimization has been identified as a powerful tool for many data mining applications, where a representative subset of moderate size needs to be extracted from a large-scale dataset. In scenarios where data points possess sensitive attributes such as age, gender, or race, it becomes imperative to integrate fairness measures into submodular optimization to mitigate bias and discrimination. In this paper, we study the fundamental problem of fair submodular maximization subject to a knapsack constraint and propose the first streaming algorithm for it with provable performance guarantees for both monotone and non-monotone submodular functions. As a byproduct, we also propose a streaming algorithm for submodular maximization subject to a partition matroid and a knapsack constraint, significantly improving the performance bounds achieved by previous work. We conduct extensive experiments on real-world applications such as movie recommendation, image summarization, and maximum coverage in social networks. The experimental results strongly demonstrate the superiority of our proposed algorithms in terms of both fairness and utility.",KDD
"Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the source bias. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias.",KDD
"We propose AGS-GNN, a novel attribute-guided sampling algorithm for Graph Neural Networks (GNNs). AGS-GNN exploits the node features and the connectivity structure of a graph while simultaneously adapting for both homophily and heterophily in graphs. In homophilic graphs, vertices of the same class are more likely to be adjacent, but vertices of different classes tend to be adjacent in heterophilic graphs. GNNs have been successfully applied to homophilic graphs, but their utility to heterophilic graphs remains challenging. The state-of-the-art GNNs for heterophilic graphs use the full neighborhood of a node instead of sampling it, and hence do not scale to large graphs and are not inductive. We develop dual-channel sampling techniques based on feature-similarity and feature-diversity to select subsets of neighbors for a node that capture adaptive information from homophilic and heterophilic neighborhoods. Currently, AGS-GNN is the only algorithm that explicitly controls homophily in the sampled subgraph through similar and diverse neighborhood samples. For diverse neighborhood sampling, we employ submodularity, a novel contribution in this context. We pre-compute the sampling distribution in parallel, achieving the desired scalability. Using an extensive dataset consisting of 35 small (< 100K nodes) and large (- 100K nodes) homophilic and heterophilic graphs, we demonstrate the superiority of AGS-GNN compared to the state-of-the-art approaches. AGS-GNN achieves test accuracy comparable to the best-performing heterophilic GNNs, even outperforming methods that use the entire graph for node classification. AGS-GNN converges faster than methods that sample neighborhoods randomly, and can be incorporated into existing GNN models that employ node or graph sampling.",KDD
"Monitoring and maintaining machine learning models are among the most critical challenges in translating recent advances in the field into real-world applications. However, current monitoring methods lack the capability of provide actionable insights answering the question of why the performance of a particular model really degraded. In this work, we propose a novel approach to explain the behavior of a black-box model under feature shifts by attributing an estimated performance change to interpretable input characteristics. We refer to our method that combines concepts from Optimal Transport and Shapley Values as Explanatory Performance Estimation (XPE). We analyze the underlying assumptions and demonstrate the superiority of our approach over several baselines on different data sets across various data modalities such as images, audio, and tabular data. We also indicate how the generated results can lead to valuable insights, enabling explanatory model monitoring by revealing potential root causes for model deterioration and guiding toward actionable countermeasures.",KDD
"Disease surveillance, traffic management, and weather forecasting are some of the key applications that could benefit from block maxima forecasting of a time series as the extreme block maxima values often signify events of critical importance such as disease outbreaks, traffic gridlock, and severe weather conditions. As the use of deep neural network models for block maxima forecasting increases, so does the need for explainable AI methods that could unravel the inner workings of such black box models. To fill this need, this paper presents a novel counterfactual explanation framework for block maxima forecasting models. Unlike existing methods, our proposed framework, DiffusionCF, combines deep anomaly detection with a conditional diffusion model to identify unusual patterns in the time series that could help explain the forecasted extreme block maxima. Experimental results on several real-world datasets demonstrate the superiority of DiffusionCF over other baseline methods when evaluated according to various metrics, particularly their informativeness and closeness. Our data and codes are available at https://github.com/yue2023cs/DiffusionCF.",KDD
"Graph neural networks (GNNs) based on message passing have achieved remarkable performance in graph machine learning. By combining it with the power of pseudo labeling, one can further push forward the performance on the task of semi-supervised node classification. However, most existing works assume that the training node labels are purely noise-free, while this strong assumption usually does not hold in practice. GNNs will overfit the noisy training labels and the adverse effects of mislabeled nodes can be exaggerated by being propagated to the remaining nodes through the graph structure, exacerbating the model failure. Worse still, the noisy pseudo labels could also largely undermine the model's reliability without special treatment. In this paper, we revisit the role of (1) message passing and (2) pseudo labels in the studied problem and try to address two denoising subproblems from the model architecture and algorithm perspective, respectively. Specifically, we first develop a label-noise robust GNN that discards the coupled message-passing scheme. Despite its simple architecture, this learning backbone prevents overfitting to noisy labels and also inherently avoids the noise propagation issue. Moreover, we propose a novel reliable graph pseudo labeling algorithm that can effectively leverage the knowledge of unlabeled nodes while mitigating the adverse effects of noisy pseudo labels. Based on those novel designs, we can attain exceptional effectiveness and efficiency in solving the studied problem. We conduct extensive experiments on benchmark datasets for semi-supervised node classification with different levels of label noise and show new state-of-the-art performance. The code is available at https://github.com/DND-NET/DND-NET.",KDD
"Deep neural network based Outlier Detection (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HP settings, the issue is ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled outliers), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on different OD tasks show that HYPER achieves competitive performance against 8 baselines with significant efficiency gains.",KDD
"Many billion-scale large language models (LLMs) have been released for resource-constraint mobile devices to provide local LLM inference service when cloud-based powerful LLMs are not available. However, the capabilities of current on-device LLMs still lag behind those of cloud-based LLMs, and how to effectively and efficiently enhance on-device LLM inference becomes a practical requirement. We thus propose to collect the user's historical interactions with the cloud-based LLM and build an external datastore on the mobile device for enhancement using nearest neighbors search. Nevertheless, the full datastore improves the quality of token generation at the unacceptable expense of much slower generation speed. To balance performance and efficiency, we propose to select an optimal subset of the full datastore within the given size limit, the optimization objective of which is proven to be submodular. We further design an offline algorithm, which selects the subset after the construction of the full datastore, as well as an online algorithm, which performs selection over the stream and can be flexibly scheduled. We theoretically analyze the performance guarantee and the time complexity of the offline and the online designs to demonstrate effectiveness and scalability. We finally take three ChatGPT related dialogue datasets and four different on-device LLMs for evaluation. Evaluation results show that the proposed designs significantly enhance LLM performance in terms of perplexity while maintaining fast token generation speed. Practical overhead testing on the smartphone reveal the efficiency of on-device datastore subset selection from memory usage and computation overhead.",KDD
"People usually interact in groups, and such groups may appear on different platforms. For instance, people often create various group chats on messaging apps (e.g., Facebook Messenger and WhatsApp) to communicate with families, friends, or colleagues. How do we identify the same people across the two platforms based on the information about the groups? This gives rise to the hypergraph alignment problem, whose objective is to find the correspondences between the sets of nodes of two hypergraphs. In a hypergraph, a node represents a person, and each hyperedge represents a group of several people. In addition, the two sets of hyperedges in the two hypergraphs can vary significantly in scales as people may use different apps at different time periods.
In this work, we propose and tackle the problem of unsupervised hypergraph alignment. Given two hypergraphs with potentially different scales and without any side information or prior ground-truth correspondences, we develop Ã˜urMethod, a learning framework, to find node correspondences across the two hypergraphs. Ã˜urMethod directly addresses each challenge of the problem. In particular, it (a) extracts node features from the hypergraph topology, (b) employs contrastive learning, as a ""supervised pseudo-alignment'' task to pre-train the learning model (c) applies topological augmentation to help a generative adversarial network to align the two embedding spaces from the two hypergraphs. The purpose of augmentation is to add virtual hyperedges from one hypergraph in order to the other to resolve the scale difference and share information across the two hypergraphs. Our extensive experiments on 12 real-world datasets demonstrate the significant and consistent superiority of Ã˜urMethod over the baseline approaches.",KDD
"Graph Neural Networks (GNNs) have been increasingly deployed in a plethora of applications. However, the graph data used for training may contain sensitive personal information of the involved individuals. Once trained, GNNs typically encode such information in their learnable parameters. As a consequence, privacy leakage may happen when the trained GNNs are deployed and exposed to potential attackers. Facing such a threat, machine unlearning for GNNs has become an emerging technique that aims to remove certain personal information from a trained GNN. Among these techniques, certified unlearning stands out, as it provides a solid theoretical guarantee of the information removal effectiveness. Nevertheless, most of the existing certified unlearning methods for GNNs are only designed to handle node and edge unlearning requests. Meanwhile, these approaches are usually tailored for either a specific design of GNN or a specially designed training objective. These disadvantages significantly jeopardize their flexibility. In this paper, we propose a principled framework named IDEA to achieve flexible and certified unlearning for GNNs. Specifically, we first instantiate four types of unlearning requests on graphs, and then we propose an approximation approach to flexibly handle these unlearning requests over diverse GNNs. We further provide theoretical guarantee of the effectiveness for the proposed approach as a certification. Different from existing alternatives, IDEA is not designed for any specific GNNs or optimization objectives to perform certified unlearning, and thus can be easily generalized. Extensive experiments on real-world datasets demonstrate the superiority of IDEA in multiple key perspectives.",KDD
"Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a <u>H</u>eterogeneity-<u>I</u>nformed Spatiotemporal <u>M</u>eta-<u>Net</u>work (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at <u>https://github.com/XDZhelheim/HimNet</u>.",KDD
"There are many applications for which we want to learn a latent scale for subjective properties, such as the excitement of a photo or the legibility of a font; however, obtaining human-labeled data is costly and time-consuming. One oft-used method for acquiring these labels, despite the cost being quadratic in the number of items, is the method of pairwise comparisons since this method minimizes the effect of biases and generally can be used effectively outside of a controlled environment.
Crowdsourcing appears to be a panacea since online platforms provide affordable access to numerous people, but these participants, judges, vary in diligence and expertise. Several methods have been proposed to assign weights to judges based on their responses relative to everyone else, the goal being to reduce exposure to poor performers, hopefully upgrading the quality of the data.
Our research focuses on two natural extensions to the Bradley-Terry-Luce formulation of scaling that jointly optimize for both scale value and judge weights. While both methods appear to perform at least as well as the unweighted formulation on average with well-behaved judges, we report a previously unknown flaw, revealing that the resultant judge weights should not be interpreted as reliabilities. Consequently, these values should not be leveraged for decisions about the judges, such as for active sampling or to validate the participant pool.",KDD
"Temporal graph representation learning has drawn considerable attention in recent years. Most existing works mainly focus on modeling local structural dependencies of temporal graphs. However, underestimating the inherent global structural role information in many real-world temporal graphs inevitably leads to sub-optimal graph representations. To overcome this shortcoming, we propose a novel Role-based Temporal Graph Convolution Network (RTGCN) that fully leverages the global structural role information in temporal graphs. Specifically, RTGCN can effectively capture the static global structural roles by using hypergraph convolution neural networks. To capture the evolution of nodes' structural roles, we further design structural role-based gated recurrent units. Finally, we integrate structural role proximity in our objective function to preserve global structural similarity, further promoting temporal graph representation learning. Experimental results on multiple real-world datasets demonstrate that RTGCN consistently outperforms state-of-the-art temporal graph representation learning methods by significant margins in various temporal link prediction and node classification tasks. Specifically, RTGCN achieves AUC improvement of up to 5.1% for link prediction and F1 improvement of up to 6.2% for new link prediction. In addition, RTGCN achieves AUC improvement up to 4.6% for node classification and 2.7% for structural role classification.",KDD
"Recommender systems play important roles in various applications such as e-commerce, social media, etc. Conventional recommendation methods usually model the collaborative signals within the tabular representation space. Despite the personalization modeling and the efficiency, the latent semantic dependencies are omitted. Methods that introduce semantics into recommendation then emerge, injecting knowledge from the semantic representation space where the general language understanding are compressed. However, existing semantic-enhanced recommendation methods focus on aligning the two spaces, during which the representations of the two spaces tend to get close while the unique patterns are discarded and not well explored. In this paper, we propose DisCo to Disentangle the unique patterns from the two representation spaces and Collaborate the two spaces for recommendation enhancement, where both the specificity and the consistency of the two spaces are captured. Concretely, we propose 1) a dual-side attentive network to capture the intra-domain patterns and the inter-domain patterns, 2) a sufficiency constraint to preserve the task-relevant information of each representation space and filter out the noise, and 3) a disentanglement constraint to avoid the model from discarding the unique information. These modules strike a balance between disentanglement and collaboration of the two representation spaces to produce informative pattern vectors, which could serve as extra features and be appended to arbitrary recommendation backbones for enhancement. Experiment results validate the superiority of our method against different models and the compatibility of DisCo over different backbones. Various ablation studies and efficiency analysis are also conducted to justify each model component.",KDD
"Recently, much effort has been devoted to modeling users' multi-interests (aka multi-faceted preferences) based on their behaviors, aiming to accurately capture users' complex preferences. Existing methods attempt to model each interest of users through a distinct representation, but these multi-interest representations easily collapse into similar ones due to a lack of effective guidance. In this paper, we propose a generic multi-interest method for sequential recommendation, achieving disentangled representation learning of diverse interests technically and theoretically. To alleviate the collapse issue of multi-interests, we propose to conduct item partition guided by their likelihood of being co-purchased in a global view. It can encourage items in each group to focus on a discriminated interest, thus achieving effective disentangled learning of multi-interests. Specifically, we first prove the theoretical connection between item partition and spectral clustering, demonstrating its effectiveness in alleviating item-level and facet-level collapse issues that hinder existing disentangled methods. To efficiently optimize this problem, we then propose a Markov Random Field (MRF)-based method that samples small-scale sub-graphs from two separate MRFs, thus it can be approximated with a cross-entropy loss and optimized through contrastive learning. Finally, we perform multi-task learning to seamlessly align item partition learning with multi-interest modeling for more accurate recommendation. Experiments on three real-world datasets show that our method significantly outperforms state-of-the-art methods and can flexibly integrate with existing multi-interest models as a plugin to enhance their performances.",KDD
"Self-supervised Heterogeneous Graph Representation (SSHGRL) learning is widely used in data mining. The latest SSHGRL methods normally use metapaths to describe the heterogeneous information (multiple relations and node types) to learn the heterogeneous graph representation and achieve impressive results. However, establishing metapaths requires lofty computational costs that are too high for the medium and large graphs. To this end, this paper proposes a Reserving-Masking-Reconstruction (RMR) model that can fully consider heterogeneous information without relying on the metapaths. In detail, we propose a reserving method to reserve to-be-masked nodes' (target nodes) information before graph masking. Second, we split the reserved graph into relation subgraphs according to the type of relations that require much less computational overheads than metapath. Then, the target nodes in each relation subgraph are randomly masked with minimal topology information loss. After, a novel reconstruction method is proposed to reconstruct the masked nodes on different relation subgraphs to establish the self-supervised signal. The proposed method requires low computational complexity and can establish a self-supervised signal without deeply changing the graph topology. Experimental results show the proposed method achieves state-of-the-art records on medium and large-scale heterogeneous graphs and competitive records on small-scale heterogeneous graphs. The code is available at https://github.com/DuanhaoranCC/RMR.",KDD
"In this paper, we present a novel method to significantly enhance the computational efficiency of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived from the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star topology as a GWT prior to training, we balance edge reduction with efficient information propagation, reducing computational demands while maintaining high model performance. Both the time and memory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from O(N2) to O(N). Our approach streamlines the ASTGNN deployment by eliminating the need for exhaustive training, pruning, and retraining cycles, and demonstrates empirically across various datasets that it is possible to achieve comparable performance to full models with substantially lower computational costs. Specifically, our approach enables training ASTGNNs on the largest scale spatial-temporal dataset using a single A6000 equipped with 48 GB of memory, overcoming the out-of-memory issue encountered during original training and even achieving state-of-the-art performance. Furthermore, we delve into the effectiveness of the GWT from the perspective of spectral graph theory, providing substantial theoretical support. This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability of the LTH in resource-constrained settings, marking a significant step forward in the field of graph neural networks. Code is available at https://anonymous.4open.science/r/paper-1430.",KDD
"We study an auction setting in which bidders bid for placement of their content within a summary generated by a large language model (LLM), e.g., an ad auction in which the display is a summary paragraph of multiple ads. This generalizes the classic ad settings such as position auctions to an LLM generated setting, which allows us to handle general display formats. We propose a novel factorized framework in which an auction module and an LLM module work together via a prediction model to provide welfare maximizing summary outputs in an incentive compatible manner. We provide a theoretical analysis of this framework and synthetic experiments to demonstrate the feasibility and validity of the system together with welfare comparisons.",KDD
"As an emerging interpretable technique, Generalized Additive Models (GAMs) adopt neural networks to individually learn non-linear functions for each feature, which are then combined through a linear model for final predictions. Although GAMs can explain deep neural networks (DNNs) at the feature level, they require large numbers of model parameters and are prone to overfitting, making them hard to train and scale. Additionally, in real-world datasets with many features, the interpretability of feature-based explanations diminishes for humans. To tackle these issues, recent research has shifted towards concept-based interpretable methods. These approaches try to integrate concept learning as an intermediate step before making predictions, explaining the predictions in terms of human-understandable concepts. However, these methods require domain experts to extensively label concepts with relevant names and their ground-truth values. In response, we propose CAT, a novel interpretable Concept-bAsed Taylor additive model to simplify this process. CAT does not require domain experts to annotate concepts and their ground-truth values. Instead, it only requires users to simply categorize input features into broad groups, which can be easily accomplished through a quick metadata review. Specifically, CAT first embeds each group of input features into one-dimensional high-level concept representation, and then feeds the concept representations into a new white-box Taylor Neural Network (TaylorNet). The TaylorNet aims to learn the non-linear relationship between the inputs and outputs using polynomials. Evaluation results across multiple benchmarks demonstrate that CAT can outperform or compete with the baselines while reducing the need of extensive model parameters. Importantly, it can effectively explain model predictions through high-level concepts. Source code is available at github.com/vduong143/CAT-KDD-2024.",KDD
"Due to the timeliness and uncertainty of data acquisition, label shift, which assumes that the source (training) and target (test) label distributions differ, occurs with the changing environment and reduces the generalization ability of traditional models. To correct the label shift, existing methods estimate the true label distribution by prediction of target data from a source classifier, which results in high variance, especially with large label shift. In this paper, we tackle this problem by proposing a novel approach termed as Label Shift Correction via Bidirectional Marginal Distribution Matching (BMDM). Our approach matchs the label and feature marginal distributions simultaneously to ensure the stability of estimated class proportions. We prove theoretically that there is a unique optimal solution, i.e., true target label distribution, for our approach under mild conditions, and an efficient optimization strategy is also proposed. On this basis, in multi-shot scenario where label distribution changes continuously, we extend BMDM by designing a new distribution matching mechanism and constructing a regularization term that constrains the direction of label distribution change. Extensive experimental results validate the effectiveness of our approach over existing state-of-the-arts methods.",KDD
"This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph's topological structure, we aim to improve view generation through language supervision. This is driven by the prevalence of textual attributes in real applications, which complement graph structures with rich semantic information. However, this presents challenges because of two major reasons. First, text attributes often vary in length and quality, making it difficulty to perturb raw text descriptions without altering their original semantic meanings. Second, although text attributes complement graph structures, they are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a novel framework for augmenting TAGs. It leverages advanced large language models like Mistral to enhance self-supervised graph learning. Specifically, we introduce a mixture-of-prompt-expert technique to generate augmented node features. This approach adaptively maps multiple prompt experts, each of which modifies raw text attributes using prompt engineering, into numerical feature space. Additionally, we devise a collaborative edge modifier to leverage structural and textual commonalities, enhancing edge augmentation by examining or building connections between nodes. Empirical results across five benchmark datasets spanning various domains underscore our framework's ability to enhance the performance of leading contrastive methods (e.g., BGRL, GraphCL, and GBT) as a plug-in tool. Notably, we observe that the augmented features and graph structure can also enhance the performance of standard generative methods (e.g., GraphMAE and S2GAE), as well as popular graph neural networks (e.g., GCN and GAT). The open-sourced implementation of our GAugLLM is available at https://github.com/NYUSHCS/GAugLLM.",KDD
"The next Point-of-interest recommendation has attracted extensive research interest recently, which predicts users' subsequent movements. The main challenge is how to effectively capture users' personalized sequential transitions in check-in trajectory, and various methods have been developed. However, most existing studies ignore the temporal information when conducting the next POI recommendation. To fill this gap, we investigate a time-specific next POI recommendation task, which additionally incorporates the target time information. We propose a brand new Time2Rotation technique to capture the temporal information. Different from conventional methods, we represent timeslots as rotation vectors and then perform the rotation operations. Based on the Time2Rotation technique, we propose a novel rotation-based temporal attention network, namely ROTAN, for the time-specific next POI recommendation task. The ROTAN begins by building a collaborative POI transition graph, capturing the asymmetric temporal influence in sequential transitions. After that, it incorporates temporal information into the modeling of individual check-in trajectories, extracting separate representations for user preference and POI influence to reflect their distinct temporal patterns. Lastly, the target time is integrated to generate recommendations. Extensive experiments are conducted on three real-world datasets, which demonstrates the advantages of the proposed Time2Rotation technique and ROTAN recommendation model.",KDD
"We consider a ubiquitous scenario in the study of Influence Maximization (IM), in which there is limited knowledge about the topology of the diffusion network. We set the IM problem in a multi-round diffusion campaign, aiming to maximize the number of distinct users that are influenced. Leveraging the capability of bandit algorithms to effectively balance the objectives of exploration and exploitation, as well as the expressivity of neural networks, our study explores the application of neural bandit algorithms to the IM problem. We propose the framework IM-GNB (Influence Maximization with Graph Neural Bandits), where we provide an estimate of the users' probabilities of being influenced by influencers (also known as diffusion seeds). This initial estimate forms the basis for constructing both an exploitation graph and an exploration one. Subsequently, IM-GNB handles the exploration-exploitation tradeoff, by selecting seed nodes in real-time using Graph Convolutional Networks (GCN), in which the pre-estimated graphs are employed to refine the influencers' estimated rewards in each contextual setting. Through extensive experiments on two large real-world datasets, we demonstrate the effectiveness of IM-GNB compared with other baseline methods, significantly improving the spread outcome of such diffusion campaigns, when the underlying network is unknown.",KDD
"Unsupervised anomaly detection in multivariate time series (MTS) has always been a challenging problem, and the modeling based on reconstruction has garnered significant attention. The insensitivity of these methods towards normal patterns poses challenges in distinguishing between normal and abnormal points. Firstly, the general reconstruction strategies may exhibit limited sensitivity to spatio-temporal dependencies, and their performance remains largely unaffected by such dependencies. Secondly, most methods fail to model the heteroscedastic uncertainty in MTS, hindering their abilities to derive a distinguishable criterion. For instance, normal data with high noise levels may lead to detection failure due to excessively high reconstruction errors. In this work, we emphasize the necessity of sensitivity to normal patterns, which could improve the discrimination between normal and abnormal points remarkably. To this end, we propose SensitiveHUE, a probabilistic network by implementing both reconstruction and heteroscedastic uncertainty estimation. Its core includes a statistical feature removal strategy to ensure the dependency sensitive property, and a novel MTS-NLL loss for modeling the normal patterns in important regions. Experimental results demonstrate that SensitiveHUE exhibits nontrivial sensitivity to normal patterns and outperforms the existing state-of-the-art alternatives by a large margin. Code is publicly available at this URL\footnotehttp://github.com/yuesuoqingqiu/SensitiveHUE.",KDD
"Mobile traffic prediction plays a crucial role in enabling efficient network management and service provisioning. Traditional prediction approaches treat different mobile application services (such as Uber, Facebook, Twitter, etc) as isolated entities, neglecting potential correlation among them. Moreover, such isolated prediction methods necessitate the uploading of historical traffic data from all regions to forecast city-wide traffic, resulting in consuming substantial bandwidth resources and risking prediction failure in the event of data loss in specific regions. To address these challenges, we propose a novel Cross-service Attention-based Spatial-Temporal Graph Convolutional Network (CsASTGCN) for precise and communication-efficient multi-service mobile traffic prediction. Our methodology allows each mobile service to transmit the traffic data of only a fraction of regions for city-wide traffic prediction of all mobile services, which reduces the resource consumption caused by data transmission. Specifically, the sparse traffic data are initially transmitted to the cloud server and the masked graph autoencoder is utilized to roughly reconstruct the traffic volume for regions with missing data. Subsequently, a cross-service attention-based predictor is designed to calculate the data correlation among different mobile services within the same region. Considering the constantly emerging mobile services, we incorporate a novel model-based adaptive transfer learning scheme to extract valuable knowledge from the existing models and expedite the training of a new model for a new service without training from scratch, thereby enhancing the scalability of our framework. Extensive experiments conducted on a large-scale real-world mobile traffic dataset demonstrate that our model greatly outperforms the existing schemes, enhancing both the communication-efficiency and robustness of large-scale multi-service traffic prediction.",KDD
"Automated machine learning (AutoML) streamlines the creation of ML models, but few specialized methods have approached the challenging domain of time series forecasting. Deep neural networks (DNNs) often deliver state-of-the-art predictive performance for forecasting data, however these models are also criticized for being computationally intensive black boxes. As a result, when searching for the ""best"" model, it is crucial to also acknowledge other aspects, such as interpretability and resource consumption. In this paper, we propose AutoXPCR - a novel method that produces DNNs for forecasting under consideration of multiple objectives in an automated and explainable fashion. Our approach leverages meta-learning to estimate any model's performance along PCR criteria, which encompass (P)redictive error, (C)omplexity, and (R)esource demand. Explainability is addressed on multiple levels, as AutoXPCR pro-vides by-product explanations of recommendations and allows to interactively control the desired PCR criteria importance and trade-offs. We demonstrate the practical feasibility AutoXPCR across 108 forecasting data sets from various domains. Notably, our method outperforms competing AutoML approaches - on average, it only requires 20% of computation costs for recommending highly efficient models with 85% of the empirical best quality.",KDD
"Due to the continuously improving capabilities of mobile edges, recommender systems start to deploy models on edges to alleviate network congestion caused by frequent mobile requests. Several studies have leveraged the proximity of edge-side to real-time data, fine-tuning them to create edge-specific models. Despite their significant progress, these methods require substantial on-edge computational resources and frequent network transfers to keep the model up to date. The former may disrupt other processes on the edge to acquire computational resources, while the latter consumes network bandwidth, leading to a decrease in user satisfaction. In response to these challenges, we propose a customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploys the same generic backbone (potentially incompatible for a specific edge) to all devices. To minimize frequent bandwidth usage and storage consumption in personalization, DIET tailors specific subnets for each edge based on its past interactions, learning to generate slimming subnets(diets) within incompatible networks for efficient transfer. It also takes the inter-layer relationships into account, empirically reducing inference time while obtaining more suitable diets. We further explore the repeated modules within networks and propose a more storage-efficient framework, DIETING, which utilizes a single layer of parameters to represent the entire network, achieving comparably excellent performance. The experiments across four state-of-the-art datasets and two widely used models demonstrate the superior accuracy in recommendation and efficiency in transmission and storage of our framework.",KDD
"Federated Graph Learning (FGL) aims to learn graph learning models over graph data distributed in multiple data owners, which has been applied in various applications such as social recommendation and financial fraud detection. Inherited from generic Federated Learning (FL), FGL similarly has the data heterogeneity issue where the label distribution may vary significantly for distributed graph data across clients. For instance, a client can have the majority of nodes from a class, while another client may have only a few nodes from the same class. This issue results in divergent local objectives and impairs FGL convergence for node-level tasks, especially for node classification. Moreover, FGL also encounters a unique challenge for the node classification task: the nodes from a minority class in a client are more likely to have biased neighboring information, which prevents FGL from learning expressive node embeddings with Graph Neural Networks (GNNs). To grapple with the challenge, we propose FedSpray, a novel FGL framework that learns local class-wise structure proxies in the latent space and aligns them to obtain global structure proxies in the server. Our goal is to obtain the aligned structure proxies that can serve as reliable, unbiased neighboring information for node classification. To achieve this, FedSpray trains a global feature-structure encoder and generates unbiased soft targets with structure proxies to regularize local training of GNN models in a personalized way. We conduct extensive experiments over four datasets, and experiment results validate the superiority of FedSpray compared with other baselines. Our code is available at https://github.com/xbfu/FedSpray.",KDD
"Causal discovery with observational and interventional data plays an important role in numerous fields. Due to the costly and potentially risky nature of intervention experiments, selecting informative interventions is critical in real-world situations. Several recent works introduce Bayesian active learning to select interventions that maximize the expected information gain about the underlying causal relationship at each optimization step. However, there are still some limitations within these methods: (1) Local optimality. With multiple intervention experiments, selecting optimal intervention myopically at each step may drop into the local optimal point. (2) Expensive time cost. Optimizing the most informative intervention at each step is time-consuming and not suitable for adaptive experiments with strict inference speed requirements. In this study, we propose a novel method called Reinforcement Learning-based Causal Bayesian Experimental Design (RL-CBED) to reduce the risk of local optimality and accelerate intervention selection inference. Specifically, we formulate the active causal discovery problem as a partially observable Markov decision process (POMDP). We design an information gain-based sparse reward function and then improve it to a dense reward function, providing fine-grained feedback to help the RL policy learn more quickly in complex environments. Moreover, we theoretically prove that the Q-function estimator can be learned using only trajectories sampled from the prior, which can significantly reduce the time cost of training process, enabling the real-world application of our method. Extensive experiments on both synthetic and real world-inspired semi-synthetic datasets demonstrate the effectiveness of our proposed method.",KDD
"The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Furthermore, to support the periodic re-condensation and expedite condensed graph updating in life-long graph learning, OpenGC reconstructs the sophisticated optimization scheme with kernel ridge regression and non-parametric graph convolution, significantly accelerating the condensation process while ensuring the exact solutions. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.",KDD
"Car-hailing services play an important role in the modern transportation system, and the utilities of the service providers highly depend on the efficiency of route planning algorithms. A widely adopted route planning framework is to assign weights to roads and compute the routes with the shortest path algorithms. Existing techniques of weight-assigning often focus on the traveling time and length of the roads, but cannot incorporate with the preferences of the passengers (users).
In this paper, a set of preference weight estimation models is employed to capture the users' preferences over paths in car-hailing with their historical choices. Since the user preferences may vary dynamically over time, it is a challenging task to make real-time decisions over the models. The main technical contribution of this paper is to propose an online learning-based preference weight chasing (PWC) algorithm to solve this problem. The worst-case performance of PWC is analyzed with the metric regret, and it is proved that PWC has a vanishing regret, which means that the time-averaged loss concerning the fixed in-hindsight best model tends to zero. Experiments based on real-world datasets are conducted to verify the effectiveness and efficiency of our algorithm. The code is available at https://github.com/GaoYucen/PWC.",KDD
"Evaluating anomaly detection algorithms in time series data is critical as inaccuracies can lead to flawed decision-making in various domains where real-time analytics and data-driven strategies are essential. Traditional performance metrics assume iid data and fail to capture the complex temporal dynamics and specific characteristics of time series anomalies, such as early and delayed detections. We introduce Proximity-Aware Time series anomaly Evaluation (PATE), a novel evaluation metric that incorporates the temporal relationship between prediction and anomaly intervals. PATE uses proximity-based weighting considering buffer zones around anomaly intervals, enabling a more detailed and informed assessment of a detection. Using these weights, PATE computes a weighted version of the area under the Precision and Recall curve. Our experiments with synthetic and real-world datasets show the superiority of PATE in providing more sensible and accurate evaluations than other evaluation metrics. We also tested several state-of-the-art anomaly detectors across various benchmark datasets using the PATE evaluation scheme. The results show that a common metric like Point-Adjusted F1 Score fails to characterize the detection performances well, and that PATE is able to provide a more fair model comparison. By introducing PATE, we redefine the understanding of model efficacy that steers future studies toward developing more effective and accurate detection models.",KDD
"Existing neural constructive solvers for routing problems have predominantly employed transformer architectures, conceptualizing the route construction as a set-to-sequence learning task. However, their efficacy has primarily been demonstrated on entirely random problem instances that inadequately capture real-world scenarios. In this paper, we introduce realistic Traveling Salesman Problem (TSP) scenarios relevant to industrial settings and derive the following insights: (1) The optimal next node (or city) to visit often lies within proximity to the current node, suggesting the potential benefits of biasing choices based on current locations. (2) Effectively solving the TSP requires robust tracking of unvisited nodes and warrants succinct grouping strategies. Building upon these insights, we propose integrating a learnable choice layer inspired by Hypernetworks to prioritize choices based on the current location, and a learnable approximate clustering algorithm inspired by the Expectation-Maximization algorithm to facilitate grouping the unvisited cities. Together, these two contributions form a hierarchical approach towards solving the realistic TSP by considering both immediate local neighbourhoods and learning an intermediate set of node representations. Our hierarchical approach yields superior performance compared to both classical and recent transformer models, showcasing the efficacy of the key designs.",KDD
"Mobile devices, especially smartphones, can support rich functions and have developed into indispensable tools in daily life. With the rise of generative AI services, smartphones can potentially transform into personalized assistants, anticipating user needs and scheduling services accordingly. Predicting user intents on smartphones, and reflecting anticipated activities based on past interactions and context, remains a pivotal step towards this vision. Existing research predominantly focuses on specific domains, neglecting the challenge of modeling diverse event sequences across dynamic contexts. Leveraging pre-trained language models (PLMs) offers a promising avenue, yet adapting PLMs to on-device user intent prediction presents significant challenges. To address these challenges, we propose PITuning, a Population-to-Individual Tuning framework. PITuning enhances common pattern extraction through dynamic event-to-intent transition modeling and addresses long-tailed preferences via adaptive unlearning strategies. Experimental results on real-world datasets demonstrate PITuning's superior intent prediction performance, highlighting its ability to capture long-tailed preferences and its practicality for on-device prediction scenarios.",KDD
"Graph neural networks have garnered notable attention for effectively processing graph-structured data. Prevalent models prioritize improving in-distribution (IND) data performance, frequently overlooking the risks from potential out-of-distribution (OOD) nodes during training and inference. In real-world graphs, the automated network construction can introduce noisy nodes from unknown distributions. Previous research into OOD node detection, typically referred to as entropy-based methods, calculates OOD measurements from the prediction entropy alongside category classification training. However, the nodes in the graph might not be pre-labeled with specific categories, rendering entropy-based OOD detectors inapplicable in such category-free situations. To tackle this issue, we propose an energy-centric density estimation framework for OOD node detection, referred to as EnergyDef. Within this framework, we introduce an energy-based GNN to compute node energies that act as indicators of node density and reveal the OOD uncertainty of nodes. Importantly, EnergyDef can efficiently identify OOD nodes with low-resource OOD node annotations, achieved by sampling hallucinated nodes via Langevin Dynamics and structure estimation, along with training through Contrastive Divergence. Our comprehensive experiments on real-world datasets substantiate that our framework markedly surpasses state-of-the-art methods in terms of detection quality, even under conditions of scarce or entirely absent OOD node annotations.",KDD
"Benefiting from the effective exploitation of the high-order correlations across multiple views, tensor-based multi-view clustering (TMVC) has garnered considerable attention in recent years. Nevertheless, prior TMVC techniques commonly involve assembling multiple view-specific spatial similarity graphs into a three-dimensional tensor, overlooking the intrinsic topological structure essential for precise clustering of data within a manifold. Additionally, mainstream techniques are constrained by equally shrinking all singular values to recover a low-rank tensor, limiting their capacity to distinguish significant variations among different singular values. In this investigation, we present an innovative TMVC framework termed toPology-driven multi-view clustering viA refined teNsorial sigmoiD rAnk minimization (PANDA ). Specifically, PANDA extracts view-specific topological structures from Euclidean graphs and intricately integrates them into a low-rank three-dimensional tensor, facilitating the concurrent utilization of intra-view topological connectivity and inter-view high-order correlations. Moreover, we develop a refined sigmoid function as the tighter surrogate to tensor rank, enabling the exploration of significant information of heterogeneous singular values. Meanwhile, the topological structures are merged into a unified structure with varying weights, associated with a connectivity constraint, empowering the significant divergence among views and the explicit cluster structure of the target graph are simultaneously leveraged. Extensive experiments demonstrate the superiority of PANDA, outperforming SOTA methods.",KDD
"Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two model-agnostic perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known GNN model architectures on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization. In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel GNN backbone model, DGat, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies. Our codes are available at https://github.com/KaiGuo20/DGAT **REMOVE 2nd URL**://github.com/KaiGuo20/DGAT.",KDD
"Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the long-standing cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a <u>C</u>onsistency and <u>D</u>iscrepancy-based graph contrastive learning method for tripartite graph-based <u>R</u>ecommendation (CDR). This approach leverages two novel meta-path-based metrics-consistency and discrepancy-to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks (GCN) layers under a multi-objective optimization framework, using the limit theory of GCN. Additionally, we introduce a novel Contrastive Divergence (CD) loss, which can seamlessly integrate the consistency and discrepancy metrics into the contrastive objective as the positive and contrastive supervision signals to learn node representations, enhancing the pairwise ranking of recommended objects and proving particularly valuable in severe cold-start scenarios. Extensive experiments demonstrate the effectiveness of the proposed CDR. The code is released at https://github.com/foodfaust/CDR.",KDD
"Rankings are increasingly used as part of human decision-making processes to most effectively allocate reviewing resources. Many of these processes have complex constraints, and we identify slot constraints as a model for a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. In this paper, we formalize the slot-constrained ranking problem as producing a ranking that maximizes the number of filled slots if candidates are evaluated by a human decision maker for slot eligibility in the order of the ranking. We show that naive adaptations of the Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, empirical evaluations show that MatchRank can provide substantial improvements over a range of synthetic and real-world tasks.",KDD
"Federated Graph Learning (FGL) has emerged as a promising way to learn high-quality representations from distributed graph data with privacy preservation. Despite considerable efforts have been made for FGL under either cross-device or cross-silo paradigm, how to effectively capture graph knowledge in a more complicated cross-silo cross-device environment remains an under-explored problem. However, this task is challenging because of the inherent hierarchy and heterogeneity of decentralized clients, diversified privacy constraints in different clients, and the cross-client graph integrity requirement. To this end, in this paper, we propose a Hierarchical Federated Graph Learning (HiFGL) framework for cross-silo cross-device FGL. Specifically, we devise a unified hierarchical architecture to safeguard federated GNN training on heterogeneous clients while ensuring graph integrity. Moreover, we propose a Secret Message Passing (SecMP) scheme to shield unauthorized access to subgraph-level and node-level sensitive information simultaneously. Theoretical analysis proves that HiFGL achieves multi-level privacy preservation with complexity guarantees. Extensive experiments on real-world datasets validate the superiority of the proposed framework against several baselines. Furthermore, HiFGL's versatile nature allows for its application in either solely cross-silo or cross-device settings, further broadening its utility in real-world FGL applications.",KDD
"For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation. Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space. In this work, we propose Binder, a novel approach for order-based representation. Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods. Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity. Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task. But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges. In particular, Binder achieves a whopping 70% higher F1-score than the second best method (98.6% vs 29%) in our largest dataset, WordNet Nouns (743,241 edges), when using only direct edges during training.",KDD
"Many evaluation metrics can be used to assess the performance of models in binary classification tasks. However, most of them are derived from a confusion matrix in a non-differentiable form, making it very difficult to generate a differentiable loss function that could directly optimize them. The lack of solutions to bridge this challenge not only hinders our ability to solve difficult tasks, such as imbalanced learning, but also requires the deployment of computationally expensive hyperparameter search processes in model selection. In this paper, we propose a general-purpose approach that transforms any confusion matrix-based metric into a loss function, AnyLoss, that is available in optimization processes. To this end, we use an approximation function to make a confusion matrix represented in a differentiable form, and this approach enables any confusion matrix-based metric to be directly used as a loss function. The mechanism of the approximation function is provided to ensure its operability and the differentiability of our loss functions is proved by suggesting their derivatives. We conduct extensive experiments under diverse neural networks with many datasets, and we demonstrate their general availability to target any confusion matrix-based metrics. Our method, especially, shows outstanding achievements in dealing with imbalanced datasets, and its competitive learning speed, compared to multiple baseline models, underscores its efficiency.",KDD
"Job recommender systems are crucial for aligning job opportunities with job-seekers in online job-seeking. However, users tend to adjust their job preferences to secure employment opportunities continually, which limits the performance of job recommendations. The inherent frequency of preference drift poses a challenge to promptly and precisely capture user preferences. To address this issue, we propose a novel session-based framework, BISTRO, to timely model user preference through fusion learning of semantic and behavioral information. Specifically, BISTRO is composed of three stages: 1) coarse-grained semantic clustering, 2) fine-grained job preference extraction, and 3) personalized top-k job recommendation. Initially, BISTRO segments the user interaction sequence into sessions and leverages session-based semantic clustering to achieve broad identification of person-job matching. Subsequently, we design a hypergraph wavelet learning method to capture the nuanced job preference drift. To mitigate the effect of noise in interactions caused by frequent preference drift, we innovatively propose an adaptive wavelet filtering technique to remove noisy interaction. Finally, a recurrent neural network is utilized to analyze session-based interaction for inferring personalized preferences. Extensive experiments on three real-world offline recruitment datasets demonstrate the significant performances of our framework. Significantly, BISTRO also excels in online experiments, affirming its effectiveness in live recruitment settings. This dual success underscores the robustness and adaptability of BISTRO. The source code is available at https://github.com/Applied-Machine-Learning-Lab/BISTRO.",KDD
"Expander decompositions of graphs have significantly advanced the understanding of many classical graph problems and led to numerous fundamental theoretical results. However, their adoption in practice has been hindered due to their inherent intricacies and large hidden factors in their asymptotic running times. Here, we introduce the first practically efficient algorithm for computing expander decompositions and their hierarchies and demonstrate its effectiveness and utility by incorporating it as the core component in a novel solver for the normalized cut graph clustering objective.
Our extensive experiments on a variety of large graphs show that our expander-based algorithm outperforms state-of-the-art solvers for normalized cut with respect to solution quality by a large margin on a variety of graph classes such as citation, e-mail, and social networks or web graphs while remaining competitive in running time.",KDD
"In many complex systems, the interactions between objects span multiple aspects. Multiplex networks are accurate paradigms to model such systems, where each edge is associated with a type. A key graph mining primitive is extracting dense subgraphs, and this has led to interesting notions such as k-cores, known as building blocks of complex networks. Despite recent attempts to extend the notion of core to multiplex networks, existing studies suffer from a subset of the following limitations: They (1) force all nodes to exhibit their high degree in the same set of relation types while in multiplex networks some connection types can be noisy for some nodes, (2) either require high computational cost or miss the complex information of multiplex networks, and (3) assume the same importance for all relation types. We introduce Score, a novel and unifying family of dense structures in multiplex networks that uses a function S(.) to summarize the degree vector of each node. We then discuss how one can choose a proper S(.) from the data. To demonstrate the usefulness of Scores, we focus on finding the densest subgraph as well as modeling user engagement in multiplex networks. We present a new density measure in multiplex networks and discuss its advantages over existing density measures. We show that the problem of finding the densest subgraph in multiplex networks is NP-hard and design an efficient approximation algorithm based on Scores. Finally, we present a new mathematical model of user engagement in the presence of different relation types. Our experiments shows the efficiency and effectiveness of our algorithms and supports the proposed mathematical model of user engagement.",KDD
"The Guaranteed Delivery (GD) advertising is a crucial component of the online advertising industry, and the allocation of inventory in GD advertising is an important procedure that influences directly the ability of the publisher to fulfill the requirements and increase its revenues. Nowadays, as the requirements of advertisers become more and more diverse and fine-grained, the focus ratio requirement, which states that the portion of allocated impressions of a designated contract on focus media among all possible media should be greater than another contract, often appears in business scenarios. However, taking these requirements into account brings hardness for the GD advertising inventory allocation as the focus ratio requirements involve non-convex multilinear constraints. Existing methods which rely on the convex properties are not suitable for processing this problem, while mathematical programming or constraint-based heuristic solvers are unable to produce high-quality solutions within the time limit. Therefore, we propose a local search framework to address this challenge. It incorporates four new operators designed for handling multilinear constraints and a two-mode algorithmic architecture. Experimental results demonstrate that our algorithm is able to compute high-quality allocations with better business metrics compared to the state-of-the-art mathematical programming or constraint based heuristic solvers. Moreover, our algorithm is able to handle the general multilinear constraints and we hope it could be used to solve other problems in GD advertising with similar requirements.",KDD
"Despite the encouraging successes in numerous applications, machine learning methods grounded on the i.i.d. assumption often experience performance deterioration when confronted with the distribution shift between training and test data. This challenge has instigated recent research endeavors focusing on out-of-distribution (OOD) generalization. A particularly pervasive and intricate OOD problem is to enhance the model's generalization ability by training it on samples drawn from a single environment. In response to the problem, we propose a simple model-agnostic method tailored for a practical OOD scenario in this paper. Our approach centers on pursuing robust weighted empirical risks, utilizing randomly shifted training distributions derived through a specific sample-based weighting strategy. Furthermore, we theoretically establish that the expected risk of the shifted training distribution can bound the expected risk of the test distribution. This theoretical foundation ensures the improved prediction performance of our method when employed in uncertain test distributions. Extensive experiments conducted on diverse real-world datasets affirm the effectiveness of our method, highlighting its potential to address the distribution shifts in machine learning applications.",KDD
"As its availability and generality in online services, implicit feedback is more commonly used in recommender systems. However, implicit feedback usually presents noisy samples in real-world recommendation scenarios (such as misclicks or non-preferential behaviors), which will affect precise user preference learning. To overcome the noisy samples problem, a popular solution is based on dropping noisy samples in the model training phase, which follows the observation that noisy samples have higher training losses than clean samples. Despite the effectiveness, we argue that this solution still has limits. (1) High training losses can result from model optimization instability or hard samples, not just noisy samples. (2) Completely dropping of noisy samples will aggravate the data sparsity, which lacks full data exploitation.
To tackle the above limitations, we propose a Double Correction Framework for Denoising Recommendation (DCF), which contains two correction components from views of more precise sample dropping and avoiding more sparse data. In the sample dropping correction component, we use the loss value of the samples over time to determine whether it is noise or not, increasing dropping stability. Instead of averaging directly, we use the damping function to reduce the bias effect of outliers. Furthermore, due to the higher variance exhibited by hard samples, we derive a lower bound for the loss through concentration inequality to identify and reuse hard samples. In progressive label correction, we iteratively re-label highly deterministic noisy samples and retrain them to further improve performance. Finally, extensive experimental results on three datasets and four backbones demonstrate the effectiveness and generalization of our proposed framework.",KDD
"We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from K arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current approaches for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, Ã¸mega-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has sublinear instance-dependent regret in general and logarithmic regret for parameter Ï â‰¥ 1, and that it outperforms existing policies consistently in synthetic and real settings.",KDD
"Placement is a critical and challenging step of modern chip design, with routability being an essential indicator of placement quality. Current routability-oriented placers typically apply an iterative two-stage approach, wherein the first stage generates a placement solution, and the second stage provides non-differentiable routing results to heuristically improve the solution quality. This method hinders jointly optimizing the routability aspect during placement. To address this problem, this work introduces RoutePlacer, an end-to-end routability-aware placement method. It trains RouteGNN, a customized graph neural network, to efficiently and accurately predict routability by capturing and fusing geometric and topological representations of placements. Well-trained RouteGNN then serves as a differentiable approximation of routability, enabling end-to-end gradient-based routability optimization. In addition, RouteGNN can improve two-stage placers as a plug-and-play alternative to external routers. Our experiments on DREAMPlace, an open-source AI4EDA platform, show that RoutePlacer can reduce Total Overflow by up to 16% while maintaining routed wirelength, compared to the state-of-the-art; integrating RouteGNN within two-stage placers leads to a 44% reduction in Total Overflow without compromising wirelength.",KDD
"Although Federated Learning (FL) enables global model training across clients without compromising their raw data, due to the unevenly distributed data among clients, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance. Specifically, different data distributions among clients lead to various optimization directions of local models. Aggregating local models usually results in a low-generalized global model, which performs worse on most of the clients. To address the above issue, inspired by the observation from a geometric perspective that a well-generalized solution is located in a flat area rather than a sharp area, we propose a novel and heuristic FL paradigm named FedMR (Federated Model Recombination). The goal of FedMR is to guide the recombined models to be trained towards a flat area. Unlike conventional FedAvg-based methods, in FedMR, the cloud server recombines collected local models by shuffling each layer of them to generate multiple recombined models for local training on clients rather than an aggregated global model. Since the area of the flat area is larger than the sharp area, when local models are located in different areas, recombined models have a higher probability of locating in a flat area. When all recombined models are located in the same flat area, they are optimized towards the same direction. We theoretically analyze the convergence of model recombination. Experimental results show that, compared with state-of-the-art FL methods, FedMR can significantly improve the inference accuracy without exposing the privacy of each client.",KDD
"In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG). Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (GDBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data which can be adaptively trained with LLMs. The usage of neural embedding storage and Complex neural logical Query Answering (CQA) provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the domain-specific or private databases. Malicious attackers can infer more sensitive information in the database using well-designed queries such as from the answer sets of where Turing Award winners born before 1950 and after 1940 lived, the living places of Turing Award winner Hinton are probably exposed, although the living places may have been deleted in the training stage due to the privacy concerns. In this work, we propose a privacy-preserved neural graph database (P-NGDB) framework to alleviate the risks of privacy leakage in NGDBs. We introduce adversarial training techniques in the training stage to enforce the NGDBs to generate indistinguishable answers when queried with private information, enhancing the difficulty of inferring sensitive information through combinations of multiple innocuous queries. Extensive experimental results on three datasets show that our framework can effectively protect private information in the graph database while delivering high-quality public answers responses to queries. The code is available at https://github.com/HKUST-KnowComp/PrivateNGDB.",KDD
"The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.",KDD
"Graph neural networks (GNNs) have demonstrated remarkable success in numerous graph analytical tasks. Yet, their effectiveness is often compromised in real-world scenarios due to distribution shifts, limiting their capacity for knowledge transfer across changing environments or domains. Recently, Unsupervised Graph Domain Adaptation (UGDA) has been introduced to resolve this issue. UGDA aims to facilitate knowledge transfer from a labeled source graph to an unlabeled target graph. Current UGDA efforts primarily focus on model-centric methods, such as employing domain invariant learning strategies and designing model architectures. However, our critical examination reveals the limitations inherent to these model-centric methods, while a data-centric method allowed to modify the source graph provably demonstrates considerable potential. This insight motivates us to explore UGDA from a data-centric perspective. By revisiting the theoretical generalization bound for UGDA, we identify two data-centric principles for UGDA: alignment principle and rescaling principle. Guided by these principles, we propose GraphAlign, a novel UGDA method that generates a small yet transferable graph. By exclusively training a GNN on this new graph with classic Empirical Risk Minimization (ERM), GraphAlign attains exceptional performance on the target graph. Extensive experiments under various transfer scenarios demonstrate the GraphAlign outperforms the best baselines by an average of 2.16%, training on the generated graph as small as 0.25~1% of the original training graph.",KDD
"Unsupervised Outlier Detection (UOD) is an important data mining task. With the advance of deep learning, deep Outlier Detection (OD) has received broad interest. Most deep UOD models are trained exclusively on clean datasets to learn the distribution of the normal data, which requires huge manual efforts to clean the real-world data if possible. Instead of relying on clean datasets, some approaches directly train and detect on unlabeled contaminated datasets, leading to the need for methods that are robust to such challenging conditions. Ensemble methods emerged as a superior solution to enhance model robustness against contaminated training sets. However, the training time is greatly increased by the ensemble mechanism.
In this study, we investigate the impact of outliers on training, aiming to halt training on unlabeled contaminated datasets before performance degradation. Initially, we noted that blending normal and anomalous data causes AUC fluctuations-a label-dependent measure of detection accuracy. To circumvent the need for labels, we propose a zero-label entropy metric named Loss Entropy for loss distribution, enabling us to infer optimal stopping points for training without labels. Meanwhile, a negative correlation between entropy metric and the label-based AUC score is demonstrated by theoretical proofs. Based on this, an automated early-stopping algorithm called EntropyStop is designed to halt training when loss entropy suggests the maximum model detection capability. We conduct extensive experiments on ADBench (including 47 real datasets), and the overall results indicate that AutoEncoder (AE) enhanced by our approach not only achieves better performance than ensemble AEs but also requires under 2% of training time. Lastly, loss entropy and EntropyStop are evaluated on other deep OD models, exhibiting their broad potential applicability.",KDD
"We study the problem of robust data augmentation for regression tasks in the presence of noisy data. Data augmentation is essential for generalizing deep learning models, but most of the techniques like the popular Mixup are primarily designed for classification tasks on image data. Recently, there are also Mixup techniques that are specialized to regression tasks like C-Mixup. In comparison to Mixup, which takes linear interpolations of pairs of samples, C-Mixup is more selective in which samples to mix based on their label distances for better regression performance. However, C-Mixup does not distinguish noisy versus clean samples, which can be problematic when mixing and lead to suboptimal model performance. At the same time, robust training has been heavily studied where the goal is to train accurate models against noisy data through multiple rounds of model training. We thus propose our data augmentation strategy RC-Mixup, which tightly integrates C-Mixup with multi-round robust training methods for a synergistic effect. In particular, C-Mixup improves robust training in identifying clean data, while robust training provides cleaner data to C-Mixup for it to perform better. A key advantage of RC-Mixup is that it is data-centric where the robust model training algorithm itself does not need to be modified, but can simply benefit from data mixing. We show in our experiments that RC-Mixup significantly outperforms C-Mixup and robust training baselines on noisy data benchmarks and can be integrated with various robust training methods.",KDD
"Gradient Boosting is a leading learning method that builds ensembles and adapts their sizes to particular tasks, consistently delivering top-tier results across various applications. However, determining the optimal number of models in the ensemble remains a critical yet underexplored aspect. Traditional approaches assume a universal ensemble size effective for all data points, which may not always hold true due to data heterogeneity.
This paper introduces an adaptive approach to early stopping in Gradient Boosting, addressing data heterogeneity by assigning different stop moments to different data regions at inference time while still training a common ensemble on the entire dataset. We propose two methods: Direct Supervised Partition (DSP) and Indirect Supervised Partition (ISP). The DSP method uses a decision tree to partition the data based on learning curves, while ISP leverages the dataset's geometric and target distribution characteristics.
An effective validation protocol is developed to determine the optimal number of early stopping regions or detect when the heterogeneity assumption does not hold. Experiments using state-of-the-art implementations of Gradient Boosting, LightGBM, and CatBoost, on standard benchmarks demonstrate that our methods enhance model precision by up to 2%, underscoring the significance of this research direction. This approach does not increase computational complexity and can be easily integrated into existing learning pipelines.",KDD
"The Gradient Boosting machine learning ensemble algorithm, well-known for its proficiency and superior performance in intricate machine learning tasks, has encountered limited success in the realm of uplift modeling. Uplift modeling is a challenging task that necessitates a known target for the precise computation of the training gradient. The prevailing two-model strategies, which separately model treatment and control outcomes, are encumbered with limitations as they fail to directly tackle the uplift problem.
This paper presents an innovative approach to uplift modeling that employs Gradient Boosting. Unlike previous works, our algorithm utilizes multioutput boosting model and calculates the uplift gradient based on intermediate surrogate predictions and directly models the concealed target. This method circumvents the requirement for a known target and addresses the uplift problem more effectively than existing solutions.
Moreover, we broaden the scope of this solution to encompass multitreatment settings, thereby enhancing its applicability. This novel approach not only overcomes the limitations of the traditional two-model strategies but also paves the way for more effective and efficient uplift modeling using Gradient Boosting.",KDD
"Over the past two decades, time series motif discovery has become a crucial subroutine for many time series data mining tasks; concurrently, it has been established that Dynamic Time Warping (DTW) outperforms other similarity measures like Euclidean Distance in most scenarios. Against this backdrop, a DTW motif discovery algorithm was recently developed; however, it is confined to working with fixed-length subsequences. In this work, we propose a novel approach that allows us to find motifs under both length differences and warping. Our algorithm exploits a promising time series representation called Spikelets and introduces the first lower bound for DTW in the Spikelet space. Extensive empirical studies demonstrate that our method scales effectively across various real-world datasets and efficiently identifies DTW motif pairs of different lengths.",KDD
"We investigate the problem of finding winner(s) given a large number of users' (voters') preferences casted as ballots, one from each of the m users, where each ballot is a ranked order of preference of up to â„“ out of n items (candidates). Given a group protected attribute with k different values and a priority that imposes a selection order among these groups, the goal is to satisfy the priority order and select a winner per group that is most representative. It is imperative that at times the original users' preferences may require further manipulation to meet these fairness and priority requirement. We consider manipulation by modifications and formalize the margin finding problem under modification problem. We study the suitability of Instant Run-off Voting (IRV) as a preference aggregation method and demonstrate its advantages over positional methods. We present a suite of technical results on the hardness of the problem, design algorithms with theoretical guarantees and further investigate efficiency opportunities. We present exhaustive experimental evaluations using multiple applications and large-scale datasets to demonstrate the effectiveness of IRV, and efficacy of our designed solutions qualitatively and scalability-wise.",KDD
"How can we leverage inherent frequency features of stock signals for effective portfolio optimization? Portfolio optimization in the domain of finance revolves around strategically allocating assets to maximize returns. Recent advancements highlight the efficacy of deep learning and reinforcement learning (RL) in capturing temporal asset patterns for portfolio optimization. However, previous methodologies focusing on time-domain often fail to detect sudden market shifts and abrupt events because their models are overly tailored to prevalent patterns, resulting in significant losses.
In this paper, we propose FreQuant (Adaptive Portfolio Optimization via Multi-<u>Fre</u>quency <u>Quant</u>itative Analysis), an effective deep RL framework for portfolio optimization that fully operates in the frequency domain, tackling the limitations of time domain-focused models. By bringing the analysis into the frequency domain with the Discrete Fourier Transform, our framework captures both prominent and subtle market frequencies, enhancing its adaptability and stability in response to market shifts. This approach allows FreQuant to adeptly identify primary asset patterns while also effectively responding to less common and abrupt market events, providing a more accurate and comprehensive asset representation. Empirical validation on diverse real-world trading datasets underscores the remarkable performance of FreQuant, showing its superiority in terms of profitability. Notably, FreQuant achieves up to 2.1x higher Annualized Rate of Return and 2.9x higher Portfolio Value than the best-performing competitors.",KDD
"Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-n recommendation for many years.
Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.",KDD
"Time series forecasting has been an essential field in many different application areas, including economic analysis, meteorology, and so forth. The majority of time series forecasting models are trained using the mean squared error (MSE). However, this training based on MSE causes a limitation known as prediction delay. The prediction delay, which implies the ground-truth precedes the prediction, can cause serious problems in a variety of fields, e.g., finance and weather forecasting --- as a matter of fact, predictions succeeding ground-truth observations are not practically meaningful although their MSEs can be low. This paper proposes a new perspective on traditional time series forecasting tasks and introduces a new solution to mitigate the prediction delay. We introduce a continuous-time gated recurrent unit (GRU) based on the neural ordinary differential equation (NODE) which can supervise explicit time-derivatives. We generalize the GRU architecture in a continuous-time manner and minimize the prediction delay through our time-derivative regularization. Our method outperforms in metrics such as MSE, Dynamic Time Warping (DTW) and Time Distortion Index (TDI). In addition, we demonstrate the low prediction delay of our method in a variety of datasets.",KDD
"The Unaligned Multi-view Clustering (UMC) problem is currently receiving widespread attention, focusing on clustering unaligned multi-view data generated in real-world applications. Although some algorithms have emerged to address this issue, there still exist the following drawbacks: 1) The fully unknown correspondence of samples across views can significantly limit the exploration of consistent clustering structure. 2) The fixed representation space makes it difficult to mine the comprehensive information in the original data. 3) Unbiased tensor rank approximation is desired to capture the high-order correlation among different views. To address these issues, we proposed a novel UMC framework termed Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning (TUMCR). Specifically, TUMCR designs a multi-scale representation learning and alignment framework, which constructs multi-scale representation spaces to comprehensively explore the unknown correspondence across views. Then, a tensorial multi-scale fusion module is proposed to fuse multi-scale representations and explore the high-order correlation hidden in different views, which utilizes the Enhanced Tensor Rank (ETR) to learn the low-rank structure. Furthermore, TUMCR is solved by an efficient algorithm with good convergence. Extensive experiments on different types of datasets demonstrate the effectiveness and superiority of our TUMCR compared with state-of-the-art methods. Our code is publicly available at: https://github.com/jijintian/TUMCR.",KDD
"Dynamic graph learning has attracted much attention in recent years due to the fact that most of the real-world graphs are dynamic and evolutionary. As a result, many dynamic learning methods have been proposed to cope with the changes of node states over time. Among these studies, a critical issue is how to update the representations of nodes when new temporal events are observed. In this paper, we provide a novel memory structure - Memory Map (MemMap) for this problem. MemMap is an adaptive and evolutionary latent memory space, where each cell corresponds to an evolving ""topic"" of the dynamic graph. Moreover, the representation of a node is generated from its semantically correlated memory cells, rather than linked neighbors of the node. We have conducted experiments on real-world datasets and compared our method with the SOTA ones. It can be concluded that: 1) By constructing an adaptive and evolving memory structure during the dynamic learning process, our method can capture the dynamic graph changes, and the learned MemMap is actually a compact evolving structure organized according to the latent ""topics"" of the graph nodes. 2) Our research suggests that it is a more effective and efficient way to generate node representations from a latent semantic space (like MemMap in our method) than from directly connected neighbors (like most of the previous graph learning methods). The reason is that the number of memory cells in latent space could be much smaller than the number of nodes in a real-world graph, and the representation learning process could well balance the global and local message passing by leveraging the semantic similarity of graph nodes via the correlated memory cells.",KDD
"This paper studies the semi-supervised partial label learning (SSPLL) problem, which aims to improve the partial label learning (PLL) by leveraging unlabeled samples. Both the existing SSPLL methods and the semi-supervised learning methods exploit the information in unlabeled samples by selecting high-confidence unlabeled samples as the pseudo labels based on the maximum value of the model output. However, the scarcity of labeled samples and the ambiguity from partial labels skew this strategy towards an unfair selection of high-confidence samples on each class, most notably during the initial phases of training, resulting in slower training and performance degradation. In this paper, we propose a novel method FairMatch, which adopts a learning state aware self-adaptive threshold for selecting the same number of high-confidence samples on each class, and uses augmentation consistency to incorporate the unlabeled samples to promote PLL. In addition, we adopt the candidate label disambiguation to utilize the partial labeled samples and mix up the partial labeled samples and the selected high-confidence unlabeled samples to prevent the model from overfitting on partial label samples. FairMatch can achieve maximum accuracy improvements of 9.53%, 4.9%, and 16.45% on CIFAR-10, CIFAR-100, and CIFAR-100H, respectively. The codes can be found at https://github.com/jhjiangSEU/FairMatch.",KDD
"Sleep stage classification has important clinical significance for the diagnosis of sleep-related diseases. To pursue more accurate sleep stage classification, multi-channel sleep signals are widely used due to the rich spatial-temporal information contained. However, it leads to a great increment in the size and computational costs, which constrain the application of multi-channel sleep models on hardware devices. Knowledge distillation is an effective way to compress models, yet existing knowledge distillation methods cannot fully extract and transfer the spatial-temporal knowledge in the multi-channel sleep signals. To solve the problem, we propose a general knowledge distillation framework for multi-channel sleep stage classification called spatial-temporal mutual distillation. Based on the spatial relationship of human body and the temporal transition rules of sleep signals, the spatial and temporal modules are designed to extract the spatial-temporal knowledge, thus help the lightweight student model learn the rich spatial-temporal knowledge from large-scale teacher model. The mutual distillation framework transfers the spatial-temporal knowledge mutually. Teacher model and student model can learn from each other, further improving the student model. The results on the ISRUC-III and MASS-SS3 datasets show that our proposed framework compresses the sleep models effectively with minimal performance loss and achieves the state-of-the-art performance compared to the baseline methods.",KDD
"Multi-task learning (MTL), which aims to make full use of knowledge contained in multiple tasks to enhance overall performance and efficiency, has been broadly applied in recommendations. The main challenge for MTL models is negative transfer. Existing MTL models, mainly built on the Mixture-of-Experts (MoE) structure, seek enhancements in performance through feature selection and specific expert sharing mode design. However, one expert sharing mode may not be universally applicable due to the complex correlations and diverse demands among various tasks. Additionally, homogeneous expert architectures in such models further limit their performance. To address these issues, in this paper, we propose an innovative automatic MTL framework, AutoMTL, leveraging neural architecture search (NAS) to design optimal expert architectures and sharing modes. The Dual-level Expert Sharing mode and Architecture Navigator (DESAN) search space of AutoMTL can not only efficiently explore expert sharing modes and feature selection schemes but also focus on the architectures of expert subnetworks. Along with this, we introduce an efficient Progressively Discretizing Differentiable Architecture Search (PD-DARTS) algorithm for search space exploration. Extensive experiments demonstrate that AutoMTL can consistently outperform state-of-the-art, human-crafted MTL models. Moreover, the insights obtained from the discovered architectures provide valuable guidance for building new multi-task recommendation models.",KDD
"In recent years, Graph Neural Networks (GNNs) and Large Language Models (LLMs) have exhibited remarkable capability in addressing different graph learning and natural language tasks, respectively. Motivated by this, integrating LLMs with GNNs has been increasingly studied to acquire transferable knowledge across modalities, which leads to improved empirical performance in language and graph domains. However, existing studies mainly focused on a single-domain scenario by designing complicated integration techniques to manage multimodal data effectively. Therefore, a concise and generic learning framework for multi-domain tasks, i.e., graph and language domains, is highly desired yet remains under-exploited due to two major challenges. First, the language corpus of downstream tasks differs significantly from graph data, making it hard to bridge the knowledge gap between modalities. Second, not all knowledge demonstrates immediate benefits for downstream tasks, potentially introducing disruptive noise to context-sensitive models like LLMs. To tackle these challenges, we propose a novel plug-and-play framework for incorporating a lightweight cross-domain prompting method into both language and graph learning tasks. Specifically, we first convert the textual input into a domain-scalable prompt, which not only preserves the semantic and logical contents of the textual input, but also highlights related graph information as external knowledge for different domains. Then, we develop a reinforcement learning-based method to learn the optimal edge selection strategy for useful knowledge extraction, which profoundly sharpens the multi-domain model capabilities. In addition, we introduce a joint multi-view optimization module to regularize agent-level collaborative learning across two domains. Finally, extensive empirical justifications over 23 public and synthetic datasets demonstrate that our approach can be applied to diverse multi-domain tasks more accurately, robustly, and reasonably, and improve the performances of the state-of-the-art graph and language models in different learning paradigms.",KDD
"Sybil attacks are a prevalent concern within the realm of crowdsourcing, underscoring the significance of quality control in this domain. Truth discovery has been extensively studied to deduce the most trustworthy information from conflicting data based on the principle that reliable workers yield reliable answers. However, existing truth discovery approaches overlook the metric of workers' reputations, e.g., workers' historical approval rates on crowdsourcing platforms, despite being inflated and noisy, they offer a rough indication of workers' ability. In this paper, we first refine the approval rate using Wilson Lower Bound to enhance its confidence, and then mitigate its noise and inflation through a method based on ranking similarity. Specifically, we propose a method called RCTD (Reputation-Constrained Truth Discovery), which introduces a similarity metric between the rankings of workers' weights and the refined approval rates. This metric serves as a penalizing factor in the objective function of the truth discovery, restricting workers' weights to avoid excessively deviating from their historical reputation during the weight estimation process. We solve the objective function by introducing the block coordinate descent coupled with heuristics approach method. Experimental results on real-world datasets demonstrate that our approach achieves more accurate inference of true results in the Sybil attack environment compared to the state-of-the-art methods.",KDD
"Continual learning closely emulates human learning, which allows a model to learn from a stream of tasks sequentially without forgetting previously learned knowledge. Replay-based continual learning methods mitigate forgetting and improve performance by reintroducing data belonging to old tasks, however a replay method's performance may deteriorate when the reintroduced data does not effectively represent all experienced data. To address this concern, we propose the Sketch-based Replay Projection (SRP) method to capture and retain the original data stream's distribution within stored memory. SRP augments existing replay frameworks and introduces a two-fold approach. First, we develop a sketch-based sample selection technique to approximate feature distributions within distinct tasks, thereby capturing a wide distribution of examples for subsequent replay. Second, we propose a data compression method which projects examples into a reduced-dimensional space while preserving inter-example relationships and emphasizing inter-class disparities, encouraging diverse representations of each class while maintaining memory requirements similar to existing replay methodologies. Our experimental results demonstrate that SRP enhances replay diversity and improves the performance of existing replay models.",KDD
"Univariate decision trees, commonly used since the 1950s, predict by asking questions about a single feature in each decision node. While they are interpretable, they often lack competitive predictive accuracy due to their inability to model feature correlations. Multivariate (oblique) trees use multiple features in each node, capturing high-dimensional correlations better, but sometimes they can be difficult to interpret. We advocate for a model that strikes a useful middle ground: bivariate decision trees, which use two features in each node. This typically produces trees that not only are more accurate than univariate trees, but much smaller, which offsets the small increase in node complexity and keeps them interpretable. They also help data mining by constructing new features that are useful for discrimination, and by providing a form of supervised, hierarchical 2D visualization that reveals patterns such as clusters or linear structure. We give two new algorithms to learn bivariate trees: a fast one based on CART; and a slower one based on alternating optimization with a feature regularization term, which produces the best trees while still scaling to large datasets.",KDD
"Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. Our method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet's superior performance in both inference time and accuracy. Code available at: https://github.com/aminK8/Masked-LoGoNet.",KDD
"Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches. In this paper, we propose Gandalf, a novel approach which makes use of a label co-occurrence graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. Surprisingly, models trained on these new training instances, although being less than half of the original dataset, can outperform models trained on the original dataset, particularly on the PSP@k metric for tail labels. With this insight, we aim to train existing XMC algorithms on both, the original and new training instances, leading to an average 5% relative improvements for 6 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3M labels. Gandalf can be applied in a plug-and-play manner to various methods and thus forwards the state-of-the-art in the domain, without incurring any additional computational overheads. Code has been open-sourced at www.github.com/xmc-aalto/InceptionXML.",KDD
"In multivariate time series (MTS) classification, finding the important features (e.g., sensors) for model performance is crucial yet challenging due to the complex, high-dimensional nature of MTS data, intricate temporal dynamics, and the necessity for domain-specific interpretations. Current explanation methods for MTS mostly focus on time-centric explanations, apt for pinpointing important time periods but less effective in identifying key features. This limitation underscores the pressing need for a feature-centric approach, a vital yet often overlooked perspective that complements time-centric analysis. To bridge this gap, our study introduces a novel feature-centric explanation and evaluation framework for MTS, named CAFO (Channel Attention and Feature Orthgonalization). CAFO employs a convolution-based approach with channel attention mechanisms, incorporating a depth-wise separable channel attention module (DepCA) and a QR decomposition-based loss for promoting feature-wise orthogonality. We demonstrate that this orthogonalization enhances the separability of attention distributions, thereby refining and stabilizing the ranking of feature importance. This improvement in feature-wise ranking enhances our understanding of feature explainability in MTS. Furthermore, we develop metrics to evaluate global and class-specific feature importance. Our framework's efficacy is validated through extensive empirical analyses on two major public benchmarks and real-world datasets, both synthetic and self-collected, specifically designed to highlight class-wise discriminative features. The results confirm CAFO's robustness and informative capacity in assessing feature importance in MTS classification tasks. This study not only advances the understanding of feature-centric explanations in MTS but also sets a foundation for future explorations in feature-centric explanations. The codes are available at https://github.com/eai-lab/CAFO.",KDD
"Given an irregular tensor from a newly emerging domain, how can we quickly and accurately capture its patterns utilizing existing irregular tensors in multiple domains? The problem is of great importance for various tasks such as finding patterns of a new disease using pre-existing diseases data. This is challenging as new target tensors have limited information due to their recent emergence. Thus, carefully utilizing the existing source tensors for analyzing the target tensor is helpful. PARAFAC2 decomposition is a strong tool for finding the patterns of irregular tensors, and the patterns are used in many applications such as missing value prediction and anomaly detection. However, previous PARAFAC2-based works cannot adaptably handle newly emerging target tensors utilizing the source tensors.
In this work, we propose Meta-P2, a fast and accurate domain adaptation method for irregular tensor decomposition. Meta-P2 generates a meta factor matrix from the multiple source domains, by domain adaptation and meta-update steps. Meta-P2 quickly and accurately finds the patterns of the new irregular tensor utilizing the meta factor matrix. Extensive experiments on real-world datasets show that Meta-P2 achieves the best performance in various downstream tasks including missing value prediction and anomaly detection tasks.",KDD
"Collaborative filtering recommender systems (CF-RecSys) have shown successive results in enhancing the user experience on social media and e-commerce platforms. However, as CF-RecSys struggles under cold scenarios with sparse user-item interactions, recent strategies have focused on leveraging modality information of user/items (e.g., text or images) based on pre-trained modality encoders and Large Language Models (LLMs). Despite their effectiveness under cold scenarios, we observe that they underperform simple traditional collaborative filtering models under warm scenarios due to the lack of collaborative knowledge. In this work, we propose an efficient All-round LLM-based Recommender system, called A-LLMRec, that excels not only in the cold scenario but also in the warm scenario. Our main idea is to enable an LLM to directly leverage the collaborative knowledge contained in a pre-trained state-of-the-art CF-RecSys so that the emergent ability of the LLM as well as the high-quality user/item embeddings that are already trained by the state-of-the-art CF-RecSys can be jointly exploited. This approach yields two advantages: (1) model-agnostic, allowing for integration with various existing CF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typically required for LLM-based recommenders. Our extensive experiments on various real-world datasets demonstrate the superiority of A-LLMRec in various scenarios, including cold/warm, few-shot, cold user, and cross-domain scenarios. Beyond the recommendation task, we also show the potential of A-LLMRec in generating natural language outputs based on the understanding of the collaborative knowledge by performing a favorite genre prediction task. Our code is available at https://github.com/ghdtjr/A-LLMRec.",KDD
"Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.",KDD
"Zero-Shot Story Visualization (ZSV) seeks to depict textual narratives through a sequence of images without relying on pre-existing text-image pairs for training. In this paper, we address the challenge of automated multi-character ZSV, aiming to create distinctive yet compatible character portraits for high-quality story visualization without the need of manual human interventions. Our study is motivated by the limitation of current ZSV approaches that necessitate inefficient manual collection of external images as initial character portraits and suffer from low-quality story visualization, especially with multi-character interactions, when the portraits are not well initiated. To overcome these issues, we develop LeMon, an LLM enhanced Multi-Character Zero-Shot Visualization framework that automates character portrait initialization and supports iterative portrait refinement by exploring the semantic content of the story. In particular, we design an LLM-based portrait generation strategy that matches the story characters with external movie characters, and leverage the matched resources as in-context learning (ICL) samples for LLMs to accurately initialize the character portraits. We then propose a graph-based Text2Image diffusion model that constructs a character interaction graph from the story to iteratively refine the character portraits by maximizing the distinctness of different characters while minimizing their incompatibility in the multi-character story visualization. Our evaluation results show that LeMon outperforms existing ZSV approaches in generating high-quality visualizations for stories across various types with multiple interacted characters. Our code is available at https://github.com/arxrean/LLM-LeMon.",KDD
"Prior attacks on graph neural networks have focused on graph poisoning and evasion, neglecting the network's weights and biases. For convolutional neural networks, however, the risk arising from bit flip attacks is well recognized. We show that the direct application of a traditional bit flip attack to graph neural networks is of limited effectivity. Hence, we discuss the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and impairing the expressivity of the Weisfeiler-Leman test. We find that exploiting mathematical properties specific to certain graph neural networks significantly increases their vulnerability to bit flip attacks. The Injectivity Bit Flip Attack can degrade the maximal expressive Graph Isomorphism Networks trained on graph property prediction datasets to random output by flipping only a small fraction of the network's bits, demonstrating its higher destructive power compared to traditional bit flip attacks transferred from convolutional neural networks. Our attack is transparent, motivated by theoretical insights and confirmed by extensive empirical results.",KDD
"One of the most well-known and simplest models for diversity maximization is the Max-Min Diversification (MMD) model, which has been extensively studied in the data mining and database literature. In this paper, we initiate the study of the Asymmetric Max-Min Diversification (AMMD) problem. The input is a positive integer k and a complete digraph over n vertices, together with a nonnegative distance function over the edges obeying the directed triangle inequality. The objective is to select a set of k vertices, which maximizes the smallest pairwise distance between them. AMMD reduces to the well-studied MMD problem in case the distances are symmetric, and has natural applications to query result diversification, web search, and facility location problems. Although the MMD problem admits a simple 1/2-approximation by greedily selecting the next-furthest point, this strategy fails for AMMD and it remained unclear how to design good approximation algorithms for AMMD.
We propose a combinatorial 1/(6k)-approximation algorithm for AMMD by leveraging connections with the Maximum Antichain problem. We discuss several ways of speeding up the algorithm and compare its performance against heuristic baselines on real-life and synthetic datasets.",KDD
"An irregular tensor is a collection of matrices with different numbers of rows. Real-world data from diverse domains, including medical and stock data, are effectively represented as irregular tensors due to the inherent variations in data length. For their analysis, various tensor decomposition methods (e.g., PARAFAC2) have been devised. While they are expected to be effective in compressing large-scale irregular tensors, akin to regular tensor decomposition methods, our analysis reveals that their compression performance is limited due to the larger number of first mode factor matrices.
In this work, we propose accurate and compact decomposition methods for lossy compression of irregular tensors. First, we propose Light-IT, which unifies all first mode factor matrices into a single matrix, dramatically reducing the size of compressed outputs. Second, motivated by the success of Tucker decomposition in regular tensor compression, we extend Light-IT to Light-IT++ to enhance its expressive power and thus reduce compression error. Finally, we generalize both methods to handle irregular tensors of any order and leverage the sparsity of tensors for acceleration.
Extensive experiments on 6 real-world datasets demonstrate that our methods are (a) Compact: their compressed output is up to 37Ã— smaller than that of the most concise baseline, (b) Accurate: our methods are up to 5Ã— more accurate, with smaller compressed output, than the most accurate baseline, and (c) Versatile: our methods are effective for sparse, dense, and higher-order tensors.",KDD
"In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields. The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors. However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc. Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs.
Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs. Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification. The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs. Empirically, \algo considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes.",KDD
"Imputation of Correlated Time Series (CTS) is essential in data preprocessing for many tasks, particularly when sensor data is often incomplete. Deep learning has enabled sophisticated models that improve CTS imputation by capturing temporal and spatial patterns. However, deep models often incur considerable consumption of computational resources and thus cannot be deployed in resource-limited settings. This paper presents ReCTSi (Resource-efficient CTS imputation), a method that adopts a new architecture for decoupled pattern learning in two phases: (1) the Persistent Pattern Extraction phase utilizes a multi-view learnable codebook mechanism to identify and archive persistent patterns common across different time series, enabling rapid pattern retrieval during inference. (2) the Transient Pattern Adaptation phase introduces completeness-aware attention modules that allocate attention to the complete and hence more reliable data segments. Extensive experimental results show that ReCTSi achieves state-of-the-art imputation accuracy while consuming much fewer computational resources than the leading existing model, consuming only 0.004% of the FLOPs for inference compared to its closest competitor. The blend of high accuracy and very low resource consumption makes ReCTSi the currently best method for resource-limited scenarios. The related code is available at https://github.com/ryanlaics/RECTSI.",KDD
"Multivariate time series classification (MTSC) has attracted significant research attention due to its diverse real-world applications. Recently, exploiting transformers for MTSC has achieved state-of-the-art performance. However, existing methods focus on generic features, providing a comprehensive understanding of data, but they ignore class-specific features crucial for learning the representative characteristics of each class. This leads to poor performance in the case of imbalanced datasets or datasets with similar overall patterns but differing in minor class-specific details. In this paper, we propose a novel Shapelet Transformer (ShapeFormer), which comprises class-specific and generic transformer modules to capture both of these features. In the class-specific module, we introduce the discovery method to extract the discriminative subsequences of each class (i.e. shapelets) from the training set. We then propose a Shapelet Filter to learn the difference features between these shapelets and the input time series. We found that the difference feature for each shapelet contains important class-specific features, as it shows a significant distinction between its class and others. In the generic module, convolution filters are used to extract generic features that contain information to distinguish among all classes. For each module, we employ the transformer encoder to capture the correlation between their features. As a result, the combination of two transformer modules allows our model to exploit the power of both types of features, thereby enhancing the classification performance. Our experiments on 30 UEA MTSC datasets demonstrate that ShapeFormer has achieved the highest accuracy ranking compared to state-of-the-art methods. The code is available at https://github.com/xuanmay2701/shapeformer.",KDD
"Knowledge distillation (KD) has emerged as a promising technique for addressing the computational challenges associated with deploying large-scale recommender systems. KD transfers the knowledge of a massive teacher system to a compact student model, to reduce the huge computational burdens for inference while retaining high accuracy. The existing KD studies primarily focus on one-time distillation in static environments, leaving a substantial gap in their applicability to real-world scenarios dealing with continuously incoming users, items, and their interactions. In this work, we delve into a systematic approach to operating the teacher-student KD in a non-stationary data stream. Our goal is to enable efficient deployment through a compact student, which preserves the high performance of the massive teacher, while effectively adapting to continuously incoming data. We propose <u>C</u>ontinual <u>C</u>ollaborative <u>D</u>istillation (CCD) framework, where both the teacher and the student continually and collaboratively evolve along the data stream. CCD facilitates the student in effectively adapting to new data, while also enabling the teacher to fully leverage accumulated knowledge. We validate the effectiveness of CCD through extensive quantitative, ablative, and exploratory experiments on two real-world datasets. We expect this research direction to contribute to narrowing the gap between existing KD studies and practical applications, thereby enhancing the applicability of KD in real-world systems.",KDD
"To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels.
In this paper, we propose SLADE (<u>S</u>elf-supervised <u>L</u>earning for <u>A</u>nomaly <u>D</u>etection in <u>E</u>dge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision. Our code and datasets are available at https://github.com/jhsk777/SLADE.",KDD
"Sharpness-aware minimization (SAM) is known to improve the generalization performance of neural networks. However, it is not widely used in real-world applications yet due to its expensive model perturbation cost. A few variants of SAM have been proposed to tackle such an issue, but they commonly do not alleviate the cost noticeably. In this paper, we propose a lightweight layer-wise gradient norm penalizing method that tackles the expensive computational cost of SAM while maintaining its superior generalization performance. Our study empirically proves that the gradient norm of the whole model can be effectively suppressed by penalizing the gradient norm of only a few critical layers. We also theoretically show that such a partial model perturbation does not harm the convergence rate of SAM, allowing them to be safely adapted in real-world applications. To demonstrate the efficacy of the proposed method, we perform extensive experiments comparing the proposed method to mini-batch SGD and the conventional SAM using representative computer vision and language modeling benchmarks.",KDD
"Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.",KDD
"Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data pooled from various task combinations, which is computationally intensive. We present a new algorithm Grad-TAG that can estimate task affinities without this repeated training.
The key idea of Grad-TAG is to train a ""base"" model for all tasks and then use a linearization technique to estimate the loss of any other model with a specific task combination. The linearization works by computing a gradient-based first-order approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression trained to predict labels for the specific task combination. We show theoretically that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity matrix, we design a semi-definite program for clustering to group similar tasks that maximize the average density of clusters.
We evaluate Grad-TAG's performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our results show that our task affinity estimates are within 2.7% distance of the true affinities while needing only 3% of FLOPs compared to full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers an estimate accurate to within 5% of the true affinities, while using only 112.3 GPU hours. Our results show that Grad-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches.",KDD
"Persistent homology, a fundamental technique within Topological Data Analysis (TDA), captures structural and shape characteristics of graphs, yet encounters computational difficulties when applied to dynamic directed graphs. This paper introduces the Dynamic Neural Dowker Network (DNDN), a novel framework specifically designed to approximate the results of dynamic Dowker filtration, aiming to capture the high-order topological features of dynamic directed graphs. Our approach creatively uses line graph transformations to produce both source and sink line graphs, highlighting the shared neighbor structures that Dowker complexes focus on. The DNDN incorporates a Source-Sink Line Graph Neural Network (SSLGNN) layer to effectively capture the neighborhood relationships among dynamic edges. Additionally, we introduce an innovative duality edge fusion mechanism, ensuring that the results for both the sink and source line graphs adhere to the duality principle intrinsic to Dowker complexes. Our approach is validated through comprehensive experiments on real-world datasets, demonstrating DNDN's capability not only to effectively approximate dynamic Dowker filtration results but also to perform exceptionally in dynamic graph classification tasks.",KDD
"Online advertising platforms leverage a two-stage auction architecture to deliver personalized ads to users with low latency. The first stage efficiently selects a small subset of promising candidates out of the complete pool of ads. In the second stage, an auction is conducted within the subset to determine the winning ad for display, using click-through-rate predictions from the second-stage machine learning model. In this work, we investigate the online learning process of the first-stage subset selection policy, while ensuring game-theoretic properties in repeated two-stage ad auctions. Specifically, we model the problem as designing a combinatorial bandit mechanism with a general reward function, as well as additional requirements of truthfulness and individual rationality (IR). We establish an O(T) regret lower bound for truthful bandit mechanisms, which demonstrates the challenge of simultaneously achieving allocation efficiency and truthfulness. To circumvent this impossibility result, we introduce truthful Î±-approximation oracles and evaluate the bandit mechanism through Î±-approximation regret. Two mechanisms are proposed, both of which are ex-post truthful and ex-post IR. The first mechanism is an explore-then-commit mechanism with regret O(T2/3 ), and the second mechanism achieves an improved O(log T /Î”Î¦2) regret where Î”Î¦ is a distribution-dependent gap, but requires additional assumptions on the oracles and information about the strategic bidders.",KDD
"Ratings of a user to most items in recommender systems are usually missing not at random (MNAR), largely because users are free to choose which items to rate. To achieve unbiased learning of the prediction model under MNAR data, three typical solutions have been proposed, including error-imputation-based (EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods. However, these methods ignore an alternative form of bias caused by the inconsistency between the observed ratings and the users' true preferences, also known as noisy feedback or outcome measurement errors (OME), e.g., due to public opinion or low-quality data collection process. In this work, we study intersectional threats to the unbiased learning of the prediction model from data MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and OME-DR estimators, which largely extend the existing estimators to combat OME in real-world recommendation scenarios. Next, we theoretically prove the unbiasedness and generalization bound of the proposed estimators. We further propose an alternate denoising training approach to achieve unbiased learning of the prediction model under MNAR data with OME. Extensive experiments are conducted on three real-world datasets and one semi-synthetic dataset to show the effectiveness of our proposed approaches. The code is available at https://github.com/haoxuanli-pku/KDD24-OME-DR.",KDD
"Urban mobility undergoes a profound decline in the aftermath of a disaster, subsequently exhibiting a complex recovery trajectory. Effectively capturing and predicting this dynamic recovery process holds paramount importance for devising more efficient post-disaster recovery strategies, such as resource allocation to areas with protracted recovery periods. Existing models for post-disaster mobility recovery predominantly employ basic mathematical methods, which are strongly based on simplifying assumptions, and their limited parameters restrict their capacity to fully capture the mobility recovery patterns. In response to this gap, we introduce the Coupled Dynamic Graph ODE Network (CDGON) to model the intricate dynamics of post-disaster mobility recovery. Our model seamlessly integrates existing physical knowledge pertaining to post-disaster mobility recovery and incorporates the nuanced interactions between intra-regional and inter-regional population flows. Extensive experimental results demonstrate the efficiency of our model in capturing the dynamic recovery patterns of urban population mobility in post-disaster scenarios, surpassing the capabilities of current dynamic graph prediction models.",KDD
"Multi-view clustering method based on anchor graph has been widely concerned due to its high efficiency and effectiveness. In order to avoid post-processing, most of the existing anchor graph-based methods learn bipartite graphs with connected components. However, such methods have high requirements on parameters, and in some cases it may not be possible to obtain bipartite graphs with clear connected components. To end this, we propose a label learning method based on tensor projection (LLMTP). Specifically, we project anchor graph into the label space through an orthogonal projection matrix to obtain cluster labels directly. Considering that the spatial structure information of multi-view data may be ignored to a certain extent when projected in different views separately, we extend the matrix projection transformation to tensor projection, so that the spatial structure information between views can be fully utilized. In addition, we introduce the tensor Schatten p-norm regularization to make the clustering label matrices of different views as consistent as possible. Extensive experiments have proved the effectiveness of the proposed method.",KDD
"Inductive relation reasoning in knowledge graphs aims at predicting missing triplets involving unseen entities and/or unseen relations. While subgraph-based methods that reason about the local structure surrounding a candidate triplet have shown promise, they often fall short in accurately modeling the causal dependence between a triplet's subgraph and its ground-truth label. This limitation typically results in a susceptibility to spurious correlations caused by confounders, adversely affecting generalization capabilities. Herein, we introduce a novel front-door adjustment-based approach designed to learn the causal relationship between subgraphs and their ground-truth labels, specifically for inductive relation prediction. We conceptualize the semantic information of subgraphs as a mediator and employ a graph data augmentation mechanism to create augmented subgraphs. Furthermore, we integrate a fusion module and a decoder within the front-door adjustment framework, enabling the estimation of the mediator's combination with augmented subgraphs. We also introduce the reparameterization trick in the fusion model to enhance model robustness. Extensive experiments on widely recognized benchmark datasets demonstrate the proposed method's superiority in inductive relation prediction, particularly for tasks involving unseen entities and unseen relations. Additionally, the subgraphs reconstructed by our decoder offer valuable insights into the model's decision-making process, enhancing transparency and interpretability.",KDD
"Educational recommendation seeks to suggest knowledge concepts that match a learner's ability, thus facilitating a personalized learning experience. In recent years, reinforcement learning (RL) methods have achieved considerable results by taking the encoding of the learner's exercise log as the state and employing an RL-based agent to make suitable recommendations. However, these approaches suffer from handling the diverse and dynamic learner's knowledge states. In this paper, we introduce the privileged feature distillation technique and propose the P rivileged K nowledge S tate D istillation (PKSD ) framework, allowing the RL agent to leverage the ""actual'' knowledge state as privileged information in the state encoding to help tailor recommendations to meet individual needs. Concretely, our PKSD takes the privileged knowledge states together with the representations of the exercise log for the state representations during training. And through distillation, we transfer the ability to adapt to learners to aknowledge state adapter. During inference, theknowledge state adapter would serve as the estimated privileged knowledge states instead of the real one since it is not accessible. Considering that there are strong connections among the knowledge concepts in education, we further propose to collaborate the graph structure learning for concepts into our PKSD framework. This new approach is termed GEPKSD (Graph-Enhanced PKSD). As our method is model-agnostic, we evaluate PKSD and GEPKSD by integrating them with five different RL bases on four public simulators, respectively. Our results verify that PKSD can consistently improve the recommendation performance with various RL methods, and our GEPKSD could further enhance the effectiveness of PKSD in all the simulations.",KDD
"In this paper, we address the challenges of data augmentation in Multi-Modal Knowledge Graphs (MMKGs), a relatively under-explored area. We propose a novel diffusion-based generative model, the Simple Denoising Probabilistic Latent Diffusion Model (SimDiff). SimDiff is capable of handling different data modalities including the graph topology in a unified manner by the same diffusion model in the latent space. It enhances the utilization of multi-modal data and encourage the multi-modal fusion and reduces the dependency on limited training data. We validate our method in downstream Entity Alignment (EA) tasks in MMKGs, demonstrating that even when using only half of the seed entities in training, our methods can still achieve superior performance. This work contributes to the field by providing a new data generation or augmentation method for MMKGs, potentially paving the way for more effective use of MMKGs in various applications. Code is made available at https://github.com/ranlislz/SimDiff.",KDD
"Trajectory prediction of moving traffic agents is crucial for the safety of autonomous vehicles, whereas previous approaches usually rely on sufficiently long-observed trajectory (e.g., 2 seconds) to predict the future trajectory of the agents. However, in many real-world scenarios, it is not realistic to collect adequate observed locations for moving agents, leading to the collapse of most prediction models. For instance, when a moving car suddenly appears and is very close to an autonomous vehicle because of the obstruction, it is quite necessary for the autonomous vehicle to quickly and accurately predict the future trajectories of the car with limited observed trajectory locations. In light of this, we focus on investigating the task of instantaneous trajectory prediction, i.e., two observed locations are available during inference. To this end, we put forward a general and plug-and-play instantaneous trajectory prediction approach, called ITPNet. Specifically, we propose a backward forecasting mechanism to reversely predict the latent feature representations of unobserved historical trajectories of the agent based on its two observed locations and then leverage them as complementary information for future trajectory prediction. Meanwhile, due to the inevitable existence of noise and redundancy in the predicted latent feature representations, we further devise a Noise Redundancy Reduction Former (NRRFormer) module, which aims to filter out noise and redundancy from unobserved trajectories and integrate the filtered features and observed features into a compact query representation for future trajectory predictions. In essence, ITPNet can be naturally compatible with existing trajectory prediction models, enabling them to gracefully handle the case of instantaneous trajectory prediction. Extensive experiments on the Argoverse and nuScenes datasets demonstrate ITPNet outperforms the baselines by a large margin and shows its efficacy with different trajectory prediction models.",KDD
"Learning complex network dynamics is fundamental for understanding, modeling, and controlling real-world complex systems. Though great efforts have been made to predict the future states of nodes on networks, the capability of capturing long-term dynamics remains largely limited. This is because they overlook the fact that long-term dynamics in complex network are predominantly governed by their inherent low-dimensional manifolds, i.e., skeletons. Therefore, we propose the <u>D</u>ynamics-<u>I</u>nvariant <u>Sk</u>eleton Neural <u>Net</u>work (DiskNet), which identifies skeletons of complex networks based on the renormalization group structure in hyperbolic space to preserve both topological and dynamics properties. Specifically, we first condense complex networks with various dynamics into simple skeletons through physics-informed hyperbolic embeddings. Further, we design graph neural ordinary differential equations to capture the condensed dynamics on the skeletons. Finally, we recover the skeleton networks and dynamics to the original ones using a degree-based super-resolution module. Extensive experiments across three representative dynamics as well as five real-world and two synthetic networks demonstrate the superior performances of the proposed DiskNet, which outperforms the state-of-the-art baselines by an average of 10.18% in terms of long-term prediction accuracy. Code for reproduction is available at: https://github.com/tsinghua-fib-lab/DiskNet.",KDD
"The advancements in disentangled representation learning significantly enhance the accuracy of counterfactual predictions by granting precise control over instrumental variables, confounders, and adjustable variables. An appealing method for achieving the independent separation of these factors is mutual information minimization, a task that presents challenges in numerous machine learning scenarios, especially within high-dimensional spaces. To circumvent this challenge, we propose the Self-Distilled Disentanglement framework, referred to as SD2. Grounded in information theory, it ensures theoretically sound independent disentangled representations without intricate mutual information estimator designs for high-dimensional representations. Our comprehensive experiments, conducted on both synthetic and real-world datasets, confirms the effectiveness of our approach in facilitating counterfactual inference in the presence of both observed and unobserved confounders.",KDD
"In today's fast-paced world, advertisers are increasingly demanding real-time and accurate personalized ad delivery based on dynamic preference modeling, which emphasizes the temporality existing in both user preference and product characteristics. Meanwhile, with the development of graph neural networks (GNNs), E-commerce knowledge graphs (KG) with rich semantic relatedness are invoked to improve accuracy and provide appropriate explanations to encourage advertisers' willingness to invest in ad expenses. However, it is still challenging for existing methods to comprehensively consider both time-series interactions and graph-structured knowledge triples in a unified model, i.e., the case in knowledge-aware dynamic advertising. The interaction graph between users and products changes rapidly over time, while the knowledge in KG remains relatively stable. This results in an uneven distribution of temporal and semantic information, causing existing GNNs to fail in this scenario. In this work, we quantitatively define the above phenomenon as temporal unevenness and introduce the Incremental Leveling Network (InLN) with three novel techniques: the periodic-focusing window for node-level dynamic modeling, the biased temporal walk for subgraph-level dynamic modeling and the incremental leveling mechanism for KG updating. Verified by comprehensive and intensive experiments, InLN outperforms nine baseline models in three tasks by substantial margins, reaching up to a 9.9% improvement and averaging a 5.7% increase.",KDD
"Contemporary systems of Guaranteed Delivery (GD) advertising work with two different stages, namely, the offline selling stage and the online serving stage. The former deals with contract allocation, and the latter fulfills the impression allocation of signed contracts. Existing work usually handles these two stages separately. For example, contracts are formulated offline without concerning practical situations in the online serving stage. Therefore, we address in this paper a bi-objective contract allocation for GD advertising, which maximizes the impressions, i.e., Ad resource assignments, allocated for the new incoming advertising orders, and at the same time, controls the balance in the inventories. Since the proposed problem is high dimensional and heavily constrained, we design an efficient local search that focuses on the two objectives alternatively. The experimental results indicate that our algorithm outperforms multi-objective evolutionary algorithms and Gurobi, the former of which is commonly applied for multi-objective optimization and the latter of which is a well-known competitive commercial tool.",KDD
"Recent studies successfully learned static graph embeddings that are structurally fair by preventing the effectiveness disparity of high- and low-degree vertex groups in downstream graph mining tasks. However, achieving structure fairness in dynamic graph embedding remains an open problem. Neglecting degree changes in dynamic graphs will significantly impair embedding effectiveness without notably improving structure fairness. This is because the embedding performance of high-degree and low-to-high-degree vertices will significantly drop close to the generally poorer embedding performance of most slightly changed vertices in the long-tail part of the power-law distribution. We first identify biased structural evolutions in a dynamic graph based on the evolving trend of vertex degree and then propose FairDGE, the first structurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biased structural evolutions by jointly embedding the connection changes among vertices and the long-short-term evolutionary trend of vertex degrees. Furthermore, a novel dual debiasing approach is devised to encode fair embeddings contrastively, customizing debiasing strategies for different biased structural evolutions. This innovative debiasing strategy breaks the effectiveness bottleneck of embeddings without notable fairness loss. Extensive experiments demonstrate that FairDGE achieves simultaneous improvement in the effectiveness and fairness of embeddings.",KDD
"Hyperbolic neural networks (HNNs) are emerging as a promising tool for representing data embedded in non-Euclidean geometries, yet their adoption has been hindered by challenges related to stability and robustness. In this work, we conduct a rigorous Lipschitz analysis for HNNs and propose using Lipschitz regularization as a novel strategy to enhance their robustness. Our comprehensive investigation spans both the PoincarÃ© ball model and the hyperboloid model, establishing Lipschitz bounds for HNN layers. Importantly, our analysis provides detailed insights into the behavior of the Lipschitz bounds as they relate to feature norms, particularly distinguishing between scenarios where features have unit norms and those with large norms. Further, we study regularization using the derived Lipschitz bounds. Our empirical validations demonstrate consistent improvements in HNN robustness against noisy perturbations.",KDD
"With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to <u>Zero</u>-shot transferability in <u>G</u>raphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models.",KDD
"Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, <u>Fair</u> <u>G</u>raph Neural Network via re-<u>B</u>alancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB.",KDD
"CAD assembly modeling, which refers to using CAD software to design new products from a catalog of existing machine components, is important in the industrial field. The graph neural network (GNN) based recommender system for CAD assembly modeling can help designers make decisions and speed up the design process by recommending the next required component based on the existing components in CAD software. These components can be represented as a graph naturally. However, present recommender systems for CAD assembly modeling adopt fixed GNN architectures, which may be sub-optimal for different manufacturers with different data distribution. Therefore, to customize a well-suited recommender system for different manufacturers, we propose a novel neural architecture search (NAS) framework, dubbed CusGNN, which can design data-specific GNN automatically. Specifically, we design a search space from three dimensions (i.e., aggregation, fusion, and readout functions), which contains a wide variety of GNN architectures. Then, we develop an effective differentiable search algorithm to search high-performing GNN from the search space. Experimental results show that the customized GNNs achieve 1.5-5.1% higher top-10 accuracy compared to previous manual designed methods, demonstrating the superiority of the proposed approach. Code and data are available at https://github.com/BUPT-GAMMA/CusGNN.",KDD
"Image similarity has been extensively studied in computer vision. In recent years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling semantic similarity, assigning a numerical score to a pair of images is impractical, making the improvement and comparisons on the task difficult. In this work, we present a more intuitive approach to build and compare image similarity models based on labelled data in the form of A:R vs B:R, i.e., determining if an image A is closer to a reference image R than another image B. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the models that were directly fine-tuned using mixed imagery data as well as existing deep embeddings, e.g., CLIP [30] and DINO [3]. This work demonstrates that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.",KDD
"Last year has witnessed the re-flourishment of tag-aware recommender systems supported by the LLM-enriched tags. Unfortunately, though large efforts have been made, current solutions may fail to describe the diversity and uncertainty inherent in user preferences with only tag-driven profiles. Recently, with the development of geometry-based techniques, e.g., box embeddings, the diversity of user preferences now could be fully modeled as the range within a box in high dimension space. However, defect still exists as these approaches are incapable of capturing high-order neighbor signals, i.e., semantic-rich multi-hop relations within the user-tag-item tripartite graph, which severely limits the effectiveness of user modeling. To deal with this challenge, in this paper, we propose a novel framework, called BoxGNN, to perform message aggregation via combinations of logical operations, thereby incorporating high-order signals. Specifically, we first embed users, items, and tags as hyper-boxes rather than simple points in the representation space, and define two logical operations, i.e., union and intersection, to facilitate the subsequent process. Next, we perform the message aggregation mechanism via the combination of logical operations, to obtain the corresponding high-order box representations. Finally, we adopt a volume-based learning objective with Gumbel smoothing techniques to refine the representation of boxes. Extensive experiments on two publicly available datasets and one LLM-enhanced e-commerce dataset have validated the superiority of BoxGNN compared with various state-of-the-art baselines. The code is released online: https://github.com/critical88/BoxGNN.",KDD
"Recently, integrated warehouse and distribution logistics systems are widely used in E-commerce industries to adjust to constantly changing customer demands. It makes the prediction of purchase demand and delivery supply capacity a crucial problem to streamline operations and improve efficiency. The interaction between such demand and supply not only relies on their economic relationships but also on consumer psychology caused by daily events, such as epidemics, promotions, and festivals. Although existing studies have made great efforts in the joint prediction of demand and supply considering modeling the demand-supply interactions, they seldom refer to the impacts of diverse events. In this work, we propose MulSTE, a Multi-view Spatio-Temporal learning framework with heterogeneous Event fusion. Firstly, an Event Fusion Representation (EFR) module is designed to fuse the textual, numerical, and categorical heterogeneous information for emergent and periodic events. Secondly, a Multi-graph Adaptive Convolution Recurrent Network (MGACRN) is developed as the spatio-temporal encoder (ST-Encoder) to capture the evolutional features of demand, supply, and events. Thirdly, the Event Gated Demand-Supply Interaction Attention (EGIA) module is designed to model the demand-supply interactions during events. The evaluations are conducted on two real-world datasets collected from JD Logistics and public websites. The experimental results show that our method outperforms state-of-the-art baselines in various metrics.",KDD
"Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs (i.e., motifs), instead of the lower-order edges, as the atomic clustering unit, which has been recognized as the state-of-the-art solution in ground truth community detection and knowledge discovery. Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability. However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning. However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question. (2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions. Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes. To overcome these dilemmas, we propose a <u>P</u>rovable and <u>S</u>calable <u>M</u>otif <u>C</u>onductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif. Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally. Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies. Finally, it outputs the locally optimal result during the above iterative process. To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs. Empirical results on real-life and synthetic demonstrate that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the state-of-the art baselines.",KDD
"In online advertising, existing auto-bidding strategies for bid shading mainly adopt the approach of first predicting the winning price distribution and then calculating the optimal bid. However, the winning price information available to the Demand Side Platforms (DSPs) is extremely limited, and the associated uncertainties make it challenging for DSPs to accurately estimate winning price distribution. To address this challenge, we conducted a comprehensive analysis of the process by which DSPs obtain winning price information, and abstracted two types of uncertainties from it: known uncertainty and unknown uncertainty. Based on these uncertainties, we proposed two levels of robust bidding strategies: Robust Bidding for Censorship (RBC) and Robust Bidding for Distribution Shift (RBDS), which offer guarantees for the surplus in the worst-case scenarios under uncertain conditions. Experimental results on public datasets demonstrate that our robust bidding strategies consistently enable DSPs to achieve superior surpluses, both on test sets and under worst-case conditions.",KDD
"Harnessing Large Language Models (LLMs) for recommendation is rapidly emerging, which relies on two fundamental steps to bridge the recommendation item space and the language space: 1) item indexing utilizes identifiers to represent items in the language space, and 2) generation grounding associates LLMs' generated token sequences to in-corpus items. However, previous methods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g., numeric IDs) and description-based identifiers (e.g., titles) either lose semantics or lack adequate distinctiveness. Moreover, prior generation grounding methods might generate invalid identifiers, thus misaligning with in-corpus items. To address these issues, we propose a novel Transition paradigm for LLM-based Recommender (named TransRec) to bridge items and language. Specifically, TransRec presents multi-facet identifiers, which simultaneously incorporate ID, title, and attribute for item indexing to pursue both distinctiveness and semantics. Additionally, we introduce a specialized data structure for TransRec to ensure generating valid identifiers only and utilize substring indexing to encourage LLMs to generate from any position of identifiers. Lastly, TransRec presents an aggregated grounding module to leverage generated multi-facet identifiers to rank in-corpus items efficiently. We instantiate TransRec on two backbone models, BART-large and LLaMA-7B.",KDD
"The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on clients with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we term as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as FedAvg but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs, thereby stimulating further advancements and research in this area.",KDD
"We introduce a method for inferring an explicit PDE from a data sample generated by previously unseen dynamics, based on a learned context. The training phase integrates knowledge of the form of the equation with a differential scheme, while the inference phase yields a PDE that fits the data sample and enables both signal prediction and data explanation. We include results of extensive experimentation, comparing our method to SOTA approaches, together with ablation studies that examine different flavors of our solution.",KDD
"Clustered Federated Learning (CFL) is an emerging paradigm to extract insights from data on IoT devices. Through iterative client clustering and model aggregation, CFL adeptly manages data heterogeneity, ensures privacy, and delivers personalized models to heterogeneous devices. Traditional CFL approaches, which operate synchronously, suffer from prolonged latency for waiting slow devices during clustering and aggregation. This paper advocates a shift to asynchronous CFL, allowing the server to process client updates as they arrive. This shift enhances training efficiency yet introduces complexities to the iterative training cycle. To this end, we present CASA, a novel CFL scheme for Clustering-Aggregation Synergy under Asynchrony. Built upon a holistic theoretical understanding of asynchrony's impact on CFL, CASA adopts a bi-level asynchronous aggregation method and a buffer-aided dynamic clustering strategy to harmonize between clustering and aggregation. Extensive evaluations on standard benchmarks show that CASA outperforms representative baselines in model accuracy and achieves 2.28-6.49Ã— higher convergence speed.",KDD
"We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models ~2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines[20]. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.",KDD
"Predicting the resilience of complex networks, which represents the ability to retain fundamental functionality amidst external perturbations or internal failures, plays a critical role in understanding and improving real-world complex systems. Traditional theoretical approaches grounded in nonlinear dynamical systems rely on prior knowledge of network dynamics. On the other hand, data-driven approaches frequently encounter the challenge of insufficient labeled data, a predicament commonly observed in real-world scenarios. In this paper, we introduce a novel resilience prediction framework for complex networks, designed to tackle this issue through generative data augmentation of network topology and dynamics. The core idea is the strategic utilization of the inherent joint distribution present in unlabeled network data, facilitating the learning process of the resilience predictor by illuminating the relationship between network topology and dynamics. Experiment results on three network datasets demonstrate that our proposed framework TDNetGen can achieve high prediction accuracy up to 85%-95%. Furthermore, the framework still demonstrates a pronounced augmentation capability in extreme low-data regimes, thereby underscoring its utility and robustness in enhancing the prediction of network resilience. We have open-sourced our code in the following link, https://github.com/tsinghua-fib-lab/TDNetGen.",KDD
"Thebiharmonic distance (BD) is a fundamental metric that measures the distance of two nodes in a graph. It has found applications in network coherence, machine learning, and computational graphics, among others. In spite of BD's importance, efficient algorithms for the exact computation or approximation of this metric on large graphs remain notably absent. In this work, we provide several algorithms to estimate BD, building on a novel formulation of this metric. These algorithms enjoy locality property (that is, they only read a small portion of the input graph) and at the same time possess provable performance guarantees. In particular, our main algorithms approximate the BD between any node pair with an arbitrarily small additive error Îµ in time O(1/Îµ2 poly(log n/Îµ)). Furthermore, we perform an extensive empirical study on several benchmark networks, validating the performance and accuracy of our algorithms.",KDD
"Vehicle routing problems (VRP) are very important in many real-world applications and has been studied for several decades. Recently, neural combinatorial optimization (NCO) has attracted growing research effort. NCO is to train a neural network model to solve an optimization problem in question. However, existing NCO methods often build a different model for each routing problem, which significantly hinders their application in some areas where there are many different VRP variants to solve. In this work, we make a first attempt to tackle the crucial challenge of cross-problem generalization in NCO. We formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization manner. In our experiments, the neural model is trained on five VRP variants and its performance is tested on eleven VRP variants. The experimental results show that the model demonstrates superior performance on these eleven VRP variants, reducing the average gap to around 5% from over 20% and achieving a notable performance boost on both benchmark datasets and real-world logistics scenarios.",KDD
"The goal of semi-supervised multi-label learning (SSMLL) is to improve model performance by leveraging the information of unlabeled data. Recent studies usually adopt the pseudo-labeling strategy to tackle unlabeled data based on the assumption that labeled and unlabeled data share the same distribution. However, in realistic scenarios, unlabeled examples are often collected through cost-effective methods, inevitably introducing out-of-distribution (OOD) data, leading to a significant decline in model performance. In this paper, we propose a safe semi-supervised multi-label learning framework based on the theory of evidential deep learning (EDL), with the goal of achieving robust and effective unlabeled data exploitation. On one hand, we propose the asymmetric beta loss to not only compensate for the lack of robustness in common MLL losses, but also to solve the inherent positive-negative imbalance problem faced by the EDL losses in MLL. On the other hand, to construct a robust SSMLL framework, we adopt a dual-head structure to generate class probabilities and instance uncertainties. The former are used to generate pseudo-labels, while the latter are utilized to filter OOD examples. To avoid the need for threshold estimation, we develop a dual-measurement weighted loss function to safely perform unlabeled training. Extensive experiments on multiple benchmark datasets verify the effectiveness of the proposed method in both OOD detection and SSMLL tasks.",KDD
"The Maximum Minimal Cut Problem (MMCP), a NP-hard combinatorial optimization (CO) problem, has not received much attention due to the demanding and challenging bi-connectivity constraint. Moreover, as a CO problem, it is also a daunting task for machine learning, especially without labeled instances. To deal with these problems, this work proposes an unsupervised learning framework combined with heuristics for MMCP that can provide valid and high-quality solutions. As far as we know, this is the first work that explores machine learning and heuristics to solve MMCP. The unsupervised solver is inspired by a relaxation-plus-rounding approach, the relaxed solution is parameterized by graph neural networks, and the cost and penalty of MMCP are explicitly written out, which can train the model end-to-end. A crucial observation is that each solution corresponds to at least one spanning tree. Based on this finding, a heuristic solver that implements tree transformations by adding vertices is utilized to repair and improve the solution quality of the unsupervised solver. Alternatively, the graph is simplified while guaranteeing solution consistency, which reduces the running time. We conduct extensive experiments to evaluate our framework and give a specific application. The results demonstrate the superiority of our method against two techniques designed.",KDD
"Complex event recognition (CER) refers to identifying specific patterns composed of several primitive events in event stores. Since full-scanning event stores to identify primitive events holding query constraint conditions will incur costly I/O overhead, a mainstream and practical approach is using index techniques to obtain these events. However, prior index-based approaches suffer from significant I/O and sorting overhead when dealing with high predicate selectivity or long query window (common in real-world applications), which leads to high query latency. To address this issue, we propose ACER, a Range Bitmap-based index, to accelerate CER. Firstly, ACER achieves a low index space overhead by grouping the events with the same type into a cluster and compressing the cluster data, alleviating the I/O overhead of reading indexes. Secondly, ACER builds Range Bitmaps in batch (block) for queried attributes and ensures that the events of each cluster in the index block are chronologically ordered. Then, ACER can always obtain ordered query results for a specific event type through merge operations, avoiding sorting overhead. Most importantly, ACER avoids unnecessary disk access in indexes and events via two-phase filtering based on the window condition, thus alleviating the I/O overhead further. Our experiments on six real-world and synthetic datasets demonstrate that ACER reduces the query latency by up to one order of magnitude compared with SOTA techniques.",KDD
"Federated Learning (FL) is susceptible to poisoning attacks, wherein compromised clients manipulate the global model by modifying local datasets or sending manipulated model updates. Experienced defenders can readily detect and mitigate the poisoning effects of malicious behaviors using Byzantine-robust aggregation rules. However, the exploration of poisoning attacks in scenarios where such behaviors are absent remains largely unexplored for Byzantine-robust FL. This paper addresses the challenging problem of poisoning Byzantine-robust FL by introducing catastrophic forgetting. To fill this gap, we first formally define generalization error and establish its connection to catastrophic forgetting, paving the way for the development of a clean-label data poisoning attack named BadSampler. This attack leverages only clean-label data (i.e., without poisoned data) to poison Byzantine-robust FL and requires the adversary to selectively sample training data with high loss to feed model training and maximize the model's generalization error. We formulate the attack as an optimization problem and present two elegant adversarial sampling strategies, Top-k sampling, and meta-sampling, to approximately solve it. Additionally, our formal error upper bound and time complexity analysis demonstrate that our design can preserve attack utility with high efficiency. Extensive evaluations on two real-world datasets illustrate the effectiveness and performance of our proposed attacks.",KDD
"Sequential Recommendation (SR) navigates users' dynamic preferences through modeling their historical interactions. The incorporation of the popular Transformer framework, which captures long relationships through pairwise dot products, has notably benefited SR. However, prevailing research in this domain faces three significant challenges: (i) Existing studies directly adopt the primary component of Transformer (i.e., the self-attention mechanism), without a clear explanation or tailored definition for its specific role in SR; (ii) The predominant focus on pairwise computations overlooks the global context or relative prevalence of item pairs within the overall sequence; (iii) Transformer primarily pursues relevance-dominated relationships, neglecting another essential objective in recommendation, i.e., diversity. In response, this work introduces a fresh perspective to elucidate the attention mechanism in SR. Here, attention is defined as dependency interactions among items, quantitatively determined under a global probabilistic model by observing the probabilities of corresponding item subsets. This viewpoint offers a precise and context-specific definition of attention, leading to the design of a distinctive attention mechanism tailored for SR. Specifically, we transmute the well-formulated global, repulsive interactions in Determinantal Point Processes (DPPs) to effectively model dependency interactions. Guided by the repulsive interactions, a theoretically and practically feasible DPP kernel is designed, enabling our attention mechanism to directly consider category/topic distribution for enhancing diversity. Consequently, the <u>P</u>robabilistic <u>Att</u>ention mechanism (PAtt) for sequential recommendation is developed. Experimental results demonstrate the excellent scalability and adaptability of our attention mechanism, which significantly improves recommendation performance in terms of both relevance and diversity.",KDD
"Graph clustering, a fundamental and challenging task in graph mining, aims to classify nodes in a graph into several disjoint clusters. In recent years, graph contrastive learning (GCL) has emerged as a dominant line of research in graph clustering and advances the new state-of-the-art. However, GCL-based methods heavily rely on graph augmentations and contrastive schemes, which may potentially introduce challenges such as semantic drift and scalability issues. Another promising line of research involves the adoption of modularity maximization, a popular and effective measure for community detection, as the guiding principle for clustering tasks. Despite the recent progress, the underlying mechanism of modularity maximization is still not well understood. In this work, we dig into the hidden success of modularity maximization for graph clustering. Our analysis reveals the strong connections between modularity maximization and graph contrastive learning, where positive and negative examples are naturally defined by modularity. In light of our results, we propose a community-aware graph clustering framework, coined Ã¸urs, which leverages modularity maximization as a contrastive pretext task to effectively uncover the underlying information of communities in graphs, while avoiding the problem of semantic drift. Extensive experiments on multiple graph datasets verify the effectiveness of Ã¸urs in terms of scalability and clustering performance compared to state-of-the-art graph clustering methods. Notably, Ã¸urs easily scales a sufficiently large graph with 100M nodes while outperforming strong baselines.",KDD
"Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named Dataset Condensation has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset Condensation for Time Series Classification via Dual Domain Matching (CondTSC) which focuses on the time series classification dataset condensation task. Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains. Specifically, CondTSC incorporates multi-view data augmentation, dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains. Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data.",KDD
"With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named Graph Data Condensation via Self-expressive Graph Structure Reconstruction (GCSR). Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure. Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets. Our code is available at https://github.com/zclzcl0223/GCSR.",KDD
"Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.
To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset under the channel-independent assumption for pretraining our model, comprising various datasets from diverse data scenarios. This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets. On the other hand, GPHT employs an auto-regressive forecasting approach, effectively modeling temporal dependencies in the output series. Importantly, no customized forecasting head is required, enablinga single model to forecast at arbitrary horizon settings. We conduct sufficient experiments on eight datasets with mainstream self-supervised pretraining models and supervised models. The results demonstrated that GPHT surpasses the baseline models across various fine-tuning and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretraining time series large models. We make our codes publicly available\footnotehttps://github.com/icantnamemyself/GPHT.",KDD
"Data collected in the real world often encapsulates historical discrimination against disadvantaged groups and individuals. Existing fair machine learning (FairML) research has predominantly focused on mitigating discriminative bias in the model prediction, with far less effort dedicated towards exploring how to trace biases present in the data, despite its importance for the transparency and interpretability of FairML. To fill this gap, we investigate a novel research problem: discovering samples that reflect biases/prejudices from the training data. Grounding on the existing fairness notions, we lay out a sample bias criterion and propose practical algorithms for measuring and countering sample bias. The derived bias score provides intuitive sample-level attribution and explanation of historical bias in data. On this basis, we further design two FairML strategies via sample-bias-informed minimal data editing. They can mitigate both group and individual unfairness at the cost of minimal or zero predictive utility loss. Extensive experiments and analyses on multiple real-world datasets demonstrate the effectiveness of our methods in explaining and mitigating unfairness. Code is available at https://github.com/ZhiningLiu1998/AIM.",KDD
"The rapid expansion of Location-Based Social Networks (LBSNs) has highlighted the importance of effective next Point-of-Interest (POI) recommendations, which leverage historical check-in data to predict users' next POIs to visit. Traditional centralized deep neural networks (DNNs) offer impressive POI recommendation performance but face challenges due to privacy concerns and limited timeliness. In response, on-device POI recommendations have been introduced, utilizing federated learning (FL) and decentralized approaches to ensure privacy and recommendation timeliness. However, these methods often suffer from computational strain on devices and struggle to adapt to new users and regions. This paper introduces a novel collaborative learning framework, Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations (DCPR), leveraging the diffusion model known for its success across various domains. DCPR operates with a cloud-edge-device architecture to offer region-specific and highly personalized POI recommendations while reducing on-device computational burdens. DCPR minimizes on-device computational demands through a unique blend of global and local learning processes. Our evaluation with two real-world datasets demonstrates DCPR's superior performance in recommendation accuracy, efficiency, and adaptability to new users and regions, marking a significant step forward in on-device POI recommendation technology.",KDD
"As the size of datasets used in statistical learning continues to grow, distributed training of models has attracted increasing attention. These methods partition the data and exploit parallelism to reduce memory and runtime, but suffer increasingly from communication costs as the data size or the number of iterations grows. Recent work on linear models has shown that a surrogate likelihood can be optimized locally to iteratively improve on an initial solution in a communication-efficient manner. However, existing versions of these methods experience multiple shortcomings as the data size becomes massive, including diverging updates and efficiently handling sparsity. In this work we develop solutions to these problems which enable us to learn a communication-efficient distributed logistic regression model even beyond millions of features. In our experiments we demonstrate a large improvement in accuracy over distributed algorithms with only a few distributed update steps needed, and similar or faster runtimes. Our code is available at https://github.com/FutureComputing4AI/ProxCSL.",KDD
"Although machine learning algorithms demonstrate impressive performance, their trustworthiness remains a critical issue, particularly concerning fairness when implemented in real-world applications. Many notions of group fairness aim to minimize disparities in performance across protected groups. However, it can inadvertently reduce performance in certain groups, leading to sub-optimal outcomes. In contrast, Min-max group fairness notion prioritizes the improvement for the worst-performing group, thereby advocating a utility-promoting approach to fairness. However, it has been proven that existing efforts to achieve Min-max fairness exhibit limited effectiveness. In response to this challenge, we leverage the recently proposed ""Neural Collapse'' framework to re-examine Empirical Risk Minimization (ERM) training, specifically investigating the root causes of poor performance in minority groups. The layer-peeled model is employed to decompose a network into two parts: an encoder to learn latent representation, and a subsequent classifier, with a systematic characterization of their training behaviors being conducted. Our analysis reveals that while classifiers achieve maximum separation, the separability of representations is insufficient, particularly for minority groups. This indicates the sub-optimal performance in minority groups stems from less separable representations, rather than classifiers. To tackle this issue, we introduce a novel strategy that incorporates a frozen classifier to directly enhance representation. Furthermore, we introduce two easily implemented loss functions to guide the learning process. The experimental assessments carried out on real-world benchmark datasets spanning the domains of Computer Vision, Natural Language Processing, and Tabular data demonstrate that our approach outperforms existing state-of-the-art methods in promoting the Min-max fairness notion.",KDD
"Graph Neural Networks (GNNs) have revolutionized graph-based machine learning, but their heavy computational demands pose challenges for latency-sensitive edge devices in practical industrial applications. In response, a new wave of methods, collectively known as GNN-to-MLP Knowledge Distillation, has emerged. They aim to transfer GNN-learned knowledge to a more efficient MLP student, which offers faster, resource-efficient inference while maintaining competitive performance compared to GNNs. However, these methods face significant challenges in situations with insufficient training data and incomplete test data, limiting their applicability in real-world applications. To address these challenges, we propose AdaGMLP, an AdaBoosting GNN-to-MLP Knowledge Distillation framework. It leverages an ensemble of diverse MLP students trained on different subsets of labeled nodes, addressing the issue of insufficient training data. Additionally, it incorporates a Node Alignment technique for robust predictions on test data with missing or incomplete features. Our experiments on seven benchmark datasets with different settings demonstrate that AdaGMLP outperforms existing G2M methods, making it suitable for a wide range of latency-sensitive real-world applications. We have submitted our code to the GitHub repository (https://github.com/WeigangLu/AdaGMLP-KDD24).",KDD
"Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN.",KDD
"Capturing molecular knowledge with representation learning approaches holds significant potential in vast scientific fields such as chemistry and life science. An effective and generalizable molecular representation is expected to capture the consensus and complementary molecular expertise from diverse views and perspectives. However, existing works fall short in learning multi-view molecular representations, due to challenges in explicitly incorporating view information and handling molecular knowledge from heterogeneous sources. To address these issues, we present MV-Mol, a molecular representation learning model that harvests multi-view molecular expertise from chemical structures, unstructured knowledge from biomedical texts, and structured knowledge from knowledge graphs. We utilize text prompts to model view information and design a fusion architecture to extract view-based molecular representations. We develop a two-stage pre-training procedure, exploiting heterogeneous data of varying quality and quantity. Through extensive experiments, we show that MV-Mol provides improved representations that substantially benefit molecular property prediction. Additionally, MV-Mol exhibits state-of-the-art performance in multi-modal comprehension of molecular structures and texts. Code and data are available at https://github.com/PharMolix/OpenBioMed.",KDD
"Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning. While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored. Our study provides a comprehensive analysis of GPL's vulnerability to backdoor attacks. We introduce CrossBA, the first cross-context backdoor attack against GPL, which manipulates only the pretraining phase without requiring knowledge of downstream applications. Our investigation reveals both theoretically and empirically that tuning trigger graphs, combined with prompt transformations, can seamlessly transfer the backdoor threat from pretrained encoders to downstream applications.Through extensive experiments involving 3 representative GPL methods across 5 distinct cross-context scenarios and 5 benchmark datasets of node and graph classification tasks, we demonstrate that CrossBA consistently achieves high attack success rates while preserving the functionality of downstream applications over clean input. We also explore potential countermeasures against CrossBA and conclude that current defenses are insufficient to mitigate CrossBA. Our study highlights the persistent backdoor threats to GPL systems, raising trustworthiness concerns in the practices of GPL techniques.",KDD
"The sparse dictionary coding framework represents signals as a linear combination of a few predefined dictionary atoms. It has been employed for images, time series, graph signals and recently for 2-way (or 2D) spatio-temporal data employing jointly temporal and spatial dictionaries. Large and over-complete dictionaries enable high-quality models, but also pose scalability challenges which are exacerbated in multi-dictionary settings. Hence, an important problem that we address in this paper is: How to scale multi-dictionary coding for large dictionaries and datasets?
We propose a multi-dictionary atom selection technique for low-rank sparse coding named LRMDS. To enable scalability to large dictionaries and datasets, it progressively selects groups of row-column atom pairs based on their alignment with the data and performs convex relaxation coding via the corresponding sub-dictionaries. We demonstrate both theoretically and experimentally that when the data has a low-rank encoding with a sparse subset of the atoms, LRMDS is able to select them with strong guarantees under mild assumptions. Furthermore, we demonstrate the scalability and quality of LRMDS in both synthetic and real-world datasets and for a range of coding dictionaries. It achieves 3 times to 10 times speed-up compared to baselines, while obtaining up to two orders of magnitude improvement in representation quality on some of the real world datasets given a fixed target number of atoms.",KDD
"Spectral Graph Neural Networks have demonstrated superior performance in graph representation learning. However, many current methods focus on employing shared polynomial coefficients for all nodes, i.e., learning node-unified filters, which limits the filters' flexibility for node-level tasks. The recent DSF attempts to overcome this limitation by learning node-wise coefficients based on positional encoding. However, the initialization and updating process of the positional encoding are burdensome, hindering scalability on large-scale graphs. In this work, we propose a scalable node-wise filter, PolyAttn. Leveraging the attention mechanism, PolyAttn can directly learn node-wise filters in an efficient manner, offering powerful representation capabilities. Building on PolyAttn, we introduce the whole model, named PolyFormer. In the lens of Graph Transformer models, PolyFormer, which calculates attention scores within nodes, shows great scalability. Moreover, the model captures spectral information, enhancing expressiveness while maintaining efficiency. With these advantages, PolyFormer offers a desirable balance between scalability and expressiveness for node-level tasks. Extensive experiments demonstrate that our proposed methods excel at learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs containing up to 100 million nodes. The code is available at https://github.com/air029/PolyFormer.",KDD
"Conventional machine learning typically assume a fixed learning objective throughout the learning process. However, for real-world tasks in open and dynamic environments, objectives can change frequently. For example, in autonomous driving, a car has several default modes, but a user's concern for speed and fuel consumption varies depending on road conditions and personal needs. We formulate this problem as learning with varied objectives (LVO), where the goal is to optimize a dynamic weighted combination of multiple sub-objectives by sequentially selecting actions that incur different losses on these sub-objectives. We propose the VaRons algorithm, which estimates the action-wise performance on each sub-objective and adaptively selects decisions according to the dynamic requirements on different sub-objectives. Further, we extend our approach to cases involving contextual representations and propose the ConVaRons algorithm, assuming parameterized linear structure that links contextual features to the main objective. Both the VaRons and ConVaRons are provably minimax optimal with respect to the time horizon T, with ConVaRons showing better dependency with the number of sub-objectives K. Experiments on dynamic classifier and real-world cluster service allocation tasks validate the effectiveness of our methods and support our theoretical findings.",KDD
"Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables).
The key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery.
To this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a ""point-estimation"", SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs.
Extensive experiments on various datasets show that SPOT substantially outperforms SOTA methods for MAG learning. SPOT also demonstrates its effectiveness in the accuracy of skeleton posterior estimation in comparison with non-parametric bootstrap-based, or more recently, variational inference-based methods. Finally, we observe that the adoption of skeleton posterior exhibits strong promise in various causal discovery tasks.",KDD
"Anomalous node detection in a static graph faces significant challenges due to the rarity of anomalies and the substantial cost of labeling their deviant structure and attribute patterns. These challenges give rise to data-centric problems, including extremely imbalanced data distributions and intricate graph learning, which significantly impede machine learning and deep learning methods from discerning the patterns of graph anomalies with few labels. While these issues remain crucial, much of the current research focuses on addressing the induced technical challenges, treating the shortage of labeled data as a given. Distinct from previous efforts, this work focuses on tackling the data-centric problems by generating auxiliary training nodes that conform to the original graph topology and attribute distribution. We categorize this approach as data-centric, aiming to enhance existing anomaly detectors by training them on our synthetic data. However, the methods for generating nodes and the effectiveness of utilizing synthetic data for graph anomaly detection remain unexplored in the realm. To answer these questions, we thoroughly investigate the denoising diffusion model. Drawing from our observations on the diffusion process, we illuminate the shifts in graph energy distribution and establish two principles for designing denoising neural networks tailored to graph anomaly generation. From the insights, we propose a diffusion-based graph generation method to synthesize training nodes, which can be promptly integrated to work with existing anomaly detectors. The empirical results on eight widely-used datasets demonstrate our generated data can effectively enhance the nine state-of-the-art graph detectors' performance.",KDD
"Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We first show that it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show we can improve utility while reducing overhead.",KDD
"A point pattern is a dataset of coordinates, typically in 2D or 3D space. Point patterns are ubiquitous in diverse applications including Geographic Information Systems, Astronomy, Ecology, Biology and Medicine. Among the statistics used to quantify point patterns, most are based on Ripley's K-function, which measures the deviation of the observed pattern from a completely random arrangement of points. This approach is useful for constructing null hypothesis tests, but Ripley's K and its variants are less suitable as quantitative effect sizes because their ranges and expected values generally depend on the scale or the size of the region in which the pattern is observed. To address this, we propose a new function that behaves like a correlation coefficient for point patterns: it is tightly bounded by -1 and 1, with a value of -1 corresponding to a maximally dispersed arrangement of points, 0 indicating complete spatial randomness, and 1 representing maximal clustering. These properties are independent of scale and observation window size assuming appropriate edge correction. Evaluating our function on simulated data, we show that it has comparable statistical calibration and power to K-based baselines. We hope that the ease of interpretation of our bounded function will facilitate the analysis of spatial data across multiple fields.",KDD
"The problem of column subset selection asks for a subset of columns from an input matrix such that the matrix can be reconstructed as accurately as possible within the span of the selected columns. A natural extension is to consider a setting where the matrix rows are partitioned into two groups, and the goal is to choose a subset of columns that minimizes the maximum reconstruction error of both groups, relative to their respective best rank-k approximation. Extending the known results of column subset selection to this fair setting is not straightforward: in certain scenarios it is unavoidable to choose columns separately for each group, resulting in double the expected column count. We propose a deterministic leverage-score sampling strategy for the fair setting and show that sampling a column subset of minimum size becomes NP-hard in the presence of two groups. Despite these negative results, we give an approximation algorithm that guarantees a solution within 1.5 times the optimal solution size. We also present practical heuristic algorithms based on rank-revealing QR factorization. Finally, we validate our methods through an extensive set of experiments using real-world data.",KDD
"Social networks represent complex ecosystems where the interactions between users or groups play a pivotal role in information dissemination, opinion formation, and social interactions. Effectively harnessing event sequence data within social networks to unearth interactions among users or groups has persistently posed a challenging frontier within the realm of point processes. Current deep point process models face inherent limitations within the context of social networks, constraining both their interpretability and expressive power. These models encounter challenges in capturing interactions among users or groups and often rely on parameterized extrapolation methods when modeling intensity over non-event intervals, limiting their capacity to capture intricate intensity patterns, particularly beyond observed events. To address these challenges, this study proposes modifications to Transformer Hawkes processes (THP), leading to the development of interpretable Transformer Hawkes processes (ITHP). ITHP inherits the strengths of THP while aligning with statistical nonlinear Hawkes processes, thereby enhancing its interpretability and providing valuable insights into interactions between users or groups. Additionally, ITHP enhances the flexibility of the intensity function over non-event intervals, making it better suited to capture complex event propagation patterns in social networks. Experimental results, both on synthetic and real data, demonstrate the effectiveness of ITHP in overcoming the identified limitations. Moreover, they highlight ITHP's applicability in the context of exploring the complex impact of users or groups within social networks. Our code is available at https://github.com/waystogetthere/Interpretable-Transformer- Hawkes-Process.git.",KDD
"Empirically-determined scaling laws have been broadly successful in predicting the evolution of large machine learning models with training data and number of parameters. As a consequence, they have been useful for optimizing the allocation of limited resources, most notably compute time.
In certain applications, storage space is an important constraint, and data format needs to be chosen carefully as a consequence. Computer vision is a prominent example: images are inherently analog, but are always stored in a digital format using a finite number of bits. Given a dataset of digital images, the number of bits L to store each of them can be further reduced using lossy data compression. This, however, can degrade the quality of the model trained on such images, since each example has lower resolution.
In order to capture this trade-off and optimize storage of training data, we propose a 'storage scaling law' that describes the joint evolution of test error with sample size and number of bits per image. We prove that this law holds within a stylized model for image compression, and verify it empirically on two computer vision tasks, extracting the relevant parameters. We then show that this law can be used to optimize the lossy compression level. At given storage, models trained on optimally compressed images present a significantly smaller test error with respect to models trained on the original data. Finally, we investigate the potential benefits of randomizing the compression level.",KDD
"In numerous real-world domains, spanning from environmental monitoring to long-term medical studies, observations do not arrive in a single batch but rather over time in episodes. This challenges the traditional assumption in causal discovery of a single, observational dataset, not only because each episode may be a biased sample of the population but also because multiple episodes could differ in the causal interactions underlying the observed variables. We address these issues using notions of context switches and episodic selection bias, and introduce a framework for causal modeling of episodic data. We show under which conditions we can apply information-theoretic scoring criteria for causal discovery while preserving consistency. To in practice discover the causal model progressively over time, we propose the CONTINENT algorithm which, taking inspiration from continual learning, discovers the causal model in an online fashion without having to re-learn the model upon arrival of each new episode. Our experiments over a variety of settings including selection bias, unknown interventions, and network changes showcase that CONTINENT works well in practice and outperforms the baselines by a clear margin.",KDD
"The intrinsic predictability of a given time series indicates how well an (ideal) algorithm could potentially predict it when trained on the time series data. Being able to compute the intrinsic predictability helps the developers of prediction algorithms immensely in deciding whether there is further optimization potential, as it tells them how close they are to what is (theoretically) achievable. We call the intrinsic predictability the predictability upper bound Â¶imax and propose a novel method for quantifying and estimating it for univariate numeric time series. So far, this has only been done for symbolic time series, even though most real-world time series are numeric by nature. We base our technique on the close relationship between entropy and predictability, utilizing the entropy rate of a time series to compute Â¶imax . Since existing entropy rate estimators, such as those based on the Lempel-Ziv compression algorithm, only work for symbolic data, we develop new estimators using tolerance thresholds for matching numeric values. We demonstrate that Â¶imax is an effective upper bound that characterizes the intrinsic predictability of a time series. We give formal proofs and we validate our arguments experimentally by comparing Â¶imax with the prediction accuracy of different state-of-the-art models on various real-world datasets from different domains.",KDD
"The popularity of decentralized finance has drawn attention to liquidity mining (LM). In LM, a user deposits her cryptocurrencies into liquidity pools to provide liquidity for exchanges and earn yields. Different liquidity pools offer varying yields and require different pairs of cryptocurrencies. A user can exchange a cryptocurrency for another with some exchange costs. Thus, an LM solution consists of exchange transactions and deposit transactions, guaranteeing (1) each exchange transaction must exchange one cryptocurrency for another at a specific rate (i.e., the exchange constraint); (2) the amounts of cryptocurrencies deposited in a liquidity pool must exceed the required threshold (i.e., the minimum constraint); (3) each deposit transaction must deposit a specific pair of cryptocurrencies at a certain rate in a liquidity pool (i.e., the deposit constraint); and (4) the cryptocurrencies used in the solution do not exceed the cryptocurrencies that the user has (i.e., the budget constraint). Selecting the most profitable LM solution is challenging due to the vast number of candidate solutions. To address this challenge, we define the yield maximization liquidity mining (YMLM) problem. Given a set of liquidity pools, a set of the user's cryptocurrencies, a set of exchange rates, and an evaluation function, YMLM aims to find an LM solution with maximal yields, satisfying the minimum, exchange, deposit, and budget constraints. We prove that YMLM is NP-hard and cannot be solved by algorithms with constant approximation ratios. To tackle YMLM, we propose two algorithms, namely YMLM\_GD and YMLM\_SK, with parameterized approximation ratios. Extensive experiments on both real and synthetic datasets show that our approaches outperform the baselines in yields.",KDD
"Missing data is a pervasive issue in both scientific and engineering tasks, especially for the modeling of spatiotemporal data. Existing imputation solutions mainly include low-rank models and deep learning models. The former assumes general structural priors but has limited model capacity. The latter possesses salient expressivity, but lacks prior knowledge of the underlying spatiotemporal structures. Leveraging the strengths of both two paradigms, we demonstrate a low rankness-induced Transformer to achieve a balance between strong inductive bias and high expressivity. The exploitation of the inherent structures of spatiotemporal data enables our model to learn balanced signal-noise representations, making it generalizable for a variety of imputation tasks. We demonstrate its superiority in terms of accuracy, efficiency, and versatility in heterogeneous datasets, including traffic flow, solar energy, smart meters, and air quality. Promising empirical results provide strong conviction that incorporating time series primitives, such as low-rankness, can substantially facilitate the development of a generalizable model to approach a wide range of spatiotemporal imputation problems.",KDD
"Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search, which aims to break the barriers between modality and language simultaneously and achieves image-text retrieval in the multi-lingual scenario with a single model. In recent years, excellent progress has been made based on cross-lingual cross-modal pre-training; particularly, the methods based on contrastive learning on large-scale data have significantly improved retrieval tasks. However, these methods directly follow the existing pre-training methods in the cross-lingual or cross-modal domain, leading to two problems of inconsistency in CCR: The methods with cross-lingual style suffer from the intra-modal error propagation, resulting in inconsistent recall performance across languages in the whole dataset. The methods with cross-modal style suffer from the inter-modal optimization direction bias, resulting in inconsistent rank across languages within each instance, which cannot be reflected by Recall@K. To solve these problems, we propose a simple but effective 1-to-K contrastive learning method, which treats each language equally and eliminates error propagation and optimization bias. In addition, we propose a new evaluation metric, Mean Rank Variance (MRV), to reflect the rank inconsistency across languages within each instance. Extensive experiments on four CCR datasets show that our method improves both recall rates and MRV with smaller-scale pre-trained data, achieving the new state-of-art.",KDD
"Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.",KDD
"Multivariate time series data suffer from the problem of missing values, which hinders the application of many analytical methods. To achieve the accurate imputation of these missing values, exploiting inter-correlation by employing the relationships between sequences (i.e., a network) is as important as the use of temporal dependency, since a sequence normally correlates with other sequences. Moreover, exploiting an adequate network depending on time is also necessary since the network varies over time. However, in real-world scenarios, we normally know neither the network structure nor when the network changes beforehand. Here, we propose a missing value imputation method for multivariate time series, namely MissNet, that is designed to exploit temporal dependency with a state-space model and inter-correlation by switching sparse networks. The network encodes conditional independence between features, which helps us understand the important relationships for imputation visually. Our algorithm, which scales linearly with reference to the length of the data, alternatively infers networks and fills in missing values using the networks while discovering the switching of the networks. Extensive experiments demonstrate that MissNet outperforms the state-of-the-art algorithms for multivariate time series imputation and provides interpretable results.",KDD
"The traditional evaluation of information retrieval (IR) systems is generally very costly as it requires manual relevance annotation from human experts. Recent advancements in generative artificial intelligence -specifically large language models (LLMs)- can generate relevance annotations at an enormous scale with relatively small computational costs. Potentially, this could alleviate the costs traditionally associated with IR evaluation and make it applicable to numerous low-resource applications. However, generated relevance annotations are not immune to (systematic) errors, and as a result, directly using them for evaluation produces unreliable results.
In this work, we propose two methods based on prediction-powered inference and conformal risk control that utilize computer-generated relevance annotations to place reliable confidence intervals (CIs) around IR evaluation metrics. Our proposed methods require a small number of reliable annotations from which the methods can statistically analyze the errors in the generated annotations. Using this information, we can place CIs around evaluation metrics with strong theoretical guarantees. Unlike existing approaches, our conformal risk control method is specifically designed for ranking metrics and can vary its CIs per query and document. Our experimental results show that our CIs accurately capture both the variance and bias in evaluation based on LLM annotations, better than the typical empirical bootstrapping estimates. We hope our contributions bring reliable evaluation to the many IR applications where this was traditionally infeasible.",KDD
"Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly given the massive scale of data. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose Ã¸urs, where we (1) enrich each node in the ontology structure with two categories of extra information:instance information for training sample augmentation andtopic information to relate types with contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that Ã¸urs achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods. Ã¸urs also enjoys strong transferability to unseen and finer-grained types. We will open source this work upon acceptance.",KDD
"Given a multidimensional array, how can we optimize the computation process for a part of Fourier coefficients? Discrete Fourier transform plays an overarching role in various data mining tasks. Recent interest has focused on efficiently calculating a small part of Fourier coefficients, exploiting the energy compaction property of real-world data. Current methods for partial Fourier transform frequently encounter efficiency issues, yet the adoption of pre-computation techniques within the PFT algorithm has shown promising performance. However, PFT still faces limitations in handling multidimensional data efficiently and requires manual hyperparameter tuning, leading to additional costs.
In this paper, we propose Auto-MPFT (Automatic Multidimensional Partial Fourier Transform), which efficiently computes a subset of Fourier coefficients in multidimensional data without the need for manual hyperparameter search. Auto-MPFT leverages multivariate polynomial approximation for trigonometric functions, generalizing its domain to multidimensional Euclidean space. Moreover, we present a convex optimization-based algorithm for automatically selecting the optimal hyperparameter of Auto-MPFT. We provide a rigorous proof for the explicit reformulation of the original optimization problem of Auto-MPFT, demonstrating the process that converts it into a well-established unconstrained convex optimization problem. Extensive experiments show that Auto-MPFT surpasses existing partial Fourier transform methods and optimized FFT libraries, achieving up to 7.6x increase in speed without sacrificing accuracy. In addition, our optimization algorithm accurately finds the optimal hyperparameter for Auto-MPFT, significantly reducing the cost associated with hyperparameter search.",KDD
"Hyperparameter optimization (HPO) is known to be costly in deep learning, especially when leveraging automated approaches. Most of the existing automated HPO methods are accuracy-based, i.e., accuracy metrics are used to guide the trials of different hyperparameter configurations amongst a specific search space. However, many trials may encounter severe training problems, such as vanishing gradients and insufficient convergence, which can hardly be reflected by accuracy metrics in the early stages of the training and often result in poor performance. This leads to an inefficient optimization trajectory because the bad trials occupy considerable computation resources and reduce the probability of finding excellent hyperparameter configurations within a time limitation. In this paper, we propose Bad Trial Tackler (BTTackler), a novel HPO framework that introduces training diagnosis to identify training problems automatically and hence tackles bad trials. BTTackler diagnoses each trial by calculating a set of carefully designed quantified indicators and triggers early termination if any training problems are detected. Evaluations are performed on representative HPO tasks consisting of three classical deep neural networks (DNN) and four widely used HPO methods. To better quantify the effectiveness of an automated HPO method, we propose two new measurements based on accuracy and time consumption. Results show the advantage of BTTackler on two-fold: (1) it reduces 40.33% of time consumption to achieve the same accuracy comparable to baseline methods on average and (2) it conducts 44.5% more top-10 trials than baseline methods on average within a given time budget. We also released an open-source Python library that allows users to easily apply BTTackler to automated HPO processes with minimal code changes\footnotehttps://github.com/thuml/BTTackler.",KDD
"Learning interpretable models has become a major focus of machine learning research, given the increasing prominence of machine learning in socially important decision-making. Among interpretable models, rule lists are among the best-known and easily interpretable ones. However, finding optimal rule lists is computationally challenging, and current approaches are impractical for large datasets.
We present a novel and scalable approach to learn nearly optimal rule lists from large datasets. Our algorithm uses sampling to efficiently obtain an approximation of the optimal rule list with rigorous guarantees on the quality of the approximation. In particular, our algorithm guarantees to find a rule list with accuracy very close to the optimal rule list when a rule list with high accuracy exists. Our algorithm builds on the VC-dimension of rule lists, for which we prove novel upper and lower bounds. Our experimental evaluation on large datasets shows that our algorithm identifies nearly optimal rule lists with a speed-up up to two orders of magnitude over state-of-the-art exact approaches. Moreover, our algorithm is as fast as, and sometimes faster than, recent heuristic approaches, while reporting higher quality rule lists. In addition, the rules reported by our algorithm are more similar to the rules in the optimal rule list than the rules from heuristic approaches.",KDD
"Multi-label text classification (MLTC) allows a given text to be associated with multiple labels, which well suits many real-world data mining scenarios. However, the annotation effort of MLTC is inevitably expensive and time-consuming. Although multi-label active learning provides a cost-effective solution, it still faces two major challenges: (i) constructing decent feature space to distinguish the confusing semantics of different labels; (ii) defining proper sampling criteria to measure a sample's joint effect over the entire label space. To bridge these gaps, we propose a Contrastive Multi-label Active Learning framework (CoMAL) that gives an effective data acquisition strategy. Specifically, a contrastive decoupling mechanism is introduced to fully release the semantic information of multiple labels into the latent space. Then, we devise a hybrid criterion that balances two data value measures: (i) similarity-enhanced label cardinality inconsistency reflects the uncertainty of data predictions. (ii) positive feature diversity evaluates the positive-propensity semantic diversity to handle the label sparsity. Extensive experiments demonstrate that our CoMAL outperforms the current state-of-the-art multi-label active learning approaches. Code for CoMAL is available at https://github.com/chengzju/CoMAL.",KDD
"Graph Convolutional Neural Network (GCN), a widely adopted method for analyzing relational data, enhances node discriminability through the aggregation of neighboring information. Usually, stacking multiple layers can improve the performance of GCN by leveraging information from high-order neighbors. However, the increase of the network depth will induce the over-smoothing problem, which can be attributed to the quality and quantity of neighbors changing: (a) neighbor quality, node's neighbors become overlapping in high order, leading to aggregated information becoming indistinguishable, (b) neighbor quantity, the exponentially growing aggregated neighbors submerges the node's initial feature by recursively aggregating operations. Current solutions mainly focus on one of the above causes and seldom consider both at once. Aiming at tackling both causes of over-smoothing in one shot, we introduce a simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet potent techniques: random masking and contrastive constraint. The random masking acts on the representation matrix's columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations. Meanwhile, the contrastive constraint, applied to the representation matrix's rows, enhances the discriminability of the nodes. Designed as a plug-in module, TSC can be easily coupled with GCN or SGC architectures. Experimental analyses on diverse real-world graph datasets verify that our approach markedly reduces the convergence of node's representation and the performance degradation in deeper GCN.",KDD
"It has been shown that the effectiveness of graph convolutional network (GCN) for recommendation is attributed to the spectral graph filtering. Most GCN-based methods consist of a graph filter or followed by a low-rank mapping optimized based on supervised training. However, we show two limitations suppressing the power of graph filtering: (1) Lack of generality. Due to the varied noise distribution, graph filters fail to denoise sparse data where noise is scattered across all frequencies, while supervised training results in worse performance on dense data where noise is concentrated in middle frequencies that can be removed by graph filters without training. (2) Lack of expressive power. We theoretically show that linear GCN (LGCN) that is effective on collaborative filtering (CF) cannot generate arbitrary embeddings, implying the possibility that optimal data representation might be unreachable.
To tackle the first limitation, we show close relation between noise distribution and the sharpness of spectrum where a sharper spectral distribution is more desirable causing data noise to be separable from important features without training. Based on this observation, we propose a generalized graph normalization (G2N) with hyperparameters adjusting the sharpness of spectral distribution in order to redistribute data noise to assure that it can be removed by graph filtering without training. As for the second limitation, we propose an individualized graph filter (IGF) adapting to the different confidence levels of the user preference that interactions can reflect, which is proved to be able to generate arbitrary embeddings. By simplifying LGCN, we further propose a simplified graph filtering for CF (SGFCF) which only requires the top-K singular values for recommendation. Finally, experimental results on four datasets with different density settings demonstrate the effectiveness and efficiency of our proposed methods.",KDD
"The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer",KDD
"The Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem is pivotal in Automatic Machine Learning (AutoML). Most leading approaches combine Bayesian optimization with post-hoc ensemble building to create advanced AutoML systems. Bayesian optimization (BO) typically focuses on identifying a singular algorithm and its hyperparameters that outperform all other configurations. Recent developments have highlighted an oversight in prior CASH methods: the lack of consideration for diversity among the base learners of the ensemble. This oversight was overcome by explicitly injecting the search for diversity into the traditional CASH problem. However, despite recent developments, BO's limitation lies in its inability to directly optimize ensemble generalization error, offering no theoretical assurance that increased diversity correlates with enhanced ensemble performance. Our research addresses this gap by establishing a theoretical foundation that integrates diversity into the core of BO for direct ensemble learning. We explore a theoretically sound framework that describes the relationship between pair-wise diversity and ensemble performance, which allows our Bayesian optimization framework Optimal Diversity Bayesian Optimization (OptDivBO) to directly and efficiently minimize ensemble generalization error. OptDivBO guarantees an optimal balance between pairwise diversity and individual model performance, setting a new precedent in ensemble learning within CASH. Empirical results on 20 public datasets show that OptDivBO achieves the best average test ranks of 1.57 and 1.4 in classification and regression tasks.",KDD
"We present GRACIE (Graph Recalibration and Adaptive Counterfactual Inspection and Explanation), a novel approach for generative classification and counterfactual explanations of dynamically changing graph data. We study graph classification problems through the lens of generative classifiers. We propose a dynamic, self-supervised latent variable model that updates by identifying plausible counterfactuals for input graphs and recalibrating decision boundaries through contrastive optimization. Unlike prior work, we do not rely on linear separability between the learned graph representations to find plausible counterfactuals. Moreover, GRACIE eliminates the need for stochastic sampling in latent spaces and graph-matching heuristics. Our work distills the implicit link between generative classification and loss functions in the latent space, a key insight to understanding recent successes with this architecture. We further observe the inherent trade-off between validity and pulling explainee instances towards the central region of the latent space, empirically demonstrating our theoretical findings. In extensive experiments on synthetic and real-world graph data, we attain considerable improvements, reaching ~99% validity when sampling sets of counterfactuals even in the challenging setting of dynamic data landscapes.",KDD
"Estimating cardinality, i.e., the number of distinct elements, of a data stream is a fundamental problem in areas like databases, computer networks, and information retrieval. This study delves into a broader scenario where each element carries a positive weight. Unlike traditional cardinality estimation, limited research exists on weighted cardinality, with current methods requiring substantial memory and computational resources, challenging for devices with limited capabilities and real-time applications like anomaly detection. To address these issues, we propose QSketch, a memory-efficient sketch method for estimating weighted cardinality in streams. QSketch uses a quantization technique to condense continuous variables into a compact set of integer variables, with each variable requiring only 8 bits, making it 8 times smaller than previous methods. Furthermore, we leverage dynamic properties during QSketch generation to significantly enhance estimation accuracy and achieve a lower time complexity of O(1) for updating estimations upon encountering a new element. Experimental results on synthetic and real-world datasets show that QSketch is approximately 30% more accurate and two orders of magnitude faster than the state-of-the-art, using only 1/8 of the memory.",KDD
"Recently, Graph Neural Networks (GNNs) have achieved inspiring performances in graph classification tasks. However, the message passing mechanism in GNNs implicitly utilizes the topological information of the graph, which may lead to a potential loss of structural information. Furthermore, the graph classification decision process based on GNNs resembles a black box and lacks sufficient transparency. The non-linear classifier following the GNNs also defaults to the assumption that each class is represented by a single vector, thereby limiting the diversity of intra-class representations.
To address these issues, we propose a novel prototype-based graph classification framework that introduces the Fused Gromov-Wasserstein (FGW) distance in Optimal Transport (OT) as the similarity measure. In this way, the model explicitly exploits the structural information on the graph through OT while leading to a more transparent and straightforward classification process. The introduction of prototypes also inherently addresses the issue of limited within-class representations. Besides, to alleviate the widely acknowledged computational complexity issue of FGW distance calculation, we devise a simple yet effective NN-based FGW distance approximator, which can enable full GPU training acceleration with a marginal performance loss. In theory, we analyze the generalization performance of the proposed method and derive an O (1 over N) generalization bound, where the proof techniques can be extended to a broader range of prototype-based classification frameworks. Experimental results show that the proposed framework achieves competitive and superior performance on several widely used graph classification benchmark datasets. The code is avaliable at https://github.com/ChnQ/PGOT.",KDD
"Cognitive diagnosis models (CDMs) are designed to learn students' mastery levels using their response logs. CDMs play a fundamental role in online education systems since they significantly influence downstream applications such as teachers' guidance and computerized adaptive testing. Despite the success achieved by existing CDMs, we find that they suffer from a thorny issue that the learned students' mastery levels are too similar. This issue, which we refer to as oversmoothing, could diminish the CDMs' effectiveness in downstream tasks. CDMs comprise two core parts: learning students' mastery levels and assessing mastery levels by fitting the response logs. This paper contends that the oversmoothing issue arises from that existing CDMs seldom utilize response signals on exercises in the learning part but only use them as labels in the assessing part. To this end, this paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing response signals in the learning part. Specifically, ORCDF introduces a novel response graph to inherently incorporate response signals as types of edges. Then, ORCDF designs a tailored response-aware graph convolution network (RGC) that effectively captures the crucial response signals within the response graph. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings with the outcome of RGC, allowing for the consideration of response signals on exercises in the learning part. Extensive experiments on real-world datasets show that ORCDF not only helps existing CDMs alleviate the oversmoothing issue but also significantly enhances the models' prediction and interpretability performance. Moreover, the effectiveness of ORCDF is validated in the downstream task of computerized adaptive testing.",KDD
"Community detection (CD) is a classic graph inference task that partitions nodes of a graph into densely connected groups. While many CD methods have been proposed with either impressive quality or efficiency, balancing the two aspects remains a challenge. This study explores the potential of deep graph learning to achieve a better trade-off between the quality and efficiency of K-agnostic CD, where the number of communities K is unknown. We propose PRoCD (<u>P</u>re-training & <u>R</u>efinement f<u>O</u>r <u>C</u>ommunity <u>D</u>etection), a simple yet effective method that reformulates K-agnostic CD as the binary node pair classification. PRoCD follows a pre-training & refinement paradigm inspired by recent advances in pre-training techniques. We first conduct the offline pre-training of PRoCD on small synthetic graphs covering various topology properties. Based on the inductive inference across graphs, we then generalize the pre-trained model (with frozen parameters) to large real graphs and use the derived CD results as the initialization of an existing efficient CD method (e.g., InfoMap) to further refine the quality of CD results. In addition to benefiting from the transfer ability regarding quality, the online generalization and refinement can also help achieve high inference efficiency, since there is no time-consuming model optimization. Experiments on public datasets with various scales demonstrate that PRoCD can ensure higher efficiency in K-agnostic CD without significant quality degradation.",KDD
"Pseudo-cliques (subgraphs with almost all possible edges) have many applications. But they do not satisfy the convertible antimonotone constraint (as we prove here). So, it is hard to reduce the search space of pseudo-cliques and list them efficiently. To our knowledge, only two exact algorithms, namely, ODES and PCE, were proposed for this purpose, but both have high execution times. Here, we present an exact algorithm named Fast Pseudo-Clique Enumerator (FPCE). It employs some pruning techniques we derived to reduce the search space. Our experiment on 15 real and 16 synthetic graphs shows that (i) on real graphs, FPCE is, on average, 38.6 and 6.5 times faster than ODES and PCE, respectively, whereas (ii) on synthetic graphs, FPCE is, on average, 39.7 and 3.1 times faster than ODES and PCE, respectively. We apply FPCE and a popular heuristic method on a PPI network to identify pseudo-cliques. FPCE outputs match with more known protein complexes, are more accurate, and are biologically more significant - suggesting that the exact computation of pseudo-cliques may give better insights. For its speed, FPCE is a suitable choice in such cases.",KDD
"We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS) framework, designed to assess the robustness of hierarchical time series forecasting models and algorithms on real-world datasets. Hierarchical time series, where lower-level forecasts must sum to upper-level ones, are prevalent in various contexts, such as retail sales across countries. Current empirical evaluations of forecasting methods are often limited to a small set of benchmark datasets, offering a narrow view of algorithm behavior. RHiOTS addresses this gap by systematically altering existing datasets and modifying the characteristics of individual series and their interrelations. It uses a set of parameterizable transformations to simulate those changes in the data distribution. Additionally, RHiOTS incorporates an innovative visualization component, turning complex, multidimensional robustness evaluation results into intuitive, easily interpretable visuals. This approach allows an in-depth analysis of algorithm and model behavior under diverse conditions. We illustrate the use of RHiOTS by analyzing the predictive performance of several algorithms. Our findings show that traditional statistical methods are more robust than state-of-the-art deep learning algorithms, except when the transformation effect is highly disruptive. Furthermore, we found no significant differences in the robustness of the algorithms when applying specific reconciliation methods, such as MinT. RHiOTS provides researchers with a comprehensive tool for understanding the nuanced behavior of forecasting algorithms, offering a more reliable basis for selecting the most appropriate method for a given problem.",KDD
"Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion. Existing work mainly chooses neighboring intersections as collaborators. However, quite a lot of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate. To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy. Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features. Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods. Code is available at https://github.com/bonaldli/CoSLight.",KDD
"Classification tasks in many real-world domains are exacerbated by class imbalance, relatively small sample sizes compared to high dimensionality, and measurement uncertainty. The problem of class imbalance has been extensively studied, and data augmentation methods based on interpolation of minority class instances have been proposed as a viable solution to mitigate imbalance. It remains to be seen whether augmentation can be applied to improve the overall performance while maintaining stability, especially with a limited number of samples. In this paper, we present a novel feature-space augmentation technique that can be applied to high-dimensional data for classification tasks and address these issues. Our method utilizes uniform random sampling and introduces synthetic instances by taking advantage of the local distributions of individual features in the observed instances. The core augmentation algorithm is class-invariant, which opens up an unexplored avenue of simultaneously improving and stabilizing performance by augmenting unlabeled instances. The proposed method is evaluated using a comprehensive performance analysis involving multiple classifiers and metrics. Comparative analysis with existing feature space augmentation methods strongly suggests that the proposed algorithm can result in improved classification performance while also increasing the overall reliability of the performance evaluation.",KDD
"As online music consumption increasingly shifts towards playlist-based listening, the task of playlist continuation, in which an algorithm suggests songs to extend a playlist in a personalized and musically cohesive manner, has become vital to the success of music streaming services. Currently, many existing playlist continuation approaches rely on collaborative filtering methods to perform their recommendations. However, such methods will struggle to recommend songs that lack interaction data, an issue known as the cold-start problem. Current approaches to this challenge design complex mechanisms for extracting relational signals from sparse collaborative signals and integrating them into content representations. However, these approaches leave content representation learning out of scope and utilize frozen, pre-trained content models that may not be aligned with the distribution or format of a specific musical setting. Furthermore, even the musical state-of-the-art content modules are either (1) incompatible with the cold-start setting or (2) unable to effectively integrate cross-modal and relational signals. In this paper, we introduce LARP, a multi-modal cold-start playlist continuation model, to effectively overcome these limitations. LARP is a three-stage contrastive learning framework that integrates both multi-modal and relational signals into its learned representations. Our framework uses increasing stages of task-specific abstraction: within-track (language-audio) contrastive loss, track-track contrastive loss, and track-playlist contrastive loss. Experimental results on two publicly available datasets demonstrate the efficacy of LARP over uni-modal and multi-modal models for playlist continuation in a cold-start setting. Finally, this work pioneers the perspective of addressing cold-start recommendation via relational representation learning. Code and dataset are released at: https://github.com/Rsalganik1123/LARP/",KDD
"Finding dense subnetworks, with density based on edges or more complex structures, such as subgraphs or k-cliques, is a fundamental algorithmic problem with many applications. While the problem has been studied extensively in static networks, much remains to be explored for temporal networks.
In this work we introduce the novel problem of identifying the temporal motif densest subnetwork, i.e., the densest subnetwork with respect to temporal motifs, which are high-order patterns characterizing temporal networks. Identifying temporal motifs is an extremely challenging task, and thus, efficient methods are required. To address this challenge, we design two novel randomized approximation algorithms with rigorous probabilistic guarantees that provide high-quality solutions. We perform extensive experiments showing that our methods outperform baselines. Furthermore, our algorithms scale on networks with up to billions of temporal edges, while baselines cannot handle such large networks. We use our techniques to analyze a financial network and show that our formulation reveals important network structures, such as bursty temporal events and communities of users with similar interests.",KDD
"Message passing on hypergraphs has been a standard framework for learning higher-order correlations between hypernodes. Recently-proposed hypergraph neural networks (HGNNs) can be categorized into spatial and spectral methods based on their design choices. In this work, we analyze the impact of change in hypergraph topology on the suboptimal performance of HGNNs and propose DPHGNN, a novel dual-perspective HGNN that introduces equivariant operator learning to capture lower-order semantics by inducing topology-aware spatial and spectral inductive biases. DPHGNN employs a unified framework to dynamically fuse lower-order explicit feature representations from the underlying graph into the super-imposed hypergraph structure. We benchmark DPHGNN over eight benchmark hypergraph datasets for the semi-supervised hypernode classification task and obtain superior performance compared to seven state-of-the-art baselines. We also provide a theoretical framework and a synthetic hypergraph isomorphism test to express the power of spatial HGNNs and quantify the expressivity of DPHGNN beyond the Generalized Weisfeiler Leman (1-GWL) test. Finally, DPHGNN was deployed by our partner e-commerce company, Meesho for the Return-to-Origin (RTO) prediction task, which shows ~7% higher macro F1-Score than the best baseline.",KDD
"Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks. Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances. Recently, diffusion-based methods have shown advanced generative capabilities. However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach. TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part. We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part. Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering. We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data.",KDD
"Temporal Graph Neural Networks (TGNN) have the ability to capture both the graph topology and dynamic dependencies of interactions within a graph over time. There has been a growing need to explain the predictions of TGNN models due to the difficulty in identifying how past events influence their predictions. Since the explanation model for a static graph cannot be readily applied to temporal graphs due to its inability to capture temporal dependencies, recent studies proposed explanation models for temporal graphs. However, existing explanation models for temporal graphs rely on post-hoc explanations, requiring separate models for prediction and explanation, which is limited in two aspects: efficiency and accuracy of explanation. In this work, we propose a novel built-in explanation framework for temporal graphs, called Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck (TGIB). TGIB provides explanations for event occurrences by introducing stochasticity in each temporal event based on the Information Bottleneck theory. Experimental results demonstrate the superiority of TGIB in terms of both the link prediction performance and explainability compared to state-of-the-art methods. This is the first work that simultaneously performs prediction and explanation for temporal graphs in an end-to-end manner. The source code of TGIB is available at https://github.com/sang-woo-seo/TGIB.",KDD
"Graph partitioning aims to divide a graph into k disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. Conventional methods, like approximation algorithms or heuristics, are designed for distinct partitioning objectives and fail to achieve generalization across other important partitioning objectives. Recently machine learning-based methods have been developed that learn directly from data. Further, these methods have a distinct advantage of utilizing node features that carry additional information. However, these methods assume differentiability of target partitioning objective functions and cannot generalize for an unknown number of partitions, i.e., they assume the number of partitions is provided in advance. In this study, we develop NeuroCut with two key innovations over previous methodologies. First, by leveraging a reinforcement learning-based framework over node representations derived from a graph neural network and positional features, NeuroCut can accommodate any optimization objective, even those with non-differentiable functions. Second, we decouple the parameter space and the partition count making NeuroCut inductive to any unseen number of partition, which is provided at query time. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits strong generalization to unseen partition count.",KDD
"Deep visual graph matching (GM) is a challenging combinatorial task that involves finding a permutation matrix that indicates the correspondence between keypoints from a pair of images. Like many learning systems, empirical studies have shown that visual GM is susceptible to adversarial attacks, with reliability issues in downstream applications. To the best of our knowledge, certifying robustness for deep visual GM remains an open challenge with two main difficulties: how to handle the paired inputs together with the heavily non-linear permutation output space (especially at large scale), and how to balance the trade-off between certified robustness and matching performance.
Inspired by the randomized smoothing (RS) technique, we propose the Certified Robustness based on the Optimal Smoothing Range Search (CR-OSRS) technique to fulfill the robustness guarantee for deep visual GM. First, unlike conventional RS methods that use isotropic Gaussian distributions for smoothing, we build the smoothed model with paired joint Gaussian distributions, which capture the structural information among keypoints, and mitigate the performance degradation caused by smoothing. For the vast space of the permutation output, we devise a similarity-based partitioning method that can lower the computational complexity and certification difficulty. We then derive a stringent robustness guarantee that links the certified space of inputs to their corresponding fixed outputs. Second, we design a global optimization method to search for optimal joint Gaussian distributions and facilitate a larger certified space and better performance. Third, we apply data augmentation and a similarity-based regularizer in training to enhance smoothed model performance. Lastly, for the high-dimensional and multivariable nature of the certified space, we propose two methods (sampling and marginal radii) to evaluate it. Experimental results on public benchmarks show that our method achieves state-of-the-art certified robustness.",KDD
"In offline Imitation Learning (IL), one of the main challenges is the covariate shift between the expert observations and the actual distribution encountered by the agent, because it is difficult to determine what action an agent should take when outside the state distribution of the expert demonstrations. Recently, the model-free solutions introduced supplementary data and identified the latent expert-similar samples to augment the reliable samples during learning. Model-based solutions build forward dynamic models with conservatism quantification and then generate additional trajectories in the neighborhood of expert demonstrations. However, without reward supervision, these methods are often over-conservative in the out-of-expert-support regions, because only in states close to expert-observed states can there be a preferred action enabling policy optimization. To encourage more exploration on expert-unobserved states, we propose a novel model-based framework, called offline Imitation Learning with Self-paced Reverse Augmentation (SRA). Specifically, we build a reverse dynamic model from the offline demonstrations, which can efficiently generate trajectories leading to the expert-observed states in a self-paced style. Then, we use the subsequent reinforcement learning method to learn from the augmented trajectories and transit from expert-unobserved states to expert-observed states. This framework not only explores the expert-unobserved states but also guides maximizing long-term returns on these states, ultimately enabling generalization beyond the expert data. Empirical results show that our proposal could effectively mitigate the covariate shift and achieve the state-of-the-art performance on the offline imitation learning benchmarks. Project website: https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/.",KDD
"Accuracy and timeliness are indeed often conflicting goals in prediction tasks. Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful. In applications such as wildfires, crimes, and traffic jams, timely forecasting are vital for safeguarding human life and property. Consequently, finding a balance between accuracy and timeliness is crucial. In this paper, we propose an early spatio-temporal forecasting model based on Multi-Objective reinforcement learning that can either implement an optimal policy given a preference or infer the preference based on a small number of samples. The model addresses two primary challenges: 1) enhancing the accuracy of early forecasting and 2) providing the optimal policy for determining the most suitable prediction time for each area. Our method demonstrates superior performance on three large-scale real-world datasets, surpassing existing methods in early spatio-temporal forecasting tasks.",KDD
"Cognitive diagnosis is a vital upstream task in intelligent education systems. It models the student-exercise interaction, aiming to infer the students' proficiency levels on each knowledge concept. This paper observes that most existing methods can hardly effectively capture the homogeneous influence due to its inherent complexity. That is to say, although students exhibit similar performance on given exercises, their proficiency levels inferred by these methods vary significantly, resulting in shortcomings in interpretability and efficacy. Given the complexity of homogeneous influence, a hypergraph could be a choice due to its flexibility and capability of modeling high-order similarity which aligns with the nature of homogeneous influence. However, before incorporating hypergraph, one at first needs to address the challenges of distorted homogeneous influence, sparsity of response logs, and over-smoothing. To this end, this paper proposes a hypergraph cognitive diagnosis model (HyperCDM) to address these challenges and effectively capture the homogeneous influence. Specifically, to avoid distortion, HyperCDM employs a divide-and-conquer strategy to learn student, exercise and knowledge representations in their own hypergraphs respectively, and interconnects them via a feature-based interaction function. To construct hypergraphs based on sparse response logs, the auto-encoder is utilized to preprocess response logs and K-means is applied to cluster students. To mitigate over-smoothing, momentum hypergraph convolution networks are designed to partially keep previous representations during the message propagation. Extensive experiments on both offline and online real-world datasets show that HyperCDM achieves state-of-the-art performance in terms of interpretability and capturing homogeneous influence effectively, and is competitive in generalization. The ablation study verifies the efficacy of each component, and the case study explicitly showcases the homogeneous influence captured by HyperCDM.",KDD
"Despite the recent progress of molecular representation learning, its effectiveness is assumed on the close-world assumptions that training and testing graphs are from identical distribution. The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions. The misleading estimations of molecules' properties in drug screening or design can result in the tremendous waste of wet-lab resources and delay the discovery of novel therapies. Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model. In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs. Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection. Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges. First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure. Second, the generative model involving iterative denoising steps is notoriously time-consuming especially when it runs on the enormous pool of drugs. To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOd Detection, dubbed as PGR-MOOD. Specifically, PGR-MOOD hinges on three innovations: i) An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules according to their discrete edges and continuous node features; ii) A creative graph generator to construct a list of prototypical graphs that are in line with ID distribution but away from OOD one; iii) An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule. Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority: PGR-MOOD achieves more than 8% of average improvement in terms of detection AUC and AUPR accompanied by the reduced cost of testing time and memory consumption. The anonymous code is in: https://github.com/se7esx/PGR-MOOD.",KDD
"Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, these optimizations do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for memory-based TGNNs that maximizes training throughput while maintaining model accuracy. Our design specifically addresses the unique challenges associated with fetching and updating node memory states in MTGNNs by integrating staleness into the memory module. However, simply introducing a predefined staleness bound in the memory module to break temporal dependencies may lead to suboptimal performance and lack of generalizability across different models and datasets. To overcome this, we introduce an online pipeline scheduling algorithm in MSPipe that strategically breaks temporal dependencies with minimal staleness and delays memory fetching to obtain fresher memory states. This is achieved without stalling the MTGNN training stage or causing resource contention. Additionally, we design a staleness mitigation mechanism to enhance training convergence and model accuracy. Furthermore, we provide convergence analysis and demonstrate that MSPipe maintains the same convergence rate as vanilla sampling-based GNN training. Experimental results show that MSPipe achieves up to 2.45Ã— speed-up without sacrificing accuracy, making it a promising solution for efficient MTGNN training. The implementation of our paper can be found at the following link: https://github.com/PeterSH6/MSPipe.",KDD
"Pre-trained vision-language models like CLIP have shown powerful zero-shot inference ability via image-text matching and prove to be strong few-shot learners in various downstream tasks. However, in real-world scenarios, adapting CLIP to downstream tasks may encounter the following challenges: 1) data may exhibit long-tailed data distributions and might not have abundant samples for all the classes; 2) There might be emerging tasks with new classes that contain no samples at all. To overcome them, we propose a novel framework to achieve efficient and long-tailed generalization, which can be termed as Candle. During the training process, we propose compensating logit-adjusted loss to encourage large margins of prototypes and alleviate imbalance both within the base classes and between the base and new classes. For efficient adaptation, we treat the CLIP model as a black box and leverage the extracted features to obtain visual and textual prototypes for prediction. To make full use of multi-modal information, we also propose cross-modal attention to enrich the features from both modalities. For effective generalization, we introduce virtual prototypes for new classes to make up for their lack of training images. Candle achieves state-of-the-art performance over extensive experiments on 11 diverse datasets while substantially reducing the training time, demonstrating the superiority of our approach. The source code is available at https://github.com/shijxcs/Candle.",KDD
"Previous works for time series classification tend to assume that both the training and testing sets originate from the same distribution. This oversimplification deviates from the complexity of reality and makes it challenging to generalize methods to out-of-distribution (OOD) time series data. Currently, there are limited works focusing on time series OOD generalization, and they typically disentangle time series into domain-agnostic and domain-specific features and design tasks to intensify the distinction between the two. However, previous models purportedly yielding domain-agnostic features continue to harbor domain-specific information, thereby diminishing their adaptability to OOD data. To address this gap, we introduce a novel model called Invariant Time Series Representation (ITSR). ITSR achieves a learnable orthogonal decomposition of time series using two sets of orthogonal axes. In detail, ITSR projects time series onto these two sets of axes separately and obtains mutually orthogonal invariant features and relevant features. ITSR theoretically ensures low similarity between these two features and further incorporates various tasks to optimize them. Furthermore, we explore the benefits of preserving orthogonality between invariant and relevant features for OOD time series classification in theory. The results on four real-world datasets underscore the superiority of ITSR over state-of-the-art methods and demonstrate the critical role of maintaining orthogonality between invariant and relevant features. Our code is available at https://github.com/CGCL-codes/ITSR.",KDD
"Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a ""pairwise encoding"" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a new method, LPFormer, which attempts to adaptively learn the pairwise encodings for each link. LPFormer models the link factors via an attention module that learns the pairwise encoding that exists between nodes by modeling multiple factors integral to link prediction. Extensive experiments demonstrate that LPFormer can achieve SOTA performance on numerous datasets while maintaining efficiency. The code is available at The code is available at https://github.com/HarryShomer/LPFormer.",KDD
"Interpretability of Deep Neural Networks using concept-based models offers a promising way to explain model behavior through human understandable concepts. A parallel line of research focuses on disentangling the data distribution into its underlying generative factors, in turn explaining the data generation process. While both directions have received extensive attention, little work has been done on explaining concepts in terms of generative factors to unify mathematically disentangled representations and human-understandable concepts as an explanation for downstream tasks. In this paper, we propose a novel method CoLiDR - which utilizes a disentangled representation learning setup for learning mutually independent generative factors and subsequently learns to aggregate the said representations into human-understandable concepts using a novel aggregation/decomposition module. Experiments are conducted on datasets with both known and unknown latent generative factors. Our method successfully aggregates disentangled generative factors into concepts while maintaining parity with state-of-the-art concept-based approaches. Quantitative and visual analysis of the learned aggregation procedure demonstrates the advantages of our work compared to commonly used concept-based models over four challenging datasets. Lastly, our work is generalizable to an arbitrary number of concepts and generative factors - making it flexible enough to be suitable for various types of data.",KDD
"Adapting large language models (LLMs) to unseen tasks with incontext training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only performs well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in metatraining literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches.",KDD
"While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes detection a difficult task. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We also show that tokens preceding a hallucination can already predict the subsequent hallucination even before it occurs.",KDD
"Data visualization (DV) has become the prevailing tool in the market due to its effectiveness into illustrating insights in vast amounts of data. To lower the barrier of using DVs, automatic DV tasks, such as natural language question (NLQ) to visualization translation (formally called text-to-vis), have been investigated in the research community. However, text-to-vis assumes the NLQ to be well-organized and expressed in a single sentence. However, in real-world settings, complex DV is needed through consecutive exchanges between the DV system and the users. In this paper, we propose a new task named CoVis, short for <u>Co</u>nversational text-to-<u>Vis</u>ualization, aiming at constructing DVs through a series of interactions between users and the system. Since it is the task which has not been studied in the literature, we first build a benchmark dataset named Dial-NVBench, including dialogue sessions with a sequence of queries from a user and responses from the system. The ultimate goal of each dialogue session is to create a suitable DV. However, this process can contain diverse dialogue queries, such as seeking information about the dataset, manipulating parts of the data, and visualizing the data. Then, we propose a multi-modal neural network named MMCoVisNet to answer these DV-related queries. In particular, MMCoVisNet first fully understands the dialogue context and determines the corresponding responses. Then, it uses adaptive decoders to provide the appropriate replies: (i) a straightforward text decoder is used to produce general responses, (ii) an SQL-form decoder is applied to synthesize data querying responses, and (iii) a DV-form decoder tries to construct the appropriate DVs. We comparatively evaluate MMCoVisNet with other baselines over our proposed benchmark dataset. Experimental results validate that MMCoVisNet performs better than existing baselines and achieves a state-of-the-art performance.",KDD
"Cross-Domain Recommendation (CDR) is a promising technique to alleviate data sparsity by transferring knowledge across domains. However, the negative transfer issue in the presence of numerous domains has received limited attention. Most existing methods transfer all information from source domains to the target domain without distinction. This introduces harmful noise and irrelevant features, resulting in suboptimal performance. Although some methods decompose user features into domain-specific and domain-shared components, they fail to consider other causes of negative transfer. Worse still, we argue that simple feature decomposition is insufficient for multi-domain scenarios. To bridge this gap, we propose TrineCDR, the TRIple-level kNowledge transferability Enhanced model for multi-target CDR. Unlike previous methods, TrineCDR captures single domain and targeted cross-domain embeddings to serve multi-domain recommendation. For the latter, we identify three fundamental causes of negative transfer, ranging from micro to macro perspectives, and correspondingly enhance knowledge transferability at three different levels: the feature level, the interaction level, and the domain level. Through these efforts, TrineCDR effectively filters out noise and irrelevant information from source domains, leading to more comprehensive and accurate representations in the target domain. We extensively evaluate the proposed model on real-world datasets, sampled from Amazon and Douban, under both dual-target and multi-target scenarios. The experimental results demonstrate the superiority of TrineCDR over state-of-the-art cross-domain recommendation methods.",KDD
"The forest matrix plays a crucial role in network science, opinion dynamics, and machine learning, offering deep insights into the structure of and dynamics on networks. In this paper, we study the problem of querying entries of the forest matrix in evolving graphs, which more accurately represent the dynamic nature of real-world networks compared to static graphs. To address the unique challenges posed by evolving graphs, we first introduce two approximation algorithms, SFQ and SFQPlus, for static graphs. SFQ employs a probabilistic interpretation of the forest matrix, while SFQPlus incorporates a novel variance reduction technique and is theoretically proven to offer enhanced accuracy. Based on these two algorithms, we further devise two dynamic algorithms centered around efficiently maintaining a list of spanning converging forests. This approach ensures O(1) runtime complexity for updates, including edge additions and deletions, as well as for querying matrix elements, and provides an unbiased estimation of forest matrix entries. Finally, through extensive experiments on various real-world networks, we demonstrate the efficiency and effectiveness of our algorithms. Particularly, our algorithms are scalable to massive graphs with more than forty million nodes.",KDD
"The recent advancements in Traffic Signal Control (TSC) have highlighted the potential of Reinforcement Learning (RL) as a promising solution to alleviate traffic congestion. Current research in this area primarily concentrates on either online or offline learning strategies, aiming to create optimized policies for specific cities. Nevertheless, the transferability of these policies to new cities is impeded by constraints such as the limited availability of high-quality data and the expensive and risky exploration process. To this end, in this paper, we present an innovative cross-city Traffic Signal Control (TSC) paradigm called CrossLight. Our approach involves meta training using offline data from source cities and adaptively fine-tuning in the target city. This novel methodology aims to address the challenges of transferring TSC policies across different cities effectively. In our proposed approach, we start by acquiring meta-decision pattern knowledge through trajectory dynamics reconstruction via pre-training in source cities. To address disparities in road network topologies between cities, we dynamically construct city topological structures based on the extracted meta-knowledge during the offline meta-training phase. These structures are then used to distill pattern-structure aware representations of decision trajectories from the source cities. To identify effective initial parameters for the learnable components, we employ the Model-Agnostic Meta-Learning (MAML) framework, a popular meta-learning approach. During adaptive fine-tuning in the target city, we introduce a replay buffer that is iteratively updated using online interactions with a rank and filter mechanism. This mechanism, along with a carefully designed exploration strategy, ensures a balance between exploitation and exploration, thereby fostering both the diversity and quality of the trajectories for fine-tuning. Finally, extensive experiments across four cities validate that CrossLight achieves comparable performance in new cities with minimal fine-tuning iterations, surpassing both existing online and offline methods. This success underscores that our CrossLight framework emerges as a groundbreaking and potent paradigm, offering a feasible and effective solution to the intelligent transportation community.",KDD
"Large Language Models (LLMs) have demonstrated efficacy in various domains, but deploying these models is economically challenging due to extensive parameter counts. Numerous efforts have been dedicated to reducing the parameter count of these models without compromising performance, employing a technique known as model pruning. Conventional pruning methods assess the significance of weights within individual layers and typically apply uniform sparsity levels across all layers, potentially neglecting the varying significance of each layer. To address this oversight, we first propose a dual-assessment driven pruning strategy that employs both intra-layer metric and global performance metric to comprehensively evaluate the impact of pruning. Then our method leverages an iterative optimization algorithm to find the optimal layer-wise sparsity distribution, thereby minimally impacting model performance. Extensive benchmark evaluations on state-of-the-art LLM architectures such as LLaMAv2 and OPT across a variety of NLP tasks demonstrate the effectiveness of our approach. When applied to the LLaMaV2-7B model with an overall pruning sparsity of 80%, our method achieves a 50% reduction in perplexity compared to the benchmark. The results indicate that our method significantly outperforms existing state-of-the-art methods in preserving performance after pruning.",KDD
"Next location prediction is a crucial task in human mobility modeling, and is pivotal for many downstream applications like location-based recommendation and transportation planning. Although there has been a large body of research tackling this problem, the usefulness of user preference and temporal regularity remains underrepresented. Specifically, previous studies usually neglect the explicit user preference information entailed from human trajectories and fall short in utilizing the arrival time of next location, as a key determinant on next location. To address these limitations, we propose a Multi-Context aware Location Prediction model (MCLP) to predict next locations for individuals, where it explicitly models user preference and the next arrival time as context. First, we utilize a topic model to extract user preferences for different types of locations from historical human trajectories. Second, we develop an arrival time estimator to construct a robust arrival time embedding based on the multi-head attention mechanism. The two components provide pivotal contextual information for the subsequent prediction. Finally, we utilize the Transformer architecture to mine sequential patterns and integrate multiple contextual information to predict the next locations. Experimental results on two real-world mobility datasets show that our proposed MCLP outperforms baseline methods.",KDD
"This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck approaches, have achieved partial success but struggle to overcome simplicity bias, often reinforcing spurious correlations. To tackle this, our study introduces a new learning paradigm for graph OOD issue. We propose DIVE, training a collection of models to focus on all label-predictive subgraphs by encouraging the models to foster divergence on the subgraph mask, which circumvents the limitation of a model solely focusing on the subgraph corresponding to simple structural patterns. Specifically, we employs a regularizer to punish overlap in extracted subgraphs across models, thereby encouraging different models to concentrate on distinct structural patterns. Model selection for robust OOD performance is achieved through validation accuracy. Tested across four datasets from GOOD benchmark and one dataset from DrugOOD benchmark, our approach demonstrates significant improvement over existing methods, effectively addressing the simplicity bias and enhancing generalization in graph machine learning.",KDD
"Social Recommendation (SR) typically exploits neighborhood influence in the social network to enhance user preference modeling. However, users' intricate social behaviors may introduce noisy social connections for user modeling and harm the models' robustness. Existing solutions to alleviate social noise either filter out the noisy connections or generate new potential social connections. Due to the absence of labels, the former approaches may retain uncertain connections for user preference modeling while the latter methods may introduce additional social noise. Through data analysis, we discover that (1) social noise likely comes from the connected users with low preference similarity; and (2) Opinion Leaders (OLs) play a pivotal role in influence dissemination, surpassing high-similarity neighbors, regardless of their preference similarity with trusting peers. Guided by these observations, we propose a novel Self-Supervised Denoising approach through Independent Cascade Graph Augmentation, for more robust SR. Specifically, we employ the independent cascade diffusion model to generate an augmented graph view, which traverses the social graph and activates the edges in sequence to simulate the cascading influence spread. To steer the augmentation towards a denoised social graph, we (1) introduce a hierarchical contrastive loss to prioritize the activation of OLs first, followed by high-similarity neighbors, while weakening the low-similarity neighbors; and (2) integrate an information bottleneck based contrastive loss, aiming to minimize mutual information between original and augmented graphs yet preserve sufficient information for improved SR. Experiments conducted on two public datasets demonstrate that our model outperforms the state-of-the-art while also exhibiting higher robustness to different extents of social noise.",KDD
"Traditional Neural Processes (NPs) and their variants aim to learn relationships between context sample points but do not consider multi-level information, resulting in a limited ability to learn complex distributions.This paper draws inspiration from features such as the hierarchical nature and interpretability of tree-like structures. This paper proposes a Hierarchical Linear Symbolized Tree-structured Neural Processes (HLNPs) architecture. This framework utilizes variables to build a top-down hierarchical linear symbolized tree-structured network architecture, enhancing positional representation information in a hierarchical manner along the deterministic path. In the latent distribution, the hierarchical linear symbolized tree-structured network approximates functions discretely through a layered approach. By decomposing the latent complex distribution into several simpler sub-problems using sum and product symbols, the upper bound of optimization is thereby increased. The tree structure discretizes variables to capture model uncertainty in the form of entropy. This approach also imparts a causal effect to the HLNPs model. Finally, we demonstrate the effectiveness of the HLNPs models for 1D data, Bayesian optimization, and 2D data.",KDD
"The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a naÃ¯ve implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the proximal gradient descent and a graph mining tree search. Our pruning strategy can ensure that the quality of the solution is maintained compared to the result without pruning. We empirically demonstrate that LAGRA has superior or comparable prediction performance to the standard existing algorithms including graph neural networks, while using only a small number of AGs in an interpretable manner.",KDD
"Structural bias or segregation of networks refers to situations where two or more disparate groups are present in the network, so that the groups are highly connected internally, but loosely connected to each other. Examples include polarized communities in social networks, antagonistic content in video-sharing or news-feed platforms, etc. In many cases it is of interest to increase the connectivity of disparate groups so as to, e.g., minimize social friction, or expose individuals to diverse viewpoints. A commonly-used mechanism for increasing the network connectivity is to add edge shortcuts between pairs of nodes. In many applications of interest, edge shortcuts typically translate to recommendations, e.g., what video to watch, or what news article to read next. The problem of reducing structural bias or segregation via edge shortcuts has recently been studied in the literature, and random walks have been an essential tool for modeling navigation and connectivity in the underlying networks. Existing methods, however, either do not offer approximation guarantees, or engineer the objective so that it satisfies certain desirable properties that simplify the optimization task.
In this paper we address the problem of adding a given number of shortcut edges in the network so as to directly minimize the average hitting time and the maximum hitting time between two disparate groups. The objectives we study are more natural than objectives considered earlier in the literature (e.g., maximizing hitting-time reduction) and the optimization task is significantly more challenging. Our algorithm for minimizing average hitting time is a greedy bicriteria that relies on supermodularity. In contrast, maximum hitting time is not supermodular. Despite, we develop an approximation algorithm for that objective as well, by leveraging connections with average hitting time and the asymmetric k-center problem.",KDD
"The detection of fake news has received increasing attention over the past few years, but there are more subtle ways of deceiving one's audience. In addition to the content of news stories, their presentation can also be made misleading or biased. In this work, we study the impact of the ordering of news stories on audience perception. We introduce the problems of detecting cherry-picked news orderings and maximizing neutrality in news orderings. We prove hardness results and present several algorithms for approximately solving these problems. Furthermore, we provide extensive experimental results and present evidence of potential cherry-picking in the real world.",KDD
"In today's digital world, interaction with online platforms is ubiquitous, and thus content moderation is important for protecting users from content that do not comply with pre-established community guidelines. Given the vast volume of content generated online daily, having an efficient content moderation system throughout every stage of planning is particularly important. We study the short-term planning problem of allocating human content reviewers to different harmful content categories. We use tools from fair division and study the application of competitive equilibrium and leximin allocation rules for addressing this problem. On top of the traditional Fisher market setup, we additionally incorporate novel aspects that are of practical importance. The first aspect is the forecasted workload of different content categories, which puts constraints on the allocation chosen by the planner. We show how a formulation that is inspired by the celebrated Eisenberg-Gale program allows us to find an allocation that not only satisfies the forecasted workload, but also fairly allocates the remaining working hours from the content reviewers among all content categories. A fair allocation of oversupply provides a guardrail in cases where the actual workload deviates from the predicted workload. The second practical consideration is time dependent allocation that is motivated by the fact that partners need scheduling guidance for the reviewers across days to achieve efficiency. To address the time component, we introduce new extensions of the various fair allocation approaches for the single-time period setting, and we show that many properties extend in essence, albeit with some modifications. Lastly, related to the time component, we additionally investigate how to satisfy markets' desire for smooth allocation (i.e, an allocation that does not vary much from time to time) so that the switch in staffing is minimized. We demonstrate the performance of our proposed approaches through real-world data obtained from Meta.",KDD
"Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downstream tasks. We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowing the model to exploit the inter-relations between them. Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing with corrupted samples that are similar to the ones used for training. However, all these methods still suffer from the token distribution shift induced by typos. In this work, we propose to tackle textual noise by equipping cross-encoders with a novel LExical-aware Attention module (LEA) that incorporates lexical similarities between words in both sentences. By using raw text similarities, our approach avoids the tokenization shift problem obtaining improved robustness. We demonstrate that the attention bias introduced by LEA helps cross-encoders to tackle complex scenarios with textual noise, specially in domains with short-text descriptions and limited context. Experiments using three popular Transformer encoders in five e-commerce datasets for product matching show that LEA consistently boosts performance under the presence of noise, while remaining competitive on the original (clean) splits. We also evaluate our approach in two datasets for textual entailment and paraphrasing showing that LEA is robust to typos in domains with longer sentences and more natural context. Additionally, we thoroughly analyze several design choices in our approach, providing insights about the impact of the decisions made and fostering future research in cross-encoders dealing with typos.",KDD
"School choice mechanism designers use discrete choice models to understand and predict families' preferences. The most widely-used choice model, the multinomial logit (MNL), is linear in school and/or household attributes. While the model is simple and interpretable, it assumes the ranked preference lists arise from a choice process that is uniform throughout the ranking, from top to bottom. In this work, we introduce two strategies for rank-heterogeneous choice modeling tailored for school choice. First, we adapt a context-dependent random utility model (CDM), considering down-rank choices as occurring in the context of earlier up-rank choices. Second, we consider stratifying the choice modeling by rank, regularizing rank-adjacent models towards one another when appropriate. Using data on household preferences from the San Francisco Unified School District (SFUSD) across multiple years, we show that the contextual models considerably improve our out-of-sample evaluation metrics across all rank positions over the non-contextual models in the literature. Meanwhile, stratifying the model by rank can yield more accurate first-choice predictions while down-rank predictions are relatively unimproved. These models provide performance upgrades that school choice researchers can adopt to improve predictions and counterfactual analyses.",KDD
"A complex logic query in a knowledge graph refers to a query expressed in logic form that conveys a complex meaning, such as where did the Canadian Turing award winner graduate from? Knowledge graph reasoning-based applications, such as dialogue systems and interactive search engines, rely on the ability to answer complex logic queries as a fundamental task. In most knowledge graphs, edges are typically used to either describe the relationships between entities or their associated attribute values. An attribute value can be in categorical or numerical format, such as dates, years, sizes, etc. However, existing complex query answering (CQA) methods simply treat numerical values in the same way as they treat entities. This can lead to difficulties in answering certain queries, such as which Australian Pulitzer award winner is born before 1927, and which drug is a pain reliever and has fewer side effects than Paracetamol. In this work, inspired by the recent advances in numerical encoding and knowledge graph reasoning, we propose numerical complex query answering. In this task, we introduce new numerical variables and operations to describe queries involving numerical attribute values. To address the difference between entities and numerical values, we also propose the framework of Number Reasoning Network (NRN) for alternatively encoding entities and numerical values into separate encoding structures. During the numerical encoding process, NRN employs a parameterized density function to encode the distribution of numerical values. During the entity encoding process, NRN uses established query encoding methods for the original CQA problem. Experimental results show that NRN consistently improves various query encoding methods on three different knowledge graphs and achieves state-of-the-art results.",KDD
"We study the classic machine learning problem of logistic regression with differential privacy (DP), under the distributed setting. While logistic regression with DP has been extensively studied in the literature, most of the research is focused on the centralized setting, where a centralized server is trusted with the entire private training dataset. However, in many real-world scenarios (e.g., federated learning), the data is distributed among multiple clients who may not trust others, including clients and the server. While the server tries to learn a model using the clients' private datasets, the clients should provide each individual record in their local datasets with a formal privacy guarantee.
Towards this end, we propose a general mechanism for logistic regression with DP under the distributed setting, based on output perturbation. We show that our solution satisfies differential privacy and enjoys privacy amplification by secure aggregation, a recent technique for DP under the distributed setting. In addition, our solution also incurs much lower communication costs (which is considered as a huge overhead in federated learning), compared with existing ones. In particular, our solution requires the clients to communicate only once throughout the entire FL process. Finally, we provide experimental results on real-world datasets to demonstrate the effectiveness of our solution.",KDD
"Despite the popularity of density-based clustering, its procedural definition makes it difficult to analyze compared to clustering methods that minimize a loss function. In this paper, we reformulate DBSCAN through a clean objective function by introducing the density-connectivity distance (dc-dist), which captures the essence of density-based clusters by endowing the minimax distance with the concept of density. This novel ultrametric allows us to show that DBSCAN, k-center, and spectral clustering are equivalent in the space given by the dc-dist, despite these algorithms being perceived as fundamentally different in their respective literatures. We also verify that finding the pairwise dc-dists gives DBSCAN clusterings across all epsilon-values, simplifying the problem of parameterizing density-based clustering. We conclude by thoroughly analyzing density-connectivity and its properties -- a task that has been elusive thus far in the literature due to the lack of formal tools. Our code recreates every experiment below: https://github.com/Andrew-Draganov/dc_dist",KDD
"Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose 4 online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on 4 real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect graph anomalies in constant memory and time.",KDD
"In this paper, we describe a new algorithm called Preferential Attac hment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others-the very things we seek to prevent.
PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to make friends by strategically targeting their initial friend requests after joining a major social network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth.
We leverage this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions.
This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance at scale on the global Facebook network, allowing it to detect fake accounts before standard methods apply and at lower computational cost. Specifically, PreAttacK converges to informative classifications (AUCâ‰ˆ0.9) after new accounts send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art network-based algorithms do not obtain this performance even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (i.e. accepted friend request) with a human.",KDD
"Graphs are a powerful mathematical model, and they are used to represent real-world structures in various fields. In many applications, real-world structures with high connectivity and robustness are preferable. For enhancing the connectivity and robustness of graphs, two operations, adding edges and anchoring nodes, have been extensively studied. However, merging nodes, which is a realistic operation in many scenarios (e.g., bus station reorganization, multiple team formation), has been overlooked. In this work, we study the problem of improving graph cohesiveness by merging nodes. First, we formulate the problem mathematically using the size of the k-truss, for a given k, as the objective. Then, we prove the NP-hardness and non-modularity of the problem. After that, we develop BATMAN, a fast and effective algorithm for choosing sets of nodes to be merged, based on our theoretical findings and empirical observations. Lastly, we demonstrate the superiority of BATMAN over several baselines, in terms of speed and effectiveness, through extensive experiments on fourteen real-world graphs.",KDD
"Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Meanwhile, rapidly developing deep learning methods offer a wide range of opportunities for better modeling brain signals, which has attracted considerable research efforts recently. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, in this paper, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose MBrain to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifically, we represent the spatial correlation by a graph structure, which is built with proposed multi-channel CPC. We theoretically prove that optimizing the goal of multi-channel CPC can lead to a better predictive representation and apply the instantaneou-time-shift prediction task based on it. Then we capture the temporal correlation by designing the delayed-time-shift prediction task. Finally, replace-discriminative-learning task is proposed to preserve the characteristics of each channel. Extensive experiments of seizure detection on both EEG and SEEG large-scale real-world datasets demonstrate that our model outperforms several state-of-the-art time series SSL and unsupervised models, and has the ability to be deployed to clinical practice.",KDD
"In recent years, graph pre-training has gained significant attention, focusing on acquiring transferable knowledge from unlabeled graph data to improve downstream performance. Despite these recent endeavors, the problem of negative transfer remains a major concern when utilizing graph pre-trained models to downstream tasks. Previous studies made great efforts on the issue of what to pre-train and how to pre-train by designing a variety of graph pre-training and fine-tuning strategies. However, there are cases where even the most advanced ""pre-train and fine-tune"" paradigms fail to yield distinct benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN first fits the pre-training data into graphon bases, each element of graphon basis (i.e., a graphon) identifies a fundamental transferable pattern shared by a collection of pre-training graphs. All convex combinations of graphon bases give rise to a generator space, from which graphs generated form the solution space for those downstream data that can benefit from pre-training. In this manner, the feasibility of pre-training can be quantified as the generation probability of the downstream data from any generator in the generator space. W2PGNN offers three broad applications: providing the application scope of graph pre-trained models, quantifying the feasibility of pre-training, and assistance in selecting pre-training data to enhance downstream performance. We provide a theoretically sound solution for the first application and extensive empirical justifications for the latter two applications.",KDD
"Recent awareness of privacy protection and compliance requirement resulted in a controversial view of recommendation system due to personal data usage. Therefore, privacy-protected recommendation emerges as a novel research direction. In this paper, we first formulate this problem as a vertical federated learning problem, i.e., features are vertically distributed over different departments. We study a contextual bandit learning problem for recommendation in the vertical federated setting. To this end, we carefully design a customized encryption scheme named orthogonal matrix-based mask mechanism (O3M). O3M mechanism, a tailored component for contextual bandits by carefully exploiting their shared structure, can ensure privacy protection while avoiding expensive conventional cryptographic techniques. We further apply the mechanism to two commonly-used bandit algorithms, LinUCB and LinTS, and instantiate two practical protocols for online recommendation. The proposed protocols can perfectly recover the service quality of centralized bandit algorithms while achieving a satisfactory runtime efficiency, which is theoretically proved and analysed in this paper. By conducting extensive experiments on both synthetic and real-world datasets, we show the superiority of the proposed method in terms of privacy protection and recommendation performance.",KDD
"Coreset selection is a technique for efficient machine learning, which selects a subset of the training data to achieve similar model performance as using the full dataset. It can be performed with or without training machine learning models. Coreset selection with training, which iteratively trains the machine model and updates data items in the coreset, is time consuming. Coreset selection without training can select the coreset before training. Gradient approximation is the typical method, but it can also be slow when dealing with large training datasets as it requires multiple iterations and pairwise distance computations for each iteration. The state-of-the-art (SOTA) results w.r.t. effectiveness are achieved by the latter approach, i.e. gradient approximation.
In this paper, we aim to significantly improve the efficiency of coreset selection while ensuring good effectiveness, by improving the SOTA approaches of using gradient descent without training machine learning models. Specifically, we present a highly efficient coreset selection framework that utilizes an approximation of the gradient. This is achieved by dividing the entire training set into multiple clusters, each of which contains items with similar feature distances (calculated using the Euclidean distance). Our framework further demonstrates that the full gradient can be bounded based on the maximum feature distance between each item and each cluster, allowing for more efficient coreset selection by iterating through these clusters. Additionally, we propose an efficient method for estimating the maximum feature distance using the product quantization technique. Our experiments on multiple real-world datasets demonstrate that we can improve the efficiency 3-10 times comparing with SOTA almost without sacrificing the accuracy.",KDD
"A classifier that is accurate on average may still underperform for ""sensitive"" subsets of people. Such subsets could be based on race, gender, age, etc. The goal of a fair classifier is to perform well for all sensitive subsets. But often, the sensitive subsets are not known a priori. So we may want the classifier to perform well on all subsets that are likely to be sensitive. We propose an iterative algorithm called SURE for this problem. In each iteration, SURE identifies high-risk zones in feature space where the risk of unfair classification is statistically significant. By changing the loss function's weights for points from these zones, SURE builds a fair classifier. The emphasis on statistical significance makes SURE robust to noise. The high-risk zones are intuitive and interpretable. Every step of our method is explainable in terms of significance tests. Finally, SURE is fast and parameter-free. Experiments on both simulated and real-world datasets show that SURE is competitive with the state-of-the-art.",KDD
"Anomaly detection (AD) plays an important role in numerous applications. In this paper, we focus on two understudied aspects of AD that are critical for integration into real-world applications. First, most AD methods cannot incorporate labeled data that are often available in practice in small quantities and can be crucial to achieve high accuracy. Second, most AD methods are not interpretable, a bottleneck that prevents stakeholders from understanding the reason behind the anomalies. In this paper, we propose a novel AD framework, DIAD, that adapts a white-box model class, Generalized Additive Models, to detect anomalies using a partial identification objective which naturally handles noisy or heterogeneous features. DIAD can incorporate a small amount of labeled data to further boost AD performances in semi-supervised settings. We demonstrate the superiority of DIAD compared to previous work in both unsupervised and semi-supervised settings on multiple datasets. We also present explainability capabilities of DIAD, on its rationale behind predicting certain samples as anomalies.",KDD
"In large-scale systems, due to system complexity and demand volatility, diverse and dynamic workloads make accurate predictions difficult. In this work, we address an online interval prediction problem (OnPred-Int) and adopt ensemble learning to solve it. We depict that the ensemble learning for OnPred-Int is a dynamic deterministic Markov Decision Process (Dd-MDP) and convert it into a stateful online learning task. Then we propose IPOC, a lightweight and flexible model able to produce effective confidence intervals, adapting the dynamics of real-time workload streams. At each time, IPOC selects a target model and executes chasing for it by a designed chasing oracle, during which process IPOC produces accurate confidence intervals. The effectiveness of IPOCis theoretically validated through sublinear regret analysis and satisfaction of confidence interval requirements. Besides, we conduct extensive experiments on 4 real-world datasets comparing with 19 baselines. To the best of our knowledge, we are the first to apply the frontier theory of online learning to time series prediction tasks.",KDD
"Multimodal spatiotemporal data (MST) consists of multiple simultaneous spatiotemporal modalities that interact with each other in a dynamic manner. Due to the complexity of MST and the recent desire for the explainability of artificial intelligent systems, disentangled representation learning for MST (DisentMST) has become a significant task, which aims to learn disentangled representations that can expose the underlying spatial semantics, temporal dynamic patterns, and inter-modality interaction modes of the complex MST. One limitation of existing approaches is that they might fail to tolerate the real-world incomplete MST data, where missing information might break the cross-modal spatiotemporal dynamics and bring noise and ambiguity to the learning process. Another limitation is that no existing work systematically reveals the structure of different types of disentangled information. To tackle the two limitations, we define a novel two-level hierarchically structured disentanglement task for MST, which reveals informative and structured disentangled representations for MST as well as digests the real-world MST with incompleteness. We propose a new framework, BiDisentMST, which leverages Gaussian Processes and Graph Factorization on the latent space to achieve our purposes. The experimental results demonstrate the effectiveness of our proposed framework compared with baselines with respect to disentanglement and imputation results.",KDD
"Semi-supervised text classification (STC) has been extensively researched and reduces human annotation. However, existing research assuming that unlabeled data only contains in-distribution texts is unrealistic. This paper extends STC to a more practical Open-set Semi-supervised Text Classification (OSTC) setting, which assumes that the unlabeled data contains out-of-distribution (OOD) texts. The main challenge in OSTC is the false positive inference problem caused by inadvertently including OOD texts during training. To address the problem, we first develop baseline models using outlier detectors for hard OOD-data filtering in a pipeline procedure. Furthermore, we propose a Latent Outlier Softening (LOS) framework that integrates semi-supervised training and outlier detection within probabilistic latent variable modeling. LOSÂ softens the OOD impacts by the Expectation-Maximization (EM) algorithm and weighted entropy maximization. Experiments on 3Â created datasets show that LOS significantly outperforms baselines.",KDD
"Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism graphs. Furthermore, the proposed SuperNorm scheme is also demonstrated to alleviate the over-smoothing phenomenon. Experimental results related to predictions of graph, node, and link properties on the eight popular datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/chenchkx/SuperNorm.",KDD
"In this work, we study the problem of finding the maximum value of a non-negative submodular function subject to a limit on the number of items selected, a ubiquitous problem that appears in many applications, such as data summarization and nonlinear regression. We provide the first deterministic, linear-time approximation algorithms for this problem that do not assume the objective is monotone. We present three deterministic, linear-time algorithms: a single-pass streaming algorithm with a ratio of 23.313 + Îµ, which is the first linear-time streaming algorithm; a simpler deterministic linear-time algorithm with a ratio of 11.657; and a (4 + O(Îµ))-approximation algorithm. Finally, we present a deterministic algorithm that obtains ratio of e + Îµ in O_Îµ (n log(n)) time, close to the best known expected ratio of e - 0.121 in polynomial time.",KDD
"Personalized PageRank Vectors are widely used as fundamental graph-learning tools for detecting anomalous spammers, learning graph embeddings, and training graph neural networks. The well-known local FwdPush algorithm[5] approximates PPVs and has a sublinear rate of O(1 over Î±Îµ). A recent study [51] found that when high precision is required, FwdPush is similar to the power iteration method, and its run time is pessimistically bounded by O(m over Î± log 1 over Îµ). This paper looks closely at calculating PPVs for both directed and undirected graphs. By leveraging the linear invariant property, we show that FwdPush is a variant of Gauss-Seidel and propose a Successive Over-Relaxation based method, FwdPushSOR to speed it up by slightly modifying FwdPush. Additionally, we prove FwdPush has local linear convergence rate O(vol (S) over Î± log 1 over Îµ) enjoying advantages of two existing bounds. We also design a new local heuristic push method that reduces the number of operations by 10-50 percent compared to FwdPush. For undirected graphs, we propose two momentum-based acceleration methods that can be expressed as one-line updates and speed up non-acceleration methods by O (1 / âˆš Î±). Our experiments on six real-world graph datasets confirm the efficiency of FwdPushSOR and the acceleration methods for directed and undirected graphs, respectively.",KDD
"We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of undirected graphical theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference performance, respectively.",KDD
"Review-Based Recommender Systems (RBRS) have attracted increasing research interest due to their ability to alleviate well-known cold-start problems. RBRS utilizes reviews to construct the user and items representations. However, in this paper, we argue that such a reliance on reviews may instead expose systems to the risk of being shilled. To explore this possibility, in this paper, we propose the first generation-based model for shilling attacks against RBRSs. Specifically, we learn a fake review generator through reinforcement learning, which maliciously promotes items by forcing prediction shifts after adding generated reviews to the system. By introducing the auxiliary rewards to increase text fluency and diversity with the aid of pre-trained language models and aspect predictors, the generated reviews can be effective for shilling with high fidelity. Experimental results demonstrate that the proposed framework can successfully attack three different kinds of RBRSs on the Amazon corpus with three domains and Yelp corpus. Furthermore, human studies also show that the generated reviews are fluent and informative. Finally, equipped with Attack Review Generators (ARGs), RBRSs with adversarial training are much more robust to malicious reviews.",KDD
"A hypergraph is a data structure composed of nodes and hyperedges, where each hyperedge is an any-sized subset of nodes. Due to the flexibility in hyperedge size, hypergraphs represent group interactions (e.g., co-authorship by more than two authors) more naturally and accurately than ordinary graphs. Interestingly, many real-world systems modeled as hypergraphs contain edge-dependent node labels, i.e., node labels that vary depending on hyperedges. For example, on co-authorship datasets, the same author (i.e., a node) can be the primary author in a paper (i.e., a hyperedge) but the corresponding author in another paper (i.e., another hyperedge).
In this work, we introduce a classification of edge-dependent node labels as a new problem. This problem can be used as a benchmark task for hypergraph neural networks, which recently have attracted great attention, and also the usefulness of edge-dependent node labels has been verified in various applications. To tackle this problem, we propose WHATsNet, a novel hypergraph neural network that represents the same node differently depending on the hyperedges it participates in by reflecting its varying importance in the hyperedges. To this end, WHATsNet models the relations between nodes within each hyperedge, using their relative centrality as positional encodings. In our experiments, we demonstrate that WHATsNet significantly and consistently outperforms ten competitors on six real-world hypergraphs, and we also show successful applications of WHATsNet to (a) ranking aggregation, (b) node clustering, and (c) product return prediction.",KDD
"In a hyper-relational knowledge graph, a triplet can be associated with a set of qualifiers, where a qualifier is composed of a relation and an entity, providing auxiliary information for the triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding them into the transformers, we reduce the computation cost of using transformers. Using HyNT, we can predict missing numeric values in addition to missing entities or relations in a hyper-relational knowledge graph. Experimental results show that HyNT significantly outperforms state-of-the-art methods on real-world datasets.",KDD
"Most media content consumed today is provided by digital platforms that aggregate input from diverse sources, where access to information is mediated by recommendation algorithms. One principal challenge in this context is dealing with content that is considered harmful. Striking a balance between competing stakeholder interests, rather than block harmful content altogether, one approach is to minimize the exposure to such content that is induced specifically by algorithmic recommendations. Hence, modeling media items and recommendations as a directed graph, we study the problem of reducing the exposure to harmful content via edge rewiring. We formalize this problem using absorbing random walks, and prove that it is NP-hard and NP-hard to approximate to within an additive error, while under realistic assumptions, the greedy method yields a (1-1/e)-approximation. Thus, we introduce Gamine, a fast greedy algorithm that can reduce the exposure to harmful content with or without quality constraints on recommendations. By performing just 100 rewirings on YouTube graphs with several hundred thousand edges, Gamine reduces the initial exposure by 50%, while ensuring that its recommendations are at most 5% less relevant than the original recommendations. Through extensive experiments on synthetic data and real-world data from video recommendation and news feed applications, we confirm the effectiveness, robustness, and efficiency of Gamine in practice.",KDD
"Graph Neural Networks (GNNs) have emerged as a prominent research topic in the field of machine learning. Existing GNN models are commonly categorized into two types: spectral GNNs, which are designed based on polynomial graph filters, and spatial GNNs, which utilize a message-passing scheme as the foundation of the model. For the expressive power and universality of spectral GNNs, a natural approach is to improve the design of basis functions for better approximation ability. As for spatial GNNs, models like Graph Isomorphism Networks (GIN) analyze their expressive power based on Graph Isomorphism Tests. Recently, there have been attempts to establish connections between spatial GNNs and geometric concepts like curvature and cellular sheaves, as well as physical phenomena like oscillators. However, despite the recent progress, there is still a lack of comprehensive analysis regarding the universality of spatial GNNs from the perspectives of geometry and physics.
In this paper, we propose MetricGNN (MGNN), a spatial GNN model inspired by the congruent-insensitivity property of classifiers in the classification phase of GNNs. We demonstrate that a GNN model is universal in the spatial domain if it can generate embedding matrices that are congruent to any given embedding matrix. This property is closely related to the Distance Geometry Problem (DGP). Since DGP is an NP-Hard combinatorial optimization problem, we propose optimizing an energy function derived from spring networks and the Multi-Dimensional Scaling (MDS) problem. This approach also allows our model to handle both homophilic and heterophilic graphs. Finally, we propose employing the iteration method to optimize our energy function. We extensively evaluate the effectiveness of our model through experiments conducted on both synthetic and real-world datasets. Our code is available at: https://github.com/GuanyuCui/MGNN.",KDD
"We study the problem of succinctly summarizing a database of event sequences in terms of generalized sequential patterns. That is, we are interested in patterns that are not exclusively defined over observed surface-level events, as is usual, but rather may additionally include generalized events that can match a set of events. To avoid spurious and redundant results we define the problem in terms of the Minimum Description Length principle, by which we are after that set of patterns and generalizations that together best compress the data without loss. The resulting optimization problem does not lend itself for exact search, which is why we propose the heuristic Flock algorithm to efficiently find high-quality models in practice. Extensive experiments on synthetic and real-world data show that Flock results in compact and easily interpretable models that accurately recover the ground truth, including rare instances of generalized patterns. Additionally Flock recovers how generalized events within patterns depend on each other, and overall provides clearer insight into the data-generating process than using state of the art algorithms that only consider surface-level patterns.",KDD
"The task of annotating a data point with labels most relevant to it from a large universe of labels is referred to as Extreme Classification (XC). State-of-the-art XC methods have applications in ranking, recommendation, and tagging and mostly employ a combination architecture comprised of a deep encoder and a high-capacity classifier. These two components are often trained in a modular fashion to conserve compute. This paper shows that in XC settings where data paucity and semantic gap issues abound, this can lead to suboptimal encoder training which negatively affects the performance of the overall architecture. The paper then proposes a lightweight alternative DEXA that augments encoder training with auxiliary parameters. Incorporating DEXA into existing XC architectures requires minimal modifications and the method can scale to datasets with 40 million labels and offer predictions that are up to 6% and 15% more accurate than embeddings offered by existing deep XC methods on benchmark and proprietary datasets, respectively. The paper also analyzes DEXA theoretically and shows that it offers provably superior encoder training than existing Siamese training strategies in certain realizable settings. Code for DEXA is available at https://github.com/Extreme-classification/dexa.",KDD
"Graph Neural Networks (GNNs) have achieved great success in modeling graph-structured data. However, recent works show that GNNs are vulnerable to adversarial attacks which can fool the GNN model to make desired predictions of the attacker. In addition, training data of GNNs can be leaked under membership inference attacks. This largely hinders the adoption of GNNs in high-stake domains such as e-commerce, finance and bioinformatics. Though investigations have been made in conducting robust predictions and protecting membership privacy, they generally fail to simultaneously consider the robustness and membership privacy. Therefore, in this work, we study a novel problem of developing robust and membership privacy-preserving GNNs. Our analysis shows that Information Bottleneck (IB) can help filter out noisy information and regularize the predictions on labeled samples, which can benefit robustness and membership privacy. However, structural noises and lack of labels in node classification challenge the deployment of IB on graph-structured data. To mitigate these issues, we propose a novel graph information bottleneck framework that can alleviate structural noises with neighbor bottleneck. Pseudo labels are also incorporated in the optimization to minimize the gap between the predictions on the labeled set and unlabeled set for membership privacy. Extensive experiments on real-world datasets demonstrate that our method can give robust predictions and simultaneously preserve membership privacy.",KDD
"Providing personalization in product search has attracted increasing attention in both industry and research communities. Most existing personalized product search methods model users' individual search interests based on their historical search logs to generate personalized search results. However, the search logs may be sparse or noisy in the real scenario, which is difficult for existing methods to learn accurate and robust user representations. To address this issue, we propose a contrastive learning framework CoPPS that aims to learn high-quality user representations for personalized product search. Specifically, we design three data augmentation and contrastive learning strategies to construct self-supervision signals from the original search behaviours. The contrastive learning tasks utilize an external knowledge graph and exploit the correlations within and between user sequences, thereby facilitating the discovery of more meaningful search patterns and ultimately enhancing the quality of personalized search. Experimental results on the public Amazon datasets verify the effectiveness of our approach.",KDD
"Matrix low rank approximation is an effective method to reduce or eliminate the statistical redundancy of its components. Compared with the traditional global low rank methods such as singular value decomposition (SVD), local low rank approximation methods are more advantageous to uncover interpretable data structures when clear duality exists between the rows and columns of the matrix. Local low rank approximation is equivalent to low rank submatrix detection. Unfortunately,existing local low rank approximation methods can detect only submatrices of specific mean structure, which may miss a substantial amount of true and interesting patterns. In this work, we develop a novel matrix computational framework called RPSP (Random Probing based submatrix Propagation) that provides an effective solution for the general matrix local low rank representation problem. RPSP detects local low rank patterns that grow from small submatrices of low rank property, which are determined by a random projection approach. RPSP is supported by theories of random projection. Experiments on synthetic data demonstrate that RPSP outperforms all state-of-the-art methods, with the capacity to robustly and correctly identify the low rank matrices when the pattern has a similar mean as the background, background noise is heteroscedastic and multiple patterns present in the data. On real-world datasets, RPSP also demonstrates its effectiveness in identifying interpretable local low rank matrices.",KDD
"Clinical trial digital twins are virtual patients that reflect personal characteristics in a high degree of granularity and can be used to simulate various patient outcomes under different conditions. With the growth of clinical trial databases captured by Electronic Data Capture (EDC) systems, there is a growing interest in using machine learning models to generate digital twins. This can benefit the drug development process by reducing the sample size required for participant recruitment, improving patient outcome predictive modeling, and mitigating privacy risks when sharing synthetic clinical trial data. However, prior research has mainly focused on generating Electronic Healthcare Records (EHRs), which often assume large training data and do not account for personalized synthetic patient record generation. In this paper, we propose a sample-efficient method TWIN for generating personalized clinical trial digital twins. TWIN can produce digital twins of patient-level clinical trial records with high fidelity to the targeting participant's record and preserves the temporal relations across visits and events. We compare our method with various baselines for generating real-world patient-level clinical trial data. The results show that TWIN generates synthetic trial data with high fidelity to facilitate patient outcome predictions in low-data scenarios and strong privacy protection against real patients from the trials.",KDD
"Network embedding, a graph representation learning method illustrating network topology by mapping nodes into lower-dimension vectors, is challenging to accommodate the ever-changing dynamic graphs in practice. Existing research is mainly based on node-by-node embedding modifications, which falls into the dilemma of efficient calculation and accuracy. Observing that the embedding dimensions are usually much smaller than the number of nodes, we break this dilemma with a novel dynamic network embedding paradigm that rotates and scales the axes of embedding space instead of a node-by-node update. Specifically, we propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which achieves an efficient and accurate dynamic network embedding by rotating and scaling the coordinate system where the network embedding resides with no more than the number of edge modifications changes of node embeddings. Moreover, a dynamic Personalized PageRank is applied to the obtained network embeddings to enhance node embeddings and capture higher-order neighbor information dynamically. Experiments of node classification, link prediction, and graph reconstruction on different-sized dynamic graphs suggest that DAMF advances dynamic network embedding. Further, we unprecedentedly expand dynamic network embedding experiments to billion-edge graphs, where DAMF updates billion-level parameters in less than 10ms.",KDD
"Prompting methods have shown impressive performance in a variety of text mining tasks and applications, especially few-shot ones. Despite the promising prospects, the performance of prompting model largely depends on the design of prompt template and verbalizer. In this work, we propose MetricPrompt, which eases verbalizer design difficulty by reformulating few-shot text classification task into text pair relevance estimation task. MetricPrompt adopts prompting model as the relevance metric, further bridging the gap between Pre-trained Language Model's (PLM) pre-training objective and text classification task, making possible PLM's smooth adaption. Taking a training sample and a query one simultaneously, MetricPrompt captures cross-sample relevance information for accurate relevance estimation. We conduct experiments on three widely used text classification datasets across four few-shot settings. Results show that MetricPrompt outperforms manual verbalizer and other automatic verbalizer design methods across all few-shot settings, achieving new state-of-the-art (SOTA) performance.",KDD
"The recent success of pre-trained language models (PLMs) such as BERT has resulted in the development of various beneficial database middlewares, including natural language query interfaces and entity matching. This shift has been greatly facilitated by the extensive external knowledge of PLMs. However, as PLMs are often provided by untrusted third parties, their lack of standardization and regulation poses significant security risks that have yet to be fully explored. This paper investigates the security threats posed by malicious PLMs to these emerging database middleware. We specifically propose a novel type of Trojan attack, where a maliciously designed PLM causes unexpected behavior in the database middleware. These Trojan attacks possess the following characteristics: (1) Triggerability: The Trojan-infected database middleware will function normally with normal input, but will likely malfunction when triggered by the attacker. (2) Imperceptibility: There is no need for noticeable modification of the input to trigger the Trojan. (3) Generalizability: The Trojan is capable of targeting a variety of downstream tasks, not just one specific task. We thoroughly evaluate the impact of these Trojan attacks through experiments and analyze potential countermeasures and their limitations. Our findings could aid in the creation of stronger mechanisms for the implementation of PLMs in database middleware.",KDD
"Spatial-temporal graph models are prevailing for abstracting and modelling spatial and temporal dependencies. In this work, we ask the following question: whether and to what extent can we localise spatial-temporal graph models? We limit our scope to adaptive spatial-temporal graph neural networks (ASTGNNs), the state-of-the-art model architecture. Our approach to localisation involves sparsifying the spatial graph adjacency matrices. To this end, we propose Adaptive Graph Sparsification (AGS), a graph sparsification algorithm which successfully enables the localisation of ASTGNNs to an extreme extent (fully localisation). We apply AGS to two distinct ASTGNN architectures and nine spatial-temporal datasets. Intriguingly, we observe that spatial graphs in ASTGNNs can be sparsified by over 99.5% without any decline in test accuracy. Furthermore, even when ASTGNNs are fully localised, becoming graph-less and purely temporal, we record no drop in accuracy for the majority of tested datasets, with only minor accuracy deterioration observed in the remaining datasets. However, when the partially or fully localised ASTGNNs are reinitialised and retrained on the same data, there is a considerable and consistent drop in accuracy. Based on these observations, we reckon that (i) in the tested data, the information provided by the spatial dependencies is primarily included in the information provided by the temporal dependencies and, thus, can be essentially ignored for inference; and (ii) although the spatial dependencies provide redundant information, it is vital for the effective training of ASTGNNs and thus cannot be ignored during training. Furthermore, the localisation of ASTGNNs holds the potential to reduce the heavy computation overhead required on large-scale spatial-temporal data and further enable the distributed deployment of ASTGNNs.",KDD
"Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their memory and compute-intensive requirements pose a critical bottleneck for long-term forecasting, despite numerous advancements in compute-aware self-attention modules. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).",KDD
"The problem of Learning from Label Proportions (LLP) has received considerable research attention and has numerous practical applications. In LLP, a hypothesis assigning labels to items is learned using knowledge of only the proportion of labels found in predefined groups, called bags. While a number of algorithmic approaches to learning in this context have been proposed, very little work has addressed the model selection problem for LLP. Nonetheless, it is not obvious how to extend straightforward model selection approaches to LLP, in part because of the lack of item labels. More fundamentally, we argue that a careful approach to model selection for LLP requires consideration of the dependence structure that exists between bags, items, and labels. In this paper we formalize this structure and show how it affects model selection. We show how this leads to improved methods of model selection that we demonstrate outperform the state of the art over a wide range of datasets and LLP algorithms.",KDD
"Heterogeneous graph neural networks have gained great popularity in tackling various network analysis tasks on heterogeneous network data. However, most existing works mainly focus on general heterogeneous networks, and assume that there is only one type of edge between two nodes, while ignoring the multiplex characteristics between multi-typed nodes in multiplex heterogeneous networks and the different importance of multiplex structures among nodes for node embedding. In addition, the over-smoothing issue of graph neural networks limits existing models to only capturing local structure signals but hardly learning the global relevant information of the network. To tackle these challenges, this work proposes a model called Behavior Pattern based Heterogeneous Graph Neural Network (BPHGNN) for multiplex heterogeneous network embedding. Specifically, BPHGNN can collaboratively learn node representations across different multiplex structures among nodes with adaptive importance learning from local and global perspectives in multiplex heterogeneous networks through depth behavior pattern aggregation and breadth behavior pattern aggregation. Extensive experiments on six real-world networks with various network analytical tasks demonstrate the significant superiority of BPHGNN against state-of-the-art approaches in terms of various evaluation metrics.",KDD
"Retrieval-based dialogue systems are a crucial component of natural language processing, employing information retrieval techniques to select responses from a predefined pool of candidates. The advent of pre-trained language models (PLMs) has significantly advanced the field, with a prevailing paradigm that involves post-training PLMs on specific dialogue corpora, followed by fine-tuning for the response selection (RS) task. This post-training process aims to capture dialogue-specific features, as most PLMs are originally trained on plain text. However, prior approaches predominantly rely on self-supervised tasks or session-level graph neural networks during post-training, focusing on capturing underlying patterns of coherent dialogues without explicitly refining the global pattern across the entire dialogue corpus. Consequently, the learned knowledge for organizing coherent dialogues remains isolated, heavily reliant on specific contexts. Additionally, interpreting or visualizing the implicit knowledge acquired through self-supervised tasks proves challenging. In this study, we address these limitations by explicitly refining the knowledge required for response selection and structuring it into a coherent global flow, known as ""dialogue structure."" This structure captures the inter-dependency of utterances and topic shifts, thereby enhancing the response selection task. To achieve this, we propose a novel structure model comprising a state recognizer and a structure planner. This model effectively captures the flow within the utterance history and plans the trajectory of future utterances. Importantly, the structure model operates orthogonally to the retrieval model, enabling seamless integration with existing retrieval models and facilitating collaborative training. Extensive experiments conducted on three benchmark datasets demonstrate the superior performance of our method over a wide range of competitive baselines, establishing a new state-of-the-art in the field.",KDD
"Antibodies are proteins that effectively protect the human body by binding to pathogens. Recently, deep learning-based computational antibody design has attracted popular attention since it automatically mines the antibody patterns from data that could be complementary to human experiences. However, the computational methods heavily rely on high-quality antibody structure data, which is quite limited. Besides, the complementarity-determining region (CDR), which is the key component of an antibody that determines the specificity and binding affinity, is highly variable and hard to predict. Therefore, the limited availability of high-quality antibody structure data exacerbates the difficulty of CDR generation. Fortunately, there is a large amount of sequence data for antibodies that can help model the CDR and reduce reliance on structure data. By witnessing the success of pre-training models for protein modeling, in this paper, we develop the antibody pre-training language model and incorporate it into the antigen-specific antibody design model in a systemic way. Specifically, we first pre-train a novel antibody language model based on the sequence data, then propose a one-shot way for sequence and structure generation of CDR to mitigate the high cost and error propagation associated with autoregressive methods, and finally leverage the pre-trained antibody model for the antigen-specific antibody generation model with some carefully designed modules. Our experiments demonstrate the superiority of our method over previous baselines in tasks such as sequence and structure generation, CDR-H3 design for antigen binding, and antibody optimization1. The code is available at https://github.com/KyGao/ABGNN.",KDD
"Spectral methods for graph neural networks (GNNs) have achieved great success. Despite their success, many works have shown that existing approaches are mainly focused on low-frequency information which may not be pertinent to the task at hand. Recent efforts have been made to design new graph filters for wider frequency profiles, but it remains an open problem how to learn multi-scale disentangled node embeddings in the graph Fourier domain. In this paper, we propose a graph (signal) sampling and filtering framework, entitled Pyramid Graph Neural Network (PyGNN), which follows the Downsampling-Filtering-Upsampling-Decoding scheme. To be specific, we develop an Ï‰-bandlimited downsampling approach to split input graph into subgraphs for the reduction of high-frequency components, then perform spectral graph filters on subgraphs to achieve node embeddings with different frequency bands, and propose a Laplacian smoothing-based upsampling approach to extrapolate the node embedding on subgraphs to the full set of vertices on the original graph. In the end, we add frequency-aware gated units to decode node embeddings of different frequencies for downstream tasks. Results on both homophilic and heterophilic graph datasets show its superiority over state-of-the-art methods.",KDD
"The NP-hard combinatorial Virtual Network Embedding (VNE) Problem refers to finding the node and edge mapping between a virtual net (request) and the physical net (resource). Learning-based methods are recently devised beyond traditional heuristic solvers. However, the efficiency and scalability hinder its applicability as reinforcement learning (RL) is often adopted in an auto-regressive node-by-node mapping manner to handle complex mapping constraints, for each coming request for mapping. Moreover, existing learning-based works often independently consider each online request, limiting the long-term online service performance. In this paper, we present a synergistic Global-And-Local learning approach for the VNE problem (GAL-VNE). At the global level across requests, RL is employed to capture the cross-request relation for better global resource accommodation to improve overall performance. At the local level within each request, we aim to replace the sequential decision-making procedure which relies much on the network size, with a more efficient one-shot solution generation scheme. The main challenge for such a one-shot model is how to encode the constraints under an end-to-end learning and inference paradigm. Accordingly, within the ""rank-then-search"" paradigm, we propose to first pretrain a graph neural network (GNN)-based node ranker with imitation supervision from an off-the-shelf solver (moderately expensive yet high quality), which is meanwhile regularized by a neighboring smooth prior. Then RL is used to finetune the GNN ranker whose supervision directly refers to the final (undifferentiable) business objectives concerning revenue and cost, etc. Experiments on benchmarks show that our method outperforms classic and learning-based methods in both efficacy and efficiency.",KDD
"Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attention mask to allow computation only at the current time step. Together, each compression technique and attention modification substantially reduces the number of non-zero operations necessary in the Transformer. We measure the computational savings of our approach over a range of metrics including parameter count, bit size, and floating point operation (FLOPs) count, showing up to a 53Ã— reduction in storage size and up to 10.5Ã— reduction in FLOPs.",KDD
"Triangular meshes are commonly used to reconstruct the surfaces of 3-dimensional (3D) objects based on the point cloud data. With an increasing demand for high-quality approximation, the sizes of point cloud data and the generated triangular meshes continue to increase, resulting in high computational cost in data processing, visualization, analysis, transmission and storage. Motivated by the process of sculpture polishing, we develop a novel progressive mesh compression approach, called the greedy 3D-polishing algorithm, to sequentially remove redundant points and triangles in a greedy manner while maintaining the approximation quality of the surface. Based on the polishing algorithm, we propose the approximate curvature radius to evaluate the scale of features polished at each iteration. By reformulating the compression rate selection as a change-point detection problem, a rank-based procedure is developed to select the optimal compression rate so that most of the global features of the 3D object surface can be preserved with statistical guarantee. Experiments on both moderate- and large-scale 3D shape datasets show that the proposed method can substantially reduce the size of the point cloud data and the corresponding triangular mesh, so that most of the surface information can be retained by a much smaller number of points and triangles.",KDD
"Explanation supervision is a technique in which the model is guided by human-generated explanations during training. This technique aims to improve both the interpretability and predictability of the model by incorporating human understanding into the training process. Since explanation supervision requires a large scale of training data, the data augmentation technique is necessary to be applied to increase the size and diversity of the original dataset. However, data augmentationÂ on sophisticated data like medical images is particularly challenging due to the following: 1) scarcity of data in training the learning-based data augmenter, 2) difficulty in generating realistic and sophisticated images, and 3) difficulty in ensuring the augmented data indeed boosts the performance of explanation-guided learning. To solve these challenges, we propose an Explanation Iterative Supervision via Saliency-guided Data Augmentation (ESSA) framework for conducting explanation supervision and adversarial-trained image data augmentation via a synergized iterative loop that handles the translation from annotation to sophisticated images and the generation of synthetic image-annotation pairs with an alternating training strategy. Extensive experiments on two datasets from the medical imaging domain demonstrate the effectiveness of our proposed framework in improving both the predictability and explainability of the model.",KDD
"This work presents CounterNet, a novel end-to-end learning framework which integrates Machine Learning (ML) model training and the generation of corresponding counterfactual (CF) explanations into a single end-to-end pipeline. Counterfactual explanations offer a contrastive case, i.e., they attempt to find the smallest modification to the feature values of an instance that changes the prediction of the ML model on that instance to a predefined output. Prior techniques for generating CF explanations suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models --- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a novel departure from the prevalent post-hoc paradigm (of generating CF explanations) by presenting CounterNet, an end-to-end learning framework which integrates predictive model training and the generation of counterfactual (CF) explanations into a single pipeline. Unlike post-hoc methods, CounterNet enables the optimization of the CF explanation generation only once together with the predictive model. We adopt a block-wise coordinate descent procedure which helps in effectively training CounterNet's network. Our extensive experiments on multiple real-world datasets show that CounterNet generates high-quality predictions, and consistently achieves 100% CF validity and low proximity scores (thereby achieving a well-balanced cost-invalidity trade-off) for any new input instance, and runs 3X faster than existing state-of-the-art baselines.",KDD
"1Estimating the quantile of distribution, especially tail distribution, is an interesting topic in data stream models, and has obtained extensive interest from many researchers. In this paper, we propose a novel sketch, namely SketchPolymer to accurately estimate per-item tail quantile. SketchPolymer uses a technique called Early Filtration to filter infrequent items, and another technique called VSS to reduce error. Our experimental results show that the accuracy of SketchPolymer is on average 32.67 times better than state-of-the-art techniques. We also implement our SketchPolymer on P4 and FPGA platforms to verify its deployment flexibility. All our codes are available at GitHub.[1]",KDD
"Collaborative filtering (CF) is an important research direction in recommender systems that aims to make recommendations given the information on user-item interactions. Graph CF has attracted more and more attention in recent years due to its effectiveness in leveraging high-order information in the user-item bipartite graph for better recommendations. Specifically, recent studies show the success of graph neural networks (GNN) for CF is attributed to its low-pass filtering effects. However, current researches lack a study of how different signal components contributes to recommendations, and how to design strategies to properly use them well. To this end, from the view of spectral transformation, we analyze the important factors that a graph filter should consider to achieve better performance. Based on the discoveries, we design JGCF, an efficient and effective method for CF based on Jacobi polynomial bases and frequency decomposition strategies. Extensive experiments on four widely used public datasets show the effectiveness and efficiency of the proposed methods, which brings at most 27.06% performance gain on Alibaba-iFashion. Besides, the experimental results also show that JGCF is better at handling sparse datasets, which shows potential in making recommendations for cold-start users.",KDD
"Graph Convolutional Networks (GCNs), which use a message-passing paradigm with stacked convolution layers, are foundational spatial methods for learning graph representations. Polynomial filters, which have an advantage on heterophilous graphs, are motivated differently from the spectral perspective of graph convolutions. Recent spatial GCN models use various residual connection techniques to alleviate the model degradation problem such as over-smoothing and gradient vanishing. However, current residual connections do not effectively harness the full potential of polynomial filters, which are commonly employed in the spectral domain of GNNs.
In this paper, we introduce ClenshawGCN, a GNN model that injects the characteristic of spectral models into spatial models by a simple residual connection submodule: the Clenshaw residual connection, which is essentially a second-order negative residual combined with an initial residual. We show that a ClenshawGCN implicitly simulates an arbitrary polynomial filter under the Chebyshev basis, since the iteration process of stacked (spatial) convolutions equipped with Clenshaw residuals can be interpreted by Clenshaw Summation Algorithm. In addition, we conduct comprehensive experiments to demonstrate the superiority of our model over spatial and spectral GNN models. Our implementation is at https://github.com/yuziGuo/KDDClenshawGNN.",KDD
"Entity Resolution (ER) is a fundamental problem in data preparation. Standard deep ER methods have achieved state-of-the-art effectiveness, assuming that relations from different organizations are centrally stored. However, due to privacy concerns, it can be difficult to centralize data in practice, rendering standard deep ER solutions inapplicable. Despite efforts to develop rule-based privacy-preserving ER methods, they often neglect subtle matching mechanisms and have poor effectiveness as a result. To bridge effectiveness and privacy, in this paper, we propose CampER, an effective framework for privacy-aware deep entity resolution. Specifically, we first design a training pair self-generation strategy to overcome the absence of manually labeled data in privacy-aware scenarios. Based on the self-constructed training pairs, we present a collaborative fine-tuning approach to learn the match-aware and uni-space individual tuple embeddings for accurate matching decisions. During the matching decision-making process, we first introduce a cryptographically secure approach to determine matches. Furthermore, we propose an order-preserving perturbation strategy to significantly accelerate the matching computation while guaranteeing the consistency of ER results. Extensive experiments on eight widely-used benchmark datasets demonstrate that CampER not only is comparable with the state-of-the-art standard deep ER solutions in effectiveness, but also preserves privacy.",KDD
"Out-of-distribution (OOD) detection, which aims to identify OOD samples from in-distribution (ID) ones in test time, has become an essential problem in machine learning. However, existing works are mostly conducted on Euclidean data, and the problem in graph-structured data remains under-explored. Several recent works begin to study graph OOD detection, but they all need to train a graph neural network (GNN) from scratch with high computational cost. In this work, we make the first attempt to endow a well-trained GNN with the OOD detection ability without modifying its parameters. To this end, we design a post-hoc framework with Adaptive Amplifier for Graph OOD Detection, named AAGOD, concentrating on data-centric manipulation. The insight of AAGOD is to superimpose a parameterized amplifier matrix on the adjacency matrix of each original input graph. The amplifier can be seen as prompts and is expected to emphasize the key patterns helpful for graph OOD detection, thereby enlarging the gap between OOD and ID graphs. Then well-trained GNNs can be reused to encode the amplified graphs into vector representations, and pre-defined scoring functions can further convert the representations into detection scores. Specifically, we design a Learnable Amplifier Generator (LAG) to customize amplifiers for different graphs, and propose a Regularized Learning Strategy (RLS) to train parameters with no OOD data required. Experiment results show that AAGOD can be applied on various GNNs to enable the OOD detection ability. Compared with the state-of-the-art baseline in graph OOD detection, on average AAGOD has 6.21% relative enhancement in AUC and a 34 times faster training speed. Code and data are available at https://github.com/BUPT-GAMMA/AAGOD.",KDD
"Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop Frigate to address all these shortcomings. Frigate is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representations. The joint fusion of this diverse information is made feasible through a novel combination of gated Lipschitz embeddings with Lstms. We prove that the proposed Gnn architecture is provably more expressive than message-passing Gnns used in state-of-the-art algorithms. The higher expressivity of Frigate naturally translates to superior empirical performance conducted on real-world network-constrained traffic data. In addition, Frigate is robust to frugal sensor deployment, changes in road network connectivity, and temporal irregularity in sensing.",KDD
"In the past decade, the technology industry has adopted online controlled experiments (a.k.a. A/B testing) to guide business decisions. In practice, A/B tests are often implemented with increasing treatment allocation: the new treatment is gradually released to an increasing number of units through a sequence of randomized experiments. In scenarios such as experimenting in a social network setting or in a bipartite online marketplace, interference among units may exist, which can harm the validity of simple inference procedures. In this work, we introduce a widely applicable procedure to test for interference in A/B testing with increasing allocation. Our procedure can be implemented on top of an existing A/B testing platform with a separate flow and does not require a priori a specific interference mechanism. In particular, we introduce two permutation tests that are valid under different assumptions. Firstly, we introduce a general statistical test for interference requiring no additional assumption. Secondly, we introduce a testing procedure that is valid under a time fixed effect assumption. The testing procedure is of very low computational complexity, it is powerful, and it formalizes a heuristic algorithm implemented already in industry. We demonstrate the performance of the proposed testing procedure through simulations on synthetic data. Finally, we discuss one application at LinkedIn, where a screening step is implemented to detect potential interference in all their marketplace experiments with the proposed methods in the paper.",KDD
"Traffic signal control plays a pivotal role in the management of urban traffic flow. With the rapid advancement of reinforcement learning, the development of signal control methods has seen a significant boost. However, a major challenge in implementing these methods is ensuring that signal lights do not change abruptly, as this can lead to traffic accidents. To mitigate this risk, a time-delay is introduced in the implementation of control actions, but usually has a negative impact on the overall efficacy of the control policy. To address this challenge, this paper presents a novel Traffic Signal Control Framework (PRLight), which leverages an On-policy Traffic Control Model (OTCM) and an Online Traffic Prediction Model (OTPM) to achieve efficient and real-time control of traffic signals. The framework collects multi-source traffic information from a local-view graph in real-time and employs a novel fast attention mechanism to extract relevant traffic features. To be specific, OTCM utilizes the predicted traffic state as input, eliminating the need for communication with other agents and maximizing computational efficiency while ensuring that the most relevant information is used for signal control. The proposed framework was evaluated on both simulated and real-world road networks and compared to various state-of-the-art methods, demonstrating its effectiveness in preventing traffic congestion and accidents.",KDD
"Recent advancements in reinforcement learning have witnessed remarkable achievements by intelligent agents ranging from game-playing to industrial applications. Of particular interest is the area of multi-agent reinforcement learning (MARL), which holds significant potential for real-world scenarios. However, typical MARL methods are limited in their ability to handle tens of agents, leaving scenarios with up to hundreds or even thousands of agents almost unexplored. The scaling up of the number of agents presents two primary challenges: (1) agent-agent interactions are crucial in multi-agent systems while the number of interactions grows quadratically with the number of agents, resulting in substantial computational complexity and difficulty in strategies-learning; (2) the strengths of interactions among agents exhibit variations both across agents and over time, making it difficult to precisely model such interactions. In this paper, we propose a novel approach named Graph Attention Mean Field (GAT-MF). By converting agent-agent interactions into interactions between each agent and a weighted mean field, we achieve a substantial reduction in computational complexity. The proposed method offers a precise modeling of interaction dynamics with mathematical proofs of its correctness. Additionally, we design a graph attention mechanism to automatically capture the diverse and time-varying strengths of interactions, ensuring an accurate representation of agent interactions. Through extensive experimentation conducted in both manual and real-world scenarios involving over 3000 agents, we validate the efficacy of our method. The results demonstrate that our method outperforms the best baseline method with a remarkable improvement of 42.7%. Furthermore, our method saves 86.4% training time and 19.2% GPU memory compared to the best baseline method. For reproducibility, our source codes and data are available at https://github.com/tsinghua-fib-lab/Large-Scale-MARL-GATMF.",KDD
"Few-shot text classification has extensive application where the sample collection is expensive or complicated. When the penalty for classification errors is high, such as early threat event detection with scarce data, we expect to know ""whether we should trust the classification results or reexamine them.'' This paper investigates the Uncertainty Estimation for Few-shot Text Classification (UEFTC), an unexplored research area. Given limited samples, a UEFTC model predicts an uncertainty score for a classification result, which is the likelihood that the classification result is false. However, many traditional uncertainty estimation models in text classification are unsuitable for implementing a UEFTC model. These models require numerous training samples, whereas the few-shot setting in UEFTC only provides a few or just one support sample for each class in an episode. We propose Contrastive Learning from Uncertainty Relations (CLUR) to address UEFTC. CLUR can be trained with only one support sample for each class with the help of pseudo uncertainty scores. Unlike previous works that manually set the pseudo uncertainty scores, CLUR self-adaptively learns them using our proposed uncertainty relations. Specifically, we explore four model structures in CLUR to investigate the performance of three common-used contrastive learning components in UEFTC and find that two of the components are effective. Experiment results prove that CLUR outperforms six baselines on four datasets, including an improvement of 4.52% AUPR on an RCV1 dataset in a 5-way 1-shot setting. Our code and data split for UEFTC are in https://github.com/he159ok/CLUR_UncertaintyEst_FewShot_TextCls.",KDD
"In this paper, we consider the alignment between an upstream dimensionality reduction task of learning a low-dimensional representation of a set of high-dimensional data and a downstream optimization task of solving a stochastic program parameterized by said representation. In this case, standard dimensionality reduction methods (e.g., principal component analysis) may not perform well, as they aim to maximize the amount of information retained in the representation and do not generally reflect the importance of such information in the downstream optimization problem. To address this problem, we develop a prescriptive dimensionality reduction framework that aims to minimize the degree of suboptimality in the optimization phase. For the case where the downstream stochastic optimization problem has an expected value objective, we show that prescriptive dimensionality reduction can be performed via solving a distributionally-robust optimization problem, which admits a semidefinite programming relaxation. Computational experiments based on a warehouse transshipment problem and a vehicle repositioning problem show that our approach significantly outperforms principal component analysis with real and synthetic data sets.",KDD
"Partial-label learning (PLL) relies on a key assumption that the true label of each training example must be in the candidate label set. This restrictive assumption may be violated in complex real-world scenarios, and thus the true label of some collected examples could be unexpectedly outside the assigned candidate label set. In this paper, we term the examples whose true label is outside the candidate label set OOC (Out-Of-Candidate) examples, and pioneer a new PLL study to learn with OOC examples. We consider two types of OOC examples in reality, i.e., the closed-set/open-set OOC examples whose true label is inside/outside the known label space. To solve this new PLL problem, we first calculate the wooden cross-entropy loss from candidate and non-candidate labels respectively, and dynamically differentiate the two types of OOC examples based on specially designed criteria. Then, for closed-set OOC examples, we conduct reversed label disambiguation in the non-candidate label set; for open-set OOC examples, we leverage them for training by utilizing an effective regularization strategy that dynamically assigns random candidate labels from the candidate label set. In this way, the two types of OOC examples can be differentiated and further leveraged for model training. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art PLL methods.",KDD
"Restless and collapsing bandits are often used to model budget-constrained resource allocation in settings where arms have action-dependent transition probabilities, such as the allocation of health interventions among patients. However, SOTA Whittle-index-based approaches to this planning problem either do not consider fairness among arms, or incentivize fairness without guaranteeing it. We thus introduce ProbFair, a probabilistically fair policy that maximizes total expected reward and satisfies the budget constraint while ensuring a strictly positive lower bound on the probability of being pulled at each timestep. We evaluate our algorithm on a real-world application, where interventions support continuous positive airway pressure (CPAP) therapy adherence among patients, as well as on a broader class of synthetic transition matrices. We find that ProbFair preserves utility while providing fairness guarantees.",KDD
"This work presents a mechanism to debias polynomial functions computed from locally differentially private data. Local differential privacy is a widely used privacy notion where users add Laplacian noise to their information before submitting it to a central server. That, however, causes bias when we calculate non-linear functions based on those noisy information. Our proposed recursive algorithm debiases these functions, with a calculation time of O(r n log n), where r is the polynomial degree and n is the number of users. We evaluate our method on the problems of k-star counting and variance estimation, comparing results with state-of-the-art algorithms. The results show that our method not only eliminates bias, but also provides at least 100 times more accuracy than previous works.",KDD
"We study the task of spatio-temporal extrapolation that generates data at target locations from surrounding contexts in a graph. This task is crucial as sensors that collect data are sparsely deployed, resulting in a lack of fine-grained information due to high deployment and maintenance costs. Existing methods either use learning-based models like Neural Networks or statistical approaches like Gaussian Processes for this task. However, the former lacks uncertainty estimates and the latter fails to capture complex spatial and temporal correlations effectively. To address these issues, we propose Spatio-Temporal Graph Neural Processes (STGNP), a neural latent variable model which commands these capabilities simultaneously. Specifically, we first learn deterministic spatio-temporal representations by stacking layers of causal convolutions and cross-set graph neural networks. Then, we learn latent variables for target locations through vertical latent state transitions along layers and obtain extrapolations. Importantly during the transitions, we propose Graph Bayesian Aggregation (GBA), a Bayesian graph aggregator that aggregates contexts considering uncertainties in context data and graph structure. Extensive experiments show that STGNP has desirable properties such as uncertainty estimates and strong learning capabilities, and achieves state-of-the-art results by a clear margin.",KDD
"The Human Mobility Signature Identification (HuMID) problem aims at determining whether the incoming trajectories were generated by a claimed agent from the historical movement trajectories of a set of individual human agents such as pedestrians and taxi drivers. The HuMID problem is significant, and its solutions have a wide range of real-world applications, such as criminal identification for police departments, risk assessment for auto insurance providers, driver verification in ride-sharing services, and so on. Though Deep neural networks (DNN) based HuMID models on spatial-temporal mobility fingerprint similarity demonstrate remarkable performance in effectively identifying human agents' mobility signatures, it is vulnerable to adversarial attacks as other DNN-based models. Therefore, in this paper, we propose a Spatial-Temporal iterative Fast Gradient Sign Method with L0 regularization - ST-iFGSM - to detect the vulnerability and enhance the robustness of HuMID models. Extensive experiments with real-world taxi trajectory data demonstrate the efficiency and effectiveness of our ST-iFGSM algorithm. We tested our method on both the ST-SiameseNet and an LSTM-based HuMID classification model. It shows that ST-iFGSM can generate successful attacks to fool the HuMID models with only a few steps of attack in a small portion of the trajectories. The generated attacks can be used as augmented data to update and improve the HuMID model accuracy significantly from 47.36% to 76.18% on testing samples after the attack(86.25% on the original testing samples).",KDD
"Traditional methods of pre-training, fine-tuning, and ensembling often overlook essential relational data and task interconnections. To address this gap, our study presents a novel approach to harnessing this relational information via a relational graph-based model. We introduce Relational grAph Model ensemBLE model, abbreviated as RAMBLE. This model distinguishes itself by performing class label inference simultaneously across all data nodes and task nodes, employing the relational graph in a transductive manner. This fine-grained approach allows us to better comprehend and model the intricate interplay between data and tasks. Furthermore, we incorporate a novel variational information bottleneck-guided scheme for embedding fusion and aggregation. This innovative technique facilitates the creation of an informative fusion embedding, honing in on embeddings beneficial for the intended task while simultaneously filtering out potential noise-laden embeddings. Our theoretical analysis, grounded in information theory, confirms that the use of relational information for embedding fusion allows us to achieve higher upper and lower bounds on our target task's accuracy. We thoroughly assess our proposed model across eight diverse datasets, and the experimental results demonstrate the model's effective utilization of relational knowledge derived from all pre-trained models, thereby enhancing its performance on our target tasks.",KDD
"Workload prediction in multi-tenant edge cloud platforms (MT-ECP) is vital for efficient application deployment and resource provisioning. However, the heterogeneous application patterns, variable infrastructure performance, and frequent deployments in MT-ECP pose significant challenges for accurate and efficient workload prediction. Clustering-based methods for dynamic MT-ECP modeling often incur excessive costs due to the need to maintain numerous data clusters and models, which leads to excessive costs. Existing end-to-end time series prediction methods are challenging to provide consistent prediction performance in dynamic MT-ECP. In this paper, we propose an end-to-end framework with global pooling and static content awareness, DynEformer, to provide a unified workload prediction scheme for dynamic MT-ECP. Meticulously designed global pooling and information merging mechanisms can effectively identify and utilize global application patterns to drive local workload predictions. The integration of static content-aware mechanisms enhances model robustness in real-world scenarios. Through experiments on five real-world datasets, DynEformer achieved state-of-the-art in the dynamic scene of MT-ECP and provided a unified end-to-end prediction scheme for MT-ECP.",KDD
"Learning multi-agent system dynamics have been extensively studied for various real-world applications, such as molecular dynamics in biology, multi-body system prediction in physics, and particle dynamics in material science. Most of the existing models are built to learn single system dynamics, which learn the dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents. We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function. The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences. To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning. Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations.",KDD
"Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generalization over dense training. We show that an ensemble of sub-models can be formed from the subsampled pathways within a network, which can achieve better performance than its densely attended counterpart. We perform experiments on a variety of NLP, computer vision and graph learning tasks in both generative and discriminative settings to provide empirical evidence for our claims and show the effectiveness of the proposed method.",KDD
"The sparse Mixture-of-Experts (Sparse-MoE) framework efficiently scales up model capacity in various domains, such as natural language processing and vision. Sparse-MoEs select a subset of the ""experts"" (thus, only a portion of the overall network) for each input sample using a sparse, trainable gate. Existing sparse gates are prone to convergence and performance issues when training with first-order optimization methods. In this paper, we introduce two improvements to current MoE approaches. First, we propose a new sparse gate: COMET, which relies on a novel tree-based mechanism. COMET is differentiable, can exploit sparsity to speed up computation, and outperforms state-of-the-art gates. Second, due to the challenging combinatorial nature of sparse expert selection, first-order methods are typically prone to low-quality solutions. To deal with this challenge, we propose a novel, permutation-based local search method that can complement first-order methods in training any sparse gate, e.g., Hash routing, Top-k, DSelect-k, and COMET. We show that local search can help networks escape bad initializations or solutions. We performed large-scale experiments on various domains, including recommender systems, vision, and natural language processing. On standard vision and recommender systems benchmarks, COMET+ (COMET with local search) achieves up to 13% improvement in ROC AUC over popular gates, e.g., Hash routing and Top-k, and up to 9% over prior differentiable gates e.g., DSelect-k. When Top-k and Hash gates are combined with local search, we see up to 100X reduction in the budget needed for hyperparameter tuning. Moreover, for language modeling, our approach improves over the state-of-the-art MoEBERT model for distilling BERT on 5/7 GLUE benchmarks as well as SQuAD dataset.",KDD
"We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself.
We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ''deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We introduce a variational Bayes algorithm for deriving the distributions of per variable attribution scores. To the best of our knowledge, this is the first probabilistic anomaly attribution framework that is free from being deviation-agnostic.",KDD
"Over the last two decades, time series motif discovery has emerged as a useful primitive for many downstream analytical tasks, including clustering, classification, rule discovery, segmentation, and summarization. In parallel, it has long been known that Dynamic Time Warping (DTW) is superior to other similarity measures such as Euclidean Distance under most settings. Recently an algorithm to allow scalable DTW motif discovery was proposed; however, it is limited to finding pairs of subsequences whose subsequence lengths are the same. Moreover, that length must be provided by the user ahead of time. In this work, we propose a novel method to discover ""warped"" motifs whose lengths may differ. Moreover, our method allows input parameters that are not fixed lengths but rather just bounds on the maximum length of motifs to find. This allows us to quickly find different-length motifs without the burdensome trial-and-error of conventional methods. With extensive empirical work, we show that our method is scalable enough for real-world datasets and enables us to find variable-length and ""warped"" motifs that would otherwise escape the attention of conventional algorithms.",KDD
"Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of different properties (i.e., the node similarity-preserving view and the adversarial view). Extensive experiments demonstrate that SP-AGCL achieves a competitive performance on several downstream tasks, and shows its effectiveness in various scenarios, e.g., a network with adversarial attacks,noisy labels, and heterophilous neighbors. Our code is available at https://github.com/yeonjun-in/torch-SP-AGCL.",KDD
"How can we efficiently and accurately analyze an irregular tensor in a dual-way streaming setting where the sizes of two dimensions of the tensor increase over time? What types of anomalies are there in the dual-way streaming setting? An irregular tensor is a collection of matrices whose column lengths are the same while their row lengths are different. In a dual-way streaming setting, both new rows of existing matrices and new matrices arrive over time. PARAFAC2 decomposition is a crucial tool for analyzing irregular tensors. Although real-time analysis is necessary in the dual-way streaming, static PARAFAC2 decomposition methods fail to efficiently work in this setting since they perform PARAFAC2 decomposition for accumulated tensors whenever new data arrive. Existing streaming PARAFAC2 decomposition methods work in a limited setting and fail to handle new rows of matrices efficiently.
In this paper, we propose Dash, an efficient and accurate PARAFAC2 decomposition method working in the dual-way streaming setting. When new data are given, Dash efficiently performs PARAFAC2 decomposition by carefully dividing the terms related to old and new data and avoiding naive computations involved with old data. Furthermore, applying a forgetting factor makes Dash follow recent movements. Extensive experiments show that Dash achieves up to 14.0x faster speed than existing PARAFAC2 decomposition methods for newly arrived data. We also provide discoveries for detecting anomalies in real-world datasets, including Subprime Mortgage Crisis and COVID-19.",KDD
"Selecting the right set of hyperparameters is crucial in time series forecasting. The classical temporal cross-validation framework for hyperparameter optimization (HPO) often leads to poor test performance because of a possible mismatch between validation and test periods. To address this test-validation mismatch, we propose a novel technique, H-Pro to drive HPO via test proxies by exploiting data hierarchies often associated with time series datasets. Since higher-level aggregated time series often show less irregularity and better predictability as compared to the lowest-level time series which can be sparse and intermittent, we optimize the hyperparameters of the lowest-level base-forecaster by leveraging the proxy forecasts for the test period generated from the forecasters at higher levels. H-Pro can be applied on any off-the-shelf machine learning model to perform HPO. We validate the efficacy of our technique with extensive empirical evaluation on five publicly available hierarchical forecasting datasets. Our approach outperforms existing state-of-the-art methods in Tourism, Wiki, and Traffic datasets, and achieves competitive result in Tourism-L dataset, without any model-specific enhancements. Moreover, our method outperforms the winning method of the M5 forecast accuracy competition.",KDD
"Anomaly detection is an important field that aims to identify unexpected patterns or data points, and it is closely related to many real-world problems, particularly to applications in finance, manufacturing, cyber security, and so on. While anomaly detection has been studied extensively in various fields, detecting future anomalies before they occur remains an unexplored territory. In this paper, we present a novel type of anomaly detection, called Precursor-of-Anomaly (PoA) detection. Unlike conventional anomaly detection, which focuses on determining whether a given time series observation is an anomaly or not, PoA detection aims to detect future anomalies before they happen. To solve both problems at the same time, we present a neural controlled differential equation-based neural network and its multi-task learning algorithm. We conduct experiments using 17 baselines and 3 datasets, including regular and irregular time series, and demonstrate that our presented method outperforms the baselines in almost all cases. Our ablation studies also indicate that the multitasking training method significantly enhances the overall performance for both anomaly and PoA detection.",KDD
"Popularity prediction, which aims to forecast how many users would like to interact with a target item or online content in the future, can help online shopping or social media platforms to identify popular items or digital contents. Many efforts have been made to study how the multi-faceted factors, such as item features, user preferences, and social influence, affect user-item interactions, but little work has focused on the evolutionary dynamics of these factors for individuals or groups. In that light, this paper develops a community-based dynamic graph learning method for popularity prediction. First, a dynamic graph learning framework is proposed to maintain a dynamic representation for each item or user entity and update the representations according to the newly observed user-item interactions. Second, a community detection module is designed to capture the evolving community structures and identify the most influential nodes. More importantly, our framework leverages a community-level message passing during the learning process to balance local and global information propagation. Finally, we predict the popularity of the target item or online content based on the learned representations. Our experimental results based on three real-world datasets demonstrate that the proposed method achieves better performance than the baselines. Our method could not only model the changes in a user's preferences, but also capture how the communities evolve over time.",KDD
"Tables are widely used for data storage and presentation due to their high flexibility in layout. The importance of tables as information carriers and the complexity of tabular data understanding attract a great deal of research on large-scale pre-training for tabular data. However, most of the works design models for specific types of tables, such as relational tables and tables with well-structured headers, neglecting tables with complex layouts. In real-world scenarios, there are many such tables beyond their target scope that cannot be well supported. In this paper, we propose GetPt, a unified pre-training architecture for general table representation applicable even to tables with complex structures and layouts. First, we convert a table to a heterogeneous graph with multiple types of edges to represent the layout of the table. Based on the graph, a specially designed transformer is applied to jointly model the semantics and structure of the table. Second, we devise the Alternate Attention Network (AAN) to better model the contextual information across multiple granularities of a table including tokens, cells, and the table. To better support a wide range of downstream tasks, we further employ three pre-training objectives and pre-train the model on a large table dataset. We fine-tune and evaluate GetPt model on two representative tasks, table type classification, and table structure recognition. Experiments show that GetPt outperforms existing state-of-the-art methods on these tasks.",KDD
"Graph neural networks (GNNs) have shown considerable promise for graph-structured data. However, they are also known to be unstable and vulnerable to perturbations and attacks. Recently, the Lipschitz constant has been adopted as a control on the stability of Euclidean neural networks, but calculating the exact constant is also known to be difficult even for very shallow networks. In this paper, we extend the Lipschitz analysis to graphs by providing a systematic scheme for estimating upper bounds of the Lipschitz constants of GNNs. We also derive concrete bounds for widely used GNN architectures including GCN, GraphSAGE and GAT. We then use these Lipschitz bounds for regularized GNN training for improved stability. Our numerical results on Lipschitz regularization of GNNs not only illustrate enhanced test accuracy under random noise, but also show consistent improvement for state-of-the-art defense methods against adversarial attacks.",KDD
"Partial label learning (PLL) is a significant weakly supervised learning framework, where each training example corresponds to a set of candidate labels among which only one is the ground-truth label. Existing works on partial label dimensionality reduction only exploit the disambiguated labels, but overlook the available semantic dissimilarity relationship hidden in the disambiguated labeling confidence, i.e., the smaller the inner product of the labeling confidences of two instances, the less likely they have the same ground-truth label. By combining such global dissimilarity relationship with local neighborhood information, we propose a novel partial label dimensionality reduction method named SDLPP, which employs an alternating procedure including candidate label disambiguation, semantic dissimilarity generation and dimensionality reduction. The labeling confidences of candidate labels and semantic dissimilarity relationship are constantly updated through the alternating procedure, where the processes in each iteration are based on the low-dimensional data obtained in the previous iteration. After the alternating procedure, SDLPP maps the original data to a pre-specified low-dimensional feature space. Comprehensive experiments on both synthetic and real-world data sets validate that SDLPP can improve the generalization performance of different PLL algorithms, and outperform state-of-the-art partial label dimensionality reduction methods. The codes can be publicly accessible on the link https://github.com/jhjiangSEU/SDLPP.",KDD
"In partial label learning (PLL), each training sample is associated with a set of candidate labels, among which only one is valid. The core of PLL is to disambiguate the candidate labels to get the ground-truth one. In disambiguation, the existing works usually do not fully investigate the effectiveness of the non-candidate label set (a.k.a. complementary labels), which accurately indicates a set of labels that do not belong to a sample. In this paper, we use the non-candidate labels to induce a complementary classifier, which naturally forms an adversarial relationship against the traditional PLL classifier, to eliminate the false-positive labels in the candidate label set. Besides, we assume the feature space and the label space share the same local topological structure captured by a dynamic graph, and use it to assist disambiguation. Extensive experimental results validate the superiority of the proposed approach against state-of-the-art PLL methods on 4 controlled UCI data sets and 6 real-world data sets and reveal the usefulness of complementary learning in PLL. The code has been released in the link https://github.com/Chongjie-Si/PL-CL",KDD
"Recent studies give more attention to the anomaly detection (AD) methods that can leverage a handful of labeled anomalies along with abundant unlabeled data. These existing anomaly-informed AD methods rely on manually predefined score target(s), e.g., prior constant or margin hyperparameter(s), to realize discrimination in anomaly scores between normal and abnormal data. However, such methods would be vulnerable to the existence of anomaly contamination in the unlabeled data, and also lack adaptation to different data scenarios.
In this paper, we propose to optimize the anomaly scoring function from the view of score distribution, thus better retaining the diversity and more fine-grained information of input data, especially when the unlabeled data contains anomaly noises in more practical AD scenarios. We design a novel loss function called Overlap loss that minimizes the overlap area between the score distributions of normal and abnormal samples, which no longer depends on prior anomaly score targets and thus acquires adaptability to various datasets. Overlap loss consists of Score Distribution Estimator and Overlap Area Calculation, which are introduced to overcome challenges when estimating arbitrary score distributions, and to ensure the boundness of training loss. As a general loss component, Overlap loss can be effectively integrated into multiple network architectures for constructing AD models. Extensive experimental results indicate that Overlap loss based AD models significantly outperform their state-of-the-art counterparts, and achieve better performance on different types of anomalies.",KDD
"Multi-agent dynamical systems refer to scenarios where multiple units (aka agents) interact with each other and evolve collectively over time. For instance, people's health conditions are mutually influenced. Receiving vaccinations not only strengthens the long-term health status of one unit but also provides protection for those in their immediate surroundings. To make informed decisions in multi-agent dynamical systems, such as determining the optimal vaccine distribution plan, it is essential for decision-makers to estimate the continuous-time counterfactual outcomes. However, existing studies of causal inference over time rely on the assumption that units are mutually independent, which is not valid for multi-agent dynamical systems. In this paper, we aim to bridge this gap and study how to estimate counterfactual outcomes in multi-agent dynamical systems. Causal inference in a multi-agent dynamical system has unique challenges: 1) Confounders are time-varying and are present in both individual unit covariates and those of other units; 2) Units are affected by not only their own but also others' treatments; 3) The treatments are naturally dynamic, such as receiving vaccines and boosters in a seasonal manner. To this end, we model a multi-agent dynamical system as a graph and propose a novel model called CF-GODE (C ounterFactual Graph Ordinary Differential Equations). CF-GODE is a causal model that estimates continuous-time counterfactual outcomes in the presence of inter-dependencies between units. To facilitate continuous-time estimation, we propose Treatment-Induced GraphODE, a novel ordinary differential equation based on graph neural networks (GNNs), which can incorporate dynamical treatments as additional inputs to predict potential outcomes over time. To remove confounding bias, we propose two domain adversarial learning based objectives that learn balanced continuous representation trajectories, which are not predictive of treatments and interference. We further provide theoretical justification to prove their effectiveness. Experiments on two semi-synthetic datasets confirm that CF-GODE outperforms baselines on counterfactual estimation. We also provide extensive analyses to understand how our model works.",KDD
"Imitation learning that replicates experts' skills via their demonstrations has shown significant success in various decision-making tasks. However, two critical challenges still hinder the deployment of imitation learning techniques in real-world application scenarios. First, existing methods lack the intrinsic interpretability to explicitly explain the underlying rationale of the learned skill and thus making learned policy untrustworthy. Second, due to the scarcity of expert demonstrations from each end user (client), learning a policy based on different data silos is necessary but challenging in privacy-sensitive applications such as finance and healthcare. To this end, we present a privacy-preserved interpretable skill learning framework (FedSkill) that enables global policy learning to incorporate data from different sources and provides explainable interpretations to each local user without violating privacy and data sovereignty. Specifically, our proposed interpretable skill learning model can capture the varying patterns in the trajectories of expert demonstrations, and extract prototypical information as skills that provide implicit guidance for policy learning and explicit explanations in the reasoning process. Moreover, we design a novel aggregation mechanism coupled with the based skill learning model to preserve global information utilization and maintain local interpretability under the federated framework. Thoroughly experiments on three datasets and empirical studies demonstrate that our proposed FedSkill framework not only outperforms state-of-the-art imitation learning methods but also exhibits good interpretability under a federated setting. Our proposed FedSkill framework is the first attempt to bridge the gaps among federated learning, interpretable machine learning, and imitation learning.",KDD
"Representation learning on networks aims to derive a meaningful vector representation for each node, thereby facilitating downstream tasks such as link prediction, node classification, and node clustering. In heterogeneous text-rich networks, this task is more challenging due to (1) presence or absence of text: Some nodes are associated with rich textual information, while others are not; (2) diversity of types: Nodes and edges of multiple types form a heterogeneous network structure. As pretrained language models (PLMs) have demonstrated their effectiveness in obtaining widely generalizable text representations, a substantial amount of effort has been made to incorporate PLMs into representation learning on text-rich networks. However, few of them can jointly consider heterogeneous structure (network) information as well as rich textual semantic information of each node effectively. In this paper, we propose Heterformer, a Heterogeneous Network-Empowered Transformer that performs contextualized text encoding and heterogeneous structure encoding in a unified model. Specifically, we inject heterogeneous structure information into each Transformer layer when encoding node texts. Meanwhile, Heterformer is capable of characterizing node/edge type heterogeneity and encoding nodes with or without texts. We conduct comprehensive experiments on three tasks (i.e., link prediction, node classification, and node clustering) on three large-scale datasets from different domains, where Heterformer outperforms competitive baselines significantly and consistently. The code can be found at https://github.com/PeterGriffinJin/Heterformer.",KDD
"Graph-based deep learning models are powerful in modeling spatio-temporal graphs for traffic forecasting. In practice, accurate forecasting models rely on sufficient traffic data, which may not be accessible in real-world applications. To address this problem, transfer learning methods are designed to transfer knowledge from the source graph with abundant data to the target graph with limited data. However, existing methods adopt pre-defined graph structures for knowledge extraction and transfer, which may be noisy or biased and negatively impact the performance of knowledge transfer. To address the problem, we propose TransGTR, a transferable structure learning framework for traffic forecasting that jointly learns and transfers the graph structures and forecasting models across cities. TransGTR consists of a node feature network, a structure generator, and a forecasting model. We train the node feature network with knowledge distillation to extract city-agnostic node features, such that the structure generator, taking the node features as inputs, can be transferred across both cities. Furthermore, we train the structure generator via a temporal decoupled regularization, such that the spatial features learned with the generated graphs share similar distributions across cities and thus facilitate knowledge transfer for the forecasting model. We evaluate TransGTR on real-world traffic speed datasets, where under a fair comparison, TransGTR outperforms state-of-the-art baselines by up to 5.4%.",KDD
"The problem of community-level information pathway prediction (CLIPP) aims at predicting the transmission trajectory of content across online communities. A successful solution to CLIPP holds significance as it facilitates the distribution of valuable information to a larger audience and prevents the proliferation of misinfor- mation. Notably, solving CLIPP is non-trivial as inter-community relationships and influence are unknown, information spread is multi-modal, and new content and new communities appear over time. In this work, we address CLIPP by collecting large-scale, multi-modal datasets to examine the diffusion of online YouTube videos on Reddit. We analyze these datasets to construct community influence graphs (CIGs) and develop a novel dynamic graph frame- work, INPAC (Information Pathway Across Online Communities), which incorporates CIGs to capture the temporal variability and multi-modal nature of video propagation across communities. Ex- perimental results in both warm-start and cold-start scenarios show that INPAC outperforms seven baselines in CLIPP. Our code and datasets are available at https://github.com/claws-lab/INPAC",KDD
"Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have hierarchical relations. Previous works assume rigid consistency over the given hierarchies and do not adapt well to real-world data that show deviation from this assumption. Moreover, recent state-of-art neural probabilistic methods also impose hierarchical relations on point predictions and samples of the predictive distribution. This does not account for full forecast distributions being consistent with the hierarchy and leading to poorly calibrated forecasts. We close both these gaps and propose PROFHiT, a probabilistic hierarchical forecasting model that jointly models forecast distributions over the entire hierarchy. PROFHiT (1) uses a flexible probabilistic Bayesian approach and (2) introduces soft distributional consistency regularization that enables end-to-end learning of the entire forecast distribution leveraging information from the underlying hierarchy. This enables calibrated forecasts as well as adaptation to real-life data with varied hierarchical consistency. PROFHiT provides 41-88% better performance in accuracy and significantly better calibration over a wide range of dataset consistency. Furthermore, PROFHiT adapts to missing data and can provide reliable forecasts even if up to 10% of input time-series data is missing, whereas other methods' performance severely degrades by over 70%",KDD
"Biological networks are commonly used in biomedical and healthcare domains to effectively model the structure of complex biological systems with interactions linking biological entities. However, due to their characteristics of high dimensionality and low sample size, directly applying deep learning models on biological networks usually faces severe overfitting. In this work, we propose R-MIXUP, a Mixup-based data augmentation technique that suits the symmetric positive definite (SPD) property of adjacency matrices from biological networks with optimized training efficiency. The interpolation process in R-MIXUP leverages the log-Euclidean distance metrics from the Riemannian manifold, effectively addressing the swelling effect and arbitrarily incorrect label issues of vanilla Mixup. We demonstrate the effectiveness of R-MIXUP with five real-world biological network datasets on both regression and classification tasks. Besides, we derive a commonly ignored necessary condition for identifying the SPD matrices of biological networks and empirically study its influence on the model performance. The code implementation can be found in Appendix D.",KDD
"Numerical reasoning is an essential task for supporting machine learning applications, such as recommendation and information retrieval. The reasoning task aims to compare two items and infer new facts (e.g., is taller than) by leveraging existing relational information and numerical attributes (e.g., the height of an entity) in knowledge graphs. However, most existing methods rely on leveraging attribute encoders or additional loss functions to predict numerical relations. Therefore, the prediction performance is often not robust in cases when attributes are sparsely observed. In this paper, we propose a Relation-AAware attribute representation learning-based Knowledge Graph Embedding method for numerical reasoning tasks, which we call RAKGE. RAKGE incorporates a newly proposed attribute representation learning mechanism, which can leverage the association between relations and their corresponding numerical attributes. In addition, we introduce a robust self-supervised learning method to generate unseen positive and negative examples, thereby making our approach more reliable when numerical attributes are sparsely available. In the evaluation of three real-world datasets, our proposed model outperformed state-of-the-art methods, achieving an improvement of up to 65.1% in Hits@1 and up to 52.6% in MRR compared to the best competitor. Our implementation code is available at https://github.com/learndatalab/RAKGE.",KDD
"k-nearest neighbor graphs, shortly k-NN graphs, are widely used in many data mining applications like recommendation, information retrieval, and similarity search. Approximate k-NN graph construction has been getting a lot of attention, and most researches focus on developing algorithms that operate efficiently and quickly on a single machine. A few pioneering studies propose distributed algorithms to increase the size of data that can be processed to billions. However, we notice that the distributed algorithms don't perform well enough due to the problems of graph fragmentation and massive data exchange. In this paper, we propose MRDF (Multiway Random Division Forest), a scalable distributed algorithm that constructs highly accurate k-NN graph from numerous high-dimensional vectors quickly. MRDF resolves the problems that the existing distributed algorithms suffer from, through coarse-grained partitioning based on tree path annotation. Experimental results on real-world datasets show that MRDF outperforms the state-of-the-art distributed algorithms with up to 7.6 times faster speed and up to 56%p better accuracy than the second best results.",KDD
"User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number of learned tasks increases while capturing the relationship between the tasks. The main idea is to introduce an embedding for each task, i.e., task embedding, which is utilized to generate task-specific soft masks that not only allow the entire model parameters to be updated until the end of training sequence, but also facilitate the relationship between the tasks to be captured. Moreover, we introduce a novel knowledge retention module with pseudo-labeling strategy that successfully alleviates the long-standing problem of continual learning, i.e., catastrophic forgetting. Extensive experiments on public and proprietary real-world datasets demonstrate the superiority and practicality of TERACON. Our code is available at https://github.com/Sein-Kim/TERACON.",KDD
"Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strategies using a limited number of training meta-tasks, allowing it to acquire meta-knowledge for a wide range of meta-tasks. By incorporating equivariant neural networks, TEG can utilize their strong generalization abilities to learn highly adaptable task-specific strategies. As a result, TEG achieves state-of-the-art performance with limited training meta-tasks. Our experiments on various benchmark datasets demonstrate TEG's superiority in terms of accuracy and generalization ability, even when using minimal meta-training data, highlighting the effectiveness of our proposed approach in addressing the challenges of meta-learning based few-shot node classification. Our code is available at the following link: https://github.com/sung-won-kim/TEG",KDD
"Many real-world interactions (e.g., researcher collaborations and email communication) occur among multiple entities. These group interactions are naturally modeled as hypergraphs. In graphs, transitivity is helpful to understand the connections between node pairs sharing a neighbor, and it has extensive applications in various domains. Hypergraphs, an extension of graphs, are designed to represent group relations. However, to the best of our knowledge, there has been no examination regarding the transitivity of real-world group interactions. In this work, we investigate the transitivity of group interactions in real-world hypergraphs. We first suggest intuitive axioms as necessary characteristics of hypergraph transitivity measures. Then, we propose a principled hypergraph transitivity measure HyperTrans, which satisfies all the proposed axioms, with a fast computation algorithm Fast-HyperTrans. After that, we analyze the transitivity patterns in real-world hypergraphs distinguished from those in random hypergraphs. Lastly, we propose a scalable hypergraph generator THera. It reproduces the observed transitivity patterns by leveraging community structures, which are pervasive in real-world hypergraphs. Our code and datasets are available at https://github.com/kswoo97/hypertrans.",KDD
"For high-quality conversational recommender systems (CRS), it is important to recommend the suitable items by capturing the items' features mentioned in the dialog and to explain the appropriate ones among the various features of the recommended item. We argue that the CRS model should be a domain-expert who is (1) knowledgeable about the relationships between items and their various features and (2) able to explain the recommended item with its features relevant to dialog context. To this end, we propose a novel framework, named as LATTE, to pre-train each core module in CRS (i.e., the recommendation and the conversation module) through abundant external data. For the recommendation module, we pre-train the recommendation module to comprehensively understand the relationships between items and their various features by leveraging both multi-reviews and a knowledge graph. For pre-training the conversation module, we create the synthetic dialogs, which contain responses providing the explanation relevant to the dialog context by using all the items' features and dialog templates. Through extensive experiments on two public CRS datasets, we demonstrate that LATTE exhibits (1) the effectiveness of each module in LATTE, (2) the superiority over 7 state-of-the art methods, and (3) the interpretations based on visualization.",KDD
"Ranking interfaces are everywhere in online platforms. There is thus an ever growing interest in their Off-Policy Evaluation (OPE), aiming towards an accurate performance evaluation of ranking policies using logged data. A de-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides an unbiased and consistent value estimate. However, it becomes extremely inaccurate in the ranking setup due to its high variance under large action spaces. To deal with this problem, previous studies assume either independent or cascade user behavior, resulting in some ranking versions of IPS. While these estimators are somewhat effective in reducing the variance, all existing estimators apply a single universal assumption to every user, causing excessive bias and variance. Therefore, this work explores a far more general formulation where user behavior is diverse and can vary depending on the user context. We show that the resulting estimator, which we call Adaptive IPS (AIPS), can be unbiased under any complex user behavior. Moreover, AIPS achieves the minimum variance among all unbiased estimators based on IPS. We further develop a procedure to identify the appropriate user behavior model to minimize the mean squared error (MSE) of AIPS in a data-driven fashion. Extensive experiments demonstrate that the empirical accuracy improvement can be significant, enabling effective OPE of ranking systems even under diverse user behavior.",KDD
"Causality-informed machine learning has been proposed as an avenue for achieving many of the goals of modern machine learning, from ensuring generalization under domain shifts to attaining fairness, robustness, and interpretability. A key component of causal machine learning is the inference of causal structures from observational data; in practice, this data may be incompletely observed. Prior work has demonstrated that adversarial perturbations of completely observed training data may be used to force the learning of inaccurate causal structural models (SCMs). However, when the data can be audited for correctness (e.g., it is cryptographically signed by its source), this adversarial mechanism is invalidated. This work introduces a novel attack methodology wherein the adversary deceptively omits a portion of the true training data to bias the learned causal structures in a desired manner (under strong signed sample input validation, this behavior seems to be the only strategy available to the adversary). Under this model, theoretically sound attack mechanisms are derived for the case of arbitrary SCMs, and a sample-efficient learning-based heuristic is given. Experimental validation of these approaches on real and synthetic data sets demonstrates the effectiveness of adversarial missingness attacks at deceiving popular causal structure learning algorithms.",KDD
"Traffic signal control is an important problem in urban mobility with a significant potential for economic and environmental impact. While there is a growing interest in Reinforcement Learning (RL) for traffic signal control, the work so far has focussed on learning through simulations which could lead to inaccuracies due to simplifying assumptions. Instead, real experience data on traffic is available and could be exploited at minimal costs. Recent progress in offline or batch RL has enabled just that. Model-based offline RL methods, in particular, have been shown to generalize from the experience data much better than others.
We build a model-based learning framework that infers a Markov Decision Process (MDP) from a dataset collected using a cyclic traffic signal control policy that is both commonplace and easy to gather. The MDP is built with pessimistic costs to manage out-of-distribution scenarios using an adaptive shaping of rewards which is shown to provide better regularization compared to the prior related work in addition to being PAC-optimal. Our model is evaluated on a complex signalized roundabout and a large multi-intersection environment, demonstrating that highly performant traffic control policies can be built in a data-efficient manner.",KDD
"This paper proposes to learn Multi-task, Multi-modal Direct Acyclic Graphs (MM-DAGs), which are commonly observed in complex systems, e.g., traffic, manufacturing, and weather systems, whose variables are multi-modal with scalars, vectors, and functions. This paper takes the traffic congestion analysis as a concrete case, where a traffic intersection is usually regarded as a DAG. In a road network of multiple intersections, different intersections can only have someoverlapping and distinct variables observed. For example, a signalized intersection has traffic light-related variables, whereas unsignalized ones do not. This encourages the multi-task design: with each DAG as a task, the MM-DAG tries to learn the multiple DAGs jointly so that their consensus and consistency are maximized. To this end, we innovatively propose a multi-modal regression for linear causal relationship description of different variables. Then we develop a novel Causality Difference (CD) measure and its differentiable approximator. Compared with existing SOTA measures, CD can penalize the causal structural difference among DAGs with distinct nodes and can better consider the uncertainty of causal orders. We rigidly prove our design's topological interpretation and consistency properties. We conduct thorough simulations and one case study to show the effectiveness of our MM-DAG. The code is available under https://github.com/Lantian72/MM-DAG.",KDD
"Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and synthetic datasets demonstrate the superiority of CMRL over state-of-the-art baseline models.",KDD
"Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss of one task in the presence of another task and a random subset of other tasks. Then, we use spectral clustering on the affinity score matrix to identify task grouping. We design several speedup techniques to compute the higher-order affinity scores efficiently and show that they can predict negative transfers more accurately than pairwise task affinities. We validate our procedure using various community detection and molecular graph prediction data sets, showing favorable results compared with existing methods. Lastly, we provide a theoretical analysis to show that under a planted block model of tasks on graphs, our affinity scores can provably separate tasks into groups.",KDD
"Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications that can be formulated as graph classification tasks. However, dense brain graphs pose computational challenges such as large time and memory consumption and poor model interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. Many prior works select noisy edges based on explainability or task-irrelevant properties, but this does not guarantee performance improvement when using the sparsified graphs. Additionally, the selection of noisy edges is often tailored to each individual graph, making it challenging to sparsify multiple graphs collectively using the same approach.
To address the issues above, we first introduce an iterative framework to analyze the effectiveness of different sparsification models. By utilizing this framework, we find that (i) methods that prioritize interpretability may not be suitable for graph sparsification, as the sparsified graphs may degenerate the performance of GNN models; (ii) it is beneficial to learn the edge selection during the training of the GNN, rather than after the GNN has converged; (iii) learning a joint edge selection shared across all graphs achieves higher performance than generating separate edge selection for each graph; and (iv) gradient information, which is task-relevant, helps with edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which improves the graph classification performance by up to 5.1% with 55.0% fewer edges than the original graphs. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature.",KDD
"Effective personalized incentives can improve user experience and increase platform revenue, resulting in a win-win situation between users and e-commerce companies. Previous studies have used uplift modeling methods to estimate the conditional average treatment effects of users' incentives, and then placed the incentives by maximizing the sum of estimated treatment effects under a limited budget. However, some users will always buy whether incentives are given or not, and they will actively collect and use incentives if provided, named ""Always Buyers"". Identifying and predicting these ""Always Buyers"" and reducing incentive delivery to them can lead to a more rational incentive allocation. In this paper, we first divide users into five strata from an individual counterfactual perspective, and reveal the failure of previous uplift modeling methods to identify and predict the ""Always Buyers"". Then, we propose principled counterfactual identification and estimation methods and prove their unbiasedness. We further propose a counterfactual entire-space multi-task learning approach to accurately perform personalized incentive policy learning with a limited budget. We also theoretically derive a lower bound on the reward of the learned policy. Extensive experiments are conducted on three real-world datasets with two common incentive scenarios, and the results demonstrate the effectiveness of the proposed approaches.",KDD
"Personalized natural language generation for explainable recommendations plays a key role in justifying why a recommendation might match a user's interests. Existing models usually control the generation process by aspect planning. While promising, these aspect-planning methods struggle to generate specific information correctly, which prevents generated explanations from being convincing. In this paper, we claim that introducing lexical constraints can alleviate the above issues. We propose a model, UCEpic, that generates high-quality personalized explanations for recommendation results by unifying aspect planning and lexical constraints in an insertion-based generation manner.
Methodologically, to ensure text generation quality and robustness to various lexical constraints, we pre-train a non-personalized text generator via our proposed robust insertion process. Then, to obtain personalized explanations under this framework of insertion-based generation, we design a method of incorporating aspect planning and personalized references into the insertion process. Hence, UCEpic unifies aspect planning and lexical constraints into one framework and generates explanations for recommendations under different settings. Compared to previous recommendation explanation generators controlled by only aspects, UCEpic incorporates specific information from keyphrases and then largely improves the diversity and informativeness of generated explanations for recommendations on datasets such as RateBeer and Yelp.",KDD
"Sequential recommendation aims to model dynamic user behavior from historical interactions. Existing methods rely on either explicit item IDs or general textual features for sequence modeling to understand user preferences. While promising, these approaches still struggle to model cold-start items or transfer knowledge to new datasets. In this paper, we propose to model user preferences and item features as language representations that can be generalized to new items and datasets. To this end, we present a novel framework, named Recformer, which effectively learns language representations for sequential recommendation. Specifically, we propose to formulate an item as a ""sentence"" (word sequence) by flattening item key-value attributes described by text so that an item sequence for a user becomes a sequence of sentences. For recommendation, Recformer is trained to understand the ""sentence"" sequence and retrieve the next ""sentence"". To encode item sequences, we design a bi-directional Transformer similar to the model Longformer but with different embedding layers for sequential recommendation. For effective representation learning, we propose novel pretraining and finetuning methods which combine language understanding and recommendation tasks. Therefore, Recformer can effectively recommend the next item based on language representations. Extensive experiments conducted on six datasets demonstrate the effectiveness of Recformer for sequential recommendation, especially in low-resource and cold-start settings.",KDD
"The last years have witnessed the emergence of a promising self-supervised learning strategy, referred to as masked autoencoding. However, there is a lack of theoretical understanding of how masking matters on graph autoencoders (GAEs). In this work, we present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled pretext task - masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to comprehensively justify the benefits of this pretext task. Theoretically, we establish close connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a variety of graph benchmarks, demonstrating the superiority of MaskGAE over several state-of-the-arts on both link prediction and node classification tasks.",KDD
"Deploying pre-trained transformer models like BERT on downstream tasks in resource-constrained scenarios is challenging due to their high inference cost, which grows rapidly with input sequence length. In this work, we propose a constraint-aware and ranking-distilled token pruning method ToP, which selectively removes unnecessary tokens as input sequence passes through layers, allowing the model to improve online inference speed while preserving accuracy. ToP overcomes the limitation of inaccurate token importance ranking in the conventional self-attention mechanism through a ranking-distilled token distillation technique, which distills effective token rankings from the final layer of unpruned models to early layers of pruned models. Then, ToP introduces a coarse-to-fine pruning approach that automatically selects the optimal subset of transformer layers and optimizes token pruning decisions within these layers through improved L0 regularization. Extensive experiments on GLUE benchmark and SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning and model compression methods with improved accuracy and speedups. ToP reduces the average FLOPs of BERT by 8.1X while achieving competitive accuracy on GLUE, and provides a real latency speedup of up to 7.4X on an Intel CPU. Code is available at https://github.com/microsoft/Moonlit/tree/main/ToP",KDD
"Learning-based ad auctions have increasingly been adopted in online advertising. However, existing approaches neglect externalities, such as the interaction between ads and organic items. In this paper, we propose a general framework, namely Score-Weighted VCG, for designing learning-based ad auctions that account for externalities. The framework decomposes the optimal auction design into two parts: designing a monotone score function and an allocation algorithm, which facilitates data-driven implementation. Theoretical results demonstrate that this framework produces the optimal incentive-compatible and individually rational ad auction under various externality-aware CTR models while being data-efficient and robust. Moreover, we present an approach to implement the proposed framework with a matching-based allocation algorithm. Experiment results on both real-world and synthetic data illustrate the effectiveness of the proposed approach.",KDD
"OPORP is a variant of the count-sketch data structure by using a fixed-length binning scheme and a normalization step for the estimation. In our experience, we find engineers like the name ""one permutation + one random projection"" as it tells the exact steps. Consider two vectors (e.g., embeddings): u, v ? R D with p = cos(u, v). In embedding-based applications (e.g., EBR), D = 256 - 4096 are common. With OPORP, we first apply a permutation on the data vectors. A vector r âˆˆ R D is generated i.i.d. with E(i) = 0, E(r 2 i ) = 1, E(r 3 i ) = 0, E(r4 i ) = s, where s â‰¥ 1. We multiply (as Hadamard product) r with all permuted data vectors. Then we break the D columns into k equal-length bins and aggregate (i.e., sum) the values in each bin to obtain k samples from each data vector. One crucial step is to normalize the k samples to the unit l2 norm. We show that the estimation variance equals:(s - 1)A + D-k/D-1 1/k [(1-p2)2 -2A ], A â‰¥ 0, s â‰¥ 1, which reveals several key properties of the proposed scheme:We need s = 1, otherwise the variance would have a term which does not decrease with increasing sample size k.The factor d-k/d-1 is beneficial in reducing variances, especially for short vectors which are common in embeddings.The term (1-p2)2 is a drastic variance reduction compared to (1+p 2 ) which is the variance term without normalization.Moreover, the technique in our work also substantially improves the ""very sparse random projections"" (VSRP) in kDD'06. Another major use of OPORP will be in differential privacy (DP).
We need s = 1, otherwise the variance would have a term which does not decrease with increasing sample size k.
The factor d-k/d-1 is beneficial in reducing variances, especially for short vectors which are common in embeddings.
The term (1-p2)2 is a drastic variance reduction compared to (1+p 2 ) which is the variance term without normalization.",KDD
"Finding multiple temporal relationships among locations can benefit a bunch of urban applications, such as dynamic offline advertising and smart public transport planning. While some efforts have been made on finding static relationships among locations, little attention is focused on studying time-aware location relationships. Indeed, abundant location-based human activities are time-varying and the availability of these data enables a new paradigm for understanding the dynamic relationships in a period among connective locations. To this end, we propose to study a new problem, namely multi-Temporal relationship inference among locations (Trial for short), where the major challenge is how to integrate dynamic and geographical influence under the relationship sparsity constraint. Specifically, we propose a solution to Trial with a graph learning scheme, which includes a spatially evolving graph neural network (SEENet) with two collaborative components: spatially evolving graph convolution module (SEConv) and spatially evolving self-supervised learning strategy (SE-SSL). SEConv performs the intra-time aggregation and inter-time propagation to capture the multifaceted spatially evolving contexts from the view of location message passing. In addition, SE-SSL designs time-aware self-supervised learning tasks in a global-local manner with additional evolving constraint to enhance the location representation learning and further handle the relationship sparsity. Finally, experiments on four real-world datasets demonstrate the superiority of our method over several state-of-the-art approaches.",KDD
"Class imbalance is the phenomenon that some classes have much fewer instances than others, which is ubiquitous in real-world graph-structured scenarios. Recent studies find that off-the-shelf Graph Neural Networks (GNNs) would under-represent minor class samples. We investigate this phenomenon and discover that the subspaces of minor classes being squeezed by those of the major ones in the latent space is the main cause of this failure. We are naturally inspired to enlarge the decision boundaries of minor classes and propose a general framework GraphSHA by Synthesizing HArder minor samples. Furthermore, to avoid the enlarged minor boundary violating the subspaces of neighbor classes, we also propose a module called SemiMixup to transmit enlarged boundary information to the interior of the minor classes while blocking information propagation from minor classes to neighbor classes. Empirically, GraphSHA shows its effectiveness in enlarging the decision boundaries of minor classes, as it outperforms various baseline methods in class-imbalanced node classification with different GNN backbone encoders over seven public benchmark datasets. Code is avilable at https://github.com/wenzhilics/GraphSHA.",KDD
"Contrastive learning (CL) has become the de-facto learning paradigm in self-supervised learning on graphs, which generally follows the ""augmenting-contrasting'' learning scheme. However, we observe that unlike CL in computer vision domain, CL in graph domain performs decently even without augmentation. We conduct a systematic analysis of this phenomenon and argue that homophily, i.e., the principle that ""like attracts like'', plays a key role in the success of graph CL. Inspired to leverage this property explicitly, we propose HomoGCL, a model-agnostic framework to expand the positive set using neighbor nodes with neighbor-specific significances. Theoretically, HomoGCL introduces a stricter lower bound of the mutual information between raw node features and node embeddings in augmented views. Furthermore, HomoGCL can be combined with existing graph CL models in a plug-and-play way with light extra computational overhead. Extensive experiments demonstrate that HomoGCL yields multiple state-of-the-art results across six public datasets and consistently brings notable performance improvements when applied to various graph CL methods. Code is avilable at https://github.com/wenzhilics/HomoGCL.",KDD
"Vector retrieval focuses on finding the k-nearest neighbors from a bunch of data points, and is widely used in a diverse set of areas such as information retrieval and recommender system. The current state-of-the-art methods represented by HNSW usually generate indexes with a big memory footprint, restricting the scale of data they can handle, except resorting to a hybrid index with external storage. The space-partitioning learned indexes, which only occupy a small memory, have made great breakthroughs in recent years. However, these methods rely on a large amount of labeled data for supervised learning, so model complexity affects the generalization.
To this end, we propose a lightweight learnable hierarchical space partitioning index based on a balanced K-ary tree, called BAlanced Tree Learner (BATL), where the same bucket of data points are represented by a path from the root to the corresponding leaf. Instead of mapping each query into a bucket, BATL classifies it into a sequence of branches (i.e. a path), which drastically reduces the number of classes and potentially improves generalization. BATL updates the classifier and the balanced tree in an alternating way. When updating the classifier, we innovatively leverage the sequence-to-sequence learning paradigm for learning to route each query into the ground-truth leaf on the balanced tree. Retrieval is then boiled down into a sequence (i.e. path) generation task, which can be simply achieved by beam search on the encoder-decoder. When updating a balanced tree, we apply the classifier for navigating each data point into the tree nodes layer by layer under the balance constraints. We finally evaluate BATL with several large-scale vector datasets, where the experimental results show the superiority of the proposed method to the SOTA baselines in the tradeoff among latency, accuracy, and memory cost.",KDD
"The prosperity of crowdsourcing geospatial data provides increasing opportunities to understand our cities. In particular, OpenStreetMap (OSM) has become a prominent vault of geospatial data on the Web. In this context, learning urban region representations from OSM data, which is unexplored in previous work, could be profitable for various downstream tasks. In this work, we utilize OSM buildings (footprints) complemented with points of interest (POIs) to learn region representations, as buildings' shapes, spatial distributions, and properties have tight linkages to different urban functions. However, appealing as it seems, urban buildings often exhibit complex patterns to form dense or sparse areas, which brings significant challenges for unsupervised feature extraction. To address the challenges, we propose RegionDCL1, an unsupervised framework to deeply mine urban buildings. In a nutshell, we leverage random points generated by Poisson Disk Sampling to tackle data-sparse areas and utilize triplet loss with a novel adaptive margin to preserve inter-region correlations. Furthermore, we train our model with group-level and region-level contrastive learning, making it adaptive to varying region partitions. Extensive experiments in two global cities demonstrate that RegionDCL consistently outperforms the state-of-the-art counterparts across different region partitions, and outputs effective representations for inferring urban land use and population density.",KDD
"Various machine learning applications take users' data to train the models. Recently enforced legislation requires companies to remove users' data upon requests, i.e.,the right to be forgotten. In the context of machine learning, the trained model potentially memorizes the training data. Machine learning algorithms have to be able to unlearn the user data that are requested to delete to meet the requirement. Gradient Boosting Decision Trees (GBDT) is a widely deployed model in many machine learning applications. However, few studies investigate the unlearning on GBDT. This paper proposes a novel unlearning framework for GBDT. To the best of our knowledge, this is the first work that considers machine unlearning on GBDT. It is not straightforward to transfer the unlearning methods of DNN to GBDT settings. We formalized the machine unlearning problem and its relaxed version. We propose an unlearning framework that efficiently and effectively unlearns a given collection of data without retraining the model from scratch. We introduce a collection of techniques, including random split point selection and random partitioning layers training, to the training process of the original tree models to ensure that the trained model requires few subtree retrainings during the unlearning. We investigate the intermediate data and statistics to store as an auxiliary data structure during the training so that we can immediately determine if a subtree is required to be retrained without touching the original training dataset. Furthermore, a lazy update technique is proposed as a trade-off between unlearning time and model functionality. We experimentally evaluate our proposed methods on public datasets. The empirical results confirm the effectiveness of our framework.",KDD
"With the widespread application of online advertising systems, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume (e.g., billions of user click logs). The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since click signals are not sufficient enough for the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs and learn more robust and effective representations. However, current works on this line are still preliminary and rudimentary, leaving self-supervised learning for CTR prediction still an open question. To this end, we propose a Model-agnostic Pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data, and more specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and we also introduce Noise Contrastive Estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining. Our extensive experiments on two real-world million-level datasets (i.e., Avazu, Criteo) demonstrate the advantages of these two methods over several strong baselines, and achieve new state-of-the-art in terms of both performance and efficiency for CTR prediction.",KDD
"We present FIRE, Fast Interpretable Rule Extraction, an optimization-based framework to extract a small but useful collection of decision rules from tree ensembles. FIRE selects sparse representative subsets of rules from tree ensembles, that are easy for a practitioner to examine. To further enhance the interpretability of the extracted model, FIRE encourages fusing rules during selection, so that many of the selected decision rules share common antecedents. The optimization framework utilizes a fusion regularization penalty to accomplish this, along with a non-convex sparsity-inducing penalty to aggressively select rules. Optimization problems in FIRE pose a challenge to off-the-shelf solvers due to problem scale and the non-convexity of the penalties. To address this, making use of problem-structure, we develop a specialized solver based on block coordinate descent principles; our solver performs up to 40x faster than existing solvers. We show in our experiments that FIRE outperforms state-of-the-art rule ensemble algorithms at building sparse rule sets, and can deliver more interpretable models compared to existing methods.",KDD
"We propose a communication and computation efficient second-order method for distributed optimization. For each iteration, our method only requires O (d) communication complexity, where d is the problem dimension. We also provide theoretical analysis to show the proposed method has the similar convergence rate as the classical second-order optimization algorithms. Concretely, our method can find (âˆˆ, âˆšdLe,)-second-order stationary points for nonconvex problem by O (âˆšdL,âˆˆ-3/2) iterations, where L is the Lipschitz constant of Hessian. Moreover, it enjoys a local superlinear convergence under the strongly-convex assumption. Experiments on both convex and nonconvex problems show that our proposed method performs significantly better than baselines.",KDD
"Machine learning-based forecasting models are commonly used in Intelligent Transportation Systems (ITS) to predict traffic patterns and provide city-wide services. However, most of the existing models are susceptible to adversarial attacks, which can lead to inaccurate predictions and negative consequences such as congestion and delays. Therefore, improving the adversarial robustness of these models is crucial for ITS. In this paper, we propose a novel framework for incorporating adversarial training into spatiotemporal traffic forecasting tasks. We demonstrate that traditional adversarial training methods designated for static domains cannot be directly applied to traffic forecasting tasks, as they fail to effectively defend against dynamic adversarial attacks. Then, we propose a reinforcement learning-based method to learn the optimal node selection strategy for adversarial examples, which simultaneously strengthens the dynamic attack defense capability and reduces the model overfitting. Additionally, we introduce a self-knowledge distillation regularization module to overcome the ""forgetting issue"" caused by continuously changing adversarial nodes during training. We evaluate our approach on two real-world traffic datasets and demonstrate its superiority over other baselines. Our method effectively enhances the adversarial robustness of spatiotemporal traffic forecasting models. The source code for our framework is available at https://github.com/usail-hkust/RDAT.",KDD
"Discovering causal structure from purely observational data (i.e., causal discovery), aiming to identify causal relationships among variables, is a fundamental task in machine learning.The recent invention of differentiable score-based DAG learners is a crucial enabler, which reframes the combinatorial optimization problem into a differentiable optimization with a DAG constraint over directed graph space. Despite their great success, these cutting-edge DAG learners incorporate DAG-ness independent score functions to evaluate the directed graph candidates, lacking in considering graph structure. As a result, measuring the data fitness alone regardless of DAG-ness inevitably leads to discovering suboptimal DAGs and model vulnerabilities.
Towards this end, we propose a dynamic csusal space for DAG structure learning, coined CASPER, that integrates the graph structure into the score function as a new measure in the causal space to faithfully reflect the causal distance between estimated and ground-truth DAG. CASPER revises the learning process as well as enhances the DAG structure learning via adaptive attention to DAG-ness. Grounded by empirical visualization, CASPER, as a space, satisfies a series of desired properties, such as structure awareness and noise-robustness. Extensive experiments on both synthetic and real-world datasets clearly validate the superiority of our CASPER over the state-of-the-art causal discovery methods in terms of accuracy and robustness.",KDD
"Exercise recommendation is a fundamental and important task in the E-learning system, facilitating students' personalized learning. Most existing exercise recommendation algorithms design a scoring criterion (e.g., weakest mastery, lowest historical correctness) in conjunction with experience, and then recommend the recommended knowledge concepts (KCs). These algorithms rely entirely on the scoring criteria by treating exercise recommendations as a centralized system. However, it is a complex problem for the centralized system to choose a limited number of exercises in a period of time to consolidate and learn the KCs efficiently. Moreover, different groups of students (e.g., different countries, schools, or classes) have different solutions for the same group of KCs according to their own situations, in the spirit of competency-based instructing. Therefore, we propose Meta Multi-Agent Exercise Recommendation (MMER). Specifically, we design the multi-agent exercise recommendation module, in which the KCs involved in exercises are considered agents with competition and cooperation among them. And the meta-training stage is designed to learn a robust recommendation module for new student groups. Extensive experiments on real-world datasets validate the satisfactory performance of the proposed model. Furthermore, the effectiveness of the multi-agent and meta-training part is demonstrated for the model in recommendation applications.",KDD
"Data imbalance is easily found in annotated data when the observations of certain continuous label values are difficult to collect for regression tasks. When they come to molecule and polymer property predictions, the annotated graph datasets are often small because labeling them requires expensive equipment and effort. To address the lack of examples of rare label values in graph regression tasks, we propose a semi-supervised framework to progressively balance training data and reduce model bias via self-training. The training data balance is achieved by (1) pseudo-labeling more graphs for under-represented labels with a novel regression confidence measurement and (2) augmenting graph examples in latent space for remaining rare labels after data balancing with pseudo-labels. The former is to identify quality examples from unlabeled data whose labels are confidently predicted and sample a subset of them with a reverse distribution from the imbalanced annotated data. The latter collaborates with the former to target a perfect balance using a novel label-anchored mixup algorithm. We perform experiments in seven regression tasks on graph datasets. Results demonstrate that the proposed framework significantly reduces the error of predicted graph properties, especially in under-represented label areas.",KDD
"In recent years, graph neural networks (GNNs) have been widely used in many domains due to their powerful capability in representation learning on graph-structured data. While a majority of extant studies focus on mitigating the over-smoothing problem, recent works also reveal the limitation of GNN from a new over-correlation perspective which states that the learned representation becomes highly correlated after feature transformation and propagation in GNNs. In this paper, we thoroughly re-examine the issue of over-correlation in deep GNNs, both empirically and theoretically. We demonstrate that the propagation operator in GNNs exacerbates the feature correlation. In addition, we discovered through empirical study that existing decorrelation solutions fall short of maintaining a low feature correlation, potentially encoding redundant information. Thus, to more effectively address the over-correlation problem, we propose a decorrelated propagation scheme (DeProp) as a fundamental component to decorrelate the feature learning in GNN models, which achieves feature decorrelation at the propagation step. Comprehensive experiments on multiple real-world datasets demonstrate that DeProp can be easily integrated into prevalent GNNs, leading to significant performance enhancements. Furthermore, we find that it can be used to solve over-smoothing and over-correlation problems simultaneously and significantly outperform state-of-the-art methods on missing feature settings. The code is available at https://github.com/hualiu829/DeProp.",KDD
"Math formulas (e.g., ""distance = speed X time'') serve as one of the fundamental commonsense knowledge in human cognition, where humans naturally acquire and manipulate them in logical thinking for mathematical reasoning problems. However, existing reasoning models mainly focus on learning heuristic linguistics or patterns to generate answers, but do not pay enough attention on learning with such formula knowledge. Thus, they are not transparent (thus uninterpretable) in terms of understanding and grasping basic mathematical logic. In this paper, to promote a step forward in the domain, we first construct two datasets (Math23K-F and MAWPS-F) with precise annotations of formula usage in each reasoning step for math word problems. Especially, our datasets are refined on the benchmark datasets, and thus ensure the generality and comparability for relevant research. Then, we propose a novel Formula-mastered Solver (FOMAS) with the guidance of mastering formula knowledge to solve the problems. Specifically, we establish FOMAS with two systems drawing insight from the dual process theory, including a Knowledge System and a Reasoning System, to learn and apply formula knowledge, respectively. The Knowledge System accumulates the math formulas, where we propose a novel pretraining manner to mimic how humans grasp the mathematical logic behind them. Then, in the Reasoning System, we develop elaborate formula-guided symbol prediction and goal generation methods that retrieve the necessary formula knowledge from Knowledge System to improve both reasoning accuracy and interpretability. It organically simulates how humans conduct complex reasoning under the explicit instruction of math formulas. Experimental results prove that FOMAS has a stronger reasoning ability and achieves a more interpretable reasoning process, which verifies the necessity of introducing formula knowledge transparently.",KDD
"Graph contrastive learning (GCL), aiming for an embedding space where semantically similar nodes are closer, has been widely applied in graph-structured data. Researchers have proposed many approaches to define positive and negative pairs (i.e., semantically similar and dissimilar pairs) on the graph, serving as labels to learn their embedding distances. Despite the effectiveness, those approaches usually suffer from two typical learning challenges. First, the number of candidate negative pairs is enormous. Thus, it is non-trivial to select representative ones to train the model in a more effective way. Second, the heuristics (e.g., graph views or meta-path patterns) to define positive and negative pairs are sometimes less reliable, causing considerable noise for both ""labelled'' positive and negative pairs. In this work, we propose a novel sampling approach B2-Sampling to address the above challenges in a unified way. On the one hand, we use balanced sampling to select the most representative negative pairs regarding both the topological and embedding diversities. On the other hand, we use biased sampling to learn and correct the labels of the most error-prone negative pairs during the training. The balanced and biased samplings can be applied iteratively for discriminating and correcting training pairs, boosting the performance of GCL models. B2-Sampling is designed as a framework to support many known GCL models. Our extensive experiments on node classification, node clustering, and graph classification tasks show that B2-Sampling significantly improves the performance of GCL models with acceptable runtime overhead. Our website[11] https://sites.google.com/view/b2-sampling/home provides access to our codes and additional experiment results.",KDD
"Graph generative models are highly important for sharing surrogate data and benchmarking purposes. Real-world complex systems often exhibit dynamic nature, where the interactions among nodes change over time in the form of a temporal network. Most temporal network generation models extend the static graph generation models by incorporating temporality in the generation process. More recently, temporal motifs are used to generate temporal networks with better success. However, existing models are often restricted to a small set of predefined motif patterns due to the high computational cost of counting temporal motifs. In this work, we develop a practical temporal graph generator, Motif Transition Model (MTM), to generate synthetic temporal networks with realistic global and local features. Our key idea is modeling the arrival of new events as temporal motif transition processes. We first calculate the transition properties from the input graph and then simulate the motif transition processes based on the transition probabilities and transition rates. We demonstrate that our model consistently outperforms the baselines with respect to preserving various global and local temporal graph statistics and runtime performance.",KDD
"We study a novel problem of continuously predicting a number of user-subscribed continuous analytics targets (CATs) in dynamic networks. Our architecture includes any dynamic graph neural network model as the back end applied over the network data, and per CAT front end models that return results with their confidence to users. We devise a data filtering algorithm that feeds a provably optimal subset of data in the embedding space from back end model to front end models. Secondly, to ensure fairness in terms of query result accuracy for different CATs and users, we propose a fairness metric and a fairness-aware training scheduling algorithm, along with accuracy guarantees on fairness estimation. Our experiments over five real-world datasets show that our proposed solution is effective, efficient, fair, extensible, and adaptive.",KDD
"Personalized recommender systems fulfill the daily demands of customers and boost online businesses. The goal is to learn a policy that can generate a list of items that matches the user's demand or interest. While most existing methods learn a pointwise scoring model that predicts the ranking score of each individual item, recent research shows that the listwise approach can further improve the recommendation quality by modeling the intra-list correlations of items that are exposed together. This has motivated the recent list reranking and generative recommendation approaches that optimize the overall utility of the entire list. However, it is challenging to explore the combinatorial space of list actions and existing methods that use cross-entropy loss may suffer from low diversity issues. In this work, we aim to learn a policy that can generate sufficiently diverse item lists for users while maintaining high recommendation quality. The proposed solution, GFN4Rec, is a generative method that takes the insight of the flow network to ensure the alignment between list generation probability and its reward. The key advantages of our solution are the log scale reward matching loss that intrinsically improves the generation diversity and the autoregressive item selection model that captures the item mutual influences while capturing future reward of the list. As validation of our method's effectiveness and its superior diversity during active exploration, we conduct experiments on simulated online environments as well as an offline evaluation framework for two real-world datasets.",KDD
"A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchmarks have verified the effectiveness of the proposed method. Codes: https://github.com/jugechengzi/Rationalization-DR.",KDD
"Graph Neural Networks (GNNs) have achieved remarkable success in various domains but most of them are developed under the in-distribution assumption. Under out-of-distribution (OOD) settings, they suffer from the distribution shift between the training set and the test set and may not generalize well to the test distribution. Several methods have tried the invariance principle to improve the generalization of GNNs in OOD settings. However, in previous solutions, the graph encoder is immutable after the invariant learning and cannot be adapted to the target distribution flexibly. Confronting the distribution shift, a flexible encoder with refinement to the target distribution can generalize better on the test set than the stable invariant encoder. To remedy these weaknesses, we propose a Flexible invariant Learning framework for Out-Of-Distribution generalization on graphs (FLOOD), which comprises two key components, invariant learning and bootstrapped learning. The invariant learning component constructs multiple environments from graph data augmentation and learns invariant representation under risk extrapolation. Besides, the bootstrapped learning component is devised to be trained in a self-supervised way with a shared graph encoder with the invariant learning part. During the test phase, the shared encoder is flexible to be refined with the bootstrapped learning on the test set. Extensive experiments are conducted for both transductive and inductive node classification tasks. The results demonstrate that FLOOD consistently outperforms other graph OOD generalization methods and effectively improves the generalization ability.",KDD
"Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D2PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.",KDD
"The timely prediction of bankruptcy is highly desirable to guarantee an upward spiral for overall societal well-being. By extracting multifaceted information from the business interaction networks, Graph Neural Networks (GNNs) may be able to automatically make more informed predictions for bankruptcy, as compared to methods that rely heavily on abundant manpower to a large extent. Yet in real applications, bankruptcy prediction faces the key issue of quantity-imbalance: data usually comes with a long-tailed distribution wherein bankrupt corporates occupy the least of the data proportion but are our target to be identified. Apart from that, the topology-imbalance issue behind graph-structural data exacerbates prediction deterioration: feature propagation is dominated by non-bankrupt nodes through messages passing between nodes; thus, bankrupt nodes receive highly confusing information and could be easily assimilated by nearby non-bankrupt nodes. Unfortunately, the existing GNN methods are not immune to these two imbalance issues. To tackle the challenging but practically useful scenario, we propose a novel bankruptcy prediction model called the Quantity and Topology Imbalance-Aware Heterogeneous Graph Neural Network (QTIAH-GNN) to boost the final performance. Specifically, QTIAH-GNN employs the multi-hierarchy label-aware neighbor selection to conquer the topology-imbalance issue by using the class-semantic representation and the learnable parameterized similarity metric, and employs the imbalance-oriented loss to obtain the optimal tradeoff between the accuracies of the majority and minority classes. In experiments, we evaluate the proposed QTIAH-GNN on two large-scale, real-world datasets. The results show that QTIAH-GNN outperforms other state-of-the-art baselines in terms of prediction accuracy with superior efficiency and generalization ability, has stronger robustness to data imbalance, and provides meaningful model interpretation.",KDD
"Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network (MIMIC) framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction units to comprehensively explore the intra-modal interaction and inter-modal fusion among features of entities and mentions. In particular, three modules, namely the Text-based Global-Local interaction Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based interaction Unit (CMFU) are designed to capture and integrate the fine-grained representation lying in abbreviated text and implicit visual cues. Afterwards, we introduce a unit-consistency objective function via contrastive learning to avoid inconsistency and model degradation. Experimental results on three public benchmark datasets demonstrate that our solution outperforms various state-of-the-art baselines, and ablation studies verify the effectiveness of designed modules.",KDD
"Partial differential equations (PDEs) that fit scientific data can represent physical laws with explainable mechanisms for various mathematically-oriented subjects, such as physics and finance. The data-driven discovery of PDEs from scientific data thrives as a new attempt to model complex phenomena in nature, but the effectiveness of current practice is typically limited by the scarcity of data and the complexity of phenomena. Especially, the discovery of PDEs with highly nonlinear coefficients from low-quality data remains largely under-addressed. To deal with this challenge, we propose a novel physics-guided learning method, which can not only encode observation knowledge such as initial and boundary conditions but also incorporate the basic physical principles and laws to guide the model optimization. We theoretically show that our proposed method strictly reduces the coefficient estimation error of existing baselines, and is also robust against noise. Extensive experiments show that the proposed method is more robust against data noise, and can reduce the estimation error by a large margin. Moreover, all the PDEs in the experiments are correctly discovered, and for the first time we are able to discover three-dimensional PDEs with highly nonlinear coefficients.",KDD
"While graph neural networks (GNNs) provide a powerful way to learn structured representations, it remains challenging to learn long-range dependencies in graphs. Recurrent GNNs only partly address this problem. In this paper, we propose a general approach for augmenting recurrent GNNs with a cache memory to improve their expressivity, especially for modeling long-range dependencies. Specifically, we first introduce a method of augmenting recurrent GNNs with a cache of previous hidden states. Then we further propose a general Cache-GNN framework by adding additional modules, including attention mechanism and positional/structural encoders, to improve the expressivity. We show that the Cache-GNNs outperforms other models on synthetic datasets as well as tasks on real-world datasets that require long-range information.",KDD
"Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.",KDD
"Graph-level anomaly detection aims at capturing anomalous individual graphs in a graph set. Due to its significance in various real-world application fields, e.g., identifying rare molecules in chemistry and detecting potential frauds in online social networks, graph-level anomaly detection has received great attention recently. In distinction from node- and edge-level anomaly detection that is devoted to identifying anomalies on a single graph, graph-level anomaly detection faces more significant challenges because both the intra- and inter- graph structural and attribute patterns need to be taken into account to distinguish anomalies that exhibit deviating structures, rare attributes or the both. Although deep graph representation learning shows effectiveness in fusing high-level representations and capturing characters of individual graphs, most of the existing works are defective in graph-level anomaly detection because of their limited capability in exploring information across graphs, the imbalanced data distribution of anomalies, and low interpretability of the black-box graph neural networks (GNNs). To overcome these limitations, we propose a novel deep evolutionary graph mapping framework named GmapAD1, which can adaptively map each graph into a new feature space based on its similarity to a set of representative nodes chosen from the graph set. By automatically adjusting the candidate nodes using a specially designed evolutionary algorithm, anomalies and normal graphs are mapped to separate areas in the new feature space where a clear boundary between them can be learned. The selected candidate nodes can therefore be regarded as a benchmark for explaining anomalies because anomalies are more dissimilar/similar to the benchmark than normal graphs. Through our extensive experiments on nine real-world datasets, we demonstrate that exploring both intra- and inter- graph structural and attribute information is critical to spot anomalous graphs, and our method has achieved statistically significant improvements compared to the state of the art in terms of precision, recall, F1 score, and AUC.",KDD
"Event forecasting has been a demanding and challenging task throughout the entire human history. It plays a pivotal role in crisis alarming and disaster prevention in various aspects of the whole society. The task of event forecasting aims to model the relational and temporal patterns based on historical events and makes forecasting to what will happen in the future. Most existing studies on event forecasting formulate it as a problem of link prediction on temporal event graphs. However, such pure structured formulation suffers from two main limitations: 1) most events fall into general and high-level types in the event ontology, and therefore they tend to be coarse-grained and offers little utility which inevitably harms the forecasting accuracy; and 2) the events defined by a fixed ontology are unable to retain the out-of-ontology contextual information.
To address these limitations, we propose a novel task of context-aware event forecasting which incorporates auxiliary contextual information. First, the categorical context provides supplementary fine-grained information to the coarse-grained events. Second and more importantly, the context provides additional information towards specific situation and condition, which is crucial or even determinant to what will happen next. However, it is challenging to properly integrate context into the event forecasting framework, considering the complex patterns in the multi-context scenario. Towards this end, we design a novel framework named Separation and Collaboration Graph Disentanglement (short as SeCoGD) for context-aware event forecasting. In the separation stage, we leverage the context as a prior guidance to disentangle the event graph into multiple sub-graphs, followed by a context-specific module to model the relational-temporal patterns within each context. In the collaboration stage, we design a cross-context module to retain the collaborative associations among multiple contexts. Since there is no available dataset for this novel task, we construct three large- scale datasets based on GDELT. Experimental results demonstrate hat our model outperforms a list of SOTA methods. The dataset and code are released via https://github.com/yecchen/SeCoGD.",KDD
"In Learning-to-Rank (LTR) problems, the task of delivering relevant search results and allocating fair exposure to items of a protected group can conflict. Previous works in Fair LTR have attempted to resolve this by combining the objectives of relevant ranking and fair ranking into a single linear combination, but this approach is limited by the nonconvexity of the objective functions and can result in suboptimal relevance in ranking outputs. To address this, we propose a solution using Multi-Objective Optimization (MOO) algorithms. We extend these algorithms to querywise MOO to reduce the exposure disparity, not only on average but also at the query level. Interestingly, for moderate fairness requirements, it improves the relevance of ranking instead of deteriorating. We attribute this improvement to the benefits of multi-task learning and study the effect of fair ranking on the relevant ranking task. Moreover, we significantly improve the computational efficiency compared to previous methods by using the Gumbel max trick to sample the Plackett-Luce distribution. We evaluate our proposed methods on three real-world datasets and show their improvement in relevance ranking over state-of-the-art solutions.",KDD
"A sizable proportion of deployed machine learning models make their decisions in a black-box manner. Such decision-making procedures are susceptible to intrinsic biases, which has led to a call for accountability in deployed decision systems. In this work, we investigate mechanisms that help audit claimed mathematical guarantees of the fairness of such systems. We construct AVOIR, a system that reduces the number of observations required for the runtime monitoring of probabilistic assertions over fairness metrics specified on decision functions associated with black-box AI models. AVOIR provides an adaptive process that automates the inference of probabilistic guarantees associated with estimating a wide range of fairness metrics. In addition, AVOIR enables the exploration of fairness violations aligned with governance and regulatory requirements. We conduct case studies with fairness metrics on three different datasets and demonstrate how AVOIR can help detect and localize fairness violations and ameliorate the issues with faulty fairness metric design.",KDD
"Guaranteed Delivery (GD) advertising plays an essential part in e-commerce marketing, where the ad publisher signs contracts with advertisers in advance by promising delivery of advertising impressions to fulfill targeting requirements for advertisers. Previous research on GD advertising mainly focused on online serving yet overlooked the importance of contract allocation at the GD selling stage. Traditional GD selling approaches consider impression inventory prediction and contract allocation as two separate stages. However, such a two-stage optimization often leads to inferior contract allocation performance. In this paper, our goal is to reduce this performance gap with a novel end-to-end approach. Specifically, we propose the Neural Lagrangian Selling (NLS) model to jointly predict the impression inventory and optimize the contract allocation of advertising impressions with a unified learning objective. To this end, we first develop a differentiable Lagrangian layer to backpropagate the allocation problem through the neural network and allow direct optimization of the allocation regret. Then, for effective optimization with various allocation targets and constraints, we design a graph convolutional neural network to extract predictive features from the bipartite allocation graph. Extensive experiments show that our approach can improve GD selling performance compared with existing two-stage approaches. Particularly, our optimization layer can outperform the baseline solvers in both computational efficiency and solution quality. To the best of our knowledge, this is the first study to apply the end-to-end prediction and optimization approach for industrial GD selling problems. Our work has implications for general prediction and allocation problems as well.",KDD
"Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify content aligned with long-term success by carefully balancing exploration and exploitation. We apply our approach to a podcast recommendation problem, where we seek to identify shows that users engage with repeatedly over two months. We empirically validate that our approach results in substantially better performance compared to approaches that either optimize for short-term proxies, or wait for the long-term outcome to be fully realized.",KDD
"Sketching algorithms are considered as promising solutions for answering approximate query on massive data stream. In real scenarios, a large number of problems can be abstracted as subset query over multiple attributes. Existing sketches are designed for query on single attributes, and therefore are inefficient for query on multiple attributes. In this work, we propose Hyper-USS, an innovative sketching algorithm that supports subset query over multiple attributes accurately and efficiently. To the best of our knowledge, this work is the first sketching algorithm designed to answer approximate query over multi-attribute data stream. We utilize the key technique, Joint Variance Optimization, to guarantee high estimation accuracy on all attributes. Experiment results show that, compared with the state-of-the-art (SOTA) sketches that support subset query on single attributes, Hyper-USS improves the accuracy by 16.67X and the throughput by 8.54X. The code is open-sourced at Github.",KDD
"Dense subgraph discovery methods are routinely used in a variety of applications including the identification of a team of skilled individuals for collaboration from a social network. However, when the network's node set is associated with a sensitive attribute such as race, gender, religion, or political opinion, the lack of diversity can lead to lawsuits.
In this work, we focus on the problem of finding a densest diverse subgraph in a graph whose nodes have different attribute values/types that we refer to as colors. We propose two novel formulations motivated by different realistic scenarios. Our first formulation, called the densest diverse subgraph problem (DDSP), guarantees that no color represents more than some fraction of the nodes in the output subgraph, which generalizes the state-of-the-art due to Anagnostopoulos et al. (CIKM 2020). By varying the fraction we can range the diversity constraint and interpolate from a diverse dense subgraph where all colors have to be equally represented to an unconstrained dense subgraph. We design a scalable Î©(1/âˆšn)-approximation algorithm, where n is the number of nodes. Our second formulation is motivated by the setting where any specified color should not be overlooked. We propose the densest at-least-â†’k-subgraph problem (Dalâ†’kS), a novel generalization of the classic DalkS, where instead of a single value k, we have a vector k of cardinality demands with one coordinate per color class. We design a 1/3-approximation algorithm using linear programming together with an acceleration technique. Computational experiments using synthetic and real-world datasets demonstrate that our proposed algorithms are effective in extracting dense diverse clusters.",KDD
"Conversational search allows a user to interact with a search system in multiple turns. A query is strongly dependent on the conversation context. An effective way to improve retrieval effectiveness is to expand the current query with historical queries. However, not all the previous queries are related to, and useful for expanding the current query. In this paper, we propose a new method to select relevant historical queries that are useful for the current query. To cope with the lack of labeled training data, we use a pseudo-labeling approach to annotate useful historical queries based on their impact on the retrieval results. The pseudo-labeled data are used to train a selection model. We further propose a multi-task learning framework to jointly train the selector and the retriever during fine-tuning, allowing us to mitigate the possible inconsistency between the pseudo labels and the changed retriever. Extensive experiments on four conversational search datasets demonstrate the effectiveness and broad applicability of our method compared with several strong baselines.",KDD
"Online hierarchical clustering algorithms, compared to their scalable batch setting counterparts, typically provide more limited accuracy and efficiency performance. Yet, when data is incrementally arriving, a crucial setting in many clustering applications (e.g., entity resolution and concept discovery), these batch setting algorithms do not apply. This paper presents a family of new algorithms for online hierarchical clustering that combine high quality trees and fast per-point insertion time--made possible through a limited number of parallel non-greedy tree re-arrangements. We analyze our methods under assumptions about the data and the separability of clusters. Empirically, we find that our proposed algorithms yield state-of-the-art results in hierarchical clustering dendrogram purity and in building compressed prototypes for a k-nearest representative classifier.",KDD
"Out-of-distribution (OOD) generalisation aims to build a model that can generalise well on an unseen target domain using knowledge from multiple source domains. To this end, the model should seek the causal dependence between inputs and labels, which may be determined by the semantics of inputs and remain invariant across domains. However, statistical or non-causal methods often cannot capture this dependence and perform poorly due to not considering spurious correlations learnt from model training via unobserved confounders. A well-known existing causal inference method like back-door adjustment cannot be applied to remove spurious correlations as it requires the observation of confounders. In this paper, we propose a novel method that effectively deals with hidden confounders by successfully implementing front-door adjustment (FA). FA requires the choice of a mediator, which we regard as the semantic information of images that helps access the causal mechanism without the need for observing confounders. Further, we propose to estimate the combination of the mediator with other observed images in the front-door formula via style transfer algorithms. Our use of style transfer to estimate FA is novel and sensible for OOD generalisation, which we justify by extensive experimental results on widely used benchmark datasets.",KDD
"Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",KDD
"The H-index of a node in a static network is the maximum value h such that at least h of its neighbors have a degree of at least h. Recently, a generalized version, the n-th order H-index, was introduced, allowing to relate degree centrality, H-index, and the k-core of a node. We extend the n-th order H-index to temporal networks and define corresponding temporal centrality measures and temporal core decompositions. Our n-th order temporal H-index respects the reachability in temporal networks leading to node rankings, which reflect the importance of nodes in spreading processes. We derive natural decompositions of temporal networks into subgraphs with strong temporal coherence. We analyze a recursive computation scheme and develop a highly scalable streaming algorithm. Our experimental evaluation demonstrates the efficiency of our algorithms and the conceptional validity of our approach. Specifically, we show that the n-th order temporal H-index is a strong heuristic for identifying possible super-spreaders in evolving social networks and detects temporally well-connected components.",KDD
"Recently, how to protect the Intellectual Property (IP) of deep neural networks (DNN) becomes a major concern for the AI industry. To combat potential model piracy, recent works explore various watermarking strategies to embed secret identity messages into the prediction behaviors or the internals (e.g., weights and neuron activation) of the target model. Sacrificing less functionality and involving more knowledge about the target model, the latter branch of watermarking schemes (i.e., white-box model watermarking) is claimed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts and applications in the industry.
In this paper, we present the first effective removal attack which cracks almost all the existing white-box watermarking schemes with provably no performance overhead and no required prior knowledge. By analyzing these IP protection mechanisms at the granularity of neurons, we for the first time discover their common dependence on a set of fragile features of a local neuron group, all of which can be arbitrarily tampered by our proposed chain of invariant neuron transforms. On nine state-of-the-art white-box watermarking schemes and a broad set of industry-level DNN architectures, our attack for the first time reduces the embedded identity message in the protected models to be almost random. Meanwhile, unlike known removal attacks, our attack requires no prior knowledge on the training data distribution or the adopted watermark algorithms, and leaves model functionality intact.",KDD
"Recent semi-supervised anomaly detection methods that are trained using small labeled anomaly examples and large unlabeled data (mostly normal data) have shown largely improved performance over unsupervised methods. However, these methods often focus on fitting abnormalities illustrated by the given anomaly examples only (i.e., seen anomalies), and consequently they fail to generalize to those that are not, i.e., new types/classes of anomaly unseen during training. To detect both seen and unseen anomalies, we introduce a novel deep weakly-supervised approach, namely Pairwise Relation prediction Network (PReNet), that learns pairwise relation features and anomaly scores by predicting the relation of any two randomly sampled training instances, in which the pairwise relation can be anomaly-anomaly, anomaly-unlabeled, or unlabeled-unlabeled. Since unlabeled instances are mostly normal, the relation prediction enforces a joint learning of anomaly-anomaly, anomaly-normal, and normal-normal pairwise discriminative patterns, respectively. PReNet can then detect any seen/unseen abnormalities that fit the learned pairwise abnormal patterns, or deviate from the normal patterns. Further, this pairwise approach also seamlessly and significantly augments the training anomaly data. Empirical results on 12 real-world datasets show that PReNet significantly outperforms nine competing methods in detecting seen and unseen anomalies. We also theoretically and empirically justify the robustness of our model w.r.t. anomaly contamination in the unlabeled data. The code is available at https://github.com/mala-lab/PReNet.",KDD
"The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution (CPA-LGC ) method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user-item MC ratings into an expanded bipartite graph to potentially learn from the collaborative signal in MC ratings. Next, to strengthen the capability of criteria preference awareness, CPA-LGC incorporates newly characterized embeddings, including user-specific criteria-preference embeddings and item-specific criterion embeddings, into our graph convolution model. Through comprehensive evaluations using four real-world datasets, we demonstrate (a) the superiority over benchmark MC recommendation methods and benchmark recommendation methods using GNNs with tremendous gains, (b) the effectiveness of core components in CPA-LGC, and (c) the computational efficiency.",KDD
"Vision-based ego-centric 3D human pose estimation (ego-HPE) is essential to support critical applications of xR-technologies. However, severe self-occlusions and strong distortion introduced by the fish-eye view from the head mounted camera, make ego-HPE extremely challenging. To address these challenges, we propose a domain-guided spatio-temporal transformer model that leverages information specific to ego-views. Powered by this domain-guided transformer, we build Egocentric Spatio-Temporal Self-Attention Network (Ego-STAN), which uses 2D image representations and spatio-temporal attention to address both distortions and self-occlusions in ego-HPE. Additionally, we introduce a spatial concept called feature map tokens (FMT) which endows Ego-STAN with the ability to draw complex spatio-temporal information encoded in ego-centric videos. Our quantitative evaluation on the contemporary xR-EgoPose dataset, achieves a 38.2% improvement on the highest error joints against the SOTA ego-HPE model, while accomplishing a 22% decrease in the number of parameters. Finally, we also demonstrate the generalization capabilities of our model to real-world HPE tasks beyond ego-views achieving 7.7% improvement on 2D human pose estimation with the Human3.6M dataset. Our code is also made available at: https://github.com/jmpark0808/Ego-STAN",KDD
"Federated learning enables learning from decentralized data sources without compromising privacy, which makes it a crucial technique. However, it is vulnerable to model poisoning attacks, where malicious clients interfere with the training process. Previous defense mechanisms have focused on the server-side by using careful model aggregation, but this may not be effective when the data is not identically distributed or when attackers can access the information of benign clients. In this paper, we propose a new defense mechanism that focuses on the client-side, called FedDefender, to help benign clients train robust local models and avoid the adverse impact of malicious model updates from attackers, even when a server-side defense cannot identify or remove adversaries. Our method consists of two main components: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation. These components are used to find noise-resilient model parameters while accurately extracting knowledge from a potentially corrupted global model. Our client-side defense strategy has a flexible structure and can work in conjunction with any existing server-side strategies. Evaluations of real-world scenarios across multiple datasets show that the proposed method enhances the robustness of federated learning against model poisoning attacks.",KDD
"Despite their capacity to convey knowledge, most existing knowledge graphs (KGs) are created for specific domains using low-resource data sources, especially those in non-global languages, and thus unavoidably suffer from the incompleteness problem. The automatic discovery of missing triples for KG completion is thus hindered by the challenging long-tail relations problem in low-resource KGs. Few-shot learning models trained on rich-resource KGs are unable to tackle this challenge due to a lack of generalization. To alleviate the impact of the intractable long-tail problem on low-resource KG completion, in this paper, we propose a novel few-shot learning framework empowered by multi-view task representation generation. The framework consists of four components, i.e., few-shot learner, perturbed few-shot learner, relation knowledge distiller, and pairwise contrastive distiller. The key idea is to utilize the different views of each few-shot task to improve and regulate the training of the few-shot learner. For each few-shot task, instead of augmenting it by complicated task designs, we generate its representation of different views using the relation knowledge distiller and perturbed few-shot learner, which are obtained by distilling knowledge from a KG encoder and perturbing the few-shot learner. Then, the generated representation of different views is utilized by the pairwise contrastive distiller based on a teacher-student framework to distill the knowledge of how to represent relations from different views into the few-shot learner and facilitate few-shot learning. Extensive experiments conducted on several real-world low-resource KGs validate the effectiveness of our proposed method.",KDD
"The identification of the set of k most central nodes of a graph, or centrality maximization, is a key task in network analysis, with various applications ranging from finding communities in social and biological networks to understanding which seed nodes are important to diffuse information in a graph. As the exact computation of centrality measures does not scale to modern-sized networks, the most practical solution is to resort to rigorous, but efficiently computable, randomized approximations. In this work we present CentRA, the first algorithm based on progressive sampling to compute high-quality approximations of the set of k most central nodes. CentRA is based on a novel approach to efficiently estimate Monte Carlo Rademacher Averages, a powerful tool from statistical learning theory to compute sharp data-dependent approximation bounds. Then, we study the sample complexity of centrality maximization using the VC-dimension, a key concept from statistical learning theory. We show that the number of random samples required to compute high-quality approximations scales with finer characteristics of the graph, such as its vertex diameter, or of the centrality of interest, significantly improving looser bounds derived from standard techniques. We apply CentRA to analyze large real-world networks, showing that it significantly outperforms the state-of-the-art approximation algorithm in terms of number of samples, running times, and accuracy.",KDD
"This paper explores parallel computing systems for efficient subgraph query processing in large graphs. We investigate how to take advantage of the inherent parallelism of parallel computing systems for both intraquery and interquery optimization during subgraph query processing. Rather than relying on widely-used hash-based methods, we utilize and extend locality sensitive hashing methods. For intraquery optimization, we use the structures of both the data graph and subgraph query to design a query-constraint locality sensitive hashing method named QCMH, which can be used to merge multiple tasks during a single subgraph query processing. For interquery optimization, we propose a query locality sensitive hashing method named QMH, which can be used to detect common subgraphs among different subgraph queries, thereby merging multiple subgraph queries. Our proposed methods can reduce the redundant computation among multiple tasks duringa single subgraph query processing or multiple queries. Extensive experimental studies on large real and synthetic graphs show that our proposed methods can improve query performance compared to state-of-the-art methods by 10% to 50%.",KDD
"In the multi-instance learning (MIL) setting instances are grouped together into bags. Labels are provided only for the bags and not on the level of individual instances. A positive bag label means that at least one instance inside the bag is positive, while a negative bag label restricts all the instances in the bag to be negative. MIL data naturally arises in many contexts, such as anomaly detection, where labels are rare and costly, and one often ends up annotating the label for sets of instances. Moreover, in many real-world anomaly detection problems, only positive labels are collected because they usually represent critical events. Such a setting, where only positive labels are provided along with unlabeled data, is called Positive and Unlabeled (PU) learning. Despite being useful for several use cases, there is no work dedicated to learning from positive and unlabeled data in a multi-instance setting for anomaly detection. Therefore, we propose the first method that learns from PU bags in anomaly detection. Our method uses an autoencoder as an underlying anomaly detector. We alter the autoencoder's objective function and propose a new loss that allows it to learn from positive and unlabeled bags of instances. We theoretically analyze this method. Experimentally, we evaluate our method on 30 datasets and show that it performs better than multiple baselines adapted to work in our setting.",KDD
"Automated Machine Learning (AutoML) is a promising direction for democratizing AI by automatically deploying Machine Learning systems with minimal human expertise. The core technical challenge behind AutoML is optimizing the pipelines of Machine Learning systems (e.g. the choice of preprocessing, augmentations, models, optimizers, etc.). Existing Pipeline Optimization techniques fail to explore deep interactions between pipeline stages/components. As a remedy, this paper proposes a novel neural architecture that captures the deep interaction between the components of a Machine Learning pipeline. We propose embedding pipelines into a latent representation through a novel per-component encoder mechanism. To search for optimal pipelines, such pipeline embeddings are used within deep-kernel Gaussian Process surrogates inside a Bayesian Optimization setup. Furthermore, we meta-learn the parameters of the pipeline embedding network using existing evaluations of pipelines on diverse collections of related datasets (a.k.a. meta-datasets). Through extensive experiments on three large-scale meta-datasets, we demonstrate that pipeline embeddings yield state-of-the-art results in Pipeline Optimization.",KDD
"Contextual bandits algorithms aim to choose the optimal arm with the highest reward out of a set of candidates based on the contextual information. Various bandit algorithms have been applied to real-world applications due to their ability of tackling the exploitation-exploration dilemma. Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs). Instead of estimating rigid user clusters as in existing works, we model the ""fine-grained"" collaborative effects through estimated user graphs in terms of exploitation and exploration respectively. Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration. Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework.",KDD
"Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data diversity and explores the latent activity properties. Then, we propose a diversity preservation module to preserve the diversity of learned features by enlarging the distribution divergence between the original and augmented domains. Meanwhile, DDLearn also enhances semantic discrimination by learning discriminative representations with supervised contrastive learning. Extensive experiments on three public HAR datasets demonstrate that our method significantly outperforms state-of-art methods by an average accuracy improvement of 9.5% under the low-resource distribution shift scenarios, while being a generic, explainable, and flexible framework. Code is available at: https://github.com/microsoft/robustlearn.",KDD
"In cross-silo federated learning (FL), the data among clients are usually statistically heterogeneous (aka not independent and identically distributed, non-IID) due to diversified data sources, lowering the accuracy of FL. Although many personalized FL (PFL) approaches have been proposed to address this issue, they are only suitable for data with specific degrees of statistical heterogeneity. In the real world, the heterogeneity of data among clients is often immeasurable due to privacy concern, making the targeted selection of PFL approaches difficult. Besides, in cross-silo FL, clients are usually from different organizations, tending to hold architecturally different private models. In this work, we propose a novel FL framework, FedAPEN, which combines mutual learning and ensemble learning to take the advantages of private and shared global models while allowing heterogeneous models. Within FedAPEN, we propose two mechanisms to coordinate and promote model ensemble such that FedAPEN achieves excellent accuracy on various data distributions without prior knowledge of data heterogeneity, and thus, obtains the adaptability to data heterogeneity. We conduct extensive experiments on four real-world datasets, including: 1) Fashion MNIST, CIFAR-10, and CIFAR-100, each with ten different types and degrees of label distribution skew; and 2) eICU with feature distribution skew. The experiments demonstrate that FedAPEN almost obtains superior accuracy on data with varying types and degrees of heterogeneity compared with baselines.",KDD
"Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS for a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.",KDD
"Diffusion on graphs is ubiquitous with numerous high-impact applications, ranging from the study of residential segregation in socioeconomics and activation cascading in neuroscience, to the modeling of disease contagion in epidemiology and malware spreading in cybersecurity. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) estimation error of diffusion parameters is unavoidable due to NP-hardness of diffusion parameter estimation, and (b) the MLE formulation is sensitive to estimation error of diffusion parameters. To overcome the inherent limitation of the MLE formulation, we propose a novel barycenter formulation: finding the barycenter of the posterior distribution of histories, which is provably stable against the estimation error of diffusion parameters. We further develop an effective solver named DIffusion hiTting Times with Optimal proposal (DITTO) by reducing the problem to estimating posterior expected hitting times via the Metropolis-Hastings Markov chain Monte Carlo method (M-H MCMC) and employing an unsupervised graph neural network to learn an optimal proposal to accelerate the convergence of M-H MCMC. We conduct extensive experiments to demonstrate the efficacy of the proposed method. Our code is available at https://github.com/q-rz/KDD23-DITTO. The appendix can be found at https://arxiv.org/abs/2306.00488.",KDD
"Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a labeled source domain to an unlabeled target domain without access to the source domain data, preserving source domain privacy. Despite its prevalence in visual applications, SFDA is largely unexplored in time series applications. The existing SFDA methods that are mainly designed for visual applications may fail to handle the temporal dynamics in time series, leading to impaired adaptation performance. To address this challenge, this paper presents a simple yet effective approach for source-free domain adaptation on time series data, namely MAsk and imPUte (MAPU). First, to capture temporal information of the source domain, our method performs random masking on the time series signals while leveraging a novel temporal imputer to recover the original signal from a masked version in the embedding space. Second, in the adaptation step, the imputer network is leveraged to guide the target model to produce target features that are temporally consistent with the source features. To this end, our MAPU can explicitly account for temporal dependency during the adaptation while avoiding the imputation in the noisy input space. Our method is the first to handle temporal consistency in SFDA for time series data and can be seamlessly equipped with other existing SFDA methods. Extensive experiments conducted on three real-world time series datasets demonstrate that our MAPU achieves significant performance gain over existing methods. Our code is available at: https://github.com/mohamedr002/MAPU_SFDA_TS.",KDD
"Survival analysis, aka time-to-event analysis, has a wide-ranging impact on patient care. Federated Survival Analysis (FSA) is an emerging Federated Learning (FL) paradigm for performing survival analysis on distributed decentralized data available at multiple medical institutions. FSA enables individual medical institutions, referred to as clients, to improve their survival predictions while ensuring privacy. However, FSA faces challenges due to non-linear and non-IID data distributions among clients, as well as bias caused by censoring. Although recent studies have adapted Cox Proportional Hazards (CoxPH) survival models for FSA, a systematic exploration of these challenges is currently lacking. In this paper, we address these critical challenges by introducing FedPseudo, a pseudo value-based deep learning framework for FSA. FedPseudo uses deep learning models to learn robust representations from non-linear survival data, leverages the power of pseudo values to handle non-uniform censoring, and employs FL algorithms such as FedAvg to learn model parameters. We propose a novel and simple approach for estimating pseudo values for FSA. We provide theoretical proof that the estimated pseudo values, referred to as Federated Pseudo Values, are consistent. Moreover, our empirical results demonstrate that they can be computed faster than traditional methods of deriving pseudo values. To ensure and enhance the privacy of both the estimated pseudo values and the shared model parameters, we systematically investigate the application of differential privacy (DP) on both the federated pseudo values and local model updates. Furthermore, we adapt V -Usable Information metric to quantify the informativeness of a client's data for training a survival model and utilize this metric to show the advantages of participating in FSA. We conducted extensive experiments on synthetic and real-world survival datasets to demonstrate that our FedPseudo framework achieves better performance than other FSA approaches and performs similarly to the best centrally trained deep survival model. Moreover, FedPseudo consistently achieves superior results across different censoring settings.",KDD
"Many real-world machine learning problems involve structured prediction beyond categorical labels. However, most existing robustness certification works are devoted to the classification case. It remains open for robustness certification for more general outputs. In this paper, we propose a novel framework of robustness certification for structured prediction problems, where the output space is modeled as a semimetric space with a distance function that satisfies non-negativity and symmetry but not necessarily the triangle inequality. We further develop our tailored certification methods for binary, numerical, and hybrid inputs in structured prediction. Experiment results show that our method achieves tighter robustness guarantees than the SOTA structured certification baseline for numerical inputs (for which it only supports) with â„“2 norm perturbation when outputs are measured by intersection over union (IoU) similarity, total variation distance, and perceptual distance. Moreover, we achieve good robustness certification for binary inputs with â„“0 norm perturbation and hybrid inputs with corresponding perturbation when outputs are measured by Manhattan distance.",KDD
"Real-world graphs such as social networks, communication networks, and rating networks are constantly evolving over time. Many deep learning architectures have been developed to learn effective node representations using both graph structure and dynamics. While being crucial for practical applications, the robustness of these representation learners for dynamic graphs in the presence of adversarial attacks is highly understudied. In this work, we design a novel adversarial attack on discrete-time dynamic graph models where we desire to perturb the input graph sequence in a manner that preserves the temporal dynamics of the graph while dropping the performance of representation learners. To this end, we motivate a novel Temporal Dynamics-Aware Perturbation (TDAP) constraint, which ensures that perturbations introduced at each time step are restricted to only a small fraction of the number of changes in the graph since the previous time step. We present a theoretically-motivated Projected Gradient Descent approach for dynamic graphs to find effective perturbations under the TDAP constraint. Experiments on two tasks - dynamic link prediction and node classification, show that our approach is up to 4x more effective than the baseline methods for attacking these models. We extend our approach to a more practical online setting where graphs become available in real-time and show up to 5x superior performance over baselines We also show that our approach successfully evades state-of-the-art neural approaches for anomaly detection, thereby promoting the need to study robustness as a part of representation-learning approaches for dynamic graphs.",KDD
"Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Validation Indices (CVIs), i.e., internal measures of cluster quality (no ground truth required). CARL-G is adaptable to different clustering methods and CVIs, and we show that with the right choice of clustering method and CVI, CARL-G outperforms node classification baselines on 4/5 datasets with up to a 79Ã— training speedup compared to the best-performing baseline. CARL-G also performs at par or better than baselines in node clustering and similarity search tasks, training up to 1,500Ã— faster than the best-performing baseline. Finally, we also provide theoretical foundations for the use of CVI-inspired losses in graph representation learning.",KDD
"Brain extraction, registration and segmentation are indispensable preprocessing steps in neuroimaging studies. The aim is to extract the brain from raw imaging scans (i.e., extraction step), align it with a target brain image (i.e., registration step) and label the anatomical brain regions (i.e., segmentation step). Conventional studies typically focus on developing separate methods for the extraction, registration and segmentation tasks in a supervised setting. The performance of these methods is largely contingent on the quantity of training samples and the extent of visual inspections carried out by experts for error correction. Nevertheless, collecting voxel-level labels and performing manual quality control on high-dimensional neuroimages (e.g., 3D MRI) are expensive and time-consuming in many medical studies. In this paper, we study the problem of one-shot joint extraction, registration and segmentation in neuroimaging data, which exploits only one labeled template image (a.k.a. atlas) and a few unlabeled raw images for training. We propose a unified end-to-end framework, called JERS, to jointly optimize the extraction, registration and segmentation tasks, allowing feedback among them. Specifically, we use a group of extraction, registration and segmentation modules to learn the extraction mask, transformation and segmentation mask, where modules are interconnected and mutually reinforced by self-supervision. Empirical results on real-world datasets demonstrate that our proposed method performs exceptionally in the extraction, registration and segmentation tasks.",KDD
"Database-native machine learning operators are highly desired for efficient I/O and computation costs. While most existing machine learning algorithms assume the time series data fully available and readily ordered by timestamps, it is not the case in practice. Commodity time series databases store the data in pages with possibly overlapping time ranges, known as LSM-Tree based storage. Data points in a page could be incomplete, owing to either missing values or out-of-order arrivals, which may be inserted by the imputed or delayed points in the following pages. Likewise, data points in a page could also be updated by others in another page, for dirty data repairing or re-transmission. A straightforward idea is thus to first merge and order the data points by timestamps, and then apply the existing learning algorithms. It is not only costly in I/O but also prevents pre-computation of model learning. In this paper, we propose to offline learn the AR models locally in each page on incomplete data, and online aggregate the stored models in different pages with the consideration of the aforesaid inserted and updated data points. Remarkably, the proposed method has been deployed and included as a function in an open source time series database, Apache IoTDB. Extensive experiments in the system demonstrate that our proposal LSMAR shows up to one order-of-magnitude improvement in learning time cost. It needs only about 10s of milliseconds for learning over 1 million data points.",KDD
"Recently, pretrained models have achieved remarkable performance not only in natural language processing but also in information retrieval (IR). Previous studies show that IR-oriented pretraining tasks can achieve better performance than only finetuning pretrained language models in IR datasets. Besides, the massive search log data obtained from mainstream search engines can be used in IR pretraining, for it contains users' implicit judgments of document relevance under a concrete query. However, existing methods mainly use direct query-document click signals to pretrain models. The potential supervision signals from search logs are far from being well explored. In this paper, we propose to comprehensively leverage four query-document relevance relations, including co-interaction and multi-hop relations, to pretrain ranking models in IR. Specifically, we focus on the user's click behavior and construct an Interaction Graph to represent the global relevance relations between queries and documents from all search logs. With the graph, we can consider the co-interaction and multi-hop q-d relationships through their neighbor nodes. Based on the relations extracted from the interaction graph, we propose four strategies to generate contrastive positive and negative q-d pairs and use these data to pretrain ranking models. Experimental results on both industrial and academic datasets demonstrate the effectiveness of our method.",KDD
"Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is Adversarial Training (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch size. We further propose a novel approach, Diffusion Enhanced Adversarial Training (DEAT), to manipulate the diffusion term to improve robust generalization with virtually no extra computational burden. We theoretically show that DEAT obtains a tighter generalization bound than PGD-AT. Our empirical investigation is extensive and firmly attests that DEAT universally outperforms PGD-AT by a significant margin.",KDD
"Federated Learning (FL) allows clients to form a consortium to train a global model under the orchestration of a central server while keeping data on the local client without sharing it, thus mitigating data privacy issues. However, training a robust global model is challenging since the local data is invisible to the server. The local data of clients are naturally heterogeneous, while some clients can use corrupted data or send malicious updates to interfere with the training process artificially. Meanwhile, communication and computation costs are inevitable challenges in designing a practical FL algorithm. In this paper, to improve the robustness of FL, we propose a Shapley value-inspired adaptive weighting mechanism, which regards the FL training as sequential cooperative games and adjusts clients' weights according to their contributions. We also develop a client sampling strategy based on importance sampling, which can reduce the communication cost by optimizing the variance of the global updates according to the weights of clients. Furthermore, to diminish the computation cost of the server, we propose a weight calculation method by estimating differences between the Shapley value of clients. Our experimental results on several real data sets demonstrate the effectiveness of our approaches.",KDD
"Quantitative stock investment is a fundamental financial task that highly relies on accurate prediction of market status and profitable investment decision making. Despite recent advances in deep learning (DL) have shown stellar performance on capturing trading opportunities in the stochastic stock market, the performance of existing DL methods is unstable with sensitivity to network initialization and hyperparameter selection. One major limitation of existing works is that investment decisions are made based on one individual neural network predictor with high uncertainty, which is inconsistent with the workflow in real-world trading firms. To tackle this limitation, we propose AlphaMix, a novel three-stage mixture-of-experts (MoE) framework for quantitative investment to mimic the efficient bottom-up hierarchical trading strategy design workflow of successful trading companies. In Stage one, we introduce an efficient ensemble learning method, whose computational and memory costs are significantly lower comparing to traditional ensemble methods, to train multiple groups of trading experts with personalised market understanding and trading styles. In Stage two, we collect diversified investment suggestions through building a pool of trading experts utilizing hyperparameter level and initialization level diversity of neural networks for post hoc ensemble construction. In Stage three, we design three different mechanisms, namely as-needed router, with-replacement selection and integrated expert soup, to dynamically pick experts from the expert pool, which takes the responsibility of a portfolio manager. Through extensive experiments on US and Chinese stock markets, we demonstrate that AlphaMix significantly outperforms many state-of-the-art baselines in terms of 7 popular financial criteria.",KDD
"Recently, ""pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ""negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",KDD
"In this paper, we present the ""joint pre-training and local re-training'' framework for learning and applying multi-source knowledge graph (KG) embeddings. We are motivated by the fact that different KGs contain complementary information to improve KG embeddings and downstream tasks. We pre-train a large teacher KG embedding model over linked multi-source KGs and distill knowledge to train a student model for a task-specific KG. To enable knowledge transfer across different KGs, we use entity alignment to build a linked subgraph for connecting the pre-trained KGs and the target KG. The linked subgraph is re-trained for three-level knowledge distillation from the teacher to the student, i.e., feature knowledge distillation, network knowledge distillation, and prediction knowledge distillation, to generate more expressive embeddings. The teacher model can be reused for different target KGs and tasks without having to train from scratch. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our framework.",KDD
"Estimating individual treatment effects from observational data is a fundamental problem in causal inference. To accurately estimate treatment effects in the spatial domain, we need to address certain aspects such as how to use the spatial coordinates of covariates and treatments and how the covariates and the treatments interact spatially. We introduce a new problem of predicting treatment effects on time series outcomes from spatial graph data with a hierarchical structure. To address this problem, we propose a spatial intervention neural network (SINet) that leverages the hierarchical structure of spatial graphs to learn a rich representation of the covariates and the treatments and exploits this representation to predict a time series of treatment outcome. Using a multi-agent simulator, we synthesized a crowd movement guidance dataset and conduct experiments to estimate the conditional average treatment effect, where we considered the initial locations of the crowds as covariates, route guidance as a treatment, and number of agents reaching a goal at each time stamp as the outcome. We employed state-of-the-art spatio-temporal graph neural networks and neural network-based causal inference methods as baselines, and show that our proposed method outperformed baselines both quantitatively and qualitatively.",KDD
"Cloud-native applications using microservice architectures are rapidly replacing traditional monolithic applications. To meet end-to-end QoS guarantees and enhance user experience, each component microservice must be provisioned with sufficient resources to handle incoming API calls. Accurately predicting the latency of microservices-based applications is critical for optimizing resource allocation, which turns out to be extremely challenging due to the complex dependencies between microservices and the inherent stochasticity. To tackle this problem, various predictors have been designed based on the Microservice Call Graph. However, Microservice Call Graphs do not take into account the API-specific information, cannot capture important temporal dependencies, and cannot scale to large-scale applications.
In this paper, we propose PERT-GNN, a generic graph neural network based framework to predict the end-to-end latency for microservice applications. PERT-GNN characterizes the interactions or dependency of component microservices observed from prior execution traces of the application using the Program Evaluation and Review Technique (PERT). We then construct a graph neural network based on the generated PERT Graphs, and formulate the latency prediction task as a supervised graph regression problem using the graph transformer method. PERT-GNN can capture the complex temporal causality of different microservice traces, thereby producing more accurate latency predictions for various applications. Evaluations based on datasets generated from common benchmarks and large-scale Alibaba microservice traces show that PERT-GNN can outperform other models by a large margin. In particular, PERT-GNN is able to predict the latency of microservice applications with less than 12% mean absolute percentage error.",KDD
"This paper presents ExplainableFold (xFold), which is an Explainable AI framework for protein structure prediction. Despite the success of AI-based methods such as AlphaFold (Î±Fold) in this field, the underlying reasons for their predictions remain unclear due to the black-box nature of deep learning models. To address this, we propose a counterfactual learning framework inspired by biological principles to generate counterfactual explanations for protein structure prediction, enabling a dry-lab experimentation approach. Our experimental results demonstrate the ability of ExplainableFold to generate high-quality explanations for AlphaFold's predictions, providing near-experimental understanding of the effects of amino acids on 3D protein structure. This framework has the potential to facilitate a deeper understanding of protein structures. Source code and data of the ExplainableFold project are available at https://github.com/rutgerswiselab/ExplainableFold.",KDD
"Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under the invariant learning framework. With contrastive learning, we propose a learning potential-guided metric for domain heterogeneity by promoting learning variant features. Then we notice the differences between seeking variance-based heterogeneity and training invariance-based generalizable model. We thus propose a novel method called H eterogeneity-based Two-stage Contrastive Learning (HTCL) for the DG task. In the first stage, we generate the most heterogeneous dividing pattern with our contrastive metric. In the second stage, we employ an invariance-aimed contrastive learning by re-building pairs with the stable relation hinted by domains and classes, which better utilizes generated domain labels for generalization learning. Extensive experiments show HTCL better digs heterogeneity and yields great generalization performance.",KDD
"In recent years, online social networks have been the target of adversaries who seek to introduce discord into societies, to undermine democracies and to destabilize communities. Often the goal is not to favor a certain side of a conflict but to increase disagreement and polarization. To get a mathematical understanding of such attacks, researchers use opinion-formation models from sociology, such as the Friedkin--Johnsen model, and formally study how much discord the adversary can produce when altering the opinions for only a small set of users. In this line of work, it is commonly assumed that the adversary has full knowledge about the network topology and the opinions of all users. However, the latter assumption is often unrealistic in practice, where user opinions are not available or simply difficult to estimate accurately.
To address this concern, we raise the following question: Can an attacker sow discord in a social network, even when only the network topology is known? We answer this question affirmatively. We present approximation algorithms for detecting a small set of users who are highly influential for the disagreement and polarization in the network. We show that when the adversary radicalizes these users and if the initial disagreement/polarization in the network is not very high, then our method gives a constant-factor approximation on the setting when the user opinions are known. To find the set of influential users, we provide a novel approximation algorithm for a variant of MaxCut in graphs with positive and negative edge weights. We experimentally evaluate our methods, which have access only to the network topology, and we find that they have similar performance as methods that have access to the network topology and all user opinions. We further present an NP-hardness proof, which was left as an open question by Chen and Racz [IEEE Transactions on Network Science and Engineering, 2021].",KDD
"Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of our method in generating diverse counterfactuals of actionability and plausibility. Our counterfactual engine is more efficient than counterparts of the same capacity while yielding the lowest re-identification risks.",KDD
"Recently, spatiotemporal graph convolutional networks are becoming popular in the field of traffic flow prediction and significantly improve prediction accuracy. However, the majority of existing traffic flow prediction models are tailored to static traffic networks and fail to model the continuous evolution and expansion of traffic networks. In this work, we move to investigate the challenge of traffic flow prediction on an expanding traffic network. And we propose an efficient and effective continual learning framework to achieve continuous traffic flow prediction without the access to historical graph data, namely Pattern Expansion and Consolidation based on Pattern Matching based (PECPM). Specifically, we first design a pattern bank based on pattern matching to store representative patterns of the road network. With the expansion of the road network, the model configured with such a bank module can achieve continuous traffic prediction by effectively managing patterns stored in the bank. The core idea is to continuously update new patterns while consolidating learned ones. Specifically, we design a pattern expansion mechanism that can detect evolved and new patterns from the updated network, then these unknown patterns are expanded into the pattern bank to adapt to the updated road network. Additionally, we propose a pattern consolidation mechanism that includes both a bank preservation mechanism and a pattern traceability mechanism. This can effectively consolidate the learned patterns in the bank without requiring access to detailed historical graph data. We construct experiments on real-world traffic datasets to demonstrate the competitive performance, superior efficiency, and strong generalization ability of PECPM.",KDD
"User financial default prediction plays a critical role in credit risk forecasting and management. It aims at predicting the probability that the user will fail to make the repayments in the future. Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions. However, these methods cannot get satisfied results, especially for users with limited information. Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns. In this paper, we fill in this gap by proposing a motif-preserving Graph Neural Network with curriculum learning (MotifGNN) to jointly learn the lower-order structures from the original graph and higher-order structures from multi-view motif-based graphs for financial default prediction. Specifically, to solve the problem of weak connectivity in motif-based graphs, we design the motif-based gating mechanism. It utilizes the information learned from the original graph with good connectivity to strengthen the learning of the higher-order structure. And considering that the motif patterns of different samples are highly unbalanced, we propose a curriculum learning mechanism on the whole learning process to more focus on the samples with uncommon motif distributions. Extensive experiments on one public dataset and two industrial datasets all demonstrate the effectiveness of our proposed method.",KDD
"Antimicrobial peptides (AMPs) are promising therapeutic approaches against drug-resistant pathogens. Recently, deep generative models are used to discover new AMPs. However, previous studies mainly focus on peptide sequence attributes and do not consider crucial structure information. In this paper, we propose a latent sequence-structure model for designing AMPs (LSSAMP). LSSAMP exploits multi-scale vector quantization in the latent space to represent secondary structures (e.g. alpha helix and beta sheet). By sampling in the latent space, LSSAMP can simultaneously generate peptides with ideal sequence attributes and secondary structures. Experimental results show that the peptides generated by LSSAMP have a high probability of antimicrobial activity. Our wet laboratory experiments verified that two of the 21 candidates exhibit strong antimicrobial activity. The code is released at https://github.com/dqwang122/LSSAMP.",KDD
"Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new model named PoGeVon which leverages variational autoencoder (VAE) to predict missing values over both node time series features and graph structures. In particular, we propose a new node position embedding based on random walk with restart (RWR) in the encoder with provable higher expressive power compared with message-passing based graph neural networks (GNNs). We further design a decoder with 3-stage predictions from the perspective of multi-task learning to impute missing values in both time series and graph structures reciprocally. Experiment results demonstrate the effectiveness of our model over baselines.",KDD
"The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.
In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on multivariate singular spectrum analysis and cumulative sum statistics. To efficiently update the RCA model, we propose an incremental disentangled causal graph learning approach to decouple the state-invariant and state-dependent information. After that, CORAL applies a random walk with restarts to the updated causal graph to accurately identify root causes. The online RCA process terminates when the causal graph and the generated root cause list converge. Extensive experiments on three real-world datasets demonstrate the effectiveness and superiority of the proposed framework.",KDD
"Computerized Adaptive Testing (CAT) refers to an online system that adaptively selects the best-suited question for students with various abilities based on their historical response records. Compared with traditional CAT methods based on heuristic rules, recent data-driven CAT methods obtain higher performance by learning from large-scale datasets. However, most CAT methods only focus on the quality objective of predicting the student ability accurately, but neglect concept diversity or question exposure control, which are important considerations in ensuring the performance and validity of CAT. Besides, the students' response records contain valuable relational information between questions and knowledge concepts. The previous methods ignore this relational information, resulting in the selection of sub-optimal test questions. To address these challenges, we propose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly, three objectives, namely quality, diversity and novelty, are introduced into the Scalarized Multi-Objective Reinforcement Learning framework of CAT, which respectively correspond to improving the prediction accuracy, increasing the concept diversity and reducing the question exposure. We use an Actor-Critic Recommender to select questions and optimize three objectives simultaneously by the scalarization function. Secondly, we utilize the graph neural network to learn relation-aware embeddings of questions and concepts. These embeddings are able to aggregate neighborhood information in the relation graphs between questions and concepts. We conduct experiments on three real-world educational datasets. The experimental results show that GMOCAT not only outperforms the state-of-the-art methods in the ability prediction, but also achieve superior performance in improving the concept diversity and alleviating the question exposure.",KDD
"In causal inference, it is common to select a subset of observed covariates, named the adjustment features, to be adjusted for estimating the treatment effect. For real-world applications, the abundant covariates are usually observed, which contain extra variables partially correlating to the treatment (treatment-only variables, e.g., instrumental variables) or the outcome (outcome-only variables, e.g., precision variables) besides the confounders (variables that affect both the treatment and outcome). In principle, unbiased treatment effect estimation is achieved once the adjustment features contain all the confounders. However, the performance of empirical estimations varies a lot with different extra variables. To solve this issue, variable separation/selection for treatment effect estimation has received growing attention when the extra variables contain instrumental variables and precision variables.
In this paper, assuming no mediator variables exist, we consider a more general setting by allowing for the existence of post-treatment and post-outcome variables rather than instrumental and precision variables in observed covariates. Our target is to separate the treatment-only variables from the adjustment features. To this end, we establish a metric named Optimal Adjustment Features(OAF), which empirically measures the asymptotic variance of the estimation. Theoretically, we show that our OAF metric is minimized if and only if adjustment features consist of the confounders and outcome-only variables, i.e., the treatment-only variables are perfectly separated. As optimizing the OAF metric is a combinatorial optimization problem, we introduce Reinforcement Learning (RL) and adopt the policy gradient to search for the optimal adjustment set. Empirical results on both synthetic and real-world datasets demonstrate that (a) our method successfully searches the optimal adjustment features and (b) the searched adjustment features achieve a more precise estimation of the treatment effect.",KDD
"The proliferation of the Internet has led to the emergence of online advertising, driven by the mechanics of online auctions. In these repeated auctions, software agents participate on behalf of aggregated advertisers to optimize for their long-term utility. To fulfill the diverse demands, bidding strategies are employed to optimize advertising objectives subject to different spending constraints. Existing approaches on constrained bidding typically rely on i.i.d. train and test conditions, which contradicts the adversarial nature of online ad markets where different parties possess potentially conflicting objectives. In this regard, we explore the problem of constrained bidding in adversarial bidding environments, which assumes no knowledge about the adversarial factors. Instead of relying on the i.i.d. assumption, our insight is to align the train distribution of environments with the potential test distribution meanwhile minimizing policy regret. Based on this insight, we propose a practical Minimax Regret Optimization (MiRO) approach that interleaves between a teacher finding adversarial environments for tutoring and a learner meta-learning its policy over the given distribution of environments. In addition, we pioneer to incorporate expert demonstrations for learning bidding strategies. Through a causality-aware policy design, we improve upon MiRO by distilling knowledge from the experts. Extensive experiments on both industrial data and synthetic data show that our method, MiRO with Causality-aware reinforcement Learning (MiROCL), outperforms prior methods by over 30%.",KDD
"Graph representation learning (GRL) is a powerful tool for graph analysis, which has gained massive attention from both academia and industry due to its superior performance in various real-world applications. However, the majority of existing works for GRL are dedicated to node-based tasks and thus focus on producing node representations. Despite such methods can be used to derive edge representations by regarding edges as nodes, they suffer from sub-par result utility in practical edge-wise applications, such as financial fraud detection and review spam combating, due to neglecting the unique properties of edges and their inherent drawbacks. Moreover, to our knowledge, there is a paucity of research devoted to edge representation learning. These methods either require high computational costs in sampling random walks or yield severely compromised representation quality because of falling short of capturing high-order information between edges. To address these challenges, we present TER and AER, which generate high-quality edge representation vectors based on the graph structure surrounding edges and edge attributes, respectively. In particular, TER can accurately encode high-order proximities of edges into low-dimensional vectors in a practically efficient and theoretically sound way, while AER augments edge attributes through a carefully-designed feature aggregation scheme. Our extensive experimental study demonstrates that the combined edge representations of TER and AER can achieve significantly superior performance in terms of edge classification on 8 real-life datasets, while being up to one order of magnitude faster than 16 baselines on large graphs.",KDD
"Graph Neural Networks (GNNs) have been a powerful tool to acquire high-quality node representations dealing with graphs, which strongly depends on a promising graph structure. In the real world scenarios, it is inevitable to introduce noises in graph topology. To prevent GNNs from the disturbance of irrelevant edges or missing edges, graph structure learning is proposed and has attracted considerable attentions in recent years. In this paper, we argue that current graph structure learning methods still pay no regard to the status of nodes and just judge all of their connections simultaneously using a monotonous standard, which will lead to indeterminacy and instability in the optimization process. We designate these methods as status-unaware models. To demonstrate the rationality of our point of view, we conduct exploratory experiments on publicly available datasets, and discover some exciting observations. Afterwards, we propose a new model named Graph Structure Learning via Progressive Strategy (PROSE) according to the observations, which uses a progressive strategy to acquire ideal graph structure in a status-aware way. Concretely, PROSE consists of progressive structure splitting module (PSS) and progressive structure refining module (PSR) to modify node connections according to their global potency, and we also introduce horizontal position encoding and vertical position encoding in order to capture fruitful graph topology information ignored by previous methods. On several widely-used graph datasets, we conduct extensive experiments to demonstrate the effectiveness of our model, and the source code 1 https://github.com/tigerbunny2023/PROSE is provided.",KDD
"Researchers recently investigated to explain Graph Neural Networks (GNNs) on the access to a task-specific GNN, which may hinder their wide applications in practice. Specifically, task-specific explanation methods are incapable of explaining pretrained GNNs whose downstream tasks are usually inaccessible, not to mention giving explanations for the transferable knowledge in pretrained GNNs. Additionally, task-specific methods only consider target models' output in the label space, which are coarse-grained and insufficient to reflect the model's internal logic. To address these limitations, we consider a two-stage explanation strategy, i.e., explainers are first pretrained in a task-agnostic fashion in the representation space and then further fine-tuned in the task-specific label space and representation space jointly if downstream tasks are accessible. The two-stage explanation strategy endows post-hoc graph explanations with the applicability to pretrained GNNs where downstream tasks are inaccessible and the capacity to explain the transferable knowledge in the pretrained GNNs. Moreover, as the two-stage explanation strategy explains the GNNs in the representation space, the fine-grained information in the representation space also empowers the explanations. Furthermore, to achieve a trade-off between the fidelity and intelligibility of explanations, we propose an explanation framework based on the Information Bottleneck principle, named Explainable Graph Information Bottleneck (EGIB). EGIB subsumes the task-specific explanation and task-agnostic explanation into a unified framework. To optimize EGIB objective, we derive a tractable bound and adopt a simple yet effective explanation generation architecture. Based on the unified framework, we further theoretically prove that task-agnostic explanation is a relaxed sufficient condition of task-specific explanation, which indicates the transferability of task-agnostic explanations. Extensive experimental results demonstrate the effectiveness of our proposed explanation method.",KDD
"Given its broad applications, time series analysis has gained substantial research attention but remains a very challenging task. Recent years have witnessed the great success of deep learning methods, eg., CNN and RNN, in time series classification and forecasting, but heterogeneity as the very nature of time series has not yet been addressed adequately and remains the performance ""treadstone."" In this light, we argue that the intra-sequence non-stationarity and inter-sequence asynchronism are two types of heterogeneities widely existed in multiple times series, and propose a hybrid attention network called WHEN as deep learning solution. WHEN features in two attention mechanisms in two different modules. In the WaveAtt module, we propose a novel data-dependent wavelet function and integrate it into the BiLSTM network as the wavelet attention, for the purpose of analyzing dynamic frequency components in nonstationary time series. In the DTWAtt module, we transform the dynamic time warping (DTW) technique into the form as the DTW attention, where all input sequences are synchronized with a universal parameter sequence to overcome the time distortion problem in multiple time series. WHEN with the hybrid attentions is then formulated as task-dependent neural network for either classification or forecasting tasks. Extensive experiments on 30 UEA datasets and 3 real-world datasets with rich competitive baselines demonstrate the excellent performance of our model. The ability of WHEN in dealing with time series heterogeneities is also elaborately explored via specially designed analysis.",KDD
"Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients can only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem remains challenging due to two major reasons: the global data variance among clients (i.e., the difference in data distributions among clients) and the local data insufficiency in each client (i.e., the lack of adequate local data for training). To overcome these two challenges, we propose a novel federated few-shot learning framework with two separately updated models and dedicated training strategies to reduce the adverse impact of global data variance and local data insufficiency. Extensive experiments on four prevalent datasets that cover news articles and images validate the effectiveness of our framework compared with the state-of-the-art baselines.",KDD
"Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and inter-class generalizability of the model. We create a novel contrastive meta-learning framework on graphs, named COSMIC, with two key designs. First, we propose to enhance the intra-class generalizability by involving a contrastive two-step optimization in each episode to explicitly align node embeddings in the same classes. Second, we strengthen the inter-class generalizability by generating hard node classes for classification via a novel similarity-sensitive mix-up strategy. Extensive experiments on prevalent few-shot node classification datasets verify the effectiveness of our framework and demonstrate its superiority over other state-of-the-art baselines.",KDD
"Conversational recommender systems~(CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data.
To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage recommendation dialogue simulator based on a conversation flow language model. Under the guidance of the learned user preference and dialogue schema, the flow language model can produce reasonable, coherent conversation flows, which can be further realized into complete dialogues. Based on the simulator, we perform the intervention at the representations of the interacted entities of target users, and design an adversarial training method with a curriculum schedule that can gradually optimize the data augmentation strategy. Extensive experiments show that our approach can consistently boost the performance of several competitive CRSs, and outperform other data augmentation methods, especially when the training data is limited. Our code is publicly available at https://github.com/RUCAIBox/CFCRS.",KDD
"Missing values, which are common in multivariate time series, is most important obstacle towards the utilization and interpretation of those data. Great efforts have been employed on how to accurately impute missing values in multivariate time series, and existing works either use deep learning networks to achieve deterministic imputations or aim at generating different plausible imputations by sampling multiple noises from a same distribution and then denoising them. However, these models either fall short of modeling the uncertainties of imputations due to their deterministic nature or perform poorly in terms of interpretability and imputation accuracy due to their ignorance of the correlations between the latent representations of both observed and missing values which are parts of samples from a same distribution. To this end, in this paper, we explicitly take the correlations between observed and missing values into account, and theoretically re-derive the Evidence Lower BOund (ELBO) of conditional diffusion model in the scenario of multivariate time series imputation. Based on the newly derived ELBO, we further propose a novel multivariate imputation diffusion model (MIDM) which is equipped with novel noise sampling, adding and denoising mechanisms for multivariate time series imputation, and the series of newly designed technologies jointly ensure the involving of the consistency between observed and missing values. Extensive experiments on both the tasks of multivariate time series imputation and forecasting witness the superiority of our proposed MIDM model on generating conditional estimations.",KDD
"Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction,3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the total energy to search for weight distribution of the three pretext tasks since total energy corresponding to the quality of 3D conformer. Extensive experiments on 2D molecular graphs are conducted to demonstrate the accuracy, efficiency and generalization ability of the proposed 3D PGT compared to various pre-training baselines.",KDD
"We propose a computationally efficient Lasso Random Project Bandit (LRP-Bandit) algorithm for sparse linear bandit problems under high-dimensional settings with limited samples. LRP-Bandit bridges Lasso and Random Projection as feature selection and dimension reduction techniques to alleviate the computational complexity and improve the regret performance. We demonstrate that for the total feature dimension d, the significant feature dimension s, and the sample size T, the expected cumulative regret under LRP-Bandit is upper bounded by Ã• (T2 over 3 s 3 over 2 log7 over 6 d), where Ã• suppresses the logarithmic dependence on T. Further, we show that when available samples are larger than a problem-dependent threshold, the regret upper bound for LRP-Bandit can be further improved to Ã• (sâˆšT log d). These regret upper bounds on T for both data-poor and data-rich regimes match the theoretical minimax lower bounds up to logarithmic factors. Through experiments, we show that LRP-Bandit is computationally efficient and outperforms other benchmarks on the expected cumulative regret.",KDD
"In this paper, we propose an adaptive learning paradigm for resource-constrained cross-device federated learning, in which heterogeneous local submodels with varying resources can be jointly trained to produce a global model. Different from existing studies, the submodel structures of different clients are formed by arbitrarily assigned neurons according to their local resources. Along this line, we first design a general resource-adaptive federated learning algorithm, namely RA-Fed, and rigorously prove its convergence with asymptotically optimal rate O(1/âˆšÎ“*TQ) under loose assumptions. Furthermore, to address both submodels heterogeneity and data heterogeneity challenges under non-uniform training, we come up with a new server aggregation mechanism RAM-Fed with the same theoretically proved convergence rate. Moreover, we shed light on several key factors impacting convergence, such as minimum coverage rate, data heterogeneity level, submodel induced noises. Finally, we conduct extensive experiments on two types of tasks with three widely used datasets under different experimental settings. Compared with the state-of-the-arts, our methods improve the accuracy up to 10% on average. Particularly, when submodels jointly train with 50% parameters, RAM-Fed achieves comparable accuracy to FedAvg trained with the full model.",KDD
"Conversion rate prediction is critical to many online applications such as digital display advertising. To capture dynamic data distribution, industrial systems often require retraining models on recent data daily or weekly. However, the delay of conversion behavior usually leads to incorrect labeling, which is called delayed feedback problem. Existing work may fail to introduce the correct information about false negative samples due to data sparsity and dynamic data distribution. To directly introduce the correct feedback label information, we propose an Unbiased delayed feedback Label Correction framework (ULC), which uses an auxiliary model to correct labels for observed negative feedback samples. Firstly, we theoretically prove that the label-corrected loss is an unbiased estimate of the oracle loss using true labels. Then, as there are no ready training data for label correction, counterfactual labeling is used to construct artificial training data. Furthermore, since counterfactual labeling utilizes only partial training data, we design an embedding-based alternative training method to enhance performance. Comparative experiments on both public and private datasets and detailed analyses show that our proposed approach effectively alleviates the delayed feedback problem and consistently outperforms the previous state-of-the-art methods.",KDD
"The success of Computer Vision (CV) relies heavily on manually annotated data. However, it is prohibitively expensive to annotate images in key domains such as healthcare, where data labeling requires significant domain expertise and cannot be easily delegated to crowd workers. To address this challenge, we propose a neuro-symbolic approach called RAPID, which infers image labeling rules from a small amount of labeled data provided by domain experts and automatically labels unannotated data using the rules. Specifically, RAPID combines pre-trained CV models and inductive logic learning to infer the logic-based labeling rules. RAPID achieves a labeling accuracy of 83.33% to 88.33% on four image labeling tasks with only 12 to 39 labeled samples. In particular, RAPID significantly outperforms finetuned CV models in two highly specialized tasks. These results demonstrate the effectiveness of RAPID in learning from small data and its capability to generalize among different tasks. Code and our dataset are publicly available at https://github.com/Neural-Symbolic-Image-Labeling/Rapid/",KDD
"Recently, the field of generative models has seen a significant advancement with the introduction of Diffusion Probabilistic Models (DPMs). The Denoising Diffusion Implicit Model (DDIM) was designed to reduce computational time by skipping a number of steps in the inference process of DPMs. However, the hand-crafted sampling schedule in DDIM, which relies on human expertise, has its limitations in considering all relevant factors in the sampling process. Additionally, the assumption that all instances should have the same schedule is not always valid. To address these problems, this paper proposes a method that leverages reinforcement learning to automatically search for an optimal sampling schedule for DPMs. This is achieved by a policy network that predicts the next step to visit based on the current state of the noisy image. The optimization of the policy network is accomplished using an episodic actor-critic framework, which incorporates reinforcement learning. Empirical results demonstrate the superiority of our approach over various datasets with different timesteps. We also observe that the trained sampling schedule has a strong generalization ability across different DPM baselines.",KDD
"Recently, the message passing neural network (MPNN) has attracted a lot of attention, which learns node representations based on the receptive field of the given node. Despite its success in many graph-related tasks, recent studies find that conventional MPNNs are incapable of handling variant receptive fields required in different graphs, and thereby some upgraded MPNNs have been developed. However, these methods are limited to designing a common solution for different graphs, which fails to capture the impact of different graph properties on the receptive fields. To alleviate such issues, we propose a novel MPNN space for data-dependent receptive fields (MpnnDRF), which enables us to dynamically design suitable MPNNs to capture the receptive field for the given graph. More concretely, we systemically investigate the capability of existing designs and propose several key design dimensions to improve them. Then, to fully explore the proposed designs and useful designs in existing works, we propose a novel search space to incorporate them and formulate a search framework. In the empirical study, the proposed MpnnDRF shows very strong robustness against the increased receptive field, which allows MpnnDRF to learn node representations based on a larger perceptual field. Therefore, MpnnDRF consistently achieves outstanding performance on benchmark node and graph classification tasks.",KDD
"The acquisition of explicit user feedback (e.g., ratings) in real-world recommender systems is often hindered by the need for active user involvement. To mitigate this issue, implicit feedback (e.g., clicks) generated during user browsing is exploited as a viable substitute. However, implicit feedback possesses a high degree of noise, which significantly undermines recommendation quality. While many methods have been proposed to address this issue by assigning varying weights to implicit feedback, two shortcomings persist: (1) the weight calculation in these methods is iteration-independent, without considering the influence of weights in previous iterations, and (2) the weight calculation often relies on prior knowledge, which may not always be readily available or universally applicable.
To overcome these two limitations, we model recommendation denoising as a bi-level optimization problem. The inner optimization aims to derive an effective model for the recommendation, as well as guiding the weight determination, thereby eliminating the need for prior knowledge. The outer optimization leverages gradients of the inner optimization and adjusts the weights in a manner considering the impact of previous weights. To efficiently solve this bi-level optimization problem, we employ a weight generator to avoid the storage of weights and a one-step gradient-matching-based loss to significantly reduce computational time. The experimental results on three benchmark datasets demonstrate that our proposed approach outperforms both state-of-the-art general and denoising recommendation models. The code is available at https://github.com/CoderWZW/BOD.",KDD
"Highly skewed long-tail item distribution commonly hurts model performance on tail items in recommendation systems, especially for graph-based recommendation models. We propose a novel idea to learn relations among items as an auxiliary graph to enhance the graph-based representation learning and make recommendations collectively in a coupled framework. This raises two challenges, 1) the long-tail downstream information may also bias the auxiliary graph learning, and 2) the learned auxiliary graph may cause negative transfer to the original user-item bipartite graph. We innovatively propose a novel Meta Graph Learning framework for long-tail recommendation (MGL) for solving both challenges. The meta-learning strategy is introduced to the learning of an edge generator, which is first tuned to reconstruct a debiased item co-occurrence matrix, and then virtually evaluated on generating item relations for recommendation. Moreover, we propose a popularity-aware contrastive learning strategy to prevent negative transfer by aligning the confident head item representations with those of the learned auxiliary graph. Experiments on public datasets demonstrate that our proposed model significantly outperforms strong baselines for tail items without compromising the overall performance.",KDD
"The rawly collected training data often comes with separate noisy labels collected from multiple imperfect annotators (e.g., via crowdsourcing). A typical way of using these separate labels is to first aggregate them into one and apply standard training methods. The literature has also studied extensively on effective aggregation approaches. This paper revisits this choice and aims to provide an answer to the question of whether one should aggregate separate noisy labels into single ones or use them separately as given. We theoretically analyze the performance of both approaches under the empirical risk minimization framework for a number of popular loss functions, including the ones designed specifically for the problem of learning with noisy labels. Our theorems conclude that label separation is preferred over label aggregation when the noise rates are high, or the number of labelers/annotations is insufficient. Extensive empirical results validate our conclusions.",KDD
"Modern health care systems are conducting continuous, automated surveillance of the electronic medical record (EMR) to identify adverse events with increasing frequency; however, many events such as sepsis do not have elucidated prodromes (i.e., event chains) that can be used to identify and intercept the adverse event early in its course. Clinically relevant and interpretable results require a framework that can (i) infer temporal interactions across multiple patient features found in EMR data (e.g., Labs, vital signs, etc.) and (ii) identify patterns that precede and are specific to an impending adverse event (e.g., sepsis). In this work, we propose a linear multivariate Hawkes process model, coupled with ReLU link function, to recover a Granger Causal (GC) graph with both exciting and inhibiting effects. We develop a scalable two-phase gradient-based method to obtain a maximum surrogate-likelihood estimator, which is shown to be effective via extensive numerical simulation. Our method is subsequently extended to a data set of patients admitted to Grady hospital system in Atlanta, GA, USA, where the estimated GC graph identifies several highly interpretable GC chains that precede sepsis. The code is available at https://github.com/SongWei-GT/two-phase-MHP.",KDD
"Automated Machine Learning (AutoML) techniques have recently been introduced to design Collaborative Filtering (CF) models in a data-specific manner. However, existing works either search architectures or hyperparameters while ignoring the fact they are intrinsically related and should be considered together. This motivates us to consider a joint hyperparameter and architecture search method to design CF models. However, this is not easy because of the large search space and high evaluation cost. To solve these challenges, we reduce the space by screening out usefulness hyperparameter choices through a comprehensive understanding of individual hyperparameters. Next, we propose a two-stage search algorithm to find proper configurations from the reduced space. In the first stage, we leverage knowledge from subsampled datasets to reduce evaluation costs; in the second stage, we efficiently fine-tune top candidate models on the whole dataset. Extensive experiments on real-world datasets show better performance can be achieved compared with both hand-designed and previous searched models. Besides, ablation and case studies demonstrate the effectiveness of our search framework.",KDD
"Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. While deep surrogate models can speed up the simulations, doing so for stochastic simulations and with active learning approaches is an underexplored area. We propose Interactive Neural Process (INP), a deep Bayesian active learning framework for learning deep surrogate models to accelerate stochastic simulations. INP consists of two components, a spatiotemporal surrogate model built upon Neural Process (NP) family and an acquisition function for active learning. For surrogate modeling, we develop Spatiotemporal Neural Process (STNP) to mimic the simulator dynamics. For active learning, we propose a novel acquisition function, Latent Information Gain (LIG), calculated in the latent space of NP based models. We perform a theoretical analysis and demonstrate that LIG reduces sample complexity compared with random sampling in high dimensions. We also conduct empirical studies on three complex spatiotemporal simulators for reaction diffusion, heat flow, and infectious disease. The results demonstrate that STNP outperforms the baselines in the offline learning setting and LIG achieves the state-of-the-art for Bayesian active learning.",KDD
"Adversarial training has been shown to be the most popular and effective technique to protect models from imperceptible adversarial samples. Despite its success, it also accompanies the significant performance degeneration to clean data. To achieve a good performance on both clean and adversarial samples, the main effort is searching for an adaptive perturbation radius for each training sample. However, this method suffers from a conflict between exact searching and computational overhead. To address this conflict, in this paper, firstly we show the superiority of adaptive perturbation radii on the accuracy and robustness respectively. Then we propose our novel self-adaptive adjustment framework for perturbation radii without tedious searching. We also discuss this framework on both deep neural networks (DNNs) and kernel support vector machines (SVMs). Finally, extensive experimental results show that our framework can improve adversarial robustness without compromising the natural generalization. It is also competitive with existing searching strategies in terms of running time.",KDD
"Recent efforts in fake news detection have witnessed a surge of interest in using graph neural networks (GNNs) to exploit rich social context. Existing studies generally leverage fixed graph structures, assuming that the graphs accurately represent the related social engagements. However, edge noise remains a critical challenge in real-world graphs, as training on suboptimal structures can severely limit the expressiveness of GNNs. Despite initial efforts in graph structure learning (GSL), prior works often leverage node features to update edge weights, resulting in heavy computational costs that hinder the methods' applicability to large-scale social graphs. In this work, we approach the fake news detection problem with a novel aspect of social graph refinement. We find that the degrees of news article nodes exhibit distinctive patterns, which are indicative of news veracity. Guided by this, we propose DECOR, a novel application of Degree-Corrected Stochastic Blockmodels to the fake news detection problem. Specifically, we encapsulate our empirical observations into a lightweight social graph refinement component that iteratively updates the edge weights via a learnable degree correction mask, which allows for joint optimization with a GNN-based detector. Extensive experiments on two real-world benchmarks validate the effectiveness and efficiency of DECOR1.",KDD
"With decentralized data collected from diverse clients, a personalized federated learning paradigm has been proposed for training machine learning models without exchanging raw data from local clients. We dive into personalized federated learning from the perspective of privacy-preserving transfer learning, and identify the limitations of previous personalized federated learning algorithms. First, previous works suffer from negative knowledge transferability for some clients, when focusing more on the overall performance of all clients. Second, high communication costs are required to explicitly learn statistical task relatedness among clients. Third, it is computationally expensive to generalize the learned knowledge from experienced clients to new clients.
To solve these problems, in this paper, we propose a novel federated parameter propagation (FEDORA) framework for personalized federated learning. Specifically, we reformulate the standard personalized federated learning as a privacy-preserving transfer learning problem, with the goal of improving the generalization performance for every client. The crucial idea behind FEDORA is to learn how to transfer and whether to transfer simultaneously, including (1) adaptive parameter propagation: one client is enforced to adaptively propagate its parameters to others based on their task relatedness (e.g., explicitly measured by distribution similarity), and (2) selective regularization: each client would regularize its local personalized model with received parameters, only when those parameters are positively correlated with the generalization performance of its local model. The experiments on a variety of federated learning benchmarks demonstrate the effectiveness of the proposed FEDORA framework over state-of-the-art personalized federated learning baselines.",KDD
"The emergence of evolving data privacy policies and regulations has sparked a growing interest in the concept of ""machine unlearning"", which involves enabling machine learning models to forget specific data instances. In this paper, we specifically focus on edge unlearning in Graph Neural Networks (GNNs), which entails training a new GNN model as if certain specified edges never existed in the original training graph. Unlike conventional unlearning scenarios where data samples are treated as independent entities, edges in graphs exhibit correlation. Failing to carefully account for this data dependency would result in the incomplete removal of the requested data from the model. While retraining the model from scratch by excluding the specific edges can eliminate their influence, this approach incurs a high computational cost. To overcome this challenge, we introduce CEU, a Certified Edge Unlearning framework. CEU expedites the unlearning process by updating the parameters of the pre-trained GNN model in a single step, ensuring that the update removes the influence of the removed edges from the model. We formally prove that CEU offers a rigorous theoretical guarantee under the assumption of convexity on the loss function. Our empirical analysis further demonstrates the effectiveness and efficiency of CEU for both linear and deep GNNs - it achieves significant speedup gains compared to retraining and existing unlearning methods while maintaining comparable model accuracy to retraining from scratch.",KDD
"Zero-Shot Learning (ZSL), which aims at automatically recognizing unseen objects, is a promising learning paradigm to understand new real-world knowledge for machines continuously. Recently, the Knowledge Graph (KG) has been proven as an effective scheme for handling the zero-shot task with large-scale and non-attribute data. Prior studies always embed relationships of seen and unseen objects into visual information from existing knowledge graphs to promote the cognitive ability of the unseen data. Actually, real-world knowledge is naturally formed by multimodal facts. Compared with ordinary structural knowledge from a graph perspective, multimodal KG can provide cognitive systems with fine-grained knowledge. For example, the text description and visual content can depict more critical details of a fact than only depending on knowledge triplets. Unfortunately, this multimodal fine-grained knowledge is largely unexploited due to the bottleneck of feature alignment between different modalities. To that end, we propose a multimodal intensive ZSL framework that matches regions of images with corresponding semantic embeddings via a designed dense attention module and self-calibration loss. It makes the semantic transfer process of our ZSL framework learns more differentiated knowledge between entities. Our model also gets rid of the performance limitation of only using rough global features. We conduct extensive experiments and evaluate our model on large-scale real-world data. The experimental results clearly demonstrate the effectiveness of the proposed model in standard zero-shot classification tasks.",KDD
"Rare categories abound in a number of real-world networks and play a pivotal role in a variety of high-stakes applications, including financial fraud detection, network intrusion detection, and rare disease diagnosis. Rare category analysis (RCA) refers to the task of detecting, characterizing, and comprehending the behaviors of minority classes in a highly-imbalanced data distribution. While the vast majority of existing work on RCA has focused on improving the prediction performance, a few fundamental research questions heretofore have received little attention and are less explored: How confident or uncertain is a prediction model in rare category analysis? How can we quantify the uncertainty in the learning process and enable reliable rare category analysis?
To answer these questions, we start by investigating miscalibration in existing RCA methods. Empirical results reveal that state-of-the-art RCA methods are mainly over-confident in predicting minority classes and under-confident in predicting majority classes. Motivated by the observation, we propose a novel individual calibration framework, named CALIRARE, for alleviating the unique challenges of RCA, thus enabling reliable rare category analysis. In particular, to quantify the uncertainties in RCA, we develop a node-level uncertainty quantification algorithm to model the overlapping support regions with high uncertainty; to handle the rarity of minority classes in miscalibration calculation, we generalize the distribution-based calibration metric to the instance level and propose the first individual calibration measurement on graphs named Expected Individual Calibration Error (EICE). We perform extensive experimental evaluations on real-world datasets, including rare category characterization and model calibration tasks, which demonstrate the significance of our proposed framework.",KDD
"Traffic signal control (TSC) is still one of the most significant and challenging research problems in the transportation field. Reinforcement learning (RL) has achieved great success in TSC but suffers from critically high learning costs in practical applications due to the excessive trial-and-error learning process. Offline RL is a promising method to reduce learning costs whereas the data distribution shift issue is still up in the air. To this end, in this paper, we formulate TSC as a sequence modeling problem with a sequence of Markov decision process described by states, actions, and rewards from the traffic environment. A novel framework, namely TransformerLight, is introduced, which does not aim to fit into value functions by averaging all possible returns, but produces the best possible actions using a gated Transformer. Additionally, the learning process of TransformerLight is much more stable by replacing the residual connections with gated transformer blocks due to a dynamic system perspective. Through numerical experiments on offline datasets, we demonstrate that the TransformerLight model: (1) can build a high-performance adaptive TSC model without dynamic programming; (2) achieves a new state-of-the-art compared to most published offline RL methods so far; and (3) shows a more stable learning process than offline RL and recent Transformer-based methods. The relevant dataset and code are available at Github.",KDD
"To address the big data challenges, serverless multi-party collaborative training has recently attracted attention in the data mining community, since they can cut down the communications cost by avoiding the server node bottleneck. However, traditional serverless multi-party collaborative training algorithms were mainly designed for balanced data mining tasks and are intended to optimize accuracy (e.g., cross-entropy). The data distribution in many real-world applications is skewed and classifiers, which are trained to improve accuracy, perform poorly when applied to imbalanced data tasks since models could be significantly biased toward the primary class. Therefore, the Area Under Precision-Recall Curve (AUPRC) was introduced as an effective metric. Although multiple single-machine methods have been designed to train models for AUPRC maximization, the algorithm for multi-party collaborative training has never been studied. The change from the single-machine to the multi-party setting poses critical challenges. For example, existing single-machine-based AUPRC maximization algorithms maintain an inner state for local each data point, thus these methods are not applicable to large-scale multi-party collaborative training due to the dependence on each local data point.
To address the above challenge, in this paper, we reformulate the serverless multi-party collaborative AUPRC maximization problem as a conditional stochastic optimization problem in a serverless multi-party collaborative learning setting and propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to directly optimize the AUPRC. After that, we use the variance reduction technique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) algorithm to improve the convergence rate, which matches the best theoretical convergence result reached by the single-machine online method. To the best of our knowledge, this is the first work to solve the multi-party collaborative AUPRC maximization problem. Finally, extensive experiments show the advantages of directly optimizing the AUPRC with distributed learning methods and also verify the efficiency of our new algorithms (i.e., SLATE and SLATE-M).",KDD
"High-accuracy real-time data stream estimations are critical for various applications, and sliding-window-based techniques have attracted wide attention. However, existing solutions struggle to achieve high accuracy, generality, and low memory usage simultaneously. To overcome these limitations, we present MicroscopeSketch, a high-accuracy sketch framework. Our key technique, called adaptive zooming, dynamically adjusts the granularity of counters to maximize accuracy while minimizing memory usage. By applying MicroscopeSketch to three specific tasks---frequency estimation, top-k frequent items discovery, and top-k heavy changes identification-we demonstrate substantial improvements over existing methods, reducing errors by roughly 4 times for frequency estimation and 3 times for identifying top-k items. The relevant source code is available in a GitHub repository.",KDD
"A comprehensive patient health history is essential for patient care and healthcare research. However, due to the distributed nature of healthcare services, patient health records are often scattered across multiple systems. Existing record linkage approaches primarily rely on patient identifiers, which have inherent limitations such as privacy invasion and identifier discrepancies. To tackle this problem, we propose linking de-identified patient health records by matching health patterns without strictly relying on sensitive patient identifiers. Our model MedLink solves two challenges faced with the patient linkage task: (1) the challenge of identifying the same patients based on data collected in different timelines as disease progression makes the record matching difficult, and (2) the challenge of identifying distinct health patterns as common medical codes dominate health records and overshadow the more informative low-prevalence codes. To address these challenges, MedLink utilizes bi-directional health prediction to predict future codes forwardly and past codes backwardly, thus accounting for the health progression. MedLink also has a prevalence-aware retrieval design to focus more on the low-prevalence but informative codes during learning. MedLink can be trained end-to-end and is lightweight for efficient inference on large patient databases. We evaluate MedLink against leading baselines on real-world patient datasets, including the critical care dataset MIMIC-III and a large health claims dataset. Results show that MedLink outperforms the best baseline by 4% in top-1 accuracy with only 8% memory cost. Additionally, when combined with existing identifier-based linkage approaches, MedLink can improve their performance by up to 15%.",KDD
"Topic segmentation is the process of dividing a text into semantically coherent segments, and segment labeling involves assigning a topic label to each of these segments. Previous work on this task has included the use of sequence labeling, segment-extraction, and generative models. While these methods have yielded impressive results, existing generative models have struggled to accurately generate strings of segment boundaries, limiting their competitiveness in this area. In this paper, we present a novel Sequence-to-Sequence approach with Mixed Pointers (Seq2Seq-MP). Seq2Seq-MP employs an encoder-decoder architecture with the pointer mechanism to generate both segment boundaries and topics, which allows for a more robust performance than string-generation models and can handle long-range dependencies better than sequence labeling and segment-extraction models. Additionally, we introduce the pairwise type encoding and type-aware relative position encoding to improve the fusion of type and position information, enhancing the interactions between sentences and topics in the encoder and decoder. Our experiments on public datasets show that Seq2Seq-MP outperforms the current state-of-the-art, with up to 2.9% and 4.0% improvements in Pk and F1, respectively.",KDD
"Recent conversational recommender systems (CRSs) have achieved considerable success on addressing the cold-start problem. While they utilize conversational key-terms to efficiently elicit user preferences, most of them, however, neglect that key-terms can also introduce biases. Systems learning key-term-level user preferences may make a biased item recommendation based on an overrated key-term instead of the item itself. As key-term conversation is a crucial part of CRSs, it is important to properly handle such bias resulting from the item-key-term relationship. While many debiasing methods have been proposed for traditional recommender systems, most of them focus on items or item groups re-ranking or re-weighting strategies such as calibration and propensity score, which are not designed to model the relation between item and key-term user preference. There is also no effective way for traditional debiasing methods to measure potentially useful biases through conversational key-terms to enhance the recommendation performance.
In this paper, we develop a deconfounded CRS, which enables the user to provide both item and key-term feedback in each round such that we can promisingly capture more accurate relation between key-term-level and item-level user preference to alleviate the bias. To better model the relations and understand such bias, we view CRSs from a causal perspective and introduce a novel structural causal model (SCM) that identifies the confounding effect of key-term-level user preference. Inspired by our causal view, we devise an online backdoor adjustment approximation to alleviate the confounding effect when making item recommendations. Consider that not all biases are harmful, we utilize the useful bias and propose DecUCB, which leverages conversational key-term feedback to regulate the influence of backdoor adjustment adaptively in a personalized fashion. Extensive experiments on real-world datasets demonstrate the advantages of our proposed method in both recommendation performance and bias mitigation.",KDD
"Graph Neural Networks (GNNs) have demonstrated promising results on exploiting node representations for many downstream tasks through supervised end-to-end training. To deal with the widespread label scarcity issue in real-world applications, Graph Contrastive Learning (GCL) is leveraged to train GNNs with limited or even no labels by maximizing the mutual information between nodes in its augmented views generated from the original graph. However, the distribution of graphs remains unconsidered in view generation, resulting in the ignorance of unseen edges in most existing literature, which is empirically shown to be able to improve GCL's performance in our experiments. To this end, we propose to incorporate graph generative adversarial networks (GANs) to learn the distribution of views for GCL, in order to i) automatically capture the characteristic of graphs for augmentations, and ii) jointly train the graph GAN model and the GCL model. Specifically, we present GACN, a novel Generative Adversarial Contrastive learning Network for graph representation learning. GACN develops a view generator and a view discriminator to generate augmented views automatically in an adversarial style. Then, GACN leverages these views to train a GNN encoder with two carefully designed self-supervised learning losses, including the graph contrastive loss and the Bayesian personalized ranking Loss. Furthermore, we design an optimization framework to train all GACN modules jointly. Extensive experiments on seven real-world datasets show that GACN is able to generate high-quality augmented views for GCL and is superior to twelve state-of-the-art baseline methods. Noticeably, our proposed GACN surprisingly discovers that the generated views in data augmentation finally conform to the well-known preferential attachment rule in online networks.",KDD
"This paper introduces a unified causal lens for understanding representative model interpretation methods. We show that their explanation scores align with the concept of average treatment effect in causal inference, which allows us to evaluate their relative strengths and limitations from a unified causal perspective. Based on our observations, we outline the major challenges in applying causal inference to model interpretation, including identifying common causes that can be generalized across instances and ensuring that explanations provide a complete causal explanation of model predictions. We then present CIMI, a Causality-Inspired Model Interpreter, which addresses these challenges. Our experiments show that CIMI provides more faithful and generalizable explanations with improved sampling efficiency, making it particularly suitable for larger pretrained models.",KDD
"Existing anomaly detection models for time series are primarily trained with normal-point-dominant data and would become ineffective when anomalous points intensively occur in certain episodes. To solve this problem, we propose a new approach, called DiffAD, from the perspective of time series imputation. Unlike previous prediction- and reconstruction-based methods that adopt either partial or complete data as observed values for estimation, DiffAD uses a density ratio-based strategy to select normal observations flexibly that can easily adapt to the anomaly concentration scenarios. To alleviate the model bias problem in the presence of anomaly concentration, we design a new denoising diffusion-based imputation method to enhance the imputation performance of missing values with conditional weight-incremental diffusion, which can preserve the information of observed values and substantially improves data generation quality for stable anomaly detection. Besides, we customize a multi-scale state space model to capture the long-term dependencies across episodes with different anomaly patterns. Extensive experimental results on real-world datasets show that DiffAD performs better than state-of-the-art benchmarks.",KDD
"Graph Neural Networks (GNNs) have been broadly applied in many urban applications upon formulating a city as an urban graph whose nodes are urban objects like regions or points of interest. Recently, a few enhanced GNN architectures have been developed to tackle heterophily graphs where connected nodes are dissimilar. However, urban graphs usually can be observed to possess a unique spatial heterophily property; that is, the dissimilarity of neighbors at different spatial distances can exhibit great diversity. This property has not been explored, while it often exists. To this end, in this paper, we propose a metric, named Spatial Diversity Score, to quantitatively measure the spatial heterophily and show how it can influence the performance of GNNs. Indeed, our experimental investigation clearly shows that existing heterophilic GNNs are still deficient in handling the urban graph with high spatial diversity score. This, in turn, may degrade their effectiveness in urban applications. Along this line, we propose a Spatial Heterophily Aware Graph Neural Network (SHGNN), to tackle the spatial diversity of heterophily of urban graphs. Based on the key observation that spatially close neighbors on the urban graph present a more similar mode of difference to the central node, we first design a rotation-scaling spatial aggregation module, whose core idea is to properly group the spatially close neighbors and separately process each group with less diversity inside. Then, a heterophily-sensitive spatial interaction module is designed to adaptively capture the commonality and diverse dissimilarity in different spatial groups. Extensive experiments on three real-world urban datasets demonstrate the superiority of our SHGNN over several its competitors.",KDD
"This work studies the problem of learning unbiased algorithms from biased feedback for recommendation. We address this problem from a novel distribution shift perspective. Recent works in unbiased recommendation have advanced the state-of-the-art with various techniques such as re-weighting, multi-task learning, and meta-learning. Despite their empirical successes, most of them lack theoretical guarantees, forming non-negligible gaps between theories and recent algorithms. In this paper, we propose a theoretical understanding of why existing unbiased learning objectives work for unbiased recommendation. We establish a close connection between unbiased recommendation and distribution shift, which shows that existing unbiased learning objectives implicitly align biased training and unbiased test distributions. Built upon this connection, we develop two generalization bounds for existing unbiased learning methods and analyze their learning behavior. Besides, as a result of the distribution shift, we further propose a principled framework, Adversarial Self-Training (AST), for unbiased recommendation. Extensive experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of AST.",KDD
"Public cloud GPU clusters are becoming emerging platforms for training distributed deep learning jobs. Under this training paradigm, the job scheduler is a crucial component to improve user experiences, i.e., reducing training fees and job completion time, which can also save power costs for service providers. However, the scheduling problem is known to be NP-hard. Most existing work divides it into two easier sub-tasks, i.e., ordering task and placement task, which are responsible for deciding the scheduling orders of jobs and placement orders of GPU machines, respectively. Due to the superior adaptation ability, learning-based policies can generally perform better than traditional heuristic-based methods. Nevertheless, there are still two main challenges that have not been well-solved. First, most learning-based methods only focus on ordering or placement policy independently, while ignoring their cooperation. Second, the unbalanced machine performances and resource contention impose huge overhead and uncertainty on job duration, but rarely be considered in existing work. To tackle these issues, this paper presents a dual-agent scheduler framework abstracted from the two sub-tasks to jointly learn the ordering and placement policies and make better-informed scheduling decisions. Specifically, we design an ordering agent with a scalable squeeze-and-communicate strategy for better cooperation; for the placement agent, we propose a novel Random Walk Gaussian Process to learn the performance similarities of GPU machines while being aware of the uncertain performance fluctuation. Finally, the dual-agent is jointly optimized with multi-agent reinforcement learning. Extensive experiments conducted on the real-world production cluster trace demonstrate the superiority of our model.",KDD
"Exploring how learners' knowledge states evolve during the learning activities is a critical task in online learning systems, which can facilitate personalized services downstream, such as course recommendation. Most of existing methods have devoted great efforts to analyzing learners' knowledge states according to their responses (i.e., right or wrong) to different questions. However, the significant effect of learners' learning behaviors (e.g., answering speed, the number of attempts) is omitted, which can reflect their knowledge acquisition deeper and ensure the reliability of the response. In this paper, we propose a Learning Behavior-oriented Knowledge Tracing (LBKT) model, with the goal of explicitly exploring the learning behavior effects on learners' knowledge states. Specifically, we first analyze and summarize several dominated learning behaviors including Speed, Attempts and Hints in the learning process. As the characteristics of different learning behaviors vary greatly, we separately estimate their various effects on learners' knowledge acquisition in a quantitative manner. Then, considering that different learning behaviors are closely dependent with each other, we assess the fused effect of multiple learning behaviors by capturing their complex dependent patterns. Finally, we integrate the forgetting factor with learners' knowledge acquisition to comprehensively update their changing knowledge states in learning. Extensive experimental results on several public datasets demonstrate that our model generates better performance prediction for learners against existing methods. Moreover, LBKT shows good interpretability in tracking learners' knowledge state by incorporating the learning behavior effects. Our codes are available at https://github.com/xbh0720/LBKT.",KDD
"Recent studies suggest that ""memorization"" is one necessary factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting ""harmful"" atypical samples and fit as more ""benign"" atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show that it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets for image classification.",KDD
"Due to the universality of graph data, node classification shows its great importance in a wide range of real-world applications. Despite the successes of Graph Neural Networks (GNNs), GNN based methods rely heavily on rich connections and perform poorly on low-degree nodes. Since many real-world graphs follow a long-tailed distribution in node degrees, they suffer from a substantial performance bottleneck as a significant fraction of nodes is of low degree. In this paper, we point out that under-represented self-representations and low neighborhood homophily ratio of low-degree nodes are two main culprits. Based on that, we propose a novel method Grace which improves the node representation by self-distillation, and increases neighborhood homophily ratio of low-degree nodes by graph completion. To avoid error propagation of graph completion, label propagation is further leveraged. Experimental evidence has shown that our method well supports real-world graphs, and is superior in balancing degree-related bias and overall performance on node classification tasks.",KDD
"Reinforcement Learning (RL) has experienced rapid advancements in recent years. The widely studied RL algorithms mainly focus on a single input form, such as pixel-based image input or symbolic vector input. These two forms have different characteristics and, in many scenarios, will appear together, while few RL algorithms have studied the problems with mixed input types. Specifically, in the scenario where both pixel and symbolic inputs are available, symbolic input usually offers abstract features with specific semantics, which is more conducive to the agent's focus. Conversely, pixel input provides more comprehensive information, enabling the agent to make well-informed decisions. Tailoring the processing approach based on the properties of these two input types can contribute to solving the problem more effectively. To tackle the above issue, we propose an Internal Logical Induction (ILI) framework that integrates deep RL and rule learning into one system. ILI utilizes the deep RL algorithm to process the pixel input and the rule learning algorithm to induce propositional logic knowledge from symbolic input. To efficiently combine these two mechanisms, we further adopt a reward shaping technique by treating valuable knowledge as intrinsic rewards for the RL procedure. Experimental results demonstrate that the ILI framework outperforms baseline approaches in RL problems with pixel-symbolic input, and its inductive knowledge exhibits transferability advantages when pixel input semantics change.",KDD
"We abstract a MIMO scenario in distributed data stream mining, where a stream of multiple items is mined by multiple nodes. We design a framework named MimoSketch for the MIMO-specific scenario, which improves the fundamental mining task of item frequency estimation. MimoSketch consists of an algorithm design and a policy to schedule items to nodes. MimoSketch's algorithm applies random counting to preserve a mathematically proven unbiasedness property, which makes it friendly to the aggregate query on multiple nodes; its memory layout is dynamically adaptive to the runtime item size distribution, which maximizes the estimation accuracy by storing more items. MimoSketch's scheduling policy balances items among nodes, avoiding nodes being overloaded or underloaded, which improves the overall mining accuracy. Our prototype and evaluation show that our algorithm can improve the item frequency estimation accuracy by an order of magnitude compared with the state-of-the-art solutions, and the scheduling policy further promotes the performance in MIMO scenarios.",KDD
"The huge volume of emerging graph datasets has become a double-bladed sword for graph machine learning. On the one hand, it empowers the success of a myriad of graph neural networks (GNNs) with strong empirical performance. On the other hand, training modern graph neural networks on huge graph data is computationally expensive. How to distill the given graph dataset while retaining most of the trained models' performance is a challenging problem. Existing efforts try to approach this problem by solving meta-learning-based bilevel optimization objectives. A major hurdle lies in that the exact solutions of these methods are computationally intensive and thus, most, if not all, of them are solved by approximate strategies which in turn hurt the distillation performance. In this paper, inspired by the recent advances in neural network kernel methods, we adopt a kernel ridge regression-based meta-learning objective which has a feasible exact solution. However, the computation of graph neural tangent kernel is very expensive, especially in the context of dataset distillation. As a response, we design a graph kernel, named LiteGNTK, tailored for the dataset distillation problem which is closely related to the classic random walk graph kernel. An effective model named Kernel rÄ±dge regression-based graph Dataset Distillation (KIDD) and its variants are proposed. KIDD shows nice efficiency in both the forward and backward propagation processes. At the same time, KIDD shows strong empirical performance over 7 real-world datasets compared with the state-of-the-art distillation methods. Thanks to the ability to find the exact solution of the distillation objective, the learned training graphs by KIDD can sometimes even outperform the original whole training set with as few as 1.65% training graphs.",KDD
"Graph neural networks (GNNs) have become core building blocks behind a myriad of graph learning tasks. The vast majority of the existing GNNs are built upon, either implicitly or explicitly, the homophily assumption, which is not always true and could heavily degrade the performance of learning tasks. In response, GNNs tailored for heterophilic graphs have been developed. However, most of the existing works are designed for the specific GNN models to address heterophily, which lacks generality. In this paper, we study the problem from the structure learning perspective and propose a family of general solutions named ALT. It can work hand in hand with most of the existing GNNs to handle graphs with either low or high homophily. At the core of our method is learning to (1) decompose a given graph into two components, (2) extract complementary graph signals from these two components, and (3) adaptively integrate the graph signals for node classification. Moreover, analysis based on graph signal processing shows that our framework can empower a broad range of existing GNNs to have adaptive filter characteristics and further modulate the input graph signals, which is critical for handling complex homophilic/heterophilic patterns. The proposed ALT brings significant and consistent performance improvement in node classification for a wide range of GNNs over a variety of real-world datasets.",KDD
"Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users' historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they do not require any expert knowledge. With PrefRec, we can fully exploit the advantages of RL in optimizing long-term goals, while avoiding complex reward engineering. PrefRec uses the preferences to automatically train a reward function in an end-to-end manner. The reward function is then used to generate learning signals to train the recommendation policy. Furthermore, we design an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance. We conduct experiments on a variety of long-term user engagement optimization tasks. The results show that PrefRec significantly outperforms previous state-of-the-art methods in all the tasks.",KDD
"Recently, many E-commerce search models are based on Graph Neural Networks (GNNs). Despite their promising performances, they are (1) lacking proper semantic representation of product contents; (2) less efficient for industry-scale graphs; and (3) less accurate on long-tail queries and cold-start products. To address these problems simultaneously, this paper proposes CC-GNN, a novel Content Collaborative Graph Neural Network. Firstly, CC-GNN enables content phrases to participate explicitly in graph propagation to capture the proper meaning of phrases and semantic drifts. Secondly, CC-GNN presents several efforts towards a more scalable graph learning framework, including efficient graph construction, MetaPath-guided Message Passing, and Difficulty-aware Representation Perturbation for graph contrastive learning. Furthermore, CC-GNN adopts Counterfactual Data Supplement at both supervised and contrastive learning to resolve the long-tail/cold-start problems. Extensive experiments on a real E-commerce dataset of 100-million-scale nodes show that CC-GNN produces significant improvements over existing methods (i.e., more than 10% improvements in terms of several key evaluation metrics for overall, long-tail queries and cold-start products) while reducing computational complexity. The proposed components of CC-GNN can be applied to other models for search and recommendation tasks. Experiments on a public dataset show that applying the proposed components can improve the performance of different recommendation models.",KDD
"Federated learning (FL) is a distributed optimization paradigm that learns from data samples distributed across a number of clients. Adaptive client selection that is cognizant of the training progress of clients has become a major trend to improve FL efficiency but not yet well-understood. Most existing FL methods such as FedAvg and its state-of-the-art variants implicitly assume that all learning phases during the FL training process are equally important. Unfortunately, this assumption has been revealed to be invalid due to recent findings on critical learning periods (CLP), in which small gradient errors may lead to an irrecoverable deficiency on final test accuracy. In this paper, we develop CriticalFL, a CLP augmented FL framework to reveal that adaptively augmenting exiting FL methods with CLP, the resultant performance is significantly improved when the client selection is guided by the discovered CLP. Experiments based on various machine learning models and datasets validate that the proposed CriticalFL framework consistently achieves an improved model accuracy while maintains better communication efficiency as compared to state-of-the-art methods, demonstrating a promising and easily adopted method for tackling the heterogeneity of FL training.",KDD
"In binary classification problems, many performance metrics evaluate the probability that some error exceeds a threshold. Nevertheless, they focus more on the probability and fail to capture the magnitude of the error, which evaluates how large this error exceeds the threshold. Capturing the magnitude of error is desired in many applications. For example, in detecting disease and predicting credit default, the magnitude of error illustrates the confidence in making the wrong prediction. We propose a novel metric, the Fragility Index (FI), to evaluate the performance of binary classifiers by capturing the magnitude of the error. FI alleviates the risk of misclassification by penalizing the large error greatly, which is seldom considered by standard metrics. Moreover, to strengthen the generalization ability and handle unseen samples, we adopt the framework of distributionally robust optimization and robust satisficing, which allows us to derive and control the maximum degree of fragility of the classifier when the distribution of samples shifts. We show that FI can be easily calculated and optimized for common probabilistic distance measures. Experiments with real datasets demonstrate the new insights brought by FI and the advantages of classifiers selected under FI, which always improve the robustness and reduce the risk of large errors as compared to classifiers selected by alternative metrics.",KDD
"Aiding humans with scientific designs is one of the most exciting of artificial intelligence (AI) and machine learning (ML), due to their potential for the discovery of new drugs, design of new materials and chemical compounds, etc. However, scientific design typically requires complex domain knowledge that is not familiar to AI researchers. Further, scientific studies involve professional skills to perform experiments and evaluations. These obstacles prevent AI researchers from developing specialized methods for scientific designs. To take a step towards easy-to-understand and reproducible research of scientific design, we propose a benchmark for the inverse design of nanophotonic devices, which can be verified computationally and accurately. Specifically, we implemented three different nanophotonic design problems, namely a radiative cooler, a selective emitter for thermophotovoltaics, and structural color filters, all of which are different in design parameter spaces, complexity, and design targets. The benchmark environments are implemented with an open-source simulator. We further implemented 10 different inverse design algorithms and compared them in a reproducible and fair framework. The results revealed the strengths and weaknesses of existing methods, which shed light on several future directions for developing more efficient inverse design algorithms. Our benchmark can also serve as the starting point for more challenging scientific design problems. The code of IDToolkit is available at https://github.com/ThyrixYang/IDToolkit.",KDD
"Data augmentation has undoubtedly enabled a significant leap forward in training a high-accuracy deep network. Besides the commonly used augmentation to target data, e.g., random cropping, flipping, and rotation, recent works have been dedicated to mining generalized knowledge by using multiple sources. However, along with plentiful data comes the huge data distribution gap between the target and different sources (hybrid shift). To mitigate this problem, existing methods tend to manually annotate more data. Unlike previous methods, this paper focuses on the study of learning deep models by gathering knowledge from multiple sources in a labor-free fashion and further proposes the ""Multi-Alignment and Pseudo-Learning'' method, dubbed MAPLE. MAPLE constructs the multi-alignment module, which consists of multiple discriminators to align different data distributions via an adversarial process. In addition, a novel semi-supervised learning (SSL) manner is introduced to further facilitate the utility of our MAPLE. Extensive evaluations conducted on four benchmarks show the effectiveness of the proposed MAPLE, which achieves state-of-the-art performance outperforming existing methods by an obvious margin.",KDD
"Subgraph learning has received considerable attention in its capacity of interpreting important structural information for predictions. Existing subgraph learning usually exploits statistics on predefined structures e.g., node degrees, occurrence frequency, to extract subgraphs, or refine the contents via only capturing label-relevant information with node-level sampling. Given diverse subgraph patterns, and mutual independence with local correlations on graphs, current solutions on subgraph learning still have two limitations in extraction and refinement stages. 1) The universality of extracting substructure patterns across domains is still lacking, 2) node-level sampling in refinement will distort the original local topology and none explicit guidance eliminating redundant information contribute to inefficiency issue. In this paper, we propose a unified subgraph learning scheme, Poly-Pivot Graph Neural Network (P2GNN) where we designate the centric node of each subgraph as the pivot. In the extraction stage, we present a general subgraph extraction principle, i.e., Local; Asymmetry between the centric and affiliated nodes. To this end, we asymmetrically model the similarity between each pair of nodes with random walk and quantify mutual affiliations in Affinity Propagation architecture, to extract subgraph structures. In the refinement, we devise a subgraph-level exclusion regularization to squash the target-independent information by considering mutual relations across subgraphs, cooperatively preserving a support set of subgraphs and facilitating the refinement process for graph representation. Empirical experiments on diverse web and biological graphs reveal 1.1%~7.3% improvements against best baselines, and visualized case studies prove the universality and interpretability of our P2GNN.",KDD
"Recent work in adversarial robustness suggests that natural data distributions are localized,
i.e., they place high probability in small volume regions of the input space, and that this
property can be utilized for designing classiï¬ers with improved robustness guarantees for
/lscript2-bounded perturbations. Yet, it is still unclear if this observation holds true for more
general metrics. In this work, we extend this theory to /lscript0-bounded adversarial perturbations,
where the attacker can modify a few pixels of the image but is unrestricted in the magnitude
of perturbation, and we show necessary and suï¬ƒcient conditions for the existence of /lscript0-robust
classiï¬ers. Theoretical certiï¬cation approaches in this regime essentially employ voting over
a large ensemble of classiï¬ers. Such procedures are combinatorial and expensive or require
complicated certiï¬cation techniques. In contrast, a simple classiï¬er emerges from our theory,
dubbed Box-NN , which naturally incorporates the geometry of the problem and improves
upon the current state-of-the-art in certiï¬ed robustness against sparse attacks for the MNIST
and Fashion-MNIST datasets.
1",TMLR
"In retail (e.g., grocery stores, apparel shops, online retailers), inventory managers have
to balance short-term risk (no items to sell) with long-term-risk (over ordering leading to
product waste). This balancing task is made especially hard due to the lack of informa-
tion about future customer purchases. In this paper, we study the problem of restocking
a grocery storeâ€™s inventory with perishable items over time, from a distributional point of
view. The objective is to maximize sales while minimizing waste, with uncertainty about
the actual consumption by costumers. This problem is of a high relevance today, given the
growing demand for food and the impact of food waste on the environment, the economy,
and purchasing power. We frame inventory restocking as a new reinforcement learning task
that exhibits stochastic behavior conditioned on the agentâ€™s actions, making the environ-
ment partially observable. We make two main contributions. First, we introduce a new
reinforcement learning environment, RetaiL, based on real grocery store data and expert
knowledge. This environment is highly stochastic, and presents a unique challenge for re-
inforcement learning practitioners. We show that uncertainty about the future behavior of
the environment is not handled well by classical supply chain algorithms, and that distri-
butional approaches are a good way to account for the uncertainty. Second, we introduce
GTDQN, a distributional reinforcement learning algorithm that learns a generalized Tukey
Lambda distribution over the reward space. GTDQN provides a strong baseline for our
environment. It outperforms other distributional reinforcement learning approaches in this
partially observable setting, in both overall reward and reduction of generated waste.
1 Published in Transactions on Machine Learning Research (04/2023)
1",TMLR
"Personalizing treatments for patients often involves a period of trial-and-error search until
an optimal choice is found. To minimize suering and other costs, it is critical to make this
process as short as possible. When treatments have primarily short-term eects, search can
be performed with multi-armed bandits (MAB), but these typically require long exploration
periods to guarantee optimality. In this work, we design MAB algorithms which provably
identify optimal treatments quickly by leveraging prior knowledge of the types of decision
processes (patients) we can encounter, in the form of a latent variable model. We present
two algorithms, the Latent LP-based Track and Stop (LLPT) explorer and the Divergence
Explorer for this setting: ï¬xed-conï¬dence pure-exploration latent bandits. We give a lower
bound on the stopping time of any algorithm which is correct at a given certainty level, and
prove that the expected stopping time of the LLPT Explorer matches the lower bound in
the high-certainty limit. Finally, we present results from an experimental study based on
realistic simulation data for Alzheimerâ€™s disease, demonstrating that our formulation and
algorithms lead to a signiï¬cantly reduced stopping time.
1",TMLR
"Importance estimators are explainability methods that quantify feature importance for deep
neural networks (DNN). In vision transformers (ViT), the self-attention mechanism natu-
rally leads to attention maps, which are sometimes interpreted as importance scores that
indicate which input features ViT models are focusing on. However, attention maps do
not account for signals from downstream tasks. To generate explanations that are sensitive
to downstream tasks, we have developed class-discriminative attention maps (CDAM), a
gradient-based extension that estimates feature importance with respect to a known class
or a latent concept. CDAM scales attention scores by how relevant the corresponding to-
kens are for the predictions of a classifier head. In addition to targeting the supervised
classifier, CDAM can explain an arbitrary concept shared by selected samples by measur-
ing similarity in the latent space of ViT. Additionally, we introduce Smooth CDAM and
Integrated CDAM, which average a series of CDAMs with slightly altered tokens. Our quan-
titative benchmarks include correctness, compactness, and class sensitivity, in comparison
to 7 other importance estimators. Vanilla, Smooth, and Integrated CDAM excel across all
three benchmarks. In particular, our results suggest that existing importance estimators
may not provide sufficient class-sensitivity. We demonstrate the utility of CDAM in med-
ical images by training and explaining malignancy and biomarker prediction models based
on lung Computed Tomography (CT) scans. Overall, CDAM is shown to be highly class-
discriminative and semantically relevant, while providing compact explanations.
Code available: https://github.com/lenbrocki/CDAM
Figure 1: Extending attention maps (AM), the proposed class-discriminative attention maps (CDAM) quan-
tify and visualize input features that are relevant for the target class in Vision Transformers (ViT). We
visualize importance scores obtained with CDAM for a linear classifier with ViT-S/8, trained with DINO
(Caron et al., 2021). Orange and blue colors correspond to positive and negative values, respectively.
1 Published in Transactions on Machine Learning Research (11/2024)
1",TMLR
"This paper introduces an unsupervised method to estimate the class separability of text
datasets from a topological point of view. Using persistent homology, we demonstrate how
tracking the evolution of embedding manifolds during training can inform about class sep-
arability. More specifically, we show how this technique can be applied to detect when the
training process stops improving the separability of the embeddings. Our results, validated
across binary and multi-class text classification tasks, show that the proposed methodâ€™s
estimates of class separability align with those obtained from supervised methods. This
approach offers a novel perspective on monitoring and improving the fine-tuning of sen-
tence transformers for classification tasks, particularly in scenarios where labeled data is
scarce. We also discuss how tracking these quantities can provide additional insights into
the properties of the trained classifier.
1",TMLR
"The fields of both Natural Language Processing (NLP) and Automated Machine Learning
(AutoML) have achieved remarkable results over the past years. In NLP, especially Large
Language Models (LLMs) have experienced a rapid series of breakthroughs very recently.
We envision that the two fields can radically push the boundaries of each other through tight
integration. To showcase this vision, we explore the potential of a symbiotic relationship
between AutoML and LLMs, shedding light on how they can benefit each other. In particular,
we investigate both the opportunities to enhance AutoML approaches with LLMs from
different perspectives and the challenges of leveraging AutoML to further improve LLMs. To
this end, we survey existing work, and we critically assess risks. We strongly believe that the
integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By
highlighting conceivable synergies, but also risks, we aim to foster further exploration at the
intersection of AutoML and LLMs.
1",TMLR
"With the advent of score-matching techniques for model training and Langevin dynamics for
sample generation, energy-based models (EBMs) have gained renewed interest as generative
models. Recent EBMs usually use neural networks to define their energy functions. In this
work, we introduce a novel hybrid approach that combines an EBM with an exponential
family model to incorporate inductive bias into data modeling. Specifically, we augment
the energy term with a parameter-free statistic function to help the model capture key data
statistics. Like an exponential family model, the hybrid model aims to align the distribution
statistics with data statistics during model training, even when it only approximately max-
imizes the data likelihood. This property enables us to impose constraints on the hybrid
model. Our empirical study validates the hybrid modelâ€™s ability to match statistics. Fur-
thermore, experimental results show that data fitting and generation improve when suitable
informative statistics are incorporated into the hybrid model.
1",TMLR
"Transformer has been considered the dominating neural architecture in NLP and CV , mostly under
supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of
reinforcement learning (RL), but it is faced with unique design choices and challenges brought by
the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In
this paper, we seek to systematically review motivations and progress on using Transformers in RL,
provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.
1",TMLR
"Continuous-time dynamic graphs ( CTDGs ) are essential for modeling interconnected,
evolving systems. Traditional methods for extracting knowledge from these graphs often
depend on feature engineering or deep learning. Feature engineering is limited by the man-
ual and time-intensive nature of crafting features, while deep learning approaches suffer from
high inference latency, making them impractical for real-time applications. This paper intro-
duces Deep-Graph-Sprints ( DGS), a novel deep learning architecture designed for efficient
representation learning on CTDGs with low-latency inference requirements. We benchmark
DGSagainststate-of-the-art(SOTA)featureengineeringandgraphneuralnetworkmethods
using five diverse datasets. The results indicate that DGS achieves competitive performance
while inference speed improves between 4x and 12x compared to other deep learning ap-
proaches on our benchmark datasets. Our method effectively bridges the gap between deep
representation learning and low-latency application requirements for CTDGs.
1",TMLR
"Randomized smoothing has recently emerged as an eï¬€ective tool that enables certiï¬cation of
deep neural network classiï¬ers at scale. All prior art on randomized smoothing has focused
on isotropic /lscriptpcertiï¬cation, which has the advantage of yielding certiï¬cates that can be easily
compared among isotropic methods via /lscriptp-norm radius. However, isotropic certiï¬cation limits
the region that can be certiï¬ed around an input to worst-case adversaries, i.e.it cannot
reason about other â€œcloseâ€, potentially large, constant prediction safe regions. To alleviate
this issue, ( i) we theoretically extend the isotropic randomized smoothing /lscript1and/lscript2certiï¬cates
to their generalized anisotropic counterparts following a simpliï¬ed analysis. Moreover, ( ii) we
propose evaluation metrics allowing for the comparison of general certiï¬cates â€“ a certiï¬cate is
superior to another if it certiï¬es a superset region â€“ with the quantiï¬cation of each certiï¬cate
through the volume of the certiï¬ed region. We introduce AnCer, a framework for obtaining
anisotropic certiï¬cates for a given test set sample via volume maximization. We achieve
it by generalizing memory-based certiï¬cation of data-dependent classiï¬ers. Our empirical
results demonstrate that AnCer achieves state-of-the-art /lscript1and/lscript2certiï¬ed accuracy on
CIFAR-10 and ImageNet in the data-dependence setting, while certifying larger regions in
terms of volume, highlighting the beneï¬ts of moving away from isotropic analysis. Our code
is available in this repository.
1",TMLR
"Privacy concerns grow with the success of modern deep learning models, especially when the
training set contains sensitive data. Differentially private generative model (DPGM) can
serve as a solution to circumvent such concerns by generating data that are distributionally
similar to the original data yet with differential privacy (DP) guarantees. While GAN has
attracted major attention, existing DPGMs based on flow generative models are limited
and only developed on low-dimensional tabular datasets. The capability of exactdensity
estimation makes the flow model exceptional when density estimation is of interest. In this
work, we will first show that it is challenging (or even infeasible) to train a DP-flow via DP-
SGD, i.e. the workhorse algorithm for private deep learning, on high-dimensional image sets
withacceptableutility,andthenwegiveaneffectivesolutionbyreducingthegenerationfrom
the pixel space to a lower dimensional latent space. We show the effectiveness and scalability
of the proposed method via extensive experiments, where the proposed method achieves a
significantly better privacy-utility trade-off compared to existing alternatives. Notably, our
method is the first DPGM to scale to high-resolution image sets (up to 256Ã—256). Our
code is available at https://github.com/dihjiang/DP-LFlow .
1",TMLR
"Whileadversarialtrainingmethodshavesignificantlyimprovedtherobustnessofdeepneural
networks against norm-bounded adversarial perturbations, the generalization gap between
their performance on training and test data is considerably greater than that of standard
empirical risk minimization. Recent studies have aimed to connect the generalization prop-
erties of adversarially trained classifiers to the min-max optimization algorithm used in their
training. In this work, we analyze the interconnections between generalization and optimiza-
tion in adversarial training using the algorithmic stability framework. Specifically, our goal
is to compare the generalization gap of neural networks trained using the vanilla adversar-
ial training method, which fully optimizes perturbations at every iteration, with the free
adversarial training method, which simultaneously optimizes norm-bounded perturbations
and classifier parameters. We prove bounds on the generalization error of these methods,
indicating that the free adversarial training method may exhibit a lower generalization gap
between training and test samples due to its simultaneous min-max optimization of classifier
weights and perturbation variables. We conduct several numerical experiments to evaluate
the train-to-test generalization gap in vanilla and free adversarial training methods. Our
empirical findings also suggest that the free adversarial training method could lead to a
smaller generalization gap over a similar number of training iterations. The paper code is
available at https://github.com/Xiwei-Cheng/Stability_FreeAT .
1",TMLR
"New transformer networks have been integrated into object tracking pipelines and have
demonstrated strong performance on the latest benchmarks. This paper focuses on un-
derstanding how transformer trackers behave under adversarial attacks and how different
attacks perform on tracking datasets as their parameters change. We conducted a series of
experiments to evaluate the effectiveness of existing adversarial attacks on object trackers
with transformer and non-transformer backbones. We experimented on 7 different trackers,
including 3 that are transformer-based, and 4 which leverage other architectures. These
trackers are tested against 4 recent attack methods to assess their performance and ro-
bustness on VOT2022ST, UAV123 and GOT10k datasets. Our empirical study focuses on
evaluating adversarial robustness of object trackers based on bounding box versus binary
mask predictions, and attack methods at different levels of perturbations. Interestingly, our
studyfoundthatalteringtheperturbationlevelmaynotsignificantlyaffecttheoverallobject
tracking results after the attack. Similarly, the sparsity and imperceptibility of the attack
perturbations may remain stable against perturbation level shifts. By applying a specific
attack on all transformer trackers, we show that new transformer trackers having a stronger
cross-attention modeling achieve a greater adversarial robustness on tracking datasets, such
as VOT2022ST and GOT10k. Our results also indicate the necessity for new attack meth-
ods to effectively tackle the latest types of transformer trackers. The codes necessary to
reproduce this study are available at https://github.com/fatemehN/ReproducibilityStudy.
1",TMLR
"We study when the neural tangent kernel (NTK) approximation is valid for training a model
with the square loss. In the lazy training setting of Chizat et al. (2019), we show that
rescaling the model by a factor of Î±=O(T)suffices for the NTK approximation to be valid
until training time T. Our bound is tight and improves on the previous bound of Chizat
et al. (2019), which required a larger rescaling factor of Î±=O(T2).
1",TMLR
"Large Language Models (LLMs) have made the ambitious quest for generalist agents sig-
nificantly far from being a fantasy. A key hurdle for building such general models is the
diversity and heterogeneity of tasks and modalities. A promising solution is unification,
allowing the support of a myriad of tasks and modalities within one unified framework.
While few large models ( e.g., Flamingo (Alayrac et al., 2022)), trained on massive datasets,
can support more than two modalities, current small to mid-scale unified models are still
limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it
possible to build efficiently a unified model that can support all modalities? To answer this,
we propose UnIVAL , a step further towards this ambitious goal. Without relying on fancy
datasets sizes or models with billions of parameters, the âˆ¼0.25B parameter UnIVAL model
goes beyond two modalities and unifies text, images, video, and audio into a single model.
Our model is efficiently pretrained on many tasks, based on task balancing and multimodal
curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art
approaches, across image and video-text tasks. The feature representations learned from
image and video-text modalities, allows the model to achieve competitive performance when
finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified
model, we propose a novel study on multimodal model merging via weight interpolation of
models trained on different multimodal tasks, showing their benefits in particular for out-of-
distribution generalization. Finally, we motivate unification by showing the synergy between
tasks. The model weights and code are available at: https://github.com/mshukor/UnIVAL.
1",TMLR
"Some argue scale is all what is needed to achieve AI, covering even causal models. We make
it clear that large language models (LLMs) cannot be causal and give reason onto why
sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup
of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about
other SCM within their variables. We conjecture that in the cases where LLM succeed in
doing causal inference, underlying was a respective meta SCM that exposed correlations
between causal facts in natural language on whose data the LLM was ultimately trained.
If our hypothesis holds true, then this would imply that LLMs are like parrots in that they
simply recite the causal knowledge embedded in the data. Our empirical analysis provides
favoring evidence that current LLMs are even weak â€˜causal parrots.â€™
1",TMLR
"We introduce DafnyBench, the largest benchmark of its kind for training and
evaluating machine learning systems for formal software verification. We test the
ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough annotations
for the Dafny formal verification engine to successfully verify over 750 programs
with about 53,000 lines of code. The best model and prompting scheme achieved
68% success rate, and we quantify how this rate improves when retrying with error
message feedback and how it deteriorates with the amount of required code and
1 Published in Transactions on Machine Learning Research (01/2025)
annotations. We hope that DafnyBench will enable rapid improvements from this
baseline as LLMs and verification techniques grow in quality.
1",TMLR
"Graph privacy is crucial in systems that present a graph structure where the confidentiality
and privacy of participants play a significant role in the integrity of the system itself. For
instance, it is necessary to ensure the integrity of banking systems and transaction net-
works, protecting the privacy of customersâ€™ financial information and transaction details.
We propose a method called GraphPrivatizer that privatizes the structure of a graph and
protects it under Differential Privacy. GraphPrivatizer performs a controlled perturbation
of the graph structure by randomly replacing the neighbors of a node with other similar
neighbors, according to some similarity metric. With regard to neighbor perturbation, we
find that aggregating features to compute similarities and imposing a minimum similarity
score between the original and the replaced nodes provides the best privacy-utility trade-off.
We use our method to train a Graph Neural Network server-side without disclosing usersâ€™
private information to the server. We conduct experiments on real-world graph datasets
and empirically evaluate the privacy of our models against privacy attacks.
1",TMLR
"Natural actor-critic (NAC) and its variants, equipped with the representation power of neu-
ral networks, have demonstrated impressive empirical success in solving Markov decision
problems with large (potentially infinite) state spaces. In this paper, we present a finite-
time analysis of NAC with neural network approximation, and identify the roles of neural
networks, regularization and optimization techniques (e.g., gradient clipping and weight
decay) to achieve provably good performance in terms of sample complexity, iteration com-
plexity and overparametrization bounds for the actor and the critic. In particular, we prove
that (i) entropy regularization and weight decay ensure stability by providing sufficient ex-
ploration to avoid near-deterministic and strictly suboptimal policies and (ii) regularization
leads to sharp sample complexity and network width bounds in the regularized MDPs, yield-
ing a favorable bias-variance tradeoff in policy optimization. In the process, we identify the
importance of uniform approximation power of the actor neural network to achieve global
optimality in policy optimization due to distributional shift.
1",TMLR
"The dynamic SchrÃ¶dinger bridge problem provides an appealing setting for solving constrained
time-series data generation tasks posed as optimal transport problems. It consists of learning
non-linear diï¬€usion processes using eï¬ƒcient iterative solvers. Recent works have demonstrated
state-of-the-art results ( e.g., in modelling single-cell embryo RNA sequences or sampling
from complex posteriors) but are limited to learning bridges with only initial and terminal
constraints. Our work extends this paradigm by proposing the Iterative Smoothing Bridge
(ISB). We integrate Bayesian ï¬ltering and optimal control into learning the diï¬€usion process,
enabling the generation of constrained stochastic processes governed by sparse observations
at intermediate stages and terminal constraints. We assess the eï¬€ectiveness of our method
on synthetic and real-world data generation tasks and we show that the ISB generalises well
to high-dimensional data, is computationally eï¬ƒcient, and provides accurate estimates of
the marginals at intermediate and terminal times.
1",TMLR
"We study the problem of approximating compactly-supported integrable functions while imple-
menting their support set using feedforward neural networks. Our ï¬rst main result transcribes this
â€œstructuredâ€ approximation problem into a universality problem. We do this by constructing a re-
ï¬nement of the usual topology on the space L1
loc(Rd,RD)of locally-integrable functions in which
compactly-supported functions can only be approximated in L1-norm by functions with matching
discretized support. We establish the universality of ReLU feedforward networks with bilinear
pooling layers in this reï¬ned topology. Consequentially, we ï¬nd that ReLU feedforward networks
with bilinear pooling can approximate compactly supported functions while implementing their
discretized support. We derive a quantitative uniform version of our universal approximation
theorem on the dense subclass of compactly-supported Lipschitz functions. This quantitative
result expresses the depth, width, and the number of bilinear pooling layers required to construct
this ReLU network via the target functionâ€™s regularity, the metric capacity and diameter of its
essential support, and the dimensions of the inputs and output spaces. Conversely, we show that
polynomial regressors and analytic feedforward networks are not universal in this space.
1",TMLR
"We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces,
and under centralprivacy. In particular, we investigate regimes where the privacy budget is
notsupposed to be constant. We consider the classical definition of central differential pri-
vacy, but also the more recent notion of central concentrated differential privacy. We recover
the result of Barber & Duchi (2014) stating that histogram estimators are optimal against
Lipschitz distributions for the L2risk and, under regular differential privacy, we extend it
to other norms and notions of privacy. Then, we investigate higher degrees of smoothness,
drawing two conclusions: First, and contrary to what happens with constant privacy budget
(Wasserman & Zhou, 2010), there areregimes where imposing privacy degrades the regular
minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are
near-optimal against the same classes of densities in this new setup with pure differential
privacy, but contrary to the constant privacy budget case, it comes at the cost of relaxation.
With zero concentrated differential privacy, there is no need for relaxation, and we prove
that the estimation is optimal.
1",TMLR
"Emergent communication, or emergent language, is the field of research which studies
how human language-like communication systems emerge de novo in deep multi-agent
reinforcement learning environments. The possibilities of replicating the emergence of a
complex behavior like language have strong intuitive appeal, yet it is necessary to complement
this with clear notions of how such research can be applicable to other fields of science,
technology, and engineering. This paper comprehensively reviews the applications of emergent
communication research across machine learning, natural language processing, linguistics,
and cognitive science. Each application is illustrated with a description of its scope, an
explication of emergent communicationâ€™s unique role in addressing it, a summary of the
extant literature working towards the application, and brief recommendations for near-term
research directions.
1",TMLR
"Novel class discovery (NCD) aims to infer novel categories in an unlabeled dataset by
leveraging prior knowledge of a labeled set comprising disjoint but related classes. Given that
most existing literature focuses primarily on utilizing supervised knowledge from a labeled
set at the methodology level, this paper considers the question: Is supervised knowledge
always helpful at different levels of semantic relevance? To proceed, we first establish a novel
metric, so-called transfer flow , to measure the semantic similarity between labeled/unlabeled
datasets. To show the validity of the proposed metric, we build up a large-scale benchmark
with various degrees of semantic similarities between labeled/unlabeled datasets on ImageNet
by leveraging its hierarchical class structure. The results based on the proposed benchmark
show that the proposed transfer flow is in line with the hierarchical class structure; and that
NCD performance is consistent with the semantic similarities (measured by the proposed
metric). Next, by using the proposed transfer flow , we conduct various empirical experiments
with different levels of semantic similarity, yielding that supervised knowledge may hurt NCD
performance . Specifically, using supervised information from a low-similarity labeled set may
lead to a suboptimal result as compared to using pure self-supervised knowledge. These
results reveal the inadequacy of the existing NCD literature which usually assumes that
supervised knowledge is beneficial. Finally, we develop a pseudo-version of the transfer
flowas a practical reference to decide if supervised knowledge should be used in NCD. Its
âˆ—Corresponding author
1 Published in Transactions on Machine Learning Research (04/2023)
effectiveness is supported by our empirical studies, which show that the pseudo transfer flow
(with or without supervised knowledge) is consistent with the corresponding accuracy based
on various datasets. Code is released at https://github.com/J-L-O/SK-Hurt-NCD
1",TMLR
"Model miscalibration has been frequently identified in modern deep neural networks. Re-
cent work aims to improve model calibration directly through a differentiable calibration
proxy. However, the calibration produced is often biased due to the binning mechanism.
In this work, we propose to learn better-calibrated models via meta-regularization, which
has two components: (1) gamma network ( Î³-Net), a meta learner that outputs sample-wise
gamma values (continuous variable) for Focal loss for regularizing the backbone network;
(2) smooth expected calibration error (SECE), a Gaussian-kernel based, unbiased, and dif-
ferentiable surrogate to ECE that enables the smooth optimization of Î³-Net. We evaluate
the effectiveness of the proposed approach in regularizing neural networks towards improved
and unbiased calibration on three computer vision datasets. We empirically demonstrate
that: (a) learning sample-wise Î³as continuous variables can effectively improve calibration;
(b) SECE smoothly optimizes Î³-Net towards unbiased and robust calibration with respect
to the binning schemes; and (c) the combination of Î³-Net and SECE achieves the best
calibration performance across various calibration metrics while retaining very competitive
predictive performance as compared to multiple recently proposed methods.
1",TMLR
"Safe Bayesian Optimization (BO) is increasingly used to optimize an unknown function un-
der safety constraints, a central task in robotics, biomedical engineering, and many other
disciplines. Due to the safety-critical nature of these applications, it is crucial that theo-
retical safety guarantees for these algorithms translate into the real world. In this work,
we investigate three safety-related issues in SafeOpt-type algorithms, a popular class of safe
BO methods. First, these algorithms critically rely on frequentist uncertainty bounds for
Gaussian Process (GP) regression, but concrete implementations typically utilize heuris-
tics that invalidate all safety guarantees. We provide a detailed analysis of this problem
and introduce Real- Î²-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP
bounds and thus retains all theoretical guarantees. Second, we identify a key technical as-
sumption in SafeOpt-like algorithms, the availability of an upper bound on the reproducing
kernel Hilbert space (RKHS) norm of the target function, as a central obstacle to real-world
usage. To address this issue, we propose to rely instead on a known Lipschitz and noise
bound, and we introduce Lipschitz-only Safe Bayesian Optimization (LoSBO), a SafeOpt-
type algorithm using the latter two assumptions. We show empirically that this algorithm
is not only safe, but also outperforms the state-of-the-art on several function classes. Third,
SafeOpt and derived algorithms rely on a discrete search space, complicating their applica-
tion to higher-dimensional problems. To broaden the applicability of these algorithms, we
introduce Lipschitz-only Safe GP-UCB (LoS-GP-UCB), a LoSBO variant that is applicable
to moderately high-dimensional problems, while retaining safety. By analyzing practical
safety issues in an important class of safe BO algorithms, and providing ready-to-use al-
gorithms that overcome these issues, this work contributes to bringing safe and reliable
machine learning techniques closer to real world applications.
1",TMLR
"Optimization problems with group sparse regularization are ubiquitous in various popu-
lar downstream applications, such as feature selection and compression for Deep Neural
Networks (DNNs). Nonetheless, the existing methods in the literature do not perform par-
ticularly well when such regularization is used in combination with a stochastic loss function.
In particular, it is challenging to design a computationally efficient algorithm with a conver-
gence guarantee and can compute group-sparse solutions. Recently, a half-space stochastic
projectedgradient( HSPG)methodwasproposedthatpartlyaddressedthesechallenges. This
paper presents a substantially enhanced version of HSPGthat we call AdaHSPG+ that makes
two noticeable advances. First, AdaHSPG+ is shown to have a stronger convergence result
under significantly looser assumptions than those required by HSPG. This improvement in
convergence is achieved by integrating variance reduction techniques with a new adaptive
strategy for iteratively predicting the support of a solution. Second, AdaHSPG+ requires
significantly less parameter tuning compared to HSPG, thus making it more practical and
user-friendly. This advance is achieved by designing automatic and adaptive strategies for
choosing the type of step employed at each iteration and for updating key hyperparam-
eters. The numerical effectiveness of our proposed AdaHSPG+ algorithm is demonstrated
on both convex and non-convex benchmark problems. The source code is available at
https://github.com/tianyic/adahspg .
1",TMLR
"Vertical federated learning (VFL) is a distributed learning paradigm, where computing
clients collectively train a model based on the partial features of the same set of samples
they possess. Current research on VFL focuses on the case when samples are independent,
but it rarely addresses an emerging scenario when samples are interrelated through a graph.
In this work, we train a graph neural network (GNN) through VFL, where each client owns
a part of the node features and a different edge set. This data scenario incurs a signifi-
cant communication overhead, not only because of the handling of distributed features but
also due to neighborhood aggregation in a GNN. Moreover, the training analysis is faced
with a challenge caused by the biased stochastic gradients. We propose a model-splitting
method that splits a backbone GNN across the clients and the server and a communication-
efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and
stale updates to skip communication in neighborhood aggregation and in model updates,
respectively, greatly reducing communication while enjoying convergence guarantees. We
conduct extensive numerical experiments on real-world datasets, showing that GLASU ef-
fectively trains a GNN that matches the accuracy of centralized training, while using only
a fraction of the time due to communication saving.
1",TMLR
"Todayâ€™s methods for uncovering causal relationships from observational data either constrain
functional assignments (linearity/additive noise assumptions) or the data generating process
(e.g., nonâ€“i.i.d. assumptions). Unlike previous works, which use conditional independence
tests, we rely on the inference functionâ€™s Jacobian to determine nonlinear cause-effect
relationships. We prove that, under strong identifiability, the inference functionâ€™s Jacobian
captures the sparsity structure of the causal graph; thus, generalizing the classic LiNGAM
method to the nonlinear case. We use nonlinear Independent Component Analysis (ICA)
to infer the underlying sources from the observed variables and show how nonlinear ICA
is compatible with causal discovery via nonâ€“i.i.d. data. Our approach avoids the cost of
exponentially many independence tests and makes our method end-to-end differentiable. We
demonstrate that the proposed method can infer the causal graph on multiple synthetic data
sets, and in most scenarios outperforms previous work.
1",TMLR
"Thisshortnoteintroducestheharmonicindeldistance(HID),anewdistancebetweenstrings
where the cost of an insertion or deletion is inversely proportional to the string length. We
present a closed-form formula and show that the HID is a proper distance metric. Then
we perform an experimental comparison of HID to normalized and unnormalized versions
of the indel distance on benchmark tasks for biomedical sequence data. We finally show
planar embeddings of the benchmark datasets to provide some insights into the geometry
of the metric spaces associated with the different distance metrics.
1",TMLR
"Existingcombinatorialsearchmethodsareoftencomplexandrequiresomelevelofexpertise.
This work introduces a simple and efficient deep learning method for solving combinatorial
problems with a predefined goal, represented by Rubikâ€™s Cube. We demonstrate that, for
such problems, training a deep neural network on random scrambles branching from the
goal state is sufficient to achieve near-optimal solutions. When tested on Rubikâ€™s Cube, 15
Puzzle, and 7Ã—7 Lights Out, our method outperformed the previous state-of-the-art method
DeepCubeA, improving the trade-off between solution optimality and computational cost,
despite significantly less training data. Furthermore, we investigate the scaling law of our
Rubikâ€™s Cube solver with respect to model size and training data volume. Our code is
available at github.com/kyo-takano/efficientcube
F' U R R D' B F L
Figure 1: The proposed learning algorithm for solving Rubikâ€™s Cube. We train a Deep Neural Network
(DNN) on sequences of random moves applied to the predefined goal. At each step, the DNN learns to
predict the last scramble move based on the resultant state. To solve a scrambled state, we sequentially
apply the reverse of the moves predicted by the trained DNN.
1",TMLR
"This paper considers the reliability of automatic differentiation for neural networks
involving the nonsmooth MaxPool operation across various precision levels (16, 32,
64 bits), architectures (LeNet, VGG, ResNet), and datasets (MNIST, CIFAR10,
SVHN, ImageNet). Although AD can be incorrect, recent research has shown that it
coincides with the derivative almost everywhere, even in the presence of nonsmooth
operations. On the other hand, in practice, AD operates with floating-point numbers,
and there is, therefore, a need to explore subsets on which AD can be numerically
incorrect. Recently, Bertoin et al. (2021) empirically studied how the choice of
ReLUâ€²(0)changes the output of AD and define a numerical bifurcation zone where
using ReLUâ€²(0) = 0differs from using ReLUâ€²(0) = 1. To extend this for a broader
class of nonsmooth operations, we propose a new numerical bifurcation zone (where
AD is incorrect over real numbers) and define a compensation zone (where AD is
incorrect over floating-point numbers but correct over reals). Using SGD for training,
we found that nonsmooth MaxPool Jacobians with lower norms maintain stable and
efficient test accuracy, while higher norms can result in instability and decreased
performance. We can use batch normalization, Adam-like optimizers, or increase
precision to reduce MaxPool Jacobians influence.
1",TMLR
"Electronic Health Records (EHRs) provide a rich source of medical information across dif-
ferent modalities such as electrocardiograms (ECG), structured EHRs (sEHR), and unstruc-
turedEHRs(text). Inspiredbythefactthatmanycardiacandnon-cardiacdiseasesinfluence
the behavior of the ECG, we leverage structured EHRs and unstructured EHRs from mul-
tiple sources by pairing with ECGs and propose a set of three new multi-modal contrastive
learning models that combine ECG, sEHR, and text modalities. The performance of these
models is compared against different baseline models such as supervised learning models
trained from scratch with random weights initialization, and self-supervised learning mod-
els trained only on ECGs. We pre-train the models on a large proprietary dataset of about
9millionECGs from around 2.4 millionpatients and evaluate the pre-trained models on
various downstream tasks such as classification, zero-shot retrieval, and out-of-distribution
detection involving the prediction of various heart conditions using ECG waveforms as in-
put, and demonstrate that the models presented in this work show significant improvements
compared to all baseline modes.
1",TMLR
"Feature preprocessing continues to play a critical role when applying machine learning
and statistical methods to tabular data. In this paper, we propose the use of the kernel
density integral transformation as a feature preprocessing step. Our approach subsumes
the two leading feature preprocessing methods as limiting cases: linear min-max scaling
and quantile transformation. We demonstrate that, without hyperparameter tuning, the
kernel density integral transformation can be used as a simple drop-in replacement for either
method, oering protection from the weaknesses of each. Alternatively, with tuning of a
single continuous hyperparameter, we frequently outperform both of these methods. Finally,
we show that the kernel density transformation can be proï¬tably applied to statistical data
analysis, particularly in correlation analysis and univariate clustering.
1",TMLR
"This work studies the problem of time series analysis with generalist (or foundation) models,
which are models trained across many data domains. Drawing inspiration from the widespread
success of large language models, we consider the simple strategy of discretely tokenizing
time series data drawn from a myriad of datasets via self-supervision, then using the ï¬xed
tokenization to solve a variety of tasks across many data domains. Canonically, time series
models are either trained on a single dataset or built in a task-speciï¬c manner (e.g., a
forecasting-only model), where many use patches of time as inputs to the model. As such,
performant generalist, discrete representation time series models explored across many
tasks are of value. Our method, TOkenized Time Series EMbeddings (TOTEM), produces
such generalist time series models with minimal or no ï¬ne-tuning while exhibiting strong
zero-shot performance. We evaluate TOTEM extensively over nearly 500 experiments on
three commonly-studied time series tasks with real-world data: imputation (17 baselines,
12 datasets), anomaly detection (19 baselines, 25 datasets), and forecasting (14 baselines,
12 datasets). We conclude that TOTEM matches or outperforms existing state-of-the-art
models in both the canonical specialist setting (i.e., training one model on one domain)
as well as the generalist setting (i.e., training a single model on many domains), which
demonstrates the eï¬ƒcacy of tokenization for general time series analysis. The open-source
implementation is available here: https://github.com/SaberaTalukder/TOTEM; a video
summary is available here: https://www.youtube.com/watch?v=OqrCpdb6MJk.
1",TMLR
"In reinforcement learning (RL), experience replay-based sampling techniques are crucial in
promoting convergence by eliminating spurious correlations. However, widely used methods
such as uniform experience replay ( UER) and prioritized experience replay ( PER) have been
shown to have sub-optimal convergence and high seed sensitivity, respectively. To address
these issues, we propose a novel approach called Introspective Experience Replay ( IER) that
selectively samples batches of data points prior to surprising events. Our method is inspired
from the reverse experience replay ( RER) technique, which has been shown to reduce bias
in the output of Q-learning-type algorithms with linear function approximation. However,
RERis not always practically reliable when using neural function approximation. Through
empirical evaluations, we demonstrate that IERwith neural function approximation yields
reliable and superior performance compared to UER,PER, and hindsight experience replay
(HER) across most tasks.
1",TMLR
"Graph neural networks (GNNs) have recently emerged as a promising approach for solving
the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional back-
tracking or local search SAT solvers. However, despite the growing volume of literature in
this field, there remains a notable absence of a unified dataset and a fair benchmark to evalu-
ate and compare existing approaches. To address this crucial gap, we present G4SATBench,
the first benchmark study that establishes a comprehensive evaluation framework for GNN-
based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT
datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN
models across various prediction tasks, training objectives, and inference algorithms. To ex-
plore the learning abilities and comprehend the strengths and limitations of GNN-based
SAT solvers, we also compare their solving processes with the heuristics in search-based
SAT solvers. Our empirical results provide valuable insights into the performance of GNN-
based SAT solvers and further suggest that existing GNN models can effectively learn a
solving strategy akin to greedy local search but struggle to learn backtracking search in the
latent space. Our codebase is available at https://github.com/zhaoyu-li/G4SATBench .
1",TMLR
"Efficiently estimating the uncertainty of neural network predictions has become an increas-
ingly important challenge as machine learning models are adopted for high-stakes industrial
applications where shifts in data distribution may occur. Thus, calibrated prediction uncer-
tainty is crucial to determine when to trust a modelâ€™s output and when to discard them as
implausible. We propose a novel deep learning module â€“ MC Layer Normalization â€“ that
acts as a drop-in replacement for Layer Normalization blocks and endows a neural network
with uncertainty estimation capabilities. Our method is motivated from an approximate
Bayesian perspective, but it is simple to deploy with no significant computational over-
head thanks to an efficient one-shot approximation of Monte Carlo integration at prediction
time. To evaluate the effectiveness of our module, we conduct experiments in two distinct
settings. First, we investigate its potential to replace existing methods such as MC-Dropout
and Prediction-Time Batch Normalization. Second, we explore its suitability for use cases
where such conventional modules are either unsuitable or sub-optimal for certain tasks (as
is the case with modules based on Batch Normalization, which is incompatible for instance
with transformers). We empirically demonstrate the competitiveness of our module in terms
of prediction accuracy and uncertainty calibration on established out-of-distribution image
classification benchmarks, as well as its flexibility by applying it on tasks and architectures
where previous methods are unsuitable. Code implementing our MC-LayerNorm module
can be found here https://github.com/IBM/mc-layernorm .
1 Published in Transactions on Machine Learning Research (02/2024)
1",TMLR
"Temporal graph neural networks have shown promising results in learning inductive rep-
resentations by automatically extracting temporal patterns. However, previous works of-
ten rely on complex memory modules or inefficient random walk methods to construct
temporal representations. To address these limitations, we present an efficient yet effec-
tive attention-based encoder that leverages temporal edge encodings and window-based
subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-
embedding architecture using non-contrastive SSL to learn rich temporal embeddings with-
out labels. Experimental results on 7 benchmark datasets indicate that on average, our
model outperforms SoTA baselines on the future link prediction task by 4.23% for the
transductive setting and 3.30% for the inductive setting while only requiring 5-10x less
training/inference time. Lastly, different aspects of the proposed framework are investi-
gated through experimental analysis and ablation studies. The code is publicly available at
https://github.com/huawei-noah/noah-research/tree/master/graph_atlas .
1",TMLR
"It is commonly observed that deep networks trained for classification exhibit class-selective
neurons in their early and intermediate layers. Intriguingly, recent studies have shown that
these class-selective neurons can be ablated without deteriorating network function. But if
class-selective neurons are not necessary, why do they exist? We attempt to answer this
question in a series of experiments on ResNet-50s trained on ImageNet. We first show that
class-selective neurons emerge during the first few epochs of training, before receding rapidly
but not completely; this suggests that class-selective neurons found in trained networks are
in fact vestigial remains of early training. With single-neuron ablation experiments, we then
show that class-selective neurons are important for network function in this early phase of
training. We also observe that the network is close to a linear regime in this early phase; we
thus speculate that class-selective neurons appear early in training as quasi-linear shortcut
solutionstotheclassificationtask. Finally, incausalexperimentswhereweregularizeagainst
class selectivity at different points in training, we show that the presence of class-selective
neurons early in training is critical to the successful training of the network; in contrast,
class-selective neurons can be suppressed later in training with little effect on final accuracy.
It remains to be understood by which mechanism the presence of class-selective neurons in
the early phase of training contributes to the successful training of networks.
1",TMLR
"Recently, diusion-based generative models have achieved remarkable success for image
generation and edition. However, existing diusion-based video editing approaches lack the
ability to oer precise control over generated content that maintains temporal consistency
in long-term videos. On the other hand, atlas-based methods provide strong temporal
consistency but are costly to edit a video and lack spatial control. In this work, we
1 Published in Transactions on Machine Learning Research (03/2024)
introduce VidEdit , a novel method for zero-shot text-based video editing that guarantees
robust temporal and spatial consistency. In particular, we combine an atlas-based video
representation with a pre-trained text-to-image diusion model to provide a training-free and
ecient video editing method, which by design fulï¬lls temporal smoothness. To grant precise
user control over generated content, we utilize conditional information extracted from o-the-
shelf panoptic segmenters and edge detectors which guides the diusion sampling process.
This method ensures a ï¬ne spatial control on targeted regions while strictly preserving
the structure of the original video. Our quantitative and qualitative experiments show
that VidEdit outperforms state-of-the-art methods on DAVIS dataset, regarding semantic
faithfulness, image preservation, and temporal consistency metrics. With this framework,
processing a single video only takes approximately one minute, and it can generate multiple
compatible edits based on a unique text prompt.
1",TMLR
"Massive backpropagated models can outperform humans on a variety of tasks but suffer
from high power consumption and poor generalization. Local learning, which focuses on
updating subsets of a modelâ€™s parameters at a time, has emerged as a promising technique
to address these issues. Recently, a novel local learning algorithm called Forward-Forward
has received widespread attention due to its innovative approach to learning. Unfortunately,
its application has been limited to smaller datasets due to scalability issues. To this end,
we propose The Trifecta, a collection of three simple techniques that drastically improve
the Forward-Forward algorithm on deeper networks. Our experiments demonstrate that
our models are on par with similarly structured, backpropagation-based models in both
training speed and test accuracy on simple datasets. Specifically, we achieve around 84%
accuracy on CIFAR-10, a notable improvement (25%) over the original FF algorithm.
1",TMLR
"A common approach to program synthesis is to use a learned function to guide the search
for a program that satisfies the userâ€™s intent. In this paper, we propose a method that offers
search guidance, through a domain-dependent auxiliary function, that can be orthogonal to
theguidancepreviousfunctionsprovide. Ourmethod,whichwecallAuxiliary-BasedLibrary
Learning ( Aulile), searches for a solution in the program space using a base algorithm. If
this search does not produce a solution, Aulileenhances the language with a library of
programs discovered in the search that optimizes for the auxiliary function. Then, it repeats
the search with this library-augmented language. This process is repeated until a solution
is found or the system reaches a timeout. We evaluate Aulile in string manipulation
tasks. Aulileimproved, in some cases by a large margin, the performance of several base
algorithms that use different search and learning strategies: BUS,Bustle,Crossbeam ,
andBee Search . Our results suggest that Aulileoffers an effective method of injecting
domain knowledge into existing systems through a library learning scheme that optimizes
for an auxiliary function.
1",TMLR
"The Vision Transformer (ViT) architecture has emerged as the backbone of choice for state-of-
the-art deep models for computer vision applications. However, ViTs are ill-suited for private
inference using secure multi-party computation (MPC) protocols, due to the large number of
non-polynomial operations (self-attention, feed-forward rectifiers, layer normalization). We
develop PriViT, a gradient-based algorithm to selectively Taylorize nonlinearities in ViTs
while maintaining their prediction accuracy. Our algorithm is conceptually very simple, easy
to implement, and achieves improved performance over existing MPC-friendly transformer
architectures in terms of the latency-accuracy Pareto frontier.
1",TMLR
"We introduce RedMotion, a transformer model for motion prediction in self-driving vehi-
cles that learns environment representations via redundancy reduction. Our first type of
redundancy reduction is induced by an internal transformer decoder and reduces a variable-
sized set of local road environment tokens, representing road graphs and agent data, to a
fixed-sized global embedding. The second type of redundancy reduction is obtained by self-
supervisedlearningandappliestheredundancyreductionprincipletoembeddingsgenerated
fromaugmentedviewsofroadenvironments. Ourexperimentsrevealthatourrepresentation
learning approach outperforms PreTraM, Traj-MAE, and GraphDINO in a semi-supervised
setting. Moreover, RedMotion achieves competitive results compared to HPTR or MTR++
in the Waymo Motion Prediction Challenge. Our open-source implementation is available
at:https://github.com/kit-mrt/future-motion
1",TMLR
"Graph neural networks (GNNs) have demonstrated significant promise in modelling relational
data and have been widely applied in various fields of interest. The key mechanism behind
GNNs is the so-called message passing where information is being iteratively aggregated to
central nodes from their neighbourhood. Such a scheme has been found to be intrinsically
linked to a physical process known as heat diffusion, where the propagation of GNNs naturally
corresponds to the evolution of heat density. Analogizing the process of message passing to
the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and
consequently informs better model design. Recently, there emerges a plethora of works that
proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate
the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we
provide the first comprehensive review of studies that leverage the continuous perspective of
GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics
to GNNs, along with a general framework for the design of graph neural dynamics. We
then review and categorize existing works based on their driven mechanisms and underlying
dynamics. We also summarize how the limitations of classic GNNs can be addressed under
the continuous framework. We conclude by identifying multiple open research directions.
1",TMLR
"Semi-supervised anomaly detection is a common problem, as often the datasets containing
anomalies are partially labeled. We propose a canonical framework: Semi-supervised Pseudo-
labeler Anomaly Detection with Ensembling (SPADE) that isnâ€™t limited by the assumption
that labeled and unlabeled data come from the same distribution. Indeed, the assumption
is often violated in many applications â€“ for example, the labeled data may contain only
anomalies unlike unlabeled data, or unlabeled data may contain diï¬€erent types of anomalies,
or labeled data may contain only â€˜easy-to-labelâ€™ samples. SPADE utilizes an ensemble of
one class classiï¬ers as the pseudo-labeler to improve the robustness of pseudo-labeling with
distribution mismatch. Partial matching is proposed to automatically select the critical
hyper-parameters for pseudo-labeling without validation data, which is crucial with limited
labeled data. SPADE shows state-of-the-art semi-supervised anomaly detection performance
across a wide range of scenarios with distribution mismatch in both tabular and image
domains. In some common real-world settings such as model facing new types of unlabeled
anomalies, SPADE outperforms the state-of-the-art alternatives by 5% AUC in average.
1",TMLR
"A major problem with Active Learning (AL) is high training costs since models are typically
retrained from scratch after every query round. We start by demonstrating that standard
AL on neural networks with warm starting fails, both to accelerate training and to avoid
catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new
class of techniques, circumventing this problem, by biasing further training towards previously
labeled sets. We accomplish this by employing existing, and developing novel, replay-based
Continual Learning (CL) algorithms that are effective at quickly learning the new without
forgetting the old, especially when data comes from an evolving distribution. We call this
paradigm ""Continual Active Learning"" (CAL) . We show CAL achieves significant speedups
usingaplethoraofreplayschemesthatusemodeldistillationandthatselectdiverse/uncertain
pointsfromthehistory. Weconductexperimentsacrossmanydatadomains, includingnatural
language, vision, medical imaging, and computational biology, each with different neural
architectures and dataset sizes. CAL consistently provides a âˆ¼3x reduction in training time,
while retaining performance and out-of-distribution robustness, showing its wide applicability.
1",TMLR
"Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring
model safety. Concept-based explanations have emerged as a superior approach that is more inter-
pretable than feature attribution estimates such as pixel saliency. However, defining the concepts
for the interpretability analysis biases the explanations by the userâ€™s expectations on the concepts.
To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the
concepts learned by deep models during training. By decomposing the latent space of a layer in sin-
gular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with
directions of high variance that are relevant to the model prediction, and that point to semantically
distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily
understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover,
we showcase the practical utility of our method in dataset exploration, where our concept vectors
successfully identify outlier training samples affected by various confounding factors. This novel
exploration technique has remarkable versatility to data types and model architectures and it will
facilitate the identification of biases and the discovery of sources of error within training data.
1 Published in Transactions on Machine Learning Research (November/2023)
1",TMLR
"The spread of an undesirable contact process, such as an infectious disease (e.g. COVID-
19), is contained through testing and isolation of infected nodes. The temporal and spatial
evolution of the process (along with containment through isolation) render such detection
as fundamentally diï¬€erent from active search detection strategies. In this work, through an
active learning approach, we design testing and isolation strategies to contain the spread and
minimize the cumulative infections under a given test budget. We prove that the objective
can be optimized, with performance guarantees, by greedily selecting the nodes to test.
We further design reward-based methodologies that eï¬€ectively minimize an upper bound
on the cumulative infections and are computationally more tractable in large networks.
These policies, however, need knowledge about the nodesâ€™ infection probabilities which are
dynamically changing and have to be learned by sequential testing. We develop a message-
passing framework for this purpose and, building on that, show novel tradeoï¬€s between
exploitation of knowledge through reward-based heuristics and exploration of the unknown
through a carefully designed probabilistic testing. The tradeoï¬€s are fundamentally distinct
fromtheclassicalcounterpartsunderactivesearchormulti-armedbanditproblems(MABs).
We provably show the necessity of exploration in a stylized network and show through
simulations that exploration can outperform exploitation in various synthetic and real-data
networks depending on the parameters of the network and the spread.
1",TMLR
"Machine learning systems are often used in settings where individuals adapt their features
to obtain a desired outcome. In such settings, strategic behavior leads to a sharp loss in
model performance in deployment. In this work, we aim to address this problem by learning
classifiers that encourage decision subjects to change their features in a way that leads to
improvement in both predicted andtrue outcome. We frame the dynamics of prediction and
adaptation as a two-stage game, and characterize optimal strategies for the model designer
and its decision subjects. In benchmarks on simulated and real-world datasets, we find
that classifiers trained using our method maintain the accuracy of existing approaches while
inducing higher levels of improvement and less manipulation.
1",TMLR
"The study of vision-and-language navigation (VLN) has typically relied on expert trajecto-
ries, which may not always be available in real-world situations due to the significant effort
required to collect them. On the other hand, existing approaches to training VLN agents
that go beyond available expert data involve data augmentations or online exploration which
can be tedious and risky. In contrast, it is easy to access large repositories of suboptimal
offline trajectories. Inspired by research in offline reinforcement learning (ORL), we intro-
duce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration
data. We introduce a simple and effective reward-conditioned approach that can account for
dataset suboptimality for training VLN agents, as well as benchmarks to evaluate progress
and promote research in this area. We empirically study various noise models for character-
izing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it
for the VLN âŸ³BERT and MTVM architectures in the R2R and RxR environments. Our ex-
periments demonstrate that the proposed reward-conditioned approach leads to significant
performance improvements, even in complex and intricate environments.1
1",TMLR
"In the ï¬eld of equation learning, exhaustively considering all possible equations derived
from a basis function dictionary is infeasible. Sparse regression and greedy algorithms
have emerged as popular approaches to tackle this challenge. However, the presence of
multicollinearity poses diï¬ƒculties for sparse regression techniques, and greedy steps may
inadvertently exclude terms of the true equation, leading to reduced identiï¬cation accuracy.
In this article, we present an approach that strikes a balance between comprehensiveness and
eï¬ƒciency in equation learning. Inspired by stepwise regression, our approach combines the
coeï¬ƒcient of determination, R2, and the Bayesian model evidence, p(y|M), in a novel way.
Our procedure is characterized by an comprehensive search with just a minor reduction of
the model space at each iteration step. With two ï¬‚avors of our approach and the adoption
ofp(y|M)for bi-directional stepwise regression, we present a total of three new avenues
for equation learning. Through three extensive numerical experiments involving random
polynomials and dynamical systems, we compare our approach against four state-of-the-art
methods and two standard approaches. The results demonstrate that our comprehensive
search approach surpasses all other methods in terms of identiï¬cation accuracy. In particular,
the second ï¬‚avor of our approach establishes an eï¬ƒcient overï¬tting penalty solely based on
R2, which achieves highest rates of exact equation recovery.
1",TMLR
"Fairness-aware learning aims at constructing classifiers that not only make accurate pre-
dictions, but also do not discriminate against specific groups. It is a fast-growing area of
machine learning with far-reaching societal impact. However, existing fair learning methods
are vulnerable to accidental or malicious artifacts in the training data, which can cause
them to unknowingly produce unfair classifiers. In this work we address the problem of
fair learning from unreliable training data in the robust multisource setting, where the
available training data comes from multiple sources, a fraction of which might not be rep-
resentative of the true data distribution. We introduce FLEA, a filtering-based algorithm
that identifies and suppresses those data sources that would have a negative impact on
fairness or accuracy if they were used for training. As such, FLEA is not a replacement of
prior fairness-aware learning methods but rather an augmentation that makes any of them
robust against unreliable training data. We show the effectiveness of our approach by a
diverse range of experiments on multiple datasets. Additionally, we prove formally that
â€“given enough dataâ€“ FLEA protects the learner against corruptions as long as the fraction of
affected data sources is less than half. Our source code and documentation are available at
https://github.com/ISTAustria-CVML/FLEA .
1",TMLR
"Photorealistic object appearance modeling from 2D images is a constant topic in vision
and graphics. While neural implicit methods (such as Neural Radiance Fields) have shown
high-fidelity view synthesis results, they cannot relight the captured objects. More recent
neural inverse rendering approaches have enabled object relighting, but they represent sur-
face properties as simple BRDFs, and therefore cannot handle translucent objects. We
propose Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct ob-
ject appearance from only images. OSFs not only support free-viewpoint object relighting,
but also can model both opaque and translucent objects. While accurately modeling sub-
surface light transport for translucent objects can be highly complex and even intractable
for neural methods, OSFs learn to approximate the radiance transfer from a distant light to
an outgoing direction at any spatial location. This approximation avoids explicitly modeling
complex subsurface scattering, making learning a neural implicit model tractable. Exper-
iments on real and synthetic data show that OSFs accurately reconstruct appearances for
both opaque and translucent objects, allowing faithful free-viewpoint relighting as well as
scene composition. Project website with video results: https://kovenyu.com/OSF .
1",TMLR
"In unsupervised representation learning, models aim to distill essential features from high-
dimensional data into lower-dimensional learned representations, guided by inductive biases.
Understanding the characteristics that make a good representation remains a topic of ongoing
research. Disentanglement of independent generative processes has long been credited
with producing high-quality representations. However, focusing solely on representations
that adhere to the stringent requirements of most disentanglement metrics, may result in
overlooking many high-quality representations, well suited for various downstream tasks.
These metrics often demand that generative factors be encoded in distinct, single dimensions
aligned with the canonical basis of the representation space.
Motivated by these observations, we propose two novel metrics: Importance-Weighted Orthog-
onality (IWO)andImportance-Weighted Rank ( IWR). These metrics evaluate the mutual
orthogonality and rank of generative factor subspaces. Throughout extensive experiments on
common downstream tasks, over several benchmark datasets and models, IWOandIWR
consistently show stronger correlations with downstream task performance than traditional
disentanglement metrics. Our findings suggest that representation quality is closer related
to the orthogonality of independent generative processes rather than their disentanglement,
offering a new direction for evaluating and improving unsupervised learning models.
1",TMLR
"Contrastive instance discrimination methods outperform supervised learning in downstream
tasks such as image classification and object detection. However, these methods rely heavily
on data augmentation during representation learning, which can lead to suboptimal results
if not implemented carefully. A common augmentation technique in contrastive learning
is random cropping followed by resizing. This can degrade the quality of representation
learning when the two random crops contain distinct semantic content. To tackle this issue,
we introduce LeOCLR (Leveraging Original Images for Contrastive Learning of Visual Rep-
resentations), a framework that employs a novel instance discrimination approach and an
adapted loss function. This method prevents the loss of important semantic features caused
by mapping different object parts during representation learning. Our experiments demon-
strate that LeOCLR consistently improves representation learning across various datasets,
outperforming baseline models. For instance, LeOCLR surpasses MoCo-v2 by 5.1% on
ImageNet-1K in linear evaluation and outperforms several other methods on transfer learn-
ing and object detection tasks.
1",TMLR
"Models of human driving behavior have long been used for prediction in autonomous vehi-
cles, but recently have also started being used to create non-playable characters for driving
simulations. While such models are in many respects realistic, they tend to suffer from unac-
ceptably high rates of driving infractions, such as collisions or off-road driving, particularly
when deployed in map locations with road geometries dissimilar to the training dataset. In
this paper we present a novel method for fine-tuning a foundation model of human driving
behavior to novel locations where human demonstrations are not available which reduces
the incidence of such infractions. The method relies on inference in the foundation model to
generate infraction-free trajectories as well as additional penalties applied when fine-tuning
the amortized inference behavioral model. We demonstrate this â€œtitrationâ€ technique using
the ITRA foundation behavior model trained on the INTERACTION dataset when trans-
ferring to CARLA map locations. We demonstrate a 76-86% reduction in infraction rate and
provide evidence that further gains are possible with more computation or better inference
algorithms.
1",TMLR
"Random features have been introduced to scale up kernel methods via randomization tech-
niques. In particular, random Fourier features and orthogonal random features were used
to approximate the popular Gaussian kernel. Random Fourier features are built in this case
using a random Gaussian matrix. In this work, we analyze the bias and the variance of
the kernel approximation based on orthogonal random features which makes use of Haar
orthogonal matrices. We provide explicit expressions for these quantities using normalized
Bessel functions, showing that orthogonal random features does not approximate the Gaus-
sian kernel but a Bessel kernel. We also derive sharp exponential bounds supporting the
view that orthogonal random features are less dispersed than random Fourier features.
1",TMLR
"Gaussian smoothed sliced Wasserstein distance has been recently introduced for comparing
probability distributions, while preserving privacy on the data. It has been shown that it
provides performances similar to its non-smoothed (non-private) counterpart. However, the
computational and statistical properties of such a metric have not yet been well-established.
This work investigates the theoretical properties of this distance as well as those of generalized
versions denoted as Gaussian-smoothed sliced divergences GÏƒSDp. We ï¬rst show that
smoothing and slicing preserve the metric property and the weak topology. To study
the sample complexity of such divergences, we then introduce Ë†Ë†Âµnthe double empirical
distribution for the smoothed-projected Âµ. The distribution Ë†Ë†Âµnis a result of a double
sampling process: one from sampling according to the origin distribution Âµand the second
according to the convolution of the projection of Âµon the unit sphere and the Gaussian
smoothing. We particularly focus on the Gaussian smoothed sliced Wasserstein distance
GÏƒSWpand prove that it converges with a rate O(nâˆ’1/2p). We also derive other properties,
including continuity, of diï¬€erent divergences with respect to the smoothing parameter. We
support our theoretical ï¬ndings with empirical studies in the context of privacy-preserving
domain adaptation.
1",TMLR
"Graphs serve as generic tools to encode the underlying relational structure of data. Often
this graph is not given, and so the task of inferring it from nodal observations becomes
important. Traditional approaches formulate a convex inverse problem with a smoothness
promoting objective and rely on iterative methods to obtain a solution. In supervised settings
where graph labels are available, one can unroll and truncate these iterations into a deep
network that is trained end-to-end. Such a network is parameter efficient and inherits
inductive bias from the optimization formulation, an appealing aspect for data constrained
settings in, e.g., medicine, finance, and the natural sciences. But typically such settings care
equally about uncertainty over edge predictions, not just point estimates. Here we introduce
novel iterations with independently interpretable parameters , i.e., parameters whose values -
independent of other parametersâ€™ settings - proportionally influence characteristics of the
estimated graph, such as edge sparsity. After unrolling these iterations, prior knowledge over
such graph characteristics shape prior distributions over these independently interpretable
network parameters to yield a Bayesian neural network (BNN) capable of graph structure
learning (GSL) from smooth signal observations. Fast execution and parameter efficiency
allow for high-fidelity posterior approximation via Markov Chain Monte Carlo (MCMC) and
thus uncertainty quantification on edge predictions. Informative priors unlock modeling tools
from Bayesian statistics like prior predictive checks. Synthetic and real data experiments
corroborate this modelâ€™s ability to provide well-calibrated estimates of uncertainty, in test
cases that include unveiling economic sector modular structure from S &P500data and
recovering pairwise digit similarities from MNIST images. Overall, this framework enables
GSL in modest-scale applications where uncertainty on the data structure is paramount.
1",TMLR
"Existing theoretical results (such as (Woodworth et al., 2020a)) predict that the performance
of federated averaging (FedAvg) is exacerbated by high data heterogeneity. However, in
practice, FedAvg converges pretty well on several naturally heterogeneous datasets. In order
to explain this seemingly unreasonable eï¬€ectiveness of FedAvg that contradicts previous
theoretical predictions, this paper introduces the client consensus hypothesis : the average of
local models updates on clients starting from the optimum is close to zero. We prove that
under this hypothesis, data heterogeneity does not exacerbate the convergence of FedAvg.
Moreover, we show that this hypothesis holds for a linear regression problem and some
naturally heterogeneous datasets such as FEMNIST and StackOverï¬‚ow. Therefore, we
believe that this hypothesis can better explain the performance of FedAvg in practice.
1",TMLR
"Denoising diffusion probabilistic models (DDPMs) are a recent family of generative models
that achieve state-of-the-art results. In order to obtain class-conditional generation, it was
suggested to guide the diffusion process by gradients from a time-dependent classifier. While
the idea is theoretically sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers may achieve good
accuracy scores, their gradients are possibly unreliable and might hinder the improvement
of the generation results. Recent work discovered that adversarially robust classifiers exhibit
gradients that are aligned with human perception, and these could better guide a generative
process towards semantically meaningful images. We utilize this observation by defining
and training a time-dependent adversarially robust classifier and use it as guidance for a
generative diffusion model. In experiments on the highly challenging and diverse ImageNet
dataset, our scheme introduces significantly more intelligible intermediate gradients, better
alignment with theoretical findings, as well as improved generation results under several
evaluation metrics. Furthermore, we conduct an opinion survey whose findings indicate
that human raters prefer our methodâ€™s results.
1",TMLR
"In some causal inference scenarios, the treatment variable is measured inaccurately, for in-
stance in epidemiology or econometrics. Failure to correct for the effect of this measurement
error can lead to biased causal effect estimates. Previous research has not studied methods
that address this issue from a causal viewpoint while allowing for complex nonlinear depen-
dencies and without assuming access to side information. For such a scenario, this study
proposes a model that assumes a continuous treatment variable that is inaccurately mea-
sured. Building on existing results for measurement error models, we prove that our modelâ€™s
causal effect estimates are identifiable, even without side information and knowledge of the
measurement error variance. Our method relies on a deep latent variable model in which
Gaussian conditionals are parameterized by neural networks, and we develop an amortized
importance-weighted variational objective for training the model. Empirical results demon-
strate the methodâ€™s good performance with unknown measurement error. More broadly, our
work extends the range of applications in which reliable causal inference can be conducted.
1",TMLR
"Deep neural networks have exhibited remarkable performance in a variety of computer vi-
sion fields, especially in semantic segmentation tasks. Their success is often attributed
to multi-level feature fusion, which enables them to understand both global and local in-
formation from an image. However, multi-level features from parallel branches exhibits
different scales, which is a universal and unwanted flaw that leads to detrimental gradient
descent, thereby degrading performance in semantic segmentation. We discover that scale
disequilibrium is caused by bilinear upsampling, which is supported by both theoretical
and empirical evidence. Based on this observation, we propose injecting scale equalizers to
achieve scale equilibrium across multi-level features after bilinear upsampling. Our proposed
scale equalizers are easy to implement, applicable to any architecture, hyperparameter-free,
implementable without requiring extra computational cost, and guarantee scale equilibrium
for any dataset. Experiments showed that adopting scale equalizers consistently improved
the mIoU index across various target datasets, including ADE20K, PASCAL VOC 2012, and
Cityscapes, as well as various decoder choices, including UPerHead, PSPHead, ASPPHead,
SepASPPHead, and FCNHead.
1",TMLR
"We demonstrate that L2 normalization over feature space can produce capable performance
for Out-of-Distribution (OoD) detection for some models and datasets. Although it does
not demonstrate outright state-of-the-art performance, this method is notable for its ex-
treme simplicity: it requires only two addition lines of code, and does not need specialized
loss functions, image augmentations, outlier exposure or extra parameter tuning. We also
observe that training may be more efficient for some datasets and architectures. Notably,
only 60 epochs with ResNet18 on CIFAR10 (or 100 epochs with ResNet50) can produce
performance within two percentage points (AUROC) of several state-of-the-art methods for
some near and far OoD datasets. We provide theoretical and empirical support for this
method, and demonstrate viability across five architectures and three In-Distribution (ID)
datasets.
1",TMLR
"A recent line of work in natural language processing has aimed to combine language models
andtopicmodels. These topic-guided language models augmentneurallanguagemodelswith
topic models, unsupervised learning methods that can discover document-level patterns of
word use. This paper compares the effectiveness of these methods in a standardized setting.
We study four topic-guided language models and two baselines, evaluating the held-out
predictive performance of each model on four corpora. Surprisingly, we find that none of
these methods outperform a standard LSTM language model baseline , and most fail to learn
good topics. Further, we train a probe of the neural language model that shows that the
baselineâ€™s hidden states already encode topic information. We make public all code used for
this study.
1",TMLR
"Generating text with autoregressive language models (LMs) is of great importance to many
natural language processing (NLP) applications. Previous solutions for this task often pro-
duce text that contains degenerative expressions (Welleck et al., 2020) or lacks semantic
consistency (Basu et al., 2021). Recently, Su et al. (2022b) introduced a new decoding
method, contrastive search , based on the isotropic representation space of the language
model and obtained new state of the art on various benchmarks. In addition, Su et al.
(2022b) argued that the representations of autoregressive LMs (e.g. GPT-2) are intrinsi-
cally anisotropic which is also shared by previous studies (Ethayarajh, 2019). Therefore, to
ensure the language model follows an isotropic distribution, Su et al. (2022b) proposed a
contrastive learning scheme, i.e. SimCTG , which calibrates the language modelâ€™s represen-
tations through additional training.
In this study, we first answer the question: â€œAre autoregressive LMs really anisotropic?â€ .
To this end, we extensively evaluate the isotropy of LMs across 16 languages. Surpris-
ingly, we find that the anisotropic problem onlyexists in the two specific English GPT-
2-small/medium models. On the other hand, allother evaluated LMs are isotropic which
is in contrast to the conclusion drawn by previous studies (Ethayarajh, 2019; Su et al.,
2022b). Based on our findings, we further assess the contrastive search decoding method
usingoff-the-shelf LMs on four generation tasks across 16 languages. Our experimen-
tal results demonstrate that contrastive search significantly outperforms previous decod-
ing methods withoutany additional training. More notably, on 12 out of the 16 evalu-
ated languages, contrastive search performs comparably with human-level performances as
judged by human evaluations. Our code and other related resources are publicly available
athttps://github.com/yxuansu/Contrastive_Search_Is_What_You_Need .
1",TMLR
"Mislabeled examples are ubiquitous in real-world machine learning datasets, advocating the
development of techniques for automatic detection. We show that most mislabeled detec-
tion methods can be viewed as probing trained machine learning models using a few core
principles. We formalize a modular framework that encompasses these methods, parame-
terized by only 4 building blocks, as well as a Python library that demonstrates that these
principles can actually be implemented. The focus is on classifier-agnostic concepts, with an
emphasis on adapting methods developed for deep learning models to non-deep classifiers
for tabular data. We benchmark existing methods on (artificial) Completely At Random
(NCAR) as well as (realistic) Not At Random (NNAR) labeling noise from a variety of tasks
with imperfect labeling rules. This benchmark provides new insights as well as limitations
of existing methods in this setup.
1",TMLR
"We propose NeuFace, a 3D face mesh pseudo annotation method on videos via neural
re-parameterized optimization. Despite the huge progress in 3D face reconstruction methods,
generating reliable 3D face labels for in-the-wild dynamic videos remains challenging. Using
NeuFace optimization, we annotate the per-view/-frame accurate and consistent face meshes
on large-scale face videos, called the NeuFace-dataset. We investigate how neural re-
parameterization helps to reconstruct 3D facial geometries, well complying with input facial
gestures and motions. By exploiting the naturalness and diversity of 3D faces in our dataset,
we demonstrate the usefulness of our dataset for 3D face-related tasks: improving the
reconstruction accuracy of an existing 3D face reconstruction model and learning 3D facial
motion prior. Project page: https://kim-youwang.github.io/neuface
1",TMLR
"Optimal transport (OT) is a powerful geometric tool used to compare and align probability
measures following the least effort principle. Despite its widespread use in machine learning
(ML), OT problem still bears its computational burden, while at the same time suffering from
the curse of dimensionality for measures supported on general high-dimensional spaces. In
this paper, we propose to tackle these challenges using representation learning. In particular,
we seek to learn an embedding space such that the samples of the two input measures become
alignable in it with a simple affine mapping that can be calculated efficiently in closed-form.
We then show that such approach leads to results that are comparable to solving the original
OT problem when applied to the transfer learning task on which many OT baselines where
previously evaluated in both homogeneous and heterogeneous DA settings. The code for our
contribution is available at https://github.com/Oleffa/LaOT .
1 Published in Transactions on Machine Learning Research (08/2023)
""To design is to devise courses of action aimed at changing existing situations into preferred ones.â€
Herbert Simon, Nobel Prize winner, 1969.
1",TMLR
"Test log-likelihood is commonly used to compare diï¬€erent models of the same data or
diï¬€erent approximate inference algorithms for ï¬tting the same probabilistic model. We
present simple examples demonstrating how comparisons based on test log-likelihood can
contradict comparisons according to other objectives. Speciï¬cally, our examples show that
(i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need
not also yield more accurate posterior approximations and (ii) conclusions about forecast
accuracy based on test log-likelihood comparisons may not agree with conclusions based on
root mean squared error.
1",TMLR
"Interest is rising in Physics-Informed Neural Networks (PINNs) as a mesh-free alternative
to traditional numerical solvers for partial differential equations (PDEs). However, PINNs
often struggle to learn high-frequency and multi-scale target solutions. To tackle this prob-
lem, we first study a strong Boundary Condition (BC) version of PINNs for Dirichlet BCs
and observe a consistent decline in relative error compared to the standard PINNs. We then
perform a theoretical analysis based on the Fourier transform and convolution theorem. We
find that strong BC PINNs can better learn the amplitudes of high-frequency components
of the target solutions. However, constructing the architecture for strong BC PINNs is
difficult for many BCs and domain geometries. Enlightened by our theoretical analysis, we
proposeFourierPINNsâ€”asimple, general, yetpowerfulmethodthataugmentsPINNswith
pre-specified, dense Fourier bases. Our proposed architecture likewise learns high-frequency
components better but places no restrictions on the particular BCs or problem domains. We
develop an adaptive learning and basis selection algorithm via alternating neural net basis
optimization, Fourier and neural net basis coefficient estimation, and coefficient truncation.
This scheme can flexibly identify the significant frequencies while weakening the nominal
frequencies to better capture the target solutionâ€™s power spectrum. We show the advantage
of our approach through a set of systematic experiments.
1",TMLR
"Target Propagation (TP) algorithms compute targets instead of gradients along neural net-
works and propagate them backward in a way that is similar to yet diï¬€erent than gradient
back-propagation (BP). The idea initially appeared as a perturbative alternative to BP that
may improve gradient evaluation accuracy when training multi-layer neural networks (Le-
Cun, 1985) and has gained popularity as a biologically plausible counterpart of BP. However,
there have been many variations of TP, and a simple version of TP still remains worthwhile.
Revisiting the insights of LeCun (1985) and Lee et al. (2015), we present a simple version
of TP based on regularized inversions of layers of recurrent neural networks. The proposed
TP algorithm is easily implementable in a diï¬€erentiable programming framework. We illus-
trate the algorithm with recurrent neural networks on long sequences in various sequence
modeling problems and delineate the regimes in which the computational complexity of TP
can be attractive compared to BP.
1",TMLR
"Variational Graph Autoencoders (VGAEs) are powerful models for unsupervised learning of
node representations from graph data. In this work, we systematically analyze modeling node
attributes in VGAEs and show that attribute decoding is important for node representation
learning. We further propose a new learning model, interpretable NOde Representation
with Attribute Decoding (NORAD). The model encodes node representations in an inter-
pretable approach: node representations capture community structures in the graph and
the relationship between communities and node attributes. We further propose a rectifying
procedure to reï¬ne node representations of isolated notes, improving the quality of these
nodesâ€™ representations. Our empirical results demonstrate the advantage of the proposed
model when learning graph data in an interpretable approach.
1",TMLR
"Difficulties in replication and reproducibility of empirical evidences in machine learning
research have become a prominent topic in recent years. Ensuring that machine learning re-
search results are sound and reliable requires reproducibility, which verifies the reliability of
research findings using the same code and data. This promotes open and accessible research,
robust experimental workflows, and the rapid integration of new findings. Evaluating the
degree to which research publications support these different aspects of reproducibility is
one goal of the present work. In order to do this, we introduce an ontology of reproducibility
in machine learning and apply it to methods for graph neural networks.
The objective of this study is to try and identify hidden effects that influence model perfor-
mance. To this end, we employ the aforementioned ontology to control for a broad selection
of sources and turn our attention to another critical challenge in machine learning. The
curse of dimensionality, which induces complication in data collection, representation, and
analysis, makes it harder to find representative data and impedes the training and inference
processes. The closely linked concept of geometric intrinsic dimension is employed to inves-
tigate the extent to which the machine learning models under consideration are influenced
by the intrinsic dimension of the data sets on which they are trained.
Keywords: Reproducibility, Replication, Curse of Dimensionality, Intrinsic Dimension
1",TMLR
"Standard imitation learning can fail when the expert demonstrators have different sensory
inputs than the imitating agent. This is because partial observability gives rise to hidden
confounders in the causal graph. Previously, to work around the confounding problem,
policies have been trained by accessing the expertâ€™s policy or using inverse reinforcement
learning (IRL). However, both approaches have drawbacks as the expertâ€™s policy may not be
available and IRL can be unstable in practice. Instead, we propose to train a variational in-
ference model to infer the expertâ€™s latent information and use it to train a latent-conditional
policy. We prove that using this method, under strong assumptions, the identification of the
correct imitation learning policy is theoretically possible from expert demonstrations alone.
In practice, we focus on a setting with less strong assumptions where we use exploration
data for learning the inference model. We show in theory and practice that this algorithm
converges to the correct interventional policy, solves the confounding issue, and can under
certain assumptions achieve an asymptotically optimal imitation performance.
1",TMLR
"To enable video models to be applied seamlessly across video tasks in different environments,
various Video Unsupervised Domain Adaptation (VUDA) methods have been proposed to
improve the robustness and transferability of video models. Despite improvements made in
modelrobustness, theseVUDAmethodsrequireaccesstobothsourcedataandsourcemodel
parametersforadaptation, raisingseriousdataprivacyandmodelportabilityissues. Tocope
with the above concerns, this paper firstly formulates Black-box Video Domain Adaptation
(BVDA) as a more realistic yet challenging scenario where the source video model is pro-
vided only as a black-box predictor. While a few methods for Black-box Domain Adaptation
(BDA) are proposed in the image domain, these methods cannot apply to the video domain
sincevideomodalityhasmorecomplicatedtemporalfeaturesthatarehardertoalign. Toad-
dress BVDA, we propose a novel Endo and eXo-TEmporal Regularized Network (EXTERN)
by applying mask-to-mix strategies and video-tailored regularizations. They are the endo-
temporal regularization and exo-temporal regularization, which are performed across both
clip and temporal features, while distilling knowledge from the predictions obtained from
the black-box predictor. Empirical results demonstrate the state-of-the-art performance of
EXTERN across various cross-domain closed-set and partial-set action recognition bench-
marks, which even surpasses most existing video domain adaptation methods with source
data accessibility. Code will be available at https://xuyu0010.github.io/b2vda.html .
âˆ—Equal contribution.
â€ Corresponding author.
1 Published in Transactions on Machine Learning Research (02/2024)
Temporal Features Virtual Temporal Features Clip Features
One Sample i nClass A One Sample in Class B Classifier
Figure 1: Clip features of target videos may be scattered, where clips from the same video may be separated
by the decision boundary, resulting in differed label predictions. Such clip features violate both the cluster
assumption and the masked temporal hypothesis . We augment the target video domain with virtual tempo-
ral features through a novel mask-to-mix strategy and apply endo-temporal regularization. The resulting
temporal features are more discriminative and comply with both the cluster assumption and the masked
temporal hypothesis .
1",TMLR
"We present a meta-method for initializing (seeding) the k-means clustering algorithm called
PNN-smoothing. It consists in splitting a given dataset into Jrandom subsets, clustering
each of them individually, and merging the resulting clusterings with the pairwise-nearest-
neighbor (PNN) method. It is a meta-method in the sense that when clustering the indi-
vidual subsets any seeding algorithm can be used. If the computational complexity of that
seeding algorithm is linear in the size of the data Nand the number of clusters k, PNN-
smoothing is also almost linear with an appropriate choice of J, and quite competitive in
practice. We show empirically, using several existing seeding methods and testing on several
synthetic and real datasets, that this procedure results in systematically better costs. In
particular, our method of enhancing k-means++ seeding proves superior in both eï¬€ective-
ness and speed compared to the popular â€œgreedyâ€ k-means++ variant. Our implementation
is publicly available at https://github.com/carlobaldassi/KMeansPNNSmoothing.jl.
1",TMLR
"Sparse training has emerged as a promising method for resource-efficient deep neural networks
(DNNs) in real-world applications. However, the reliability of sparse models remains a crucial
concern, particularly in detecting unknown out-of-distribution (OOD) data. This study
addresses the knowledge gap by investigating the reliability of sparse training from an
OOD perspective and reveals that sparse training exacerbates OOD unreliability. The
lack of unknown information and the sparse constraints hinder the effective exploration
of weight space and accurate differentiation between known and unknown knowledge. To
tackle these challenges, we propose a new unknown-aware sparse training method, which
incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight
space exploration and mitigate confusion between known and unknown information without
incurring significant additional costs or requiring access to additional OOD data. Theoretical
insights demonstrate how our method reduces model confidence when faced with OOD
samples. Empirical experiments across multiple datasets, model architectures, and sparsity
levels validate the effectiveness of our method, with improvements of up to 8.4%in AUROC
while maintaining comparable or higher accuracy and calibration. This research enhances the
understanding and readiness of sparse DNNs for deployment in resource-limited applications.
Our code is available on: https://github.com/StevenBoys/MOON .
1",TMLR
"Fourier Neural Operators (FNO) offer a principled approach to solving challenging partial
differential equations (PDE) such as turbulent flows. At the core of FNO is a spectral layer
that leverages a discretization-convergent representation in the Fourier domain, and learns
weights over a fixed set of frequencies. However, training FNO presents two significant
challenges, particularly in large-scale, high-resolution applications: (i) Computing Fourier
transform on high-resolution inputs is computationally intensive but necessary since fine-
scale details are needed for solving many PDEs, such as fluid flows, (ii) selecting the relevant
set of frequencies in the spectral layers is challenging, and too many modes can lead to
overfitting, while too few can lead to underfitting. To address these issues, we introduce
theIncremental Fourier Neural Operator (iFNO), which progressively increases both the
number of frequency modes used by the model as well as the resolution of the training data.
We empirically show that iFNO reduces total training time while maintaining or improving
generalization performance across various datasets. Our method demonstrates a 38% lower
testing error, using 20% fewer frequency modes compared to the existing FNO, while also
achieving up to 46% faster training and a 2.8x reduction in model size.
1",TMLR
"Many algorithms for ranked data become computationally intractable as the number of
objects grows due to the complex geometric structure induced by rankings. An additional
challenge is posed by partial rankings, i.e. rankings in which the preference is only known for
a subset of all objects. For these reasons, state-of-the-art methods cannot scale to real-world
applications, such as recommender systems. We address this challenge by exploiting the
geometric structure of ranked data and additional available information about the objects to
derive a kernel for ranking based on the graph cut function. The graph cut kernel combines
the eï¬ƒciency of submodular optimization with the theoretical properties of kernel-based
methods. We demonstrate that our novel kernel drastically reduces the computational cost
while maintaining the same accuracy as state-of-the-art methods.
1",TMLR
"In many sequential decision-making tasks, the agent is not able to model the full complex-
ity of the world, which consists of multitudes of relevant and irrelevant information. For
example, a person walking along a city street who tries to model all aspects of the world
would quickly be overwhelmed by a multitude of shops, cars, and people moving in and
out of view, each following their own complex and inscrutable dynamics. Is it possible to
turn the agentâ€™s firehose of sensory information into a minimal latent state that is both
necessary and sufficient for an agent to successfully act in the world? We formulate this
question concretely, and propose the Agent Control-Endogenous State Discovery algorithm
(AC-State ), which has theoretical guarantees and is practically demonstrated to discover
theminimal control-endogenous latent state which contains all of the information necessary
for controlling the agent, while fully discarding all irrelevant information. This algorithm
consists of a multi-step inverse model (predicting actions from distant observations) with an
information bottleneck. AC-State enables localization, exploration, and navigation without
reward or demonstrations. We demonstrate the discovery of the control-endogenous latent
state in three domains: localizing a robot arm with distractions (e.g., changing lighting
conditions and background), exploring a maze alongside other agents, and navigating in the
Matterport house simulator.
1",TMLR
"Stochastic processes defined on integer valued state spaces are popular within the physical
and biological sciences. These models are necessary for capturing the dynamics of small
systems where the individual nature of the populations cannot be ignored and stochastic
effects are important. The inference of the parameters of such models, from time series data,
is challenging due to intractability of the likelihood. To work at all, current simulation based
inferencemethodsrequirethegenerationofrealisationsofthemodel conditional onthedata,
which can be both tricky to implement and computationally expensive. In this paper we
instead construct a neural likelihood approximation that can be trained using unconditional
simulation of the underlying model, which is much simpler. We demonstrate our method by
performing inference on a number of ecological and epidemiological models, showing that
we can accurately approximate the true posterior while achieving significant computational
speed ups compared to current best methods.
1",TMLR
Deep Learning on Graphs was recently made possible with the,TMLR
"We present AI-SARAH , a practical variant of SARAH. As a variant of SARAH, this algorithm
employs the stochastic recursive gradient yet adjusts step-size based on local geometry. AI-
SARAH implicitly computes step-size and eï¬ƒciently estimates local Lipschitz smoothness
of stochastic functions. It is fully adaptive, tune-free, straightforward to implement, and
computationally eï¬ƒcient. We provide technical insight and intuitive illustrations on its
design and convergence. We conduct extensive empirical analysis and demonstrate its strong
performance compared with its classical counterparts and other state-of-the-art ï¬rst-order
methods in solving convex machine learning problems.
1",TMLR
"Decision trees provide a rich family of highly non-linear but efficient models, due to which
they continue to be the go-to family of predictive models by practitioners across domains.
But learning trees is challenging due to their discrete decision boundaries. The state-of-the-
art (SOTA) techniques resort to (a) learning softtrees thereby losing logarithmic inference
time; or (b) using methods tailored to specific supervised learning settings, requiring access
to labeled examples and loss function. In this work, by leveraging techniques like overpa-
rameterization and straight-through estimators, we propose a unified method that enables
accurate end-to-end gradient based tree training and can be deployed in a variety of settings
like offline supervised learning and online learning with bandit feedback. Using extensive
validation on standard benchmarks, we demonstrate that our method provides best of both
worlds, i.e., it is competitive to, and in some cases more accurate than methods designed
specifically forthesupervisedsettings; andinbanditsettings, wheremostexistingtreelearn-
ing techniques are not applicable, our models are still accurate and significantly outperform
the applicable SOTA methods.
1",TMLR
"A core component of the recent success of self-supervised learning is cropping data augmenta-
tion, which selects sub-regions of an image to be used as positive views in the self-supervised
loss. The underlying assumption is that randomly cropped and resized regions of a given
image share information about the objects of interest, which is captured by the learned
representation. This assumption is mostly satisfied in datasets such as ImageNet where
there is a large, centered object, which is highly likely to be present in random crops of
the full image. However, in other datasets such as OpenImages or COCO, which are more
representative of real world uncurated data, there are typically multiple small objects in
an image. In this work, we show that self-supervised learning based on the usual random
cropping performs poorly on such datasets (measured by the difference from fully-supervised
learning). Instead of using pairs of random crops, we propose to leverage an unsupervised
object proposal technique; the first view is a crop obtained from this algorithm, and the
second view is a dilated version of the first view. This encourages the self-supervised model
to learn both object and scene level semantic representations. Using this approach, which we
callobject-aware cropping , results in significant improvements over random scene cropping on
classification and object detection benchmarks. For example, for pre-training on OpenImages,
our approach achieves an improvement of 8.8%mAP over random scene cropping (both meth-
ods using MoCo-v2). We also show significant improvements on COCO and PASCAL-VOC
object detection and segmentation tasks over the state-of-the-art self-supervised learning
approaches. Our approach is efficient, simple and general, and can be used in most existing
contrastive and non-contrastive self-supervised learning frameworks.
1",TMLR
"Many problems in machine learning rely on multi-task learning (MTL) , in which the goal is
to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant
for privacy-sensitive applications in areas such as healthcare, ï¬nance, and IoT computing,
where sensitive data from multiple, varied sources are shared for the purpose of learning. In
this work, we formalize notions of client-level privacy for MTL via billboard privacy (BP), a
relaxation of diï¬€erential privacy for mechanism design and distributed optimization. We
then propose an algorithm for mean-regularized MTL, an objective commonly used for
applications in personalized federated learning, subject to BP. We analyze our objective and
solver, providing certiï¬able guarantees on both privacy and utility. Empirically, we ï¬nd that
our method provides improved privacy/utility trade-oï¬€s relative to global baselines across
common federated learning benchmarks.
1",TMLR
"Failure detection in automated image classification is a critical safeguard for clinical deploy-
ment. Detected failure cases can be referred to human assessment, ensuring patient safety
in computer-aided clinical decision making. Despite its paramount importance, there is in-
sufficient evidence about the ability of state-of-the-art confidence scoring methods to detect
test-time failures of classification models in the context of medical imaging. This paper pro-
vides a reality check, establishing the performance of in-domain misclassification detection
methods, benchmarking 9 widely used confidence scores on 6 medical imaging datasets with
different imaging modalities, in multiclass and binary classification settings. Our experi-
ments show that the problem of failure detection is far from being solved. We found that
none of the benchmarked advanced methods proposed in the computer vision and machine
learning literature can consistently outperform a simple softmax baseline, demonstrating
that improved out-of-distribution detection or model calibration do not necessarily trans-
late to improved in-domain misclassification detection. Our developed testbed facilitates
future work in this important area1.
1",TMLR
"Many real-world problems with multiple objectives require reinforcement learning solutions
that can handle trade-offs in a user-preferred manner. In the multi-objective framework, a
single algorithm adapting to different user preferences based on a pre-defined reward function
and a subjectively defined scalarisation function may be developed. The scalarisation
function approximation can be done by fitting a meta-model with information gained from
the interaction between the user and the environment or the agent. The interaction requires
exact formulation of a constructive feedback, which is also simple for the user to give.
In this paper, we propose a novel algorithm, Conciliator steering, that leverages priority
weighting and reward transfer to seek optimal user-preferred policies in multi-objective
reinforcement learning under expected scalarised returns criterion. We test Conciliator
steering on DeepSeaTreasure v1 benchmark problem and demonstrate that it can find user-
preferred policies with effortless and simple user-agent interaction and negligible bias, which
has not been possible before. Additionally, we show that on average Conciliator steering
results in a fraction of carbon dioxide emissions and total energy consumption when compared
to a training of fully connected MNIST classifier, both run on a personal laptop.
1",TMLR
"Simulation-based inference techniques are indispensable for parameter estimation of mechanis-
tic and simulable models with intractable likelihoods. While traditional statistical approaches
like approximate Bayesian computation and Bayesian synthetic likelihood have been studied
under well-specified and misspecified settings, they often suffer from inefficiencies due to
wasted model simulations. Neural approaches, such as sequential neural likelihood (SNL)
avoid this wastage by utilising all model simulations to train a neural surrogate for the
likelihood function. However, the performance of SNL under model misspecification is
unreliable and can result in overconfident posteriors centred around an inaccurate parameter
estimate. In this paper, we propose a novel SNL method, which through the incorporation
of additional adjustment parameters, is robust to model misspecification and capable of
identifying features of the data that the model is not able to recover. We demonstrate the
efficacy of our approach through several illustrative examples, where our method gives more
accurate point estimates and uncertainty quantification than SNL.
1",TMLR
"Gradient flows are a powerful tool for optimizing functionals in general metric spaces,
including the space of probabilities endowed with the Wasserstein metric. A typical approach
to solving this optimization problem relies on its connection to the dynamic formulation of
optimal transport and the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this
formulation involves optimization over convex functions, which is challenging, especially in
high dimensions. In this work, we propose an approach that relies on the recently introduced
input-convex neural networks (ICNN) to parametrize the space of convex functions in
order to approximate the JKO scheme, as well as in designing functionals over measures
that enjoy convergence guarantees. We derive a computationally efficient implementation
of this JKO-ICNN framework and experimentally demonstrate its feasibility and validity
in approximating solutions of low-dimensional partial differential equations with known
solutions. We also demonstrate its viability in high-dimensional applications through an
experiment in controlled generation for molecular discovery.
1",TMLR
"Given a finite set of sample points, meta-learning algorithms aim to learn an optimal
adaptation strategy for new, unseen tasks. Often, this data can be ambiguous as it might
belong to different tasks concurrently. This is particularly the case in meta-regression tasks.
In such cases, the estimated adaptation strategy is subject to high variance due to the
limited amount of support data for each task, which often leads to sub-optimal generalization
performance. In this work, we address the problem of variance reduction in gradient-based
meta-learning and formalize the class of problems prone to this, a condition we refer to
astask overlap . Specifically, we propose a novel approach that reduces the variance of the
gradient estimate by weighing each support point individually by the variance of its posterior
over the parameters. To estimate the posterior, we utilize the Laplace approximation, which
allows us to express the variance in terms of the curvature of the loss landscape of our
meta-learner. Experimental results demonstrate the effectiveness of the proposed method
and highlight the importance of variance reduction in meta-learning.
1",TMLR
"We pose and study the problem of satisfying fairness in the online Reinforcement Learning
(RL) setting. We focus on the group notions of fairness, according to which agents belonging
to different groups should have similar performance based on some given measure. We
consider the setting of maximizing return in an unknown environment (unknown transition
and reward function) and show that it is possible to have RL algorithms that learn the
best fair policies without violating the fairness requirements at any point in time during
the learning process. In the tabular finite-horizon episodic setting, we provide an algorithm
that combines the principle of optimism and pessimism under uncertainty to achieve zero
fairness violation with arbitrarily high probability while also maintaining sub-linear regret
guarantees. For the high-dimensional Deep-RL setting, we present algorithms based on the
performance-difference style approximate policy improvement update step and we report
encouraging empirical results on various traditional RL-inspired benchmarks showing that
our algorithms display the desired behavior of learning the optimal policy while performing
a fair learning process.
1",TMLR
"Deep active learning (DAL) studies the optimal selection of labeled data for training deep
neural networks (DNNs). While data selection in traditional active learning is mostly op-
timized for given features, in DNN these features are learned and change with the learning
process as well as the choices of DNN architectures. How is the optimal selection of data
affected by this change is not well understood in DAL. To shed light on this question, we
present the first systematic investigation on: 1) the relative performance of representative
modern DAL data selection strategies, as the architecture types and sizes change in the un-
derlying DNN architecture (Focus 1), and 2) the effect of optimizing the DNN architecture
of a DNN on DAL (Focus 2). The results suggest that the change in the DNN architecture
significantly influences and outweighs the benefits of data selection in DAL. These results
cautions the community in generalizing DAL findings obtained on specific architectures,
while suggesting the importance to optimize the DNN architecture in order to maximize the
effect of active data selection in DAL.
1",TMLR
"The challenge in the widely applicable online matching problem lies in making irrevocable
assignments while there is uncertainty about future inputs. Most theoretically-grounded
policies are myopic or greedy in nature. In real-world applications where the matching
process is repeated on a regular basis, the underlying data distribution can be leveraged
for better decision-making. We present an end-to-end Reinforcement Learning framework
for deriving better matching policies based on trial-and-error on historical data. We de-
vise a set of neural network architectures, design feature representations, and empirically
evaluate them across two online matching problems: Edge-Weighted Online Bipartite
Matching and Online Submodular Bipartite Matching. We show that most of the learning
approaches perform consistently better than classical baseline algorithms on four synthetic
and real-world datasets. On average, our proposed models improve the matching quality
by 3â€“10% on a variety of synthetic and real-world datasets.Our code is publicly available
athttps://github.com/lyeskhalil/CORL .
1",TMLR
"Enhancing the generalisation abilities of neural networks (NNs) through integrating noise
such as MixUp or Dropout during training has emerged as a powerful and adaptable tech-
nique. Despite the proven efficacy of noise in NN training, there is no consensus regarding
which noise sources, types and placements yield maximal benefits in generalisation and con-
fidence calibration. This study thoroughly explores diverse noise modalities to evaluate their
impacts on NNâ€™s generalisation and calibration under in-distribution or out-of-distribution
settings, paired with experiments investigating the metric landscapes of the learnt repre-
sentations across a spectrum of NN architectures, tasks, and datasets. Our study shows
that AugMix and weak augmentation exhibit cross-task effectiveness in computer vision,
emphasising the need to tailor noise to specific domains. Our findings emphasise the ef-
ficacy of combining noises and successful hyperparameter transfer within a single domain
but the difficulties in transferring the benefits to other domains. Furthermore, the study
underscores the complexity of simultaneously optimising for both generalisation and cali-
bration, emphasising the need for practitioners to carefully consider noise combinations and
hyperparameter tuning for optimal performance in specific tasks and datasets.
1",TMLR
"Reinforcement Learning (RL) has demonstrated promising results in learning policies for
complex tasks, but it often suffers from low sample efficiency and limited transferability.
Hierarchical RL (HRL) methods aim to address the difficulty of learning long-horizon tasks
by decomposing policies into skills, abstracting states, and reusing skills in new tasks. How-
ever, many HRL methods require some initial task success to discover useful skills, which
paradoxically may be very unlikely without access to useful skills. On the other hand,
reward-free HRL methods often need to learn far too many skills to achieve proper cover-
age in high-dimensional domains. In contrast, we introduce the Chain of Interaction Skills
(COInS) algorithm, which focuses on controllability in factored domains to identify a small
number of task-agnostic skills that still permit a high degree of control. COInS uses learned
detectors to identify interactions between state factors and then trains a chain of skills to
control each of these factors successively. We evaluate COInS on a robotic pushing task
with obstaclesâ€”a challenging domain where other RL and HRL methods fall short. We
also demonstrate the transferability of skills learned by COInS, using variants of Breakout,
a common RL benchmark, and show 2-3x improvement in both sample efficiency and final
performance compared to standard RL baselines.
1",TMLR
"Self-supervised features are the cornerstone of modern machine learning systems. They
are typically pre-trained on data collections whose construction and curation typically re-
quire extensive human effort. This manual process has some limitations similar to those
encountered in supervised learning, e.g., the crowd-sourced selection of data is costly and
time-consuming, preventing scaling the dataset size. In this work, we consider the problem
of automatic curation of high-quality datasets for self-supervised pre-training. We posit
that such datasets should be large, diverse and balanced, and propose a clustering-based
approach for building ones satisfying all these criteria. Our method involves successive and
hierarchical applications of k-means on a large and diverse data repository to obtain clus-
ters that distribute uniformly among data concepts, followed by a hierarchical, balanced
sampling step from these clusters. Extensive experiments on three different data domains
including web-based images, satellite images and text show that features trained on our
automatically curated datasets outperform those trained on uncurated data while being on
par or better than ones trained on manually curated data. Our code is publicly available at
https://github.com/facebookresearch/ssl-data-curation .
1",TMLR
"In the context of distributed deep learning, the issue of stale weights or gradients could
result in poor algorithmic performance. This issue is usually tackled by delay tolerant algo-
rithms with some mild assumptions on the objective functions and step sizes. In this paper,
we propose a diï¬€erent approach to develop a new algorithm, called Predicting Clipping
Asynchronous Stochastic Gradient Descent (aka, PC-ASGD). Speciï¬cally, PC-ASGD has
two steps - the predicting step leverages the gradient prediction using Taylor expansion to
reduce the staleness of the outdated weights while the clipping step selectively drops the
outdated weights to alleviate their negative eï¬€ects. A tradeoï¬€ parameter is introduced to
balance the eï¬€ects between these two steps. Theoretically, we present the convergence rate
considering the eï¬€ects of delay of the proposed algorithm with constant step size when the
smooth objective functions are weakly strongly-convex, general convex, and nonconvex. One
practical variant of PC-ASGD is also proposed by adopting a condition to help with the
determination of the tradeoï¬€ parameter. For empirical validation, we demonstrate the per-
formance of the algorithm with four deep neural network architectures on three benchmark
datasets.
1",TMLR
"Symbolic regression (SR) is the task of learning a model of data in the form of a mathematical
expression. By their nature, SR models have the potential to be accurate and human-
interpretable at the same time. Unfortunately, finding such models, i.e., performing SR,
appears to be a computationally intensive task. Historically, SR has been tackled with
heuristics such as greedy or genetic algorithms and, while some works have hinted at the
possible hardness of SR, no proof has yet been given that SR is, in fact, NP-hard. This
begs the question: Is there an exact polynomial-time algorithm to compute SR models? We
provide evidence suggesting that the answer is probably negative by showing that SR is
NP-hard.
1",TMLR
"Wedescribeanalgorithmthatlearnstwo-layerresidualunitsusingrectifiedlinearunit(ReLU)
activation: suppose the input xis from a distribution with support space Rdand the ground-
truth generative model is a residual unit of this type, given by y=Bâˆ—/bracketleftï£¬ig
(Aâˆ—x)++x/bracketrightï£¬ig
, where
ground-truth network parameters Aâˆ—âˆˆRdÃ—drepresent a full-rank matrix with nonnegative
entriesandBâˆ—âˆˆRmÃ—disfull-rankwith mâ‰¥dandforcâˆˆRd,[c+]i=max{0,ci}. Wedesign
layer-wise objectives as functionals whose analytic minimizers express the exact ground-truth
network in terms of its parameters and nonlinearities. Following this objective landscape,
learning residual units from finite samples can be formulated using convex optimization of a
nonparametric function: for each layer, we first formulate the corresponding empirical risk
minimization (ERM) as a positive semi-definite quadratic program (QP), then we show the
solution space of the QP can be equivalently determined by a set of linear inequalities, which
can then be efficiently solved by linear programming (LP). We further prove the strong
statistical consistency of our algorithm, and demonstrate its robustness and sample efficiency
through experimental results on synthetic data and a set of benchmark regression datasets.1
1",TMLR
"In this paper, we study the OOD generalization of neural algorithmic reasoning tasks,
where the goal is to learn an algorithm ( e.g., sorting, breadth-first search, and depth-first
search) from input-output pairs using deep neural networks. First, we argue that OOD
generalization in this setting is significantly different than common OOD settings. For
example, some phenomena in OOD generalization of image classifications such as accuracy
on the line are not observed here, and techniques such as data augmentation methods
do not help as assumptions underlying many augmentation techniques are often violated.
Second, we analyze the main challenges ( e.g., input distribution shift, non-representative
data generation, and uninformative validation metrics) of the current leading benchmark,
i.e., CLRS (VeliÄkoviÄ‡ et al., 2021), which contains 30 algorithmic reasoning tasks. We
propose several solutions, including a simple-yet-effective fix to the input distribution shift
and improved data generation. Finally, we propose an attention-based 2WL-graph neural
network (GNN) processor which complements message-passing GNNs so their combination
outperforms the state-of-the-art model by a 3%margin averaged over all algorithms. Our
code is available at: https://github.com/smahdavi4/clrs .
1",TMLR
"Recently, the stochastic Polyak step size ( SPS) has emerged as a competitive adaptive step
size scheme for stochastic gradient descent. Here we develop ProxSPS, aproximal variant
ofSPSthat can handle regularization terms. Developing a proximal variant of SPSis par-
ticularly important, since SPSrequires a lower bound of the objective function to work well.
When the objective function is the sum of a loss and a regularizer, available estimates of
a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound
for the loss which is often readily available. As a consequence, we show that ProxSPS is
easier to tune and more stable in the presence of regularization. Furthermore for image
classiï¬cation tasks, ProxSPS performs as well as AdamWwith little to no tuning, and results
in a network with smaller weight parameters. We also provide an extensive convergence
analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly
convex setting.
1",TMLR
"Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learning
â€˜simpleâ€™ features over more â€˜complexâ€™ ones, even when the latter may be more informative.
Simplicity bias can lead to the model making biased predictions which have poor out-of-
distribution (OOD) generalization and subgroup robustness. To address this, we propose a
hypothesis about spurious features that directly connects to simplicity bias: we hypothesize
that spurious features on many datasets are simple features that are still predictive of the
label. We empirically validate this hypothesis, and subsequently develop a framework which
leverages this hypothesis to learn more robust models. In our proposed framework, we first
train a simple model, and then regularize the conditional mutual information with respect to
it to obtain the final model. We theoretically study the effect of this regularization and show
that it provably reduces reliance on spurious features in certain settings. We also empirically
demonstrate the effectiveness of this framework in various problem settings and real-world
applications, showing that it effectively addresses simplicity bias and leads to more features
being used, enhances OOD generalization, and improves subgroup robustness and fairness.
1",TMLR
"This paper introduces consistency models to the problem of sequential decision-making.
Previous work applying diffusion models to planning within a model-based reinforcement
learning framework often struggles with high computational cost during the inference pro-
cess, primarily due to their reliance on iterative reverse diffusion processes. Consistency
models, known for their computational efficiency, have already shown promise in reinforce-
ment learning within the actor-critic algorithm. Therefore, we combine guided consistency
distillation with a continuous-time diffusion model in the framework of Decision Diffuser.
Our approach, named Consistency Planning, combines the robust planning capabilities of
diffusion models with the speed of consistency models. We validate our method on Gym
tasks in the D4RL framework, demonstrating that, when compared to its diffusion model
counterparts, our method achieves more than a 12-fold increase in speed without any loss
in performance.
1",TMLR
"We consider when a sparse nonnegative matrix Scan be recovered, via an elementwise non-
linearity, from a real-valued matrix Lof significantly lower rank. Of particular interest is
the setting where the positive elements of Sencode the similarities of nearby points on a low
dimensional manifold. The recovery can then be posed as a problem in manifold learningâ€”
in this case, how to learn a norm-preserving and neighborhood-preserving mapping of high
dimensional inputs into a lower dimensional space. We describe an algorithm for this prob-
lem based on a generalized low-rank decomposition of sparse matrices. This decomposition
has the interesting property that it can be encoded by a neural network with one layer
of rectified linear units; since the algorithm discovers this encoding, it can be viewed as a
layerwise primitive for deep learning. The algorithm regards the inputs xiandxjas similar
whenever the cosine of the angle between them exceeds some threshold Ï„âˆˆ(0,1). Given this
threshold, the algorithm attempts to discover a mapping xiâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’yiby matching the elements
of two sparse matrices; in particular, it seeks a mapping for which S= max(0,L), where
Sij= max(0,xiÂ·xjâˆ’Ï„âˆ¥xiâˆ¥âˆ¥xjâˆ¥)andLij=yiÂ·yjâˆ’Ï„âˆ¥yiâˆ¥âˆ¥yjâˆ¥. We apply the algorithm to
data sets where vector magnitudes and small cosine distances have interpretable meanings
(e.g., the brightness of an image, the similarity to other words). On these data sets, the
algorithm discovers much lower dimensional representations that preserve these meanings.
1",TMLR
"We address catastrophic forgetting issues in graph learning as the arrival of new data from
diverse task distributions often leads graph models to prioritize the current task, causing
them to forget valuable insights from previous tasks. Whereas prior studies primarily tackle
one setting of graph continual learning such as incremental node classification, we focus on a
universal approach wherein each data point in a task can be a node or a graph, and the task
varies from node to graph classification. We refer to this setting as Universal Graph Continual
Learning (UGCL), which includes node-unit node classification (NUNC), graph-unit node
classification (GUNC), and graph-unit graph classification (GUGC). Our novel method
maintains a replay memory of nodes and neighbours to remind the model of past graph
structures through distillation. Emphasizing the importance of preserving distinctive graph
structures across tasks, we enforce that coarse-to-grain graph representations stay close to
previous ones by minimizing our proposed global and local structure losses. We benchmark
our method against various continual learning baselines in 8 real-world graph datasets and
achieve significant improvement in average performance and forgetting across tasks.
1",TMLR
"Many deep learning methods are data-driven, often converging to local minima due to
limited training data. This situation poses a challenge in domains where acquiring adequate
data is difficult for model training or fine-tuning, such as generalized few-shot semantic
segmentation (GFSSeg) and monocular depth estimation (MDE). To this end, we propose
a self-trained geometry regularization framework to enhance model training or fine-tuning
in scenarios with limited training data using geometric knowledge. Specifically, we propose
to leverage low-level geometry information extracted from the training data and define a
novel regularization term, which is a plug-and-play module jointly trained with the primary
task via multi-task learning. Our proposed regularization neither relies on extra manual
labels and data in training nor requires extra computation during the inference stage. We
demonstrate the effectiveness of this regularization on GFSSeg and MDE tasks. Notably, it
improvesthestate-of-the-artGFSSegby5.61%and4.26%mIoUofnovelclassesonPASCAL
and COCO in the 1-shot scenario. In MDE, it achieves a relative reduction of SILog error
by 16.6% and 9.4% for two recent methods in the KITTI dataset.
1",TMLR
"Graph convolutional neural networks (GCNs) operate by aggregating messages over local
neighborhoods given the prediction task under interest. Many GCNs can be understood as
a form of generalized diffusion of input features on the graph, and significant work has been
dedicated to improving predictive accuracy by altering the ways of message passing. In this
work, weproposeanewconvolutionkernelthateffectivelyrewiresthegraphaccordingtothe
occupation correlations of the vertices by trading on the generalized diffusion paradigm for
the propagation of a quantum particle over the graph. We term this new convolution kernel
the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale
variant that combines messages from the QDC operator and the traditional combinatorial
Laplacian. To understand our method, we explore the spectral dependence of homophily
and the importance of quantum dynamics in the construction of a bandpass filter. Through
these studies, as well as experiments on a range of datasets, we observe that QDC improves
predictive performance on the widely used benchmark datasets when compared to similar
methods.
1",TMLR
"In this work, we highlight and perform a comprehensive study on calibration attacks , a form
of adversarial attacks that aim to trap victim models to be heavily miscalibrated without
altering their predicted labels, hence endangering the trustworthiness of the models and
follow-up decision making based on their confidence. We propose four typical forms of
calibration attacks: underconfidence ,overconfidence ,maximum miscalibration , andrandom
confidence attacks , conducted in both black-box and white-box setups. We demonstrate that
the attacks are highly effective on both convolutional and attention-based models: with a
small number of queries, they seriously skew confidence without changing the predictive
performance. Given the potential danger, we further investigate the effectiveness of a wide
range of adversarial defence and recalibration methods, including our proposed defences
specifically designed for calibration attacks to mitigate the harm. From the ECE and KS
scores, we observe that there are still significant limitations in handling calibration attacks.
To the best of our knowledge, this is the first dedicated study that provides a comprehensive
investigationoncalibration-focusedattacks. Wehopethisstudyhelpsattractmoreattention
to these types of attacks and hence hamper their potential serious damages. To this end,
this work also provides detailed analyses to understand the characteristics of the attacks.1
1",TMLR
"Representative Selection (RS) is the problem of ï¬nding a small subset of exemplars from
a dataset that is representative of the dataset. In this paper, we study RS for attributed
graphs, and focus on ï¬nding representative nodes that optimize the accuracy of a model
trained on the selected representatives. Theoretically, we establish a new hardness result for
RS (in the absence of a graph structure) by proving that a particular, highly practical variant
of it (RS for Learning) is hard to approximate in polynomial time within any reasonable
factor, which implies a signiï¬cant potential gap between the optimum solution of widely-used
surrogate functions and the actual accuracy of the model. We then study the setting where
a (homophilous) graph structure is available, or can be constructed, between the data points.
We show that with an appropriate modeling approach, the presence of such a structure can
turn a hard RS (for learning) problem into one that can be eï¬€ectively solved. To this end,
we develop RS-GNN , a representation learning-based RSmodel based on GraphNeural
Networks. Empirically, we demonstrate the eï¬€ectiveness of RS-GNN on problems with
predeï¬ned graph structures as well as problems with graphs induced from node feature
similarities, by showing that RS-GNN achieves signiï¬cant improvements over established
baselines on a suite of eight benchmarks.
1",TMLR
"Learning the mapping between two function spaces has garnered considerable research
attention. However, learning the solution operator of partial differential equations (PDEs)
remains a challenge in scientific computing. Fourier neural operator (FNO) was recently
proposed to learn solution operators, and it achieved an excellent performance. In this study,
we propose a novel pseudo-differential integral operator (PDIO) to analyze and generalize the
Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which
is a generalized differential operator characterized by a certain symbol. We parameterize
this symbol using a neural network and demonstrate that the neural network-based symbol
is contained in a smooth symbol class. Subsequently, we verify that the PDIO is a bounded
linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with
the neural operator to develop a pseudo-differential neural operator (PDNO) and learn the
nonlinear solution operator of PDEs. We experimentally validate the effectiveness of the
proposed model by utilizing Darcy flow and the Navier-Stokes equation. The obtained results
indicate that the proposed PDNO outperforms the existing neural operator approaches in
most experiments.
1",TMLR
"Subject-driven image generation aims at generating images containing customized subjects,
which has recently drawn enormous attention from the research community. Nevertheless,
the previous works cannot precisely control the background and position of the target sub-
ject. In this work, we aspire to fill the void of the existing subject-driven generation tasks.
To this end, we propose two novel subject-driven editing sub-tasks, i.e., Subject Replace-
ment and Subject Addition. The new tasks are challenging in multiple aspects: replacing
a subject with a customized one can totally change its shape, texture, and color, while
adding a target subject to a designated position in a provided scene necessitates a rational
context-aware posture of the subject. To conquer these two novel tasks, we first manually
curate a new dataset called DreamEditBench containing 22 different types of subjects, and
440 source images, which cover diverse scenarios with different difficulty levels. We plan to
host DreamEditBench and hire trained evaluators for enable standardized human evaluation.
We also devise an innovative method DreamEditor to resolve these tasks by performing it-
erative generation, which enables a smooth adaptation to the customized subject. In this
project, we conduct automatic and human evaluations to understand the performance of our
DreamEditor and baselines on DreamEditBench . We found that the new tasks are challeng-
1 Published in Transactions on Machine Learning Research (11/2023)
ing for the existing models. For Subject Replacement, we found that the existing models are
particularly sensitive to the shape and color of the original subject. When the original sub-
ject and the customized subject are highly different, the model failure rate will dramatically
increase. For Subject Addition, we found that the existing models cannot easily blend the
customized subjects into the background smoothly, which causes noticeable artifacts in the
generated image. We believe DreamEditBench can become a standardized platform to en-
able future investigations towards building more controllable subject-driven image editing.
Our project and benchmark homepage is https://dreameditbenchteam.github.io/ .
1",TMLR
"Deep Operator Networks are an increasingly popular paradigm for solving regression in
infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to
establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required
for them to be able to reduce empirical error on noisy data. In particular, we show that for
low training errors to be obtained on ndata points it is necessary that the common output
dimension of the branch and the trunk net be scaling as â„¦(4âˆšn).
ThisinspiresourexperimentswithDeepONetssolvingtheadvection-diffusion-reactionPDE,
where we demonstrate the possibility that at a fixed model size, to leverage increase in this
common output dimension and get monotonic lowering of training error, the size of the
training data might necessarily need to scale at least quadratically with it.
1",TMLR
"Reinforcement learning is a powerful framework for training agents to navigate different
situations, but it is susceptible to changes in environmental dynamics. Generating an
algorithm that can find environmentally robust policies efficiently and handle different
model parameterizations without imposing stringent assumptions on the uncertainty set of
transitions is difficult due to the intricate interactions between policy and environment. In
this paper, we address both of these issues with a No-Regret Dynamics framework that utilizes
policy gradient methods and iteratively approximates the worst case environment during
training, avoiding assumptions on the uncertainty set. Alongside a toolbox of nonconvex
online learning algorithms, we demonstrate that our framework can achieve fast convergence
rates for many different problem settings and relax assumptions on the uncertainty set of
transitions.
1",TMLR
"Recent vision-language models have shown impressive multi-modal generation capabilities.
However, typically they require training huge models on massive datasets. As a more scalable
alternative, we introduce Prismer, a data- and parameter-efficient vision-language model
that leverages an ensemble of task-specific experts. Prismer only requires training of a
small number of components, with the majority of network weights inherited from multiple
readily-available, pre-trained experts, and kept frozen during training. By leveraging experts
from a wide range of domains, we show Prismer can efficiently pool this expert knowledge
and adapt it to various vision-language reasoning tasks. In our experiments, we show that
Prismer achieves fine-tuned and few-shot learning performance which is competitive with
current state-of-the-arts, whilst requiring up to two orders of magnitude less training data.
Code is available at https://github.com/NVlabs/prismer .
1",TMLR
"Likelihood-based deep generative models such as score-based diffusion models and variational
autoencoders are state-of-the-art machine learning models approximating high-dimensional
distributions of data such as images, text, or audio. One of many downstream tasks they
can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work
by Nalisnick et al. which we reproduce showed that deep generative models consistently
infer higher log-likelihoods for OOD data than data they were trained on, marking an open
problem. In this work, we analyse using the gradient of a data point with respect to the
parameters of the deep generative model for OOD detection, based on the simple intuition
that OOD data should have larger gradient norms than training data. We formalise measuring
the size of the gradient as approximating the Fisher information metric. We show that the
Fisher information matrix (FIM) has large absolute diagonal values, motivating the use of
chi-square distributed, layer-wise gradient norms as features. We combine these features to
make a simple, model-agnostic and hyperparameter-free method for OOD detection which
estimates the joint density of the layer-wise gradient norms for a given data point. We
find that these layer-wise gradient norms are weakly correlated, rendering their combined
usage informative, and prove that the layer-wise gradient norms satisfy the principle of (data
representation) invariance. Our empirical results indicate that this method outperforms the
Typicality test for most deep generative models and image dataset pairings.
1",TMLR
"Current machine learning methods struggle to solve Bongard problems, which are a type of
IQ test that requires deriving an abstract â€œconceptâ€ from a set of positive and negative â€œsup-
portâ€images, andthenclassifyingwhetherornotanewqueryimagedepictsthekeyconcept.
On Bongard-HOI, a benchmark for natural-image Bongard problems, most existing methods
have reached at best 69%accuracy (where chance is 50%). Low accuracy is often attributed
to neural netsâ€™ lack of ability to find human-like symbolic rules. In this work, we point out
that many existing methods are forfeiting accuracy due to a much simpler problem: they do
not adapt image features given information contained in the support set as a whole, and rely
instead on information extracted from individual supports. This is a critical issue, because
the â€œkey conceptâ€ in a typical Bongard problem can often only be distinguished using multi-
ple positives and multiple negatives. We explore simple methods to incorporate this context
and show substantial gains over prior works, leading to new state-of-the-art accuracy on
Bongard-LOGO ( 75.3%) and Bongard-HOI ( 76.4%) compared to methods with equivalent
vision backbone architectures and strong performance on the original Bongard problem set
(60.8%). Code is available at https://github.com/nraghuraman/bongard-context .
1",TMLR
"Recent works extend classification group fairness measures to sequential decision processes
such as reinforcement learning (RL) by measuring fairness as the difference in decision-
maker utility (e.g. accuracy) of each group. This approach suffers when decision-maker
utility is not perfectly aligned with group utility, such as in repeat loan applications where
a false positive (loan default) impacts the groups (applicants) and decision-maker (lender)
by different magnitudes. Some works remedy this by measuring fairness in terms of group
utility, typically referred to as their ""qualification"", but few works offer solutions that yield
group qualification equality. Those that do are prone to violating the ""no-harm"" principle
where one or more groupsâ€™ qualifications are lowered in order to achieve equality. In this
work, we characterize this problem space as having three implicit objectives: maximizing
decision-maker utility, maximizing group qualification, and minimizing the difference in
qualification between groups. We provide a RL policy learning technique that optimizes
for these objectives directly by constructing a multi-objective reward function that encodes
these objectives as distinct reward signals. Under suitable parameterizations our approach
is guaranteed to respect the ""no-harm"" principle.
1",TMLR
"In supervised learning â€” for instance in image classification â€” modern massive datasets
are commonly labeled by a crowd of workers. The obtained labels in this crowdsourcing
setting are then aggregated for training, generally leveraging a per-worker trust score. Yet,
such workers oriented approaches discard the tasksâ€™ ambiguity. Ambiguous tasks might fool
expert workers, which often impacts the learning step. In standard supervised learning set-
tings â€“ with one label per task â€“ the Area Under the Margin (AUM) was tailored to identify
mislabeled data. We adapt the AUM to identify ambiguous tasks in crowdsourced learning
scenarios, introducing the Weighted Areas Under the Margin (WAUM). The WAUM is an
average of AUMs weighted according to task-dependent scores. We show that the WAUM
can help discarding ambiguous tasks from the training set, leading to better generalization
performance. We report improvements over existing strategies for learning with a crowd,
both on simulated settings, and on real datasets such as CIFAR-10H (a crowdsourced dataset
withahighnumberofansweredlabels), LabelMe andMusic(twodatasetswithfewanswered
votes).
1",TMLR
"Understanding the high-density crowd dynamics of urbanization plays an important role in
architectural design and urban planning, preventing the occurrence of crowd crush. Most
traditional methods rely on formulas designed based on expert knowledge, which are in-
flexible and incomplete to model complex real-world crowd trajectories. To address the
issue, recent studies propose to simulate crowds via data-driven models. However, these
models fail to learn the inherent symmetry of high-density crowd trajectories, leading to
insufficient generalization ability. For example, existing models can not predict left-to-right
trajectories by learning right-to-left trajectories, even though they share similar patterns.
In this work, we propose a novel Equivariant Graph Learning framework for high-density
crowd dynamic modeling, called CrowdEGL. It utilizes an additional objective to encourage
models to predict the transformed output given the input under the same transformation.
We summarize three types of transformation groups, which are determined by the symme-
try of environments. To explicitly incorporate these augmented data, a multi-channel GNN
is employed to learn the latent graph embedding of pedestrian patterns. Finally, to model
dense crowd interactions, future positions of original and transformed inputs are obtained by
multiple independent graph decoders. Extensive experiments on 8 datasets from 5 different
environments show that CrowdEGL outperforms existing models by a large margin.
1",TMLR
"Camera images are ubiquitous in machine learning research. They also play a central role
in the delivery of important public services spanning medicine or environmental surveying.
However, the application of machine learning models in these domains has been limited
because of robustness concerns. A primary failure mode are performance drops due to
differences between the training and deployment data. While there are methods to prospec-
tively validate the robustness of machine learning models to such dataset drifts, existing
approaches do not account for explicit models of machine learningâ€™s primary object of interest:
the data. This limits our ability to study and understand the relationship between data
generation and downstream machine learning model performance in a physically accurate
manner. In this study, we demonstrate how to overcome this limitation by pairing traditional
machine learning with physical optics to obtain explicit and differentiable data models. We
demonstrate how such data models can be constructed for image data and used to control
downstream machine learning model performance related to dataset drift. The findings
are distilled into three applications. First, drift synthesis enables the controlled generation
of physically faithful drift test cases to power model selection and targeted generalization.
Second, the gradient connection between machine learning task model and data model allows
advanced, precise tolerancing of task model sensitivity to changes in the data generation.
These drift forensics can be used to precisely specify the acceptable data environments
in which a task model may be run. Third, drift optimization opens up the possibility to
create drifts that can help the task model learn better faster, effectively optimizing the
data generating process itself to support the downstream machine vision task. This is an
interesting upgrade to existing imaging pipelines which traditionally have been optimized to
be consumed by human users but not machine learning models. The data models require
access to raw sensor images as commonly processed at scale in industry domains such as
microscopy, biomedicine, autonomous vehicles or remote sensing. Alongside the data model
code we release two datasets to the public that we collected as part of this work. In total,
the two datasets, Raw-Microscopy and Raw-Drone, comprise 1,488 scientifically calibrated
reference raw sensor measurements, 8,928 raw intensity variations as well as 17,856 images
processed through twelve data models with different configurations. A guide to access the
open code and datasets is available at https://github.com/aiaudit-org/raw2logit .
1",TMLR
"In the realm of real-world devices, centralized servers in Federated Learning (FL) present
challenges including communication bottlenecks and susceptibility to a single point of fail-
ure. Additionally, contemporary devices inherently exhibit model and data heterogeneity.
Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such
heterogeneity without imposing architectural restrictions or assuming the availability of
additional data. To address these issues, we propose a Decentralized FederatedMutual
Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous mod-
els, and avoids reliance on additional data. DFML effectively handles model and data
heterogeneity through mutual learning, which distills knowledge between clients, and cycli-
cally varying the amount of supervision and distillation signals. Extensive experimental
results demonstrate consistent effectiveness of DFML in both convergence speed and global
accuracy, outperforming prevalent baselines under various conditions. For example, with
the CIFAR-100 dataset and 50 clients, DFML achieves a substantial increase of +17.20%
and+19.95% in global accuracy under Independent and Identically Distributed (IID) and
non-IID data shifts, respectively.
1",TMLR
"The cold posterior effect (CPE) (Wenzel et al., 2020) in Bayesian deep learning shows
that, for posteriors with a temperature T <1, the resulting posterior predictive could have
better performance than the Bayesian posterior ( T= 1). As the Bayesian posterior is
known to be optimal under perfect model specification, many recent works have studied the
presence of CPE as a model misspecification problem, arising from the prior and/or from
the likelihood. In this work, we provide a more nuanced understanding of CPE as we show
thatmisspecification leads to CPE only when the resulting Bayesian posterior underfits . In
fact, we theoretically show that if there is no underfitting, there is no CPE. Furthermore,
we show that these tempered posteriors withT <1are indeed proper Bayesian posteriors
with a different combination of likelihoods and priors parameterized by T. This observation
validates the adjustment of the temperature hyperparameter Tas a straightforward approach
to mitigate underfitting in the Bayesian posterior. In essence, we show that by fine-tuning
the temperature Twe implicitly utilize alternative Bayesian posteriors, albeit with less
misspecified likelihood and prior distributions. The code for replicating the experiments can
be found at https://github.com/pyijiezhang/cpe-underfit .
1",TMLR
"Network embedding, which maps graphs to distributed representations, is a unified frame-
work for various graph inference tasks. According to the topology properties (e.g., structural
roles and community memberships of nodes) to be preserved, it can be categorized into the
identity and position embedding. Most existing methods can only capture one type of prop-
erty. Some approaches can support the inductive inference that generalizes the embedding
model to new nodes or graphs but relies on the availability of attributes. Due to the compli-
cated correlations between topology and attributes, it is unclear for some inductive methods
which type of property they can capture. In this study, we explore a unified framework for
the joint inductive inference of identity and position embeddings without attributes. An
inductive random walk embedding (IRWE) method is proposed, which combines multiple
attention units to handle the random walk (RW) on graph topology and simultaneously
derives identity and position embeddings that are jointly optimized. We demonstrate that
some RW statistics can characterize node identities and positions while supporting the in-
ductive inference. Experiments validate the superior performance of IRWE over various
baselines for the transductive and inductive inference of identity and position embeddings.
1",TMLR
"Explainable Artificial Intelligence (XAI) aims to make learning machines less opaque, and
offers researchers and practitioners various tools to reveal the decision-making strategies of
neural networks. In this work, we investigate how XAI methods can be used for exploring
and visualizing the diversity of feature representations learned by Bayesian Neural Networks
(BNNs). Our goal is to provide a global understanding of BNNs by making their decision-
making strategies a) visible and tangible through feature visualizations and b) quantitatively
measurable with a distance measure learned by contrastive learning. Our work provides new
insights into the posterior distribution in terms of human-understandable feature information
with regard to the underlying decision-making strategies. The main findings of our work
are the following: 1) global XAI methods can be applied to explain the diversity of decision-
making strategies of BNN instances, 2) Monte Carlo dropout with commonly used Dropout
rates exhibit increased diversity in feature representations compared to the multimodal
posterior approximation of MultiSWAG, 3) the diversity of learned feature representations
highly correlates with the uncertainty estimate for the output and 4) the inter-mode diversity
of the multimodal posterior decreases as the network width increases, while the intra-mode
diversity increases. These findings are consistent with the recent Deep Neural Networks
theory, providing additional intuitions about what the theory implies in terms of humanly
understandable concepts.
1 Published in Transactions on Machine Learning Research (11/2023)
1",TMLR
"In this paper, we address the anomaly detection problem in the context of heterogeneous
normalobservationsandproposeanapproachthataccountsforthisheterogeneity. Although
prediction-basedmethodsarecommontolearnnormality, thevastmajorityofpreviouswork
predicts a single outcome, which is generally not sufficient to account for the multiplicity
of possible normal observations. To address this issue, we introduce a new masked multi-
prediction (MMP) approach that produces multiple likely normal outcomes, and show both
theoretically and experimentally that it improves normality learning and leads to a better
anomaly detection performance. In addition, we observed that normality can be character-
ized from multiple aspects, depending on the types of anomalies to be detected. Therefore,
we propose an adaptation (MMP-AMS) of our approach to cover multiple aspects of nor-
mality such as appearance, motion, semantics and location. Since we model each aspect
separately, our approach has the advantage of being interpretable and modular, as we can
select only a subset of normality aspects. The experiments conducted on several benchmarks
show the effectiveness of the proposed approach.
1",TMLR
"Machine learning models often encounter samples that are diverged from the training
distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently
assign that sample to an in-class label, significantly compromises the reliability of a model.
The problem has gained significant attention due to its importance for safety deploying
models in open-world settings. Detecting OOD samples is challenging due to the intractability
of modeling all possible unknown distributions. To date, several research domains tackle
the problem of detecting unfamiliar samples, including anomaly detection, novelty detection,
one-class learning, open set recognition, and out-of-distribution detection. Despite having
similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been
investigated independently. Accordingly, these research avenues have not cross-pollinated,
creating research barriers. While some surveys intend to provide an overview of these
approaches, they seem to only focus on a specific domain without examining the relationship
between different domains. This survey aims to provide a cross-domain and comprehensive
review of numerous eminent works in respective areas while identifying their commonalities.
Researchers can benefit from the overview of research advances in different fields and develop
future methodology synergistically. Furthermore, to the best of our knowledge, while there
are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-
date survey on out-of-distribution detection, which this survey covers extensively. Finally,
having a unified cross-domain perspective, this study discusses and sheds light on future
lines of research, intending to bring these fields closer together. All the implementations and
benchmarks reported in the paper can be found at : https://github.com/taslimisina/
osr-ood-ad-methods
1 Published in Transactions on Machine Learning Research (11/2022)
1",TMLR
"The phenomenon of adversarial examples illustrates one of the most basic vulnerabilities
of deep neural networks. Among the variety of techniques introduced to surmount this
inherent weakness, adversarial training has emerged as the most effective strategy for learning
robust models. Typically, this is achieved by balancing robust and natural objectives. In
this work, we aim to further optimize the trade-off between robust and standard accuracy
by enforcing a domain-invariant feature representation. We present a new adversarial
training method, Domain Invariant Adversarial Learning (DIAL), which learns a feature
representation that is both robust and domain invariant. DIAL uses a variant of Domain
Adversarial Neural Network (DANN) on the natural domain and its corresponding adversarial
domain. In the case where the source domain consists of natural examples and the target
domain is the adversarially perturbed examples, our method learns a feature representation
constrained not to discriminate between the natural and adversarial examples, and can
therefore achieve a more robust representation. DIAL is a generic and modular technique that
can be easily incorporated into any adversarial training method. Our experiments indicate
that incorporating DIAL in the adversarial training process improves both robustness and
standard accuracy.
1",TMLR
"A large variety of real-world Reinforcement Learning (RL) tasks are characterized by a
complex and heterogeneous structure that makes end-to-end (or flat) approaches hardly
applicable or even infeasible. Hierarchical Reinforcement Learning (HRL) provides general
solutions to address these problems thanks to a convenient multi-level decomposition of the
tasks, making their solution accessible. Although often used in practice, few works provide
theoretical guarantees to justify this outcome effectively. Thus, it is not yet clear when
to prefer such approaches compared to standard flat ones. In this work, we provide an
option-dependent upper bound to the regret suffered by regret minimization algorithms in
finite-horizon problems. We illustrate that the performance improvement derives from the
planning horizon reduction induced by the temporal abstraction enforced by the hierarchical
structure. Then, focusing on a sub-setting of HRL approaches, the options framework, we
highlight how the average duration of the available options affects the planning horizon
and, consequently, the regret itself. Finally, we relax the assumption of having pre-trained
options to show how, in particular situations, is still preferable a hierarchical approach over
a standard one.
1",TMLR
"Binary Neural Networks (BNNs), operating with ultra-low precision weights, incur a sig-
nificant reduction in storage and compute cost compared to the traditional Deep Neural
Networks (DNNs). However, vulnerability of such models against various hardware attacks
are yet to be fully unveiled. Towards understanding the potential threat imposed on such
highly efficient models, in this paper, we explore a novel adversarial attack paradigm per-
taining to BNNs. In specific, we assume the attack to be executed during deployment phase,
prior to inference, to achieve malicious intentions, via manipulation of accessible network
parameters. We aim to accomplish a graceless degradation in BNN accuracy to a point,
where the fully functional network can behave as a random output generator at best, thus
subverting the confidence in the system. To this end, we propose an Outlier Gradient-based
Evolutionary (OGE) attack, that learns injection of minimal amount of critical bit flips in
the pre-trained binary network weights, to introduce classification errors in the inference
execution. To the best of our knowledge, this is the first work that leverages the outlier gra-
dient weights to orchestrate a hardware-based bit-flip attack, that is highly effective against
the typically resilient low-quantization BNNs. Exhaustive evaluations on popular image
recognition datasets including Fashion-MNIST, CIFAR10, GTSRB, and ImageNet demon-
strate that, OGE can drop up to 68.1% of the test images mis-classification, by flipping
as little as 150 binary weights, out of 10.3 millions in a BNN architecture. Code is open
sourced at: https://github.com/isnadnr/OGE .
1",TMLR
"A two-sample hypothesis test is a statistical procedure used to determine whether the distri-
butions generating two samples are identical. We consider the two-sample testing problem
in a new scenario where the sample measurements (or sample features) are inexpensive to
access, but their group memberships (or labels) are costly. To address the problem, we de-
vise the first active sequential two-sample testing framework that not only sequentially but
alsoactively queries . Our test statistic is a likelihood ratio where one likelihood is found by
maximization over all class priors, and the other is provided by a probabilistic classification
model. The classification model is adaptively updated and used to predict where the (unla-
belled) features have a high dependency on labels; labeling the â€œhigh-dependencyâ€ features
leads to the increased power of the proposed testing framework. In theory, we provide the
proof that our framework produces an anytime-valid p-value. In addition, we characterize
the proposed frameworkâ€™s gain in testing power by analyzing the mutual information be-
tween the feature and label variables in asymptotic and finite-sample scenarios. In practice,
we introduce an instantiation of our framework and evaluate it using several experiments;
the experiments on the synthetic, MNIST, and application-specific datasets demonstrate
that the testing power of the instantiated active sequential test significantly increases while
the Type I error is under control.
1",TMLR
"Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA)
rely heavily on well-annotated groups in the training data. We show, both in theory and
practice, thatannotation-baseddataaugmentationsusingeitherdownsamplingorupweighting
for WGA are susceptible to domain annotation noise. The WGA gap is exacerbated in high-
noise regimes for models trained with vanilla empirical risk minimization (ERM). To this end,
we introduce Regularized Annotation of Domains (RAD) to train robust last layer classifiers
without needing explicit domain annotations. Our results show that RAD is competitive
with other recently proposed domain annotation-free techniques. Most importantly, RAD
outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the
training data for several publicly available datasets.
1",TMLR
"We demonstrate that neural networks can be FLOP-efficient integrators of one-dimensional
oscillatory integrands. We train a feed-forward neural network to compute integrals of highly
oscillatory 1D functions. The training set is a parametric combination of functions with
varying characters and oscillatory behavior degrees. Numerical examples show that these
networks are FLOP-efficient for sufficiently oscillatory integrands with an average FLOP gain
of103FLOPs. The network calculates oscillatory integrals better than traditional quadrature
methods under the same computational budget or number of floating point operations. We
find that feed-forward networks of 5 hidden layers are satisfactory for a relative accuracy
of10âˆ’3. The computational burden of inference of the neural network is relatively small,
even compared to inner-product pattern quadrature rules. We postulate that our result
follows from learning latent patterns in the oscillatory integrands that are otherwise opaque
to traditional numerical integrators.
1",TMLR
"Anomaly detection in continuous-time dynamic graphs is an emerging field yet under-
explored in the context of learning algorithms. In this paper, we pioneer structured anal-
yses of link-level anomalies and graph representation learning for identifying categorically
anomalous graph links. First, we introduce a fine-grained taxonomy for edge-level anomalies
leveraging structural, temporal, and contextual graph properties. Based on these properties,
we introduce a method for generating andinjecting typed anomalies into graphs. Next, we
introduce a novel method to generate continuous-time dynamic graphs featuring consisten-
cies across either or combinations of time, structure, and context. To enable temporal graph
learning methods to detect specific types of anomalous links rather than the bare existence
of a link, we extend the generic link prediction setting by: (1) conditioning link existence
on contextual edge attributes; and (2) refining the training regime to accommodate diverse
perturbations in the negative edge sampler. Comprehensive benchmarks on synthetic and
real-world datasets â€“ featuring synthetic and labeled organic anomalies and employing six
state-of-the-art link prediction methods â€“ validate our taxonomy and generation processes
for anomalies and benign graphs, as well as our approach to adapting methods for anomaly
detection. Our results reveal that different learning methods excel in capturing different
aspects of graph normality and detecting different types of anomalies. We conclude with
a comprehensive list of findings highlighting opportunities for future research. The code is
available at https://github.com/timpostuvan/CTDG-link-anomaly-detection .
1",TMLR
"In this paper we investigate transformer architectures designed for partially observable on-
line reinforcement learning. The self-attention mechanism in the transformer architecture is
capable of capturing long-range dependencies and it is the main reason behind its effective-
ness in processing sequential data. Nevertheless, despite their success, transformers have
two significant drawbacks that still limit their applicability in online reinforcement learning:
(1) in order to remember all past information, the self-attention mechanism requires access
to the whole history to be provided as context. (2) The inference cost in transformers is ex-
pensive. In this paper, we introduce recurrent alternatives to the transformer self-attention
mechanism that offer context-independent inference cost, leverage long-range dependencies
effectively, and performs well in online reinforcement learning task. We quantify the impact
of the different components of our architecture in a diagnostic environment and assess per-
formance gains in 2D and 3D pixel-based partially-observable environments (e.g. T-Maze,
Mystery Path, Craftax, and Memory Maze). Compared with a state-of-the-art architecture,
GTrXL, inference in our approach is at least 40% cheaper while reducing memory use more
than 50%. Our approach either performs similarly or better than GTrXL, improving more
than 37% upon GTrXL performance in harder tasks.
1",TMLR
"Causal discovery, the learning of causality in a data mining scenario, has been of strong scien-
tific and theoretical interest as a starting point to identify â€œwhat causes what?â€ Contingent
on assumptions and a proper learning algorithm, it is sometimes possible to identify and
accurately estimate an underlying directed acyclic graph (DAG), as opposed to a Markov
equivalence class of graphs that gives ambiguity of causal directions. The focus of this paper
is in highlighting the identifiability and estimation of DAGs through a sequential sorting
procedure that orders variables one at a time, starting at root nodes, followed by children of
the root nodes, and so on until completion. We demonstrate a novel application of this general
sequential approach to estimate the topological ordering of the DAG corresponding to a linear
structural equation model with a non-Gaussian error distribution family. At each step of the
procedure, only simple likelihood ratio scores are calculated on regression residuals to decide
the next node to append to the current partial ordering. The computational complexity of
our algorithm on a p-node problem is O(pd), wheredis the maximum neighborhood size.
Under mild assumptions, the population version of our procedure provably identifies a true
ordering of the underlying DAG. We provide extensive numerical evidence to demonstrate
that this sequential procedure scales to possibly thousands of nodes and works well for
high-dimensional data. We accompany these numerical experiments with an application to a
single-cell gene expression dataset. Our Rpackage with examples and installation instructions
can be found at https://gabriel-ruiz.github.io/scorelingam/ .
1",TMLR
"Backdoor attacks are dangerous and difficult to prevent in federated learning (FL), where
training data is sourced from untrusted clients over long periods of time. These difficulties
arise because: (a) defenders in FL do not have access to raw training data, and (b) a
phenomenon we identify called backdoor leakage causes models trained continuously to
eventually suffer from backdoors due to cumulative errors in defense mechanisms. We
propose a framework called shadow learning for defending against backdoor attacks in the FL
setting under long-range training. Shadow learning trains two models in parallel: a backbone
model and a shadow model. The backbone is trained without any defense mechanism to
obtain good performance on the main task. The shadow model combines filtering of malicious
clients with early-stopping to control the attack success rate even as the data distribution
changes. We theoretically motivate our design and show experimentally that our framework
significantly improves upon existing defenses against backdoor attacks.
1",TMLR
"Generative models have emerged as an essential building block for many image synthesis
and editing tasks. Recent advances in this field have also enabled high-quality 3D or video
content to be generated that exhibits either multi-view or temporal consistency. With
our work, we explore 4D generative adversarial networks (GANs) that learn unconditional
generation of 3D-aware videos. By combining neural implicit representations with time-aware
discriminator, we develop a GAN framework that synthesizes 3D video supervised only with
monocular videos. We show that our method learns a rich embedding of decomposable 3D
structures and motions that enables new visual effects of spatio-temporal renderings while
producing imagery with quality comparable to that of existing 3D or video GANs.
Figure 1: 3D-Aware video generation. We show multiple frames and viewpoints of two 3D videos,
generated using our model trained on the FaceForensics dataset (RÃ¶ssler et al., 2019). Our 4D GAN generates
3D content of high quality while permitting control of time and camera extrinsics. Video results can be
viewed on our website: https://sherwinbahmani.github.io/3dvidgen
1",TMLR
"Deep latent variable generative models excel at generating complex, high-dimensional data,
often exhibiting impressive generalization beyond the training distribution. However, many
such models in use today are black-boxestrained on large unlabelled datasets with statistical
objectivesandlackaninterpretableunderstandingofthelatentspacerequiredforcontrolling
the generative process. We propose CAGE, a framework for controllable generation in latent
variable models based on causal reasoning. Given a pair of attributes, CAGE infers the
implicit cause-effect relationships between these attributes as induced by a deep generative
model. This is achieved by defining and estimating a novel notion of unit-level causal effects
in the latent space of the generative model. Thereafter, we use the inferred cause-effect
relationships to design a novel strategy for controllable generation based on counterfactual
sampling. Through a series of large-scale synthetic and human evaluations, we demonstrate
that generating counterfactual samples which respect the underlying causal relationships
inferred via CAGE leads to subjectively more realistic images.
1",TMLR
"In variable selection, a selection rule that prescribes the permissible sets of selected variables
(called a â€œselection dictionaryâ€) is desirable due to the inherent structural constraints among
the candidate variables. Such selection rules can be complex in real-world data analyses,
and failing to incorporate such restrictions could not only compromise the interpretability
of the model but also lead to decreased prediction accuracy. However, no general frame-
work has been proposed to formalize selection rules and their applications, which poses a
significant challenge for practitioners seeking to integrate these rules into their analyses. In
this work, we establish a framework for structured variable selection that can incorporate
universal structural constraints. We develop a mathematical language for constructing ar-
bitrary selection rules, where the selection dictionary is formally defined. We demonstrate
that all selection rules can be expressed as combinations of operations on constructs, facil-
itating the identification of the corresponding selection dictionary. We use a detailed and
complex example to illustrate the developed framework. Once this selection dictionary is
derived, practitioners can apply their own user-defined criteria to select the optimal model.
Additionally, our framework enhances existing penalized regression methods for variable se-
lection by providing guidance on how to appropriately group variables to achieve the desired
selection rule. Furthermore, our innovative framework opens the door to establishing new
â„“0-based penalized regression techniques that can be tailored to respect arbitrary selection
rules, thereby expanding the possibilities for more robust and tailored model development.
1",TMLR
"Self-Supervised Learning (SSL) is an important paradigm for learning representations from
unlabelled data, and SSL with neural networks has been highly successful in practice. How-
ever current theoretical analysis of SSL is mostly restricted to generalisation error bounds.
In contrast, learning dynamics often provide a precise characterisation of the behaviour
of neural networks based models but, so far, are mainly known in supervised settings.
In this paper, we study the learning dynamics of SSL models, specifically representations
obtained by minimising contrastive and non-contrastive losses. We show that a nÃ¤ive ex-
tension of the dymanics of multivariate regression to SSL leads to learning trivial scalar
representations that demonstrates dimension collapse in SSL. Consequently, we formulate
SSL objectives with orthogonality constraints on the weights, and derive the exact (network
width independent) learning dynamics of the SSL models trained using gradient descent on
the Grassmannian manifold. We also argue that the infinite width approximation of SSL
models significantly deviate from the neural tangent kernel approximations of supervised
models. We numerically illustrate the validity of our theoretical findings, and discuss how
the presented results provide a framework for further theoretical analysis of contrastive and
non-contrastive SSL.
1",TMLR
"PAC-Bayes is a well-established framework for analyzing generalization performance in ma-
chine learning models. This framework provides a bound on the expected population error
by considering the sum of training error and the divergence between posterior and prior
distributions. In addition to being a successful generalization bound analysis tool, the
PAC-Bayesian bound can also be incorporated into an objective function for training prob-
abilistic neural networks, which we refer to simply as Deep PAC-Bayesian Learning . Deep
PAC-Bayesian learning has been shown to achieve competitive expected test set error and
provide a tight generalization bound in practice at the same time through gradient descent
training. Despite its empirical success, theoretical analysis of deep PAC-Bayesian learning
for neural networks is rarely explored. To this end, this paper proposes a theoretical con-
vergence and generalization analysis for Deep PAC-Bayesian learning. For a deep and wide
probabilistic neural network, our analysis shows that PAC-Bayesian learning corresponds
to solving a kernel ridge regression when the probabilistic neural tangent kernel (PNTK) is
used as the kernel. We utilize this outcome in conjunction with the PAC-Bayes C-bound,
enabling us to derive an analytical and guaranteed PAC-Bayesian generalization bound for
the first time. Finally, drawing insight from our theoretical results, we propose a proxy
measure for efficient hyperparameter selection, which is proven to be time-saving on various
benchmarks. Our work not only provides a better understanding of the theoretical under-
pinnings of Deep PAC-Bayesian learning, but also offers practical tools for improving the
training and generalization performance of these models.
âˆ—Equal Contribution.
1 Published in Transactions on Machine Learning Research (05/2023)
1",TMLR
"Identifying dynamical systems from experimental data is a notably difficult task. Prior
knowledge generally helps, but the extent of this knowledge varies with the application, and
customized models are often needed. Neural ordinary differential equations can be written
as a flexible framework for system identification and can incorporate a broad spectrum
of physical insight, giving physical interpretability to the resulting latent space. In the
case of partial observations, however, the data points cannot directly be mapped to the
latent state of the ODE. Hence, we propose to design recognition models, in particular
inspired by nonlinear observer theory, to link the partial observations to the latent state.
We demonstrate the performance of the proposed approach on numerical simulations and
on an experimental dataset from a robotic exoskeleton.
1",TMLR
"Ensuring the safety of reinforcement learning (RL) algorithms is crucial to unlock their
potential for many real-world tasks. However, vanilla RL and most safe RL approaches
do not guarantee safety. In recent years, several methods have been proposed to provide
hard safety guarantees for RL, which is essential for applications where unsafe actions could
have disastrous consequences. Nevertheless, there is no comprehensive comparison of these
provably safe RL methods. Therefore, we introduce a categorization of existing provably
safeRLmethods, presenttheconceptualfoundationsforbothcontinuousanddiscreteaction
spaces, and empirically benchmark existing methods. We categorize the methods based on
how they adapt the action: action replacement, action projection, and action masking. Our
experimentson an inverted pendulum and a quadrotorstabilization task indicatethat action
replacement is the best-performing approach for these applications despite its comparatively
simple realization. Furthermore, adding a reward penalty, every time the safety verification
is engaged, improved training performance in our experiments. Finally, we provide practical
guidance on selecting provably safe RL approaches depending on the safety specification,
RL algorithm, and type of action space.
1",TMLR
"What distinguishes robust models from non-robust ones? While for ImageNet distribution
shifts it has been shown that such differences in robustness can be traced back predomi-
nantly to differences in training data, so far it is not known what that translates to in terms
of what the model has learned. In this work, we bridge this gap by probing the represen-
tation spaces of 16 robust zero-shot CLIP vision encoders with various backbones (ResNets
and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M
and DataComp), and comparing them to the representation spaces of less robust models
with identical backbones, but different (pre)training sets or objectives (CLIP pretraining
on ImageNet-Captions, and supervised training or finetuning on ImageNet). Through this
analysis, we generate three novel insights. Firstly, we detect the presence of outlier features
in robust zero-shot CLIP vision encoders, which to the best of our knowledge is the first
time these are observed in non-language and non-transformer models. Secondly, we find the
existence of outlier features to be an indication of ImageNet shift robustness in models, since
we only find them in robust models in our analysis. Lastly, we also investigate the number
of unique encoded concepts in the representation space and find zero-shot CLIP models to
encode a higher number of unique concepts in their representation space. However, we do
not find this to be an indicator of ImageNet shift robustness and hypothesize that it is rather
related to the language supervision.
1 Published in Transactions on Machine Learning Research (10/2024)
1",TMLR
"Large language models (LLMs) specializing in natural language generation (NLG) have
recently started exhibiting promising capabilities across a variety of domains. However,
gauging the trustworthiness of responses generated by LLMs remains an open challenge,
with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing
literature typically assumes white-box access to language models, which is becoming unreal-
istic either due to the closed-source nature of the latest LLMs or computational constraints.
In this work, we investigate UQ in NLG for black-box LLMs. We first differentiate uncer-
taintyvsconfidence : the former refers to the â€œdispersionâ€ of the potential predictions for a
fixed input, and the latter refers to the confidence on a particular prediction/generation. We
then propose and compare several confidence/uncertainty measures, applying them to selec-
tive NLG where unreliable results could either be ignored or yielded for further assessment.
Experiments were carried out with several popular LLMs on question-answering datasets
(for evaluation purposes). Results reveal that a simple measure for the semantic dispersion
can be a reliable predictor of the quality of LLM responses, providing valuable insights for
practitioners on uncertainty management when adopting LLMs. The code to replicateour
experiments is available at https://github.com/zlin7/UQ-NLG .
1",TMLR
"Dataset distillation or condensation aims to generate a smaller but representative subset
from a large dataset, which allows a model to be trained more efficiently, meanwhile eval-
uating on the original testing data distribution to achieve decent performance. Previous
decoupled methods like SRe2L simply use a unified gradient update scheme for synthesizing
data from Gaussian noise, while, we notice that the initial several update iterations will
determine the final outline of synthesis, thus an improper gradient update strategy may
dramatically affect the final generation quality. To address this, we introduce a simple yet
effective global-to-local gradient refinement approach enabled by curriculum data augmen-
tation ( CDA) during data synthesis. The proposed framework achieves the current published
highest accuracy on both large-scale ImageNet-1K and 21K with 63.2% under IPC (Images
Per Class) 50 and 36.1% under IPC 20, using a regular input resolution of 224 Ã—224 with
faster convergence speed and less synthetic time. The proposed model outperforms the
current state-of-the-art methods like SRe2L, TESLA, and MTT by more than 4% Top-1
accuracy on ImageNet-1K/21K and for the first time, reduces the gap to its full-data train-
ing counterparts to less than absolute 15%. Moreover, this work represents the inaugural
success in dataset distillation on the larger-scale ImageNet-21K dataset under the standard
224Ã—224 resolution. Our code and distilled ImageNet-21K dataset of 20 IPC, 2K recovery
budget are available at https://github.com/VILA-Lab/SRe2L/tree/main/CDA .
1",TMLR
"This work aims to reproduce the findings of the paper ""Fair Attribute Completion on Graph
with Missing Attributes"" written by Guo, Chu, and Li [1] by investigating the claims made
in the paper. This paper suggests that the results of the original paper are reproducible
and thus, the claims hold. However, the claim that FairAC is a generic framework for many
downstream tasks is very broad and could therefore only be partially tested. Moreover,
we show that FairAC is generalizable to various datasets and sensitive attributes and show
evidence that the improvement in group fairness of the FairAC framework does not come at
the expense of individual fairness. Lastly, the codebase of FairAC has been refactored and
is now easily applicable for various datasets and models.
1",TMLR
"Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm
for recent advances in private deep learning. It provides a single privacy guarantee to
all datapoints in the dataset. We propose output-specific (Îµ,Î´)-DP to characterize privacy
guarantees for individual examples when releasing models trained by DP-SGD. We also design
an efficient algorithm to investigate individual privacy across a number of datasets. We find
that most examples enjoy stronger privacy guarantees than the worst-case bound. We further
discover that the training loss and the privacy parameter of an example are well-correlated.
This implies groups that are underserved in terms of model utility simultaneously experience
weaker privacy guarantees. For example, on CIFAR-10, the average Îµof the class with the
lowest test accuracy is 44.2% higher than that of the class with the highest accuracy. Our
code is available at https://github.com/dayu11/individual_privacy_of_DPSGD .
1",TMLR
"Video Diffusion Models have been developed for video generation, usually integrating text
and image conditioning to enhance control over the generated content. Despite the progress,
ensuringconsistencyacrossframesremainsachallenge,particularlywhenusingtextprompts
as control conditions. To address this problem, we introduce UniCtrl , a novel, plug-
and-play method that is universally applicable to improve the spatiotemporal consistency
and motion diversity of videos generated by text-to-video models without additional train-
ing. UniCtrl ensures semantic consistency across different frames through cross-frame self-
attention control , and meanwhile, enhances the motion quality and spatiotemporal consis-
tency through motion injection andspatiotemporal synchronization . Our experimental re-
sults demonstrate UniCtrlâ€™s efficacy in enhancing various text-to-video models, confirming
its effectiveness and universality.
1",TMLR
"Estimating the quantiles of a large dataset is a fundamental problem in both the streaming
algorithms literature and the diï¬€erential privacy literature. However, all existing private
mechanisms for distribution-independent quantile computation require space at least lin-
ear in the input size n. In this work, we devise a diï¬€erentially private algorithm for the
quantile estimation problem, with strongly sublinear space complexity, in the one-shot and
continual observation settings. Our basic mechanism estimates any Î±-approximate quantile
of a length- nstream over a data universe Xwith probability 1âˆ’Î²usingO/parenleftBig
log(|X|/Î²) logn
Î±/epsilon1/parenrightBig
space while satisfying /epsilon1-diï¬€erential privacy at a single time point. Our approach builds
upon deterministic streaming algorithms for non-private quantile estimation instantiating
the exponential mechanism using a utility function deï¬ned on sketch items, while (privately)
sampling from intervals deï¬ned by the sketch. We also present another algorithm based on
histograms that is especially well-suited to the multiple quantiles case. We implement our
algorithms and experimentally evaluate them on synthetic and real-world datasets.
1",TMLR
"Robust generalization (RG), concerning how deep neural networks could perform over ad-
versarial examples generated from unseen dataset , has emerged as an active research topic.
Albeit its crucial importance, most previous studies lack a well-founded theoretical analysis
and certified error bounds. In this paper, we make a novel attempt to theoretically and em-
pirically study how we could attain a better RG by learning discriminative representation,
where the inconsistency of the inter-sample similarity matrix between clean and adversarial
examples should be reduced. Our theoretical investigation discloses that introducing this
inconsistency as a regularization term, named Gram matrix difference (GMD), will lead to
tighteruppererrorboundandcertifyabetterRG.Meanwhile, wedemonstratethatprevious
efforts to reduce inter-class similarity and increase intra-class similarity among adversarial
examples for enhanced adversarial robustness are approximate optimizations of our GMD
approach. Furthermore, to avoid the vast optimization complexity introduced by the sim-
ilarity matrix, we propose to optimize GMD by building a diverging spanned latent space
for adversarial examples. On the algorithmic side, this regularization term is implemented
as a novel adversarial training (AT) method â€” Subspace Diverging (SD) â€” to expand the
volume difference between the whole latent spaceâ€™s linear span and subspacesâ€™ linear spans.
Extensive experiments show that the proposed method can improve advanced AT methods
and work remarkably well in various datasets, including CIFAR-10, CIFAR-100, SVHN, and
Tiny-ImageNet.
âˆ—Equal contribution.
â€ Corresponding authors.
1 Published in Transactions on Machine Learning Research (12/2024)
1",TMLR
"Deep learning has achieved great success in the past few years. However, the performance
of deep learning is likely to impede in face of non-IID situations. Domain generalization
(DG) enables a model to generalize to an unseen test distribution, i.e., to learn domain-
invariant representations. In this paper, we argue that domain-invariant features should
be originating from both internal and mutual sides. Internal invariance means that the
features can be learned with a single domain and the features capture intrinsic semantics
of data, i.e., the property within a domain, which is agnostic to other domains. Mutual
invariance means that the features can be learned with multiple domains (cross-domain)
and the features contain common information, i.e., the transferable features w.r.t. other
domains. We then propose DIFEX for Domain-Invariant Feature EXploration. DIFEX
employs a knowledge distillation framework to capture the high-level Fourier phase as the
internally-invariant features and learn cross-domain correlation alignment as the mutually-
invariant features. We further design an exploration loss to increase the feature diversity
for better generalization. Extensive experiments on both time-series and visual benchmarks
demonstrate that the proposed DIFEX achieves state-of-the-art performance.
1",TMLR
"Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on
graph-structured data. The most popular class of GNNs operate by exchanging information
between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs).
While understanding the expressive power of MPNNs is a key question, existing results
typically consider settings with uninformative node features. In this paper, we provide a
rigorous analysis to determine which function classes of node features can be learned by an
MPNN of a given capacity. We do so by measuring the level of pairwise interactions between
nodes that MPNNs allow for. This measure provides a novel quantitative characterization
of the so-called over-squashing effect, which is observed to occur when a large volume of
messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee
sufficient communication between pairs of nodes, the capacity of the MPNNmust be large
enough, depending on properties of the input graph structure, such as commute times. For
many relevant scenarios, our analysis results in impossibility statements in practice, showing
thatover-squashing hinders the expressive power of MPNNs. Our theory also holds for
geometric graphs and hence extends to equivariant MPNNs on point clouds. We validate
our analysis through extensive controlled experiments and ablation studies.
1",TMLR
"Repeated applications of the same neural block primarily based on self-attention characterize
the current state-of-the-art in neural architectures for machine translation. In such architec-
tures the decoder adopts a masked version of the same encoding block. Although simple
this strategy doesnâ€™t encode the various inductive biases such as locality that arise from
alternative architectures and that are central to the modelling of translation. We propose
Lasagna, an encoder-decoder model that aims to combine the inductive beneï¬ts of diï¬€erent
architectures by layering multiple instances of diï¬€erent blocks. Lasagnaâ€™s encoder ï¬rst grows
the representation from local to mid-sized using convolutional blocks and only then applies
a pair of ï¬nal self-attention blocks. Lasagnaâ€™s decoder uses only convolutional blocks that
attend to the encoder representation. On a large suite of machine translation tasks, we
ï¬nd that Lasagna not only matches or outperforms the Transformer baseline, but it does
so more eï¬ƒciently thanks to widespread use of the eï¬ƒcient convolutional blocks. These
ï¬ndings suggest that the widespread use of uniform architectures may be suboptimal in
certain scenarios and exploiting the diversity of inductive architectural biases can lead to
substantial gains.
1",TMLR
"Transfer learning is a powerful technique that enables model training with limited amounts
of data, making it crucial in many data-scarce real-world applications. Typically, transfer
learning protocols require first to transfer all the feature-extractor layers of a network pre-
trained on a data-rich source task, and then to adapt only the task-specific readout layers
to a data-poor target task. This workflow is based on two main assumptions: first, the
feature maps of the pre-trained model are qualitatively similar to the ones that would have
been learned with enough data on the target task; second, the source representations of
the last hidden layers are always the most expressive. In this work, we demonstrate that
this is not always the case and that the largest performance gain may be achieved when
smaller portions of the pre-trained network are transferred. In particular, we perform a set
of numerical experiments in a controlled setting, showing how the optimal transfer depth
depends non-trivially on the amount of available training data and on the degree of source-
target task similarity, and it is often convenient to transfer only the first layers. We then
propose a strategy to detect the most promising source task among the available candidates.
This approach compares the internal representations of a network trained entirely from
scratch on the target task with those of the networks pre-trained on the potential source
tasks.
1",TMLR
"Diï¬€usion probabilistic models have been shown to generate state-of-the-art results on sev-
eralcompetitiveimagesynthesisbenchmarksbutlackalow-dimensional, interpretablelatent
space, and are slow at generation. On the other hand, standard Variational Autoencoders
(VAEs) typically have access to a low-dimensional latent space but exhibit poor sample
quality. We present Diï¬€useVAE, a novel generative framework that integrates VAE within
a diï¬€usion model framework, and leverage this to design novel conditional parameteriza-
tions for diï¬€usion models. We show that the resulting model equips diï¬€usion models with
a low-dimensional VAE inferred latent code which can be used for downstream tasks like
controllable synthesis. The proposed method also improves upon the speed vs quality trade-
oï¬€ exhibited in standard unconditional DDPM/DDIM models (for instance, FID of 16.47
vs 34.36 using a standard DDIM on the CelebA-HQ-128 benchmark using T=10reverse
process steps) without having explicitly trained for such an objective. Furthermore, the
proposed model exhibits synthesis quality comparable to state-of-the-art models on stan-
dard image synthesis benchmarks like CIFAR-10 and CelebA-64 while outperforming most
existing VAE-based methods. Lastly, we show that the proposed method exhibits inherent
generalization to diï¬€erent types of noise in the conditioning signal. For reproducibility, our
source code is publicly available at https://github.com/kpandey008/DiffuseVAE .
1",TMLR
"Randomized smoothing is the current state-of-the-art method for producing provably robust
classifiers. While randomized smoothing typically yields robust â„“2-ball certificates, recent
research has generalized provable robustness to different norm balls as well as anisotropic
regions. This work considers a classifier architecture that first projects onto a low-dimensional
approximation of the data manifold and then applies a standard classifier. By performing
randomized smoothing in the low-dimensional projected space, we characterize the certified
region of our smoothed composite classifier back in the high-dimensional input space and
prove a tractable lower bound on its volume. We show experimentally on CIFAR-10 and
SVHN that classifiers without the initial projection are vulnerable to perturbations that are
normal to the data manifold and yet are captured by the certified regions of our method.
We compare the volume of our certified regions against various baselines and show that our
method improves on the state-of-the-art by many orders of magnitude.1.
1",TMLR
"This paper proposes a novel diffusion-based model, CompoDiff, for solving zero-shot Com-
posed Image Retrieval (ZS-CIR) with latent diffusion. This paper also introduces a new syn-
thetic dataset, named SynthTriplets18M, with 18.8 million reference images, conditions, and
corresponding target image triplets to train CIR models. CompoDiff and SynthTriplets18M
tackle the shortages of the previous CIR approaches, such as poor generalizability due to
the small dataset scale and the limited types of conditions. CompoDiff not only achieves
a new state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO,
and GeneCIS, but also enables a more versatile and controllable CIR by accepting various
conditions, such as negative text, and image mask conditions. CompoDiff also shows the
controllability of the condition strength between text and image queries and the trade-off
between inference speed and performance, which are unavailable with existing CIR methods.
The code and dataset are available at https://github.com/navervision/CompoDiff .
1",TMLR
"Data pruning algorithms are commonly used to reduce the memory and computational cost
of the optimization process. Recent empirical results (Guo, B. Zhao, and Bai, 2022) reveal
that random data pruning remains a strong baseline and outperforms most existing data
pruning methods in the high compression regime, i.e. where a fraction of 30%or less of
the data is kept. This regime has recently attracted a lot of interest as a result of the role
of data pruning in improving the so-called neural scaling laws; see (Sorscher et al., 2022),
where the authors showed the need for high-quality data pruning algorithms in order to beat
the sample power law. In this work, we focus on score-based data pruning algorithms and
show theoretically and empirically why such algorithms fail in the high compression regime.
We demonstrate â€œNo Free Lunch"" theorems for data pruning and discuss potential solutions
to these limitations.
1",TMLR
"Protecting privacy during inference with deep neural networks is possible by adding Gaus-
siannoisetotheactivationsinthelastlayerspriortothefinalclassifiersorothertask-specific
layers. The activations in such layers are known as â€œfeaturesâ€ (or, less commonly, as â€œem-
beddingsâ€ or â€œfeature embeddingsâ€). The added noise helps prevent reconstruction of the
inputs from the noisy features. Lower bounding the variance of every possible unbiased esti-
mator of the inputs quantifies the confidentiality arising from such added noise. Convenient,
computationally tractable bounds are available from classic inequalities of Hammersley and
of Chapman and Robbins â€” the HCR bounds. Numerical experiments indicate that the
HCR bounds are on the precipice of being effectual for small neural nets with the data
sets, â€œMNISTâ€ and â€œCIFAR-10,â€ which contain 10 classes each for image classification. The
HCR bounds appear to be insufficient on their own to guarantee confidentiality of the in-
puts to inference with standard deep neural nets, â€œResNet-18â€ and â€œSwin-T,â€ pre-trained
on the data set, â€œImageNet-1000,â€ which contains 1000 classes. Supplementing the addition
of Gaussian noise to features with other methods for providing confidentiality may be war-
ranted in the case of ImageNet. In all cases, the results reported here limit consideration to
amounts of added noise that incur little degradation in the accuracy of classification from
the noisy features. Thus, the added noise enhances confidentiality without much reduction
in the accuracy on the task of image classification.
1",TMLR
"Style transfer methods put a premium on two objectives: (1) completeness which encour-
ages the encoding of a complete set of style patterns; (2) coherence which discourages the
production of spurious artifacts not found in input styles. While existing methods pursue
the two objectives either partially or implicitly, we present the Completeness and Coherence
Network (CCNet) which jointly learns completeness and coherence components and rejects
their incompatibility, both in an explicit manner. Specifically, we develop an attention
mechanism integrated with bi-directional softmax operations for explicit imposition of the
two objectives and for their collaborative modelling. We also propose CCLoss as a quanti-
tative measure for evaluating the quality of a stylized image in terms of completeness and
coherence. Through an empirical evaluation, we demonstrate that compared with existing
methods, our method strikes a better tradeoff between computation costs, generalization
ability and stylization quality.
1
1",TMLR
"Incorporating group symmetry directly into the learning process has proved to be an eï¬€ec-
tive guideline for model design. By producing features that are guaranteed to transform
covariantly to the group actions on the inputs, group-equivariant convolutional neural net-
works (G-CNNs) achieve signiï¬cantly improved generalization performance in learning tasks
withintrinsicsymmetry. GeneraltheoryandpracticalimplementationofG-CNNshavebeen
studied for planar images under either rotation or scaling transformation, but only individu-
ally. We present, in this paper, a roto-scale-translation equivariant CNN ( RST-CNN), that
is guaranteed to achieve equivariance jointly over these three groups via coupled group con-
volutions. Moreover, as symmetry transformations in reality are rarely perfect and typically
subject to input deformation, we provide a stability analysis of the equivariance of repre-
sentation to input distortion, which motivates the truncated expansion of the convolutional
ï¬lters under (pre-ï¬xed) low-frequency spatial modes. The resulting model provably achieves
deformation-robust RSTequivariance, i.e., the RSTsymmetry is still â€œapproximatelyâ€
preserved when the transformation is â€œcontaminatedâ€ by a nuisance data deformation, a
property that is especially important for out-of-distribution generalization. Numerical ex-
periments on MNIST, Fashion-MNIST, and STL-10 demonstrate that the proposed model
yields remarkable gains over prior arts, especially in the small data regime where both
rotation and scaling variations are present within the data.
1",TMLR
"The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural
information of the data by aggregating the neighboring nodes using a â€˜graph convolutionâ€™
in conjunction with a suitable choice for the network architecture, such as depth and ac-
tivation functions. Therefore, understanding the influence of each of the design choice on
the network performance is crucial. Convolutions based on graph Laplacian have emerged
as the dominant choice with the symmetric normalization of the adjacency matrix as the
most widely adopted one. However, some empirical studies show that row normalization of
the adjacency matrix outperforms it in node classification. Despite the widespread use of
GNNs, there is no rigorous theoretical study on the representation power of these convo-
lutions, that could explain this behavior. Similarly, the empirical observation of the linear
GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.
In this work, we theoretically analyze the influence of different aspects of the GNN architec-
ture using the Graph Neural Tangent Kernel in a semi-supervised node classification setting.
Under the population Degree Corrected Stochastic Block Model , we prove that: (i) linear
networks capture the class information as good as ReLU networks; (ii) row normalization
preserves the underlying class structure better than other convolutions; (iii) performance
degrades with network depth due to over-smoothing, but the loss in class information is
the slowest in row normalization; (iv) skip connections retain the class information even
at infinite depth, thereby eliminating over-smoothing. We finally validate our theoretical
findings numerically and on real datasets such as CoraandCiteseer.
1",TMLR
"In this study, we undertake a reproducibility analysis of ""Learning Fair Graph Representa-
tions Via Automated Data Augmentations"" by Ling et al. (2022). We assess the validity
of the original claims focused on node classification tasks and explore the performance of
the Graphair framework in link prediction tasks. Our investigation reveals that we can
partially reproduce one of the original three claims and fully substantiate the other two.
Additionally, we broaden the application of Graphair from node classification to link pre-
diction across various datasets. Our findings indicate that, while Graphair demonstrates
a comparable fairness-accuracy trade-off to baseline models for mixed dyadic-level fairness,
it has a superior trade-off for subgroup dyadic-level fairness. These findings underscore
Graphairâ€™s potential for wider adoption in graph-based learning. Our code base can be
found on GitHub at https://github.com/juellsprott/graphair-reproducibility.
1",TMLR
"Label hierarchy is an important source of external knowledge that can enhance classifica-
tion performance. However, most existing methods rely on predefined label hierarchies that
may not match the data distribution. To address this issue, we propose Simultaneous label
hierarchy Exploration AndLearning (SEAL), a new framework that explores the label hi-
erarchy by augmenting the observed labels with latent labels that follow a prior hierarchical
structure. Our approach uses a 1-Wasserstein metric over the tree metric space as an ob-
jective function, which enables us to simultaneously learn a data-driven label hierarchy and
perform (semi-)supervised learning. We evaluate our method on several standard bench-
marks and show that it achieves improved results in semi-supervised image classification
scenarios.
1",TMLR
"This paper introduces a novel ML-based method for Inertial Motion Tracking (IMT) that
fundamentally changes the way this technology is used. The proposed method, named RING1
(Recurrent Inertial Graph-Based Estimator), provides a pluripotent, problem-unspecific
plug-and-play IMT solution that, in contrast to conventional IMT solutions, eliminates the
need for expert knowledge to identify, select, and parameterize the appropriate method.
RINGâ€™s pluripotency is enabled by a novel online-capable neural network architecture
that uses a decentralized network of message-passing, parameter-sharing recurrent neural
networks, which map local IMU measurements and nearest-neighbour messages to local
orientations. This architecture enables RING to address a broad range of IMT problems
that vary greatly in aspects such as the number of attached sensors, or the number of
segments in the kinematic chain, and even generalize to previously unsolved IMT problems,
including the challenging combination of magnetometer-free and sparse sensing with unknown
sensor-to-segment parameters. Remarkably, RING is trained solely on simulated data, yet
evaluated on experimental data, which indicates its exceptional ability to zero-shot generalize
from simulation to experiment, while outperforming several state-of-the-art problem-specific
solutions. For example, RING can, for the first time, accurately track a four-segment
kinematic chain (which requires estimating four orientations) using only two magnetometer-
free inertial measurement units. This research not only makes IMT more powerful and less
restrictive in established domains ranging from biomechanics to autonomous systems, but
also opens its application to new users and fields previously untapped by motion tracking
technology. Code and data is available here.
1one to track them all
1 Published in Transactions on Machine Learning Research (10/2024)
After Training 
in Simulation
A Single Solution
RINGIMUsGraph
Pose of ChainAxes
Real-World Inertial Motion Tracking Problems
Applications
0
0
0
3
34
3
RING
Figure 1: RING is a ML-based method that provides a versatile, pluripotent IMT solution applicable
across a broad range of challenging IMT problems, designed for use without the need for expert knowledge.
Remarkably, RING is trained solely on simulated data, yet zero-shot generalizes to real-world experiments
and outperforms several problem-specific state-of-the-art solutions.
1",TMLR
"We study the adaptation of Soft Actor-Critic (SAC), which is considered as a state-of-
the-art reinforcement learning algorithm, from continuous action space to discrete action
space. We revisit vanilla discrete SAC, i.e., SAC for discrete action space, and provide an
in-depth understanding of its Q value underestimation and performance instability issues
when applied to discrete settings. We thereby propose Stable Discrete SAC (SD-SAC),
an algorithm that leverages entropy-penalty and double average Q-learning with Q-clip to
address these issues. Extensive experiments on typical benchmarks with discrete action
space, including Atari games and a large-scale MOBA game, show the efficacy of SD-SAC.
Our code is at: https://github.com/coldsummerday/SD-SAC.git .
1",TMLR
"Hyperbolic space is becoming a popular choice for representing data due to the hierarchi-
cal structure â€” whether implicit or explicit â€” of many real-world datasets. Along with
it comes a need for algorithms capable of solving fundamental tasks, such as classification,
in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to
hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these
approaches struggle with more complex hierarchical data. We, therefore, propose to gen-
eralize the well-known random forests to hyperbolic space. We do this by redefining the
notion of a split using horospheres. Since finding the globally optimal split is computa-
tionally intractable, we find candidate horospheres through a large-margin classifier. To
make hyperbolic random forests work on multi-class data and imbalanced experiments, we
furthermore outline new methods for combining classes based on the lowest common ances-
tor and class-balanced large-margin losses. Experiments on standard and new benchmarks
show that our approach outperforms both conventional random forest algorithms and recent
hyperbolic classifiers.
1",TMLR
"The gold standard for the identification of causal effects are randomized controlled
trials (RCT), but RCTs may not always be feasible to conduct. When treatments
dependonathresholdhowever, suchasthebloodsugarthresholdfordiabetesdiag-
nosis, we can still sometimes estimate causal effects with regression discontinuities
(RDs). RDs are valid when units just above and below the threshold have the
same distribution of covariates and thus no confounding in the presence of noise,
establishing an as-if randomization. In practice however, implementing RD studies
can be difficult as identifying treatment thresholds require considerable domain ex-
pertise â€“ furthermore, the thresholds may differ across subgroups (e.g., the blood
sugar threshold for diabetes may differ across demographics), and ignoring these
differences can lower statistical power. Finding the thresholds and to whom they
apply is an important problem currently solved manually by domain experts, and
data-driven approaches are needed when domain expertise is not sufficient. Here,
we introduce Regression Discontinuity SubGroup Discovery (RDSGD), a machine-
learning method that identifies statistically powerful and interpretable subgroups
for RD thresholds. Using a medical claims dataset with over 60 million patients,
we apply RDSGD to multiple clinical contexts and identify subgroups with in-
creased compliance to treatment assignment thresholds. As treatment thresholds
matter for many diseases and policy decisions, RDSGD can be a powerful tool for
discovering new avenues for causal estimation.
1 Published in Transactions on Machine Learning Research (10/2023)
1",TMLR
"The interest of the machine learning community in image synthesis has grown significantly
in recent years, with the",TMLR
"Normalizing ï¬‚ows model a complex target distribution in terms of a bijective transform
operating on a simple base distribution. As such, they enable tractable computation of a
number of important statistical quantities, particularly likelihoods and samples. Despite
these appealing properties, the computation of more complex inference tasks, such as the
cumulative distribution function (CDF) over a complex region (e.g., a polytope) remains
challenging. Traditional CDF approximations using Monte-Carlo techniques are unbiased but
have unbounded variance and low sample eï¬ƒciency. Instead, we build upon the diï¬€eomorphic
properties of normalizing ï¬‚ows and leverage the divergence theorem to estimate the CDF
over a closed region in target space in terms of the ï¬‚ux across its boundary , as induced by the
normalizing ï¬‚ow. We describe both deterministic and stochastic instances of this estimator:
while the deterministic variant iteratively improves the estimate by strategically subdividing
the boundary, the stochastic variant provides unbiased estimates. Our experiments on
popular ï¬‚ow architectures and UCI benchmark datasets show a marked improvement in
sample eï¬ƒciency as compared to traditional estimators.
1",TMLR
"Besides natural language processing, transformers exhibit extraordinary performance in
solving broader applications, including scientific computing and computer vision. Previ-
ous works try to explain this from the expressive power and capability perspectives that
standard transformers are capable of performing some algorithms. To empower transform-
ers with algorithmic capabilities and motivated by the recently proposed looped transformer
(Yang et al., 2024; Giannou et al., 2023), we design a novel transformer framework, dubbed
Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient
transformer architectures can be designed by leveraging prior knowledge of tasks and the
underlying structure of potential algorithms. Compared with the standard transformer and
vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm
representation in some specific tasks. In particular, inspired by the structure of human-
designed learning algorithms, our transformer framework consists of a pre-transformer that
is responsible for task preprocessing, a looped transformer for iterative optimization al-
gorithms, and a post-transformer for producing the desired results after post-processing.
We provide theoretical evidence of the expressive power of the AlgoFormer in solving some
challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical
and empirical results are presented to show that the designed transformer has the poten-
tial to perform algorithm representation and learning. Experimental results demonstrate
the empirical superiority of the proposed transformer in that it outperforms the standard
transformer and vanilla looped transformer in some specific tasks. An extensive experiment
on real language tasks (e.g., neural machine translation of German and English, and text
classification) further validates the expressiveness and effectiveness of AlgoFormer.
1",TMLR
"Recent studies show that pretraining a deep neural network with fine-grained labeled data,
followed by fine-tuning on coarse-labeled data for downstream tasks, often yields better
generalization than pretraining with coarse-labeled data. While there is ample empirical
evidence supporting this, the theoretical justification remains an open problem. This paper
addresses this gap by introducing a â€œhierarchical multi-viewâ€ structure to confine the input
data distribution. Under this framework, we prove that: 1) coarse-grained pretraining only
allows a neural network to learn the common features well, while 2) fine-grained pretraining
helps the network learn the rare features in addition to the common ones, leading to improved
accuracy on hard downstream test samples.
1",TMLR
"In recent years, camera-based 3D object detection has gained widespread attention for its
ability to achieve high performance with low computational cost. However, the robust-
ness of these methods to adversarial attacks has not been thoroughly examined, especially
when considering their deployment in safety-critical domains like autonomous driving. In
this study, we conduct the first comprehensive investigation of the robustness of leading
camera-based 3D object detection approaches under various adversarial conditions. We sys-
tematically analyze the resilience of these models under two attack settings: white-box and
black-box; focusing on two primary objectives: classification and localization. Additionally,
we delve into two types of adversarial attack techniques: pixel-based and patch-based. Our
experiments yield four interesting findings: (a) birdâ€™s-eye-view-based representations exhibit
stronger robustness against localization attacks; (b) depth-estimation-free approaches have
the potential to show stronger robustness; (c) accurate depth estimation effectively improves
robustnessfordepth-estimation-basedmethods; (d)incorporatingmulti-framebenigninputs
can effectively mitigate adversarial attacks. We hope our findings can steer the development
of future camera-based object detection models with enhanced adversarial robustness. The
code is available at: https://github.com/Daniel-xsy/BEV-Attack .
1",TMLR
"Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental
challenge in machine learning. Two branches of methods have been proposed to seek
flat minima and improve generalization: one led by sharpness-aware minimization (SAM)
minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP),
and the other minimizes the expected Bayes objective with random weight perturbation
(RWP). While RWP offers advantages in computation and is closely linked to AWP on
a mathematical basis, its empirical performance has consistently lagged behind that of
AWP. In this paper, we revisit the use of RWP for improving generalization and propose
improvements from two perspectives: i) the trade-off between generalization and convergence
and ii) the random perturbation generation. Through extensive experimental evaluations,
we demonstrate that our enhanced RWP methods achieve greater efficiency in enhancing
generalization, particularly in large-scale problems, while also offering comparable or even
superior performance to SAM. The code is released at https://github.com/nblt/mARWP .
1",TMLR
"Insemantic segmentation , we aim to train a pixel-level classifier to assign category labels to
allpixels in an image, where labeled training images and unlabeled test images are from the
same distribution and share the same label set . However, in an open world, the unlabeled test
images probably contain unknown categories and have different distributions from the labeled
images. Hence, in this paper, we consider a new, more realistic, and more challenging problem
setting where the pixel-level classifier has to be trained with labeled images and unlabeled
open-world imagesâ€”we name it open-set domain adaptation segmentation (OSDAS). In
OSDAS, the trained classifier is expected to identify unknown-class pixels and classify known-
class pixels well. To solve OSDAS, we first investigate which distribution that unknown-class
pixels obey. Then, motivated by the goodness-of-fit test, we use statistical measurements to
show how a pixel fitsthe distribution of an unknown class and select highly-fitted pixels to
form the unknown region in each test image. Eventually, we propose an end-to-end learning
framework, known-region-aware domain alignment (KRADA), to distinguish unknown classes
while aligning the distributions of known classes in labeled and unlabeled open-world images.
The effectiveness of KRADA has been verified on two synthetic tasks and one COVID-19
segmentation task.
âˆ—CHZ and FL contributed equally to this paper.
BH is the corresponding author.
Our source code is available at https://github.com/chenhong-zhou/KRADA
1 Published in Transactions on Machine Learning Research (02/2023)
(a)
 (b)
Figure 1: Illustration of the differences in two CT datasets. (a) Examples of normal CT scans. (b) Examples
of COVID-19 CT scans, where the infected area is circled by red boxes. These two datasets vary at the visual
level (domain shift) and are not consistent at the semantic level (category shift).
1",TMLR
"We consider the problem of estimating probability density functions based on sample data,
using a finite mixture of densities from some component class. To this end, we introduce the
h-lifted Kullbackâ€“Leibler (KL) divergence as a generalization of the standard KL divergence
and a criterion for conducting risk minimization. Under a compact support assumption, we
prove anO(1/âˆšn)bound on the expected estimation error when using the h-lifted KL
divergence, which extends the results of Rakhlin et al. (2005, ESAIM: Probability and
Statistics, Vol. 9) and Li & Barron (1999, Advances in Neural Information Processing
Systems, Vol. 12) to permit the risk bounding of density functions that are not strictly
positive. We develop a procedure for the computation of the corresponding maximum h-
lifted likelihood estimators ( h-MLLEs) using the Majorization-Maximization framework and
provide experimental results in support of our theoretical bounds.
1",TMLR
"Transferring trained policies and value functions from one task to another, such as one game
to another with a diï¬€erent board size, board shape, or more substantial rule changes, is a
challenging problem. Popular benchmarks for reinforcement learning (RL), such as Atari
games and ProcGen, have limited variety especially in terms of action spaces. Due to a focus
on such benchmarks, the development of transfer methods that can also handle changes in
action spaces has received relatively little attention. Furthermore, we argue that progress
towards more general methods should include benchmarks where new problem instances can
be described by domain experts, rather than machine learning experts, using convenient,
high-level domain speciï¬c languages (DSLs). In addition to enabling end users to more
easily describe their problems, user-friendly DSLs also contain relevant task information
which can be leveraged to make eï¬€ective zero-shot transfer plausibly achievable. As an
example, we use the Ludii general game system, which includes a highly varied set of over
1000 distinct games described in such a language. We propose a simple baseline approach
for transferring fully convolutional policy-value networks, which are used to guide search
agents similar to AlphaZero, between any pair of games modelled in this system. Extensive
resultsâ€”including various cases of highly successful zero-shot transferâ€”are provided for a
wide variety of source and target games.
1",TMLR
"Graph Neural Networks (GNNs) have been predominant for graph learning tasks; how-
ever, recent studies showed that a well-known graph algorithm, Label Propagation (LP),
combined with a shallow neural network can achieve comparable performance to GNNs in
semi-supervised node classification on graphs with high homophily. In this paper, we show
that this approach falls short on graphs with low homophily, where nodes often connect to
the nodes of the opposite classes. To overcome this, we carefully design a combination of a
base predictor with LP algorithm that enjoys a closed-form solution as well as convergence
guarantees. Our algorithm first learns the class compatibility matrix and then aggregates
label predictions using LP algorithm weighted by class compatibilities. On a wide variety of
benchmarks, we show that our approach achieves the leading performance on graphs with
various levels of homophily. Meanwhile, it has orders of magnitude fewer parameters and
requires less execution time.
1",TMLR
"Can adaptive strategies outperform non-adaptive ones for quantum hypothesis selection?
We exhibit problems where adaptive strategies provably reduce the number of required
samples by a factor four in the worst case, and possibly more when the actual difficulty of
the problem makes it possible. In addition, we exhibit specific hypotheses classes for which
there is a provable polynomial separation between adaptive and non-adaptive strategies â€“ a
specificity of the quantum framework that does not appear in classical testing.
1",TMLR
"Advancements in recording techniques have enabled the ability to record thousands of neu-
rons simultaneously, shifting the needs within the field of computational neuroscience to
that of powerful computational and statistical techniques. Copula-GP is a recently devel-
oped state-of-the-art parametric mutual information estimator found to outperform other
novel non-parametric methods when utilized on highly dimensional data. Here, we utilized
Copula-GP together with Gaussian Process Factor Analysis (GPFA) to investigate the in-
formation interaction between neuronal processes within the visual cortex of live mice and
pupil dilation. We found usage of GPFA as a preprocessing step to Copula-GP was an
effective means of investigating neuronal dependence, allowing flexibility in analysis and
finding results in agreement with prior literature. We additionally extended Copula-GP
with a bagging framework, allowing for the aggregation of model estimations and allow-
ing for more accurate estimation accuracy and representation of dependency shape. We
validated our bagging algorithm on simulated data sampled from known distributions, and
utilized bagged Copula-GP with GPFA on said neuronal data to find results in agreement
with baseline Copula-GP but with more stability.
1",TMLR
"We study the scaling properties of latent diï¬€usion models (LDMs) with an emphasis on their
sampling eï¬ƒciency. While improved network architecture and inference algorithms have
shown to eï¬€ectively boost sampling eï¬ƒciency of diï¬€usion models, the role of model sizeâ€”a
critical determinant of sampling eï¬ƒciencyâ€”has not been thoroughly examined. Through
empirical analysis of established text-to-image diï¬€usion models, we conduct an in-depth
investigation into how model size inï¬‚uences sampling eï¬ƒciency across varying sampling
steps. Our ï¬ndings unveil a surprising trend: when operating under a given inference bud-
get, smaller models frequently outperform their larger equivalents in generating high-quality
results. Moreover, we extend our study to demonstrate the generalizability of the these ï¬nd-
ings by applying various diï¬€usion samplers, exploring diverse downstream tasks, evaluating
post-distilled models, as well as comparing performance relative to training compute. These
ï¬ndings open up new pathways for the development of LDM scaling strategies which can be
employed to enhance generative capabilities within limited inference budgets.
1",TMLR
"Despite the outstanding performance of transformers in both language and vision tasks, the
expanding computation and model size have increased the demand for efficient deployment.
To address the heavy computation and parameter drawbacks, quantization is frequently
studied in the community as a representative model compression technique and has seen
extensive use on ConvNets. However, due to the unique properties of transformers, the
extreme low-bit quantization applications are still limited and underexplored. In this paper,
we identify the difficulty of transformer-based low-bit quantization-aware training on its
uniquevariation behaviors, which significantly differ from ConvNets. The term variation
is defined based on comprehensive quantitative analysis in three hierarchies: various module
quantizationsensitivities, outliersinstaticweightandactivationdistribution, andoscillation
in dynamic parameter fluctuations. These variations of transformers bring instability to the
quantization-aware training (QAT) and negatively influence the performance. We explore
the best practices to alleviate the variationâ€™s influence during low-bit transformer QAT and
propose a variation-aware quantization scheme for both vision and language transformers.
We extensively verify and demonstrate our scheme can alleviate the variation and improve
the performance of transformers across various models and tasks. For the 2-bit Swin-T
and binary BERT-base, our solutions achieve a 3.35%and1.4%accuracy improvement
over previous state-of-the-art methods on the ImageNet-1K dataset and GLUE benchmark.
Codesandmodelsareavailableathttps://github.com/HuangOwen/Quantization-Variation.
1",TMLR
"Many practical applications, ranging from paper-reviewer assignment in peer review to job-
applicant matching for hiring, require human decision makers to identify relevant matches
by combining their expertise with predictions from machine learning models. In many such
model-assisted document matching tasks, the decision makers have stressed the need for
assistive information about the model outputs (or the data) to facilitate their decisions. In
this paper, we devise a proxy matching task that allows us to evaluate which kinds of assistive
information improve decision makersâ€™ performance (in terms of accuracy and time). Through
a crowdsourced ( N= 271participants) study, we find that providing black-box model
explanations reduces usersâ€™ accuracy on the matching task, contrary to the commonly-held
belief that they can be helpful by allowing better understanding of the model. On the other
hand, custom methods that are designed to closely attend to some task-specific desiderata
are found to be effective in improving user performance. Surprisingly, we also find that
the usersâ€™ perceived utility of assistive information is misaligned with their objective utility
(measured through their task performance).
1",TMLR
"Adversarial Training ( AT) has been demonstrated to improve the robustness of deep neural
networks (DNNs) to adversarial attacks. ATis a min-max optimization procedure wherein
adversarial examples are generated to train a robust DNN. The inner maximization step of
AT maximizes the losses of inputs w.r.t their actual classes. The outer minimization involves
minimizing the losses on the adversarial examples obtained from the inner maximization.
This work proposes a standard-deviation-inspired ( SDI) regularization term for improving
adversarial robustness and generalization. We argue that the inner maximization is akin to
minimizing a modified standard deviation of a modelâ€™s output probabilities. Moreover, we
argue that maximizing the modified standard deviation measure may complement the outer
minimization of the ATframework. To corroborate our argument, we experimentally show
that the SDImeasure may be utilized to craft adversarial examples. Furthermore, we show
that combining the proposed SDIregularization term with existing ATvariants improves
the robustness of DNNs to stronger attacks (e.g., CW and Auto-attack) and improves robust
generalization.
1",TMLR
"Edgeapplications, suchascollaborativeroboticsandspacecraftrendezvous, demandefficient
6D object pose estimation on resource-constrained embedded platforms. Existing 6D object
poseestimationnetworksareoftentoolargeforsuchdeployments, necessitatingcompression
while maintaining reliable performance. To address this challenge, we introduce Modular
Quantization-AwareTraining(MQAT),anadaptiveandmixed-precisionquantization-aware
training strategy that exploits the modular structure of modern 6D object pose estimation
architectures. MQAT guides a systematic gradated modular quantization sequence and de-
termines module-specific bit precisions, leading to quantized models that outperform those
produced by state-of-the-art uniform and mixed-precision quantization techniques. Our ex-
periments showcase the generality of MQAT across datasets, architectures, and quantization
algorithms. Additionally, we observe that MQAT quantized models can achieve an accuracy
boost (>7%ADI-0.1d) over the baseline full-precision network while reducing model size
by a factor of 4Ã—or more. https://saqibjaved1.github.io/MQAT_
1",TMLR
"In this paper, we extend the study of concept ablation within pre-trained models as intro-
duced in â€˜Ablating Concepts in Text-to-Image Diffusion Modelsâ€™ by (Kumari et al., 2022).
Our work focuses on reproducing the results achieved by the different variants of concept
ablation proposed and validated through predefined metrics. We also introduce a novel vari-
ant of concept ablation, namely â€˜trademark ablationâ€™. This variant combines the principles
of memorization and instance ablation to tackle the nuanced influence of proprietary or
branded elements in model outputs. Further, our research contributions include an obser-
vational analysis of the modelâ€™s limitations. Moreover, we investigate the modelâ€™s behavior
in response to ablation leakage-inducing prompts, which aim to indirectly ablate concepts,
revealing insights into the modelâ€™s resilience and adaptability. We also observe the modelâ€™s
performance degradation on images generated by concepts far from its target ablation con-
cept, documented in the appendix.
1",TMLR
"Solving complex classiï¬cation tasks using deep neural networks typically requires large
amounts of annotated data. However, corresponding class labels are noisy when provided
by error-prone annotators, e.g., crowdworkers. Training standard deep neural networks
leads to subpar performances in such multi-annotator supervised learning settings. We ad-
dress this issue by presenting a probabilistic training framework named multi-annotator
deep learning (MaDL). A downstream ground truth and an annotator performance model
are jointly trained in an end-to-end learning approach. The ground truth model learns to
predict instancesâ€™ true class labels, while the annotator performance model infers proba-
bilistic estimates of annotatorsâ€™ performances. A modular network architecture enables us
to make varying assumptions regarding annotatorsâ€™ performances, e.g., an optional class or
instance dependency. Further, we learn annotator embeddings to estimate annotatorsâ€™ den-
sities within a latent space as proxies of their potentially correlated annotations. Together
with a weighted loss function, we improve the learning from correlated annotation patterns.
In a comprehensive evaluation, we examine three research questions about multi-annotator
supervised learning. Our ï¬ndings show MaDLâ€™s state-of-the-art performance and robustness
against many correlated, spamming annotators.
1",TMLR
"We propose DiffQa differentiable method for model compression for quantizing model
parameters without gradient approximations (e.g., Straight Through Estimator ). We suggest
adding independent pseudo quantization noise to model parameters during training to
approximate the effect of a quantization operator. DiffQis differentiable both with respect
to the unquantized weights and the number of bits used. Given a single hyper-parameter
balancingbetweenthequantizedmodelsizeandaccuracy, DiffQoptimizesthenumberofbits
used per individual weight or groups of weights, in end-to-end training. We experimentally
verify that our method is competitive with STE based quantization techniques on several
benchmarks and architectures for image classification, language modeling, and audio source
separation. For instance, on the ImageNet dataset, DiffQcompresses a 12 layers transformer-
based model by more than a factor of 8, (lower than 4 bits precision per weight on average),
with a loss of 0.3 %in model accuracy. Code is available at github.com/facebookresearch/diffq.
1",TMLR
"Conservation of energy is at the core of many physical phenomena and dynamical systems.
There have been a signiï¬cant number of works in the past few years aimed at predicting the
trajectory of motion of dynamical systems using neural networks while adhering to the law
of conservation of energy. Most of these works are inspired by classical mechanics such as
Hamiltonian and Lagrangian mechanics as well as Neural Ordinary Diï¬€erential Equations.
While these works have been shown to work well in speciï¬c domains respectively, there is
a lack of a unifying method that is more generally applicable without requiring signiï¬cant
changes to the neural network architectures. In this work, we aim to address this issue by
providing a simple method that could be applied to not just energy-conserving systems, but
also dissipative systems, by including a diï¬€erent inductive bias in diï¬€erent cases in the form
of a regularisation term in the loss function. The proposed method does not require changing
the neural network architecture and could form the basis to validate a novel idea, therefore
showing promises to accelerate research in this direction.
1",TMLR
"Min-max optimization problems, also known as saddle point problems, have attracted
significant attention due to their applications in various fields, such as fair beamforming,
generative adversarial networks (GANs), and adversarial learning. However, understanding
the properties of these min-max problems has remained a substantial challenge. This study
introduces a statistical mechanical formalism for analyzing the equilibrium values of min-
max problems in the high-dimensional limit, while appropriately addressing the order of
operations for min and max. As a first step, we apply this formalism to bilinear min-max
games and simple GANs, deriving the relationship between the amount of training data and
generalization error and indicating the optimal ratio of fake to real data for effective learning.
This formalism provides a groundwork for a deeper theoretical analysis of the equilibrium
properties in various machine learning methods based on min-max problems and encourages
the development of new algorithms and architectures.
1",TMLR
"Despite their great practical successes, the understanding of neural network behavior is still
a topical research issue. In particular, the class of functions learnable in the context of a
finite precision configuration is an open question. In this paper, we propose to study the
limits of gradient descent when such a configuration is set for the class of Simple Recurrent
Networks (SRNs). We exhibit conditions under which the gradient descent will provably
fail. We also design a class of SRN based on Deterministic finite State Automata (DFA) that
fulfills the failure requirements. The definition of this class is constructive: we propose an
algorithm that, from any DFA, constructs an SRN that computes exactly the same function,
a result of interest on its own.
1",TMLR
"Adeterminantalpointprocess(DPP)isapowerfulprobabilisticmodelthatgeneratesdiverse
random subsets from a ground set. Since a DPP is characterized by a positive definite
kernel, a DPP on a finite ground set can be parameterized by a kernel matrix. Recently,
DPPs have gained attention in the machine learning community and have been applied to
various practical problems; however, there is still room for further research on the learning
of DPPs. In this paper, we propose a simple learning rule for full-rank DPPs based on a
minorization-maximization (MM) algorithm, which monotonically increases the likelihood
in each iteration. We show that our minorizer of the MM algorithm provides a tighter
lower-bound compared to an existing method locally. We also generalize the algorithm for
further acceleration. In our experiments on both synthetic and real-world datasets, our
method outperforms existing methods in most settings. Our code is available at https:
//github.com/ISMHinoLab/DPPMMEstimation .
1",TMLR
"The nuclear norm and Schatten- pquasi-norm are popular rank proxies in low-rank matrix
recovery. However, computing the nuclear norm or Schatten- pquasi-norm of a tensor is
hard in both theory and practice, hindering their application to low-rank tensor completion
(LRTC) and tensor robust principal component analysis (TRPCA). In this paper, we propose
a new class of tensor rank regularizers based on the Euclidean norms of the CP component
vectors of a tensor and show that these regularizers are monotonic transformations of tensor
Schatten-pquasi-norm. This connection enables us to minimize the Schatten- pquasi-norm in
LRTC and TRPCA implicitly via the component vectors. The method scales to big tensors
and provides an arbitrarily sharper rank proxy for low-rank tensor recovery compared to the
nuclear norm. On the other hand, we study the generalization abilities of LRTC with the
Schatten-pquasi-norm regularizer and LRTC with the proposed regularizers. The theorems
show that a relatively sharper regularizer leads to a tighter error bound, which is consistent
with our numerical results. Particularly, we prove that for LRTC with Schatten- pquasi-
norm regularizer on d-order tensors, p= 1/dis always better than any p>1/din terms of
the generalization ability. We also provide a recovery error bound to verify the usefulness
of smallpin the Schatten- pquasi-norm for TRPCA. Numerical results on synthetic data
and real data demonstrate the eï¬€ectiveness of the regularization methods and theorems.
1",TMLR
"As various post hoc explanation methods are increasingly being leveraged to explain complex
models in high-stakes settings, it becomes critical to develop a deeper understanding of if
and when the explanations output by these methods disagree with each other, and how such
disagreements are resolved in practice. However, there is little to no research that provides
answers to these critical questions. In this work, we introduce and study the disagreement
problem in explainable machine learning. More specifically, we formalize the notion of
disagreement between explanations, analyze how often such disagreements occur in practice,
and how practitioners resolve these disagreements. We first conduct interviews with data
scientists to understand what constitutes disagreement between explanations generated by
different methods for the same model prediction and introduce a novel quantitative framework
to formalize this understanding. We then leverage this framework to carry out a rigorous
empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation
methods, and six different predictive models, to measure the extent of disagreement between
the explanations generated by various popular explanation methods. In addition, we carry out
an online user study with data scientists to understand how they resolve the aforementioned
disagreements. Our results indicate that (1) state-of-the-art explanation methods often
disagree in terms of the explanations they output, and (2) machine learning practitioners
often employ ad hoc heuristics when resolving such disagreements. These findings suggest
that practitioners may be relying on misleading explanations when making consequential
decisions. They also underscore the importance of developing principled frameworks for
effectively evaluating and comparing explanations output by various explanation techniques.
1",TMLR
"In this work, we address the problem of long-distance navigation for battery electric ve-
hicles (BEVs), where one or more charging sessions are required to reach the intended
destination. We consider the availability and performance of the charging stations to be
unknown and stochastic, and develop a combinatorial semi-bandit framework for exploring
the road network to learn the parameters of the queue time and charging power distribu-
tions. Within this framework, we first outline a method for transforming the road network
graph into a graph of feasible paths between charging stations to handle the constrained
combinatorial optimization problem in an efficient way. Then, for the feasibility graph, we
use a Bayesian approach to model the stochastic edge weights, utilizing conjugate priors
for the one-parameter exponential and two-parameter gamma distributions, the latter of
which is novel to multi-armed bandit literature. Finally, we apply combinatorial versions of
Thompson Sampling, BayesUCB and Epsilon-greedy to the problem. We demonstrate the
performance of our framework on long-distance navigation problem instances in large-scale
country-sized road networks, with simulation experiments in Norway, Sweden and Finland1.
1",TMLR
"Causal discovery (CD) from time-varying data is important in neuroscience, medicine,
and machine learning. Techniques for CD encompass randomized experiments, which are
generally unbiased but expensive, and algorithms such as Granger causality, conditional-
independence-based, structural-equation-based, and score-based methods that are only ac-
curate under strong assumptions made by human designers. However, as demonstrated in
other areas of machine learning, human expertise is often not entirely accurate and tends
to be outperformed in domains with abundant data. In this study, we examine whether
we can enhance domain-specific causal discovery for time series using a data-driven ap-
proach. Our findings indicate that this procedure significantly outperforms human-designed,
domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM,
and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and
the Dream3 gene dataset. We argue that, when feasible, the causality field should consider
a supervised approach in which domain-specific CD procedures are learned from extensive
datasets with known causal relationships, rather than being designed by human specialists.
Our findings promise a new approach toward improving CD in neural and medical data and
for the broader machine learning community.
1",TMLR
"Algorithmic recourse is a process that leverages counterfactual explanations, going beyond
understanding why a system produced a given classiï¬cation, to providing a user with actions
they can take to change their predicted outcome. Existing approaches to compute such
interventionsâ€”known as recourseâ€”identify a set of points that satisfy some desiderataâ€”e.g.
an intervention in the underlying causal graph, minimizing a cost function, etc. Satisfying
these criteria, however, requires extensive knowledge of the underlying model structure, an
often unrealistic amount of information in several domains. We propose a data-driven and
model-agnostic framework to compute counterfactual explanations. We introduce StEP,
a computationally eï¬ƒcient method that oï¬€ers incremental steps along the data manifold
that directs users towards their desired outcome. We show that StEP uniquely satisï¬es a
desirable set of axioms. Furthermore, via a thorough empirical and theoretical investigation,
we show that StEP oï¬€ers provable robustness and privacy guarantees while outperforming
popular methods along important metrics.
1",TMLR
"Large Language Models (LLMs) present an intriguing avenue for exploration in the field
of formal theorem proving. Nevertheless, their full potential, particularly concerning the
mitigation of hallucinations and refinement through prover error messages, remains an area
that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field,
we introduce the Lyra, a new framework that employs two distinct correction mechanisms:
Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the
post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover
tools (the tool is used to prove the connection. The tool can be by simp, Sledgehammer and
so on) for guiding the replacement of incorrect tools. Tool Correction significantly contributes
to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition,
we introduce Conjecture Correction , an error feedback mechanism designed to interact with
prover to refine formal proof conjectures with prover error messages. Compared to the
previous refinement framework, the proposed Conjecture Correction refines generation with
instruction but does not collect paired (generation, error & refinement) prompts. Therefore,
DSP(Jiang et al., 2023a)+Tool Correction + Conjecture Correction = Lyra. Our method has
achieved state-of-the-art (SOTA) performance on both miniF2F validation ( 48.0%â†’55.3%)
and test ( 45.5%â†’51.2%). Using GPT-4, our Lyra enhances the baseline DSP, achieving
improvements on the miniF2F validation from 50.4% to 55.3%, and on the test from 42.6% to
1 Published in Transactions on Machine Learning Research (07/2024)
51.2%. We also present 3 IMO (International Mathematical Olympiad) problems solved by
Lyra. We believe Tool Correction (post-process for hallucination mitigation) and Conjecture
Correction (subgoal adjustment from interaction with the environment) could provide a
promising avenue for future research in this field.
1",TMLR
"Temporal Graph Neural Networks have garnered substantial attention for their capacity to
model evolving structural and temporal patterns while exhibiting impressive performance.
However, it is known that these architectures are encumbered by issues that constrain their
performance, such as over-squashing and over-smoothing. Meanwhile, Transformers have
demonstrated exceptional computational capacity to effectively address challenges related
to long-range dependencies. Consequently, we introduce Todyformerâ€”a novel Transformer-
based neural network tailored for dynamic graphs. It unifies the local encoding capacity
of Message-Passing Neural Networks (MPNNs) with the global encoding of Transformers
through i) a novel patchifying paradigm for dynamic graphs to improve over-squashing, ii)
a structure-aware parametric tokenization strategy leveraging MPNNs, iii) a Transformer
with temporal positional-encoding to capture long-range dependencies, and iv) an encod-
ing architecture that alternates between local and global contextualization, mitigating over-
smoothinginMPNNs. Experimentalevaluationsonpublicbenchmarkdatasetsdemonstrate
that Todyformer consistently outperforms the state-of-the-art methods for downstream
tasks. Furthermore, we illustrate the underlying aspects of the proposed model in effectively
capturing extensive temporal dependencies in dynamic graphs. The code is publicly avail-
able at https://github.com/huawei-noah/noah-research/tree/master/graph_atlas
1",TMLR
"Certiï¬ed defenses based on convex relaxations are an established technique for training
provably robust models. The key component is the choice of relaxation, varying from simple
intervals to tight polyhedra. Counterintuitively, loose interval-based training often leads
to higher certiï¬ed robustness than what can be achieved with tighter relaxations, which
is a well-known but poorly understood paradox. While recent works introduced various
improvements aiming to circumvent this issue in practice, the fundamental problem of
training models with high certiï¬ed robustness remains unsolved. In this work, we investigate
the underlying reasons behind the paradox and identify two key properties of relaxations,
beyond tightness, that impact certiï¬ed training dynamics: continuity and sensitivity. Our
extensive experimental evaluation with a number of popular convex relaxations provides
strong evidence that these factors can explain the drop in certiï¬ed robustness observed for
tighter relaxations. We also systematically explore modiï¬cations of existing relaxations and
discover that improving unfavorable properties is challenging, as such attempts often harm
other properties, revealing a complex tradeoï¬€. Our ï¬ndings represent an important ï¬rst step
towards understanding the intricate optimization challenges involved in certiï¬ed training.
1",TMLR
"Genome-wide association studies (GWAS) are used to identify relationships between genetic
variations and specific traits. When applied to high-dimensional medical imaging data,
a key step is to extract lower-dimensional, yet informative representations of the data as
traits. Representation learning for imaging genetics is largely under-explored due to the
unique challenges posed by GWAS in comparison to typical visual representation learning.
In this study, we tackle this problem from the mutual information (MI) perspective by
identifying key limitations of existing methods. We introduce a trans-modal learning
framework Genetic InfoMax (GIM), including a regularized MI estimator and a novel
genetics-informed transformer to address the specific challenges of GWAS. We evaluate GIM
on human brain 3D MRI data and establish standardized evaluation protocols to compare it
to existing approaches. Our results demonstrate the effectiveness of GIM and a significantly
improved performance on GWAS.
1",TMLR
"The convergence of the conjugate gradient method for solving large-scale and sparse linear
equation systems depends on the spectral properties of the system matrix, which can be
improved by preconditioning. In this paper, we develop a computationally efficient data-
driven approach to accelerate the generation of effective preconditioners. We, therefore,
replace the typically hand-engineered preconditioners by the output of graph neural networks.
Our method generates an incomplete factorization of the matrix and is, therefore, referred to
as neural incomplete factorization (NeuralIF). Optimizing the condition number of the linear
system directly is computationally infeasible. Instead, we utilize a stochastic approximation
of the Frobenius loss which only requires matrix-vector multiplications for efficient training.
At the core of our method is a novel message-passing block, inspired by sparse matrix
theory, that aligns with the objective of finding a sparse factorization of the matrix. We
evaluate our proposed method on both synthetic problem instances and on problems arising
from the discretization of the Poisson equation on varying domains. Our experiments
show that by using data-driven preconditioners within the conjugate gradient method we
are able to speed up the convergence of the iterative procedure. The code is available at
https://github.com/paulhausner/neural-incomplete-factorization .
1",TMLR
"Bayesian inference in non-linear dynamical systems seeks to find good posterior approxi-
mations of a latent state given a sequence of observations. Gaussian filters and smoothers,
including the (extended/unscented) Kalman filter/smoother, which are commonly used in
engineering applications, yield Gaussian posteriors on the latent state. While they are
computationally efficient, they are often criticised for their crude approximation of the pos-
terior state distribution. In this paper, we address this criticism by proposing a message
passing scheme for iterative state estimation in non-linear dynamical systems, which yields
more informative (Gaussian) posteriors on the latent states. Our message passing scheme
is based on expectation propagation (EP). We prove that classical Rauchâ€“Tungâ€“Striebel
(RTS) smoothers, such as the extended Kalman smoother (EKS) or the unscented Kalman
smoother (UKS), are special cases of our message passing scheme. Running the message
passing scheme more than once can lead to significant improvements of the classical RTS
smoothers, so that more informative state estimates can be obtained. We address potential
convergence issues of EP by generalising our state estimation framework to damped updates
and the consideration of general Î±-divergences.
1",TMLR
"The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set
of graph learning tasks strongly dependent on long-range interaction between vertices. Em-
pirical evidence suggests that on these tasks Graph Transformers signiï¬cantly outperform
Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN
baselines as well as the Graph Transformer GPS (RampÃ¡ek et al. 2022) on LRGB. Through
a rigorous empirical analysis, we demonstrate that the reported performance gap is overes-
timated due to suboptimal hyperparameter choices. It is noteworthy that across multiple
datasets the performance gap completely vanishes after basic hyperparameter optimiza-
tion. In addition, we discuss the impact of lacking feature normalization for LRGBâ€™s vision
datasets and highlight a spurious implementation of LRGBâ€™s link prediction metric. The
principal aim of our paper is to establish a higher standard of empirical rigor within the
graph machine learning community.
1",TMLR
"The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of
the objective gradient. This generally requires an expensive hyperparameter tuning process
to appropriately calibrate a stepsize for a given problem. In this work we introduce a local
ï¬rst-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients
smoothness condition and is applicable to any twice-diï¬€erentiable function. We show that
this oracle can encode all relevant problem information for tuning stepsizes for a suitably
modiï¬ed gradient descent method and give global and local convergence results. We also
show that LFSOs in this modiï¬ed ï¬rst-order method can yield global linear convergence
rates for non-strongly convex problems with extremely ï¬‚at minima.
1",TMLR
"Statistical heterogeneity severely limits the performance of federated learning (FL), moti-
vating several explorations e.g.,FedProx, MOON and FedDyn, to alleviate this problem.
Despite effectiveness, their considered scenario generally requires samples from almost all
classes during the local training of each client, although some covariate shifts may exist
among clients. In fact, the natural case of partially class-disjoint data (PCDD), where each
client contributes a few classes (instead of all classes) of samples, is practical yet underex-
plored. Specifically, the unique collapse and invasion characteristics of PCDD can induce
the biased optimization direction in local training, which prevents the efficiency of federated
learning. To address this dilemma, we propose a manifold reshaping approach called FedMR
to calibrate the feature space of local training. Our FedMR adds two interplaying losses to
the vanilla federated learning: one is intra-class loss to decorrelate feature dimensions for
anti-collapse; and the other one is inter-class loss to guarantee the proper margin among cat-
egories in the feature expansion. We conduct extensive experiments on a range of datasets
to demonstrate that our FedMR achieves much higher accuracy and better communication
efficiency. Source code is available at: https://github.com/MediaBrain-SJTU/FedMR.
1",TMLR
"Adaptive gradient methods are workhorses in deep learning. However, the convergence guar-
antees of adaptive gradient methods for nonconvex optimization have not been thoroughly
studied. In this paper, we provide a fine-grained convergence analysis for a general class
of adaptive gradient methods including AMSGrad, RMSProp and AdaGrad. For smooth
nonconvex functions, we prove that adaptive gradient methods in expectation converge to a
first-order stationary point. Our convergence rate is better than existing results for adaptive
gradient methods in terms of dimension. In addition, we also prove high probability bounds
on the convergence rates of AMSGrad, RMSProp as well as AdaGrad, which have not been
established before. Our analyses shed light on better understanding the mechanism behind
adaptive gradient methods in optimizing nonconvex objectives.
1",TMLR
"A classic inferential problem in statistics is the goodness-of-fit (GOF) test. Performing such
tests can be challenging when the hypothesized parametric model has an intractable like-
lihood and its distributional form is not available. Bayesian methods for GOF testing can
be appealing due to their ability to incorporate expert knowledge through prior distribu-
tions. However, standard Bayesian methods for this test often require strong distributional
assumptions on the data and their relevant parameters. To address this issue, we propose a
semi-Bayesian nonparametric (semi-BNP) procedure based on the maximum mean discrep-
ancy (MMD) measure that can be applied to the GOF test. We introduce a novel Bayesian
estimator for the MMD, which enables the development of a measure-based hypothesis test
for intractable models. Through extensive experiments, we demonstrate that our proposed
test outperforms frequentist MMD-based methods by achieving a lower false rejection and
acceptance rate of the null hypothesis. Furthermore, we showcase the versatility of our ap-
proachbyembeddingtheproposedestimatorwithinagenerativeadversarialnetwork(GAN)
framework. It facilitates a robust BNP learning approach as another significant application
of our method. With our BNP procedure, this new GAN approach can enhance sample
diversity and improve inferential accuracy compared to traditional techniques.
1",TMLR
"Continual learning research has shown that neural networks suffer from catastrophic for-
getting â€œat the output levelâ€, but it is debated whether this is also the case at the level
of learned representations. Multiple recent studies ascribe representations a certain level
of innate robustness against forgetting â€“ that they only forget minimally in comparison
with forgetting at the output level. We revisit and expand upon the experiments that re-
vealed this difference in forgetting and illustrate the coexistence of two phenomena that
affect the quality of continually learned representations: knowledge accumulation and fea-
ture forgetting. Taking both aspects into account, we show that, even though forgetting in
the representation ( i.e. feature forgetting) can be small in absolute terms, when measuring
relative to how much was learned during a task, forgetting in the representation tends to be
just as catastrophic as forgetting at the output level. Next we show that this feature forget-
ting is problematic as it substantially slows down the incremental learning of good general
representations ( i.e. knowledge accumulation). Finally, we study how feature forgetting and
knowledge accumulation are affected by different types of continual learning methods.
1",TMLR
"Batch active learning is a popular approach for efficiently training machine learning models
on large, initially unlabelled datasets by repeatedly acquiring labels for batches of data
points. However, many recent batch active learning methods are white-box approaches and
are often limited to differentiable parametric models: they score unlabeled points using
acquisition functions based on model embeddings or first- and second-order derivatives. In
this paper, we propose black-box batch active learning for regression tasks as an extension of
white-box approaches. Crucially, our method only relies on model predictions. This approach
is compatible with a wide range of machine learning models including regular and Bayesian
deep learning models and non-differentiable models such as random forests. It is rooted in
Bayesian principles and utilizes recent kernel-based approaches. This allows us to extend a
wide range of existing state-of-the-art white-box batch active learning methods (BADGE,
BAIT, LCMD) to black-box models. We demonstrate the effectiveness of our approach
through extensive experimental evaluations on regression datasets, achieving surprisingly
strong performance compared to white-box approaches for deep learning models.
1",TMLR
"Feature attribution methods attempt to explain neural network predictions by identifying
relevant features. However, establishing a cohesive framework for assessing feature attribu-
tion remains a challenge. There are several views through which we can evaluate attribu-
tions. One principal lens is to observe the effect of perturbing attributed features on the
modelâ€™s behavior (i.e., faithfulness). While providing useful insights, existing faithfulness
evaluations suffer from shortcomings that we reveal in this paper. To address the limitations
ofpreviousevaluations, inthiswork, weproposetwonewperspectiveswithinthefaithfulness
paradigm that reveal intuitive properties: soundness andcompleteness . Soundness assesses
the degree to which attributed features are truly predictive features, while completeness
examines how well the resulting attribution reveals all the predictive features. The two
perspectives are based on a firm mathematical foundation and provide quantitative metrics
that are computable through efficient algorithms. We apply these metrics to mainstream
attribution methods, offering a novel lens through which to analyze and compare feature at-
tribution methods. Our code is provided at https://github.com/sandylaker/soco.git .
1",TMLR
"Large Language Models (LLMs) have demonstrated remarkable capabilities in important
tasks such as natural language understanding and language generation, and thus have the
potential to make a substantial impact on our society. Such capabilities, however, come with
the considerable resources they demand, highlighting the strong need to develop effective
techniques for addressing their efficiency challenges. In this survey, we provide a systematic
and comprehensive review of efficient LLMs research. We organize the literature in a taxon-
omy consisting of three main categories, covering distinct yet interconnected efficient LLMs
topicsfrommodel-centric, data-centric, andframework-centricperspective, respectively. We
have also created a GitHub repository where we organize the papers featured in this survey
at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain
the repository and incorporate new research as it emerges. We hope our survey can serve as
a valuable resource to help researchers and practitioners gain a systematic understanding of
efficient LLMs research and inspire them to contribute to this important and exciting field.
â€¡â€¡The work is done outside Amazon.
1 Published in Transactions on Machine Learning Research (May/2024)
1",TMLR
"This paper introduces a novel approach to personalised federated learning within the X-armed
bandit framework, addressing the challenge of optimising both local and global objectives in
a highly heterogeneous environment. Our method employs a surrogate objective function
that combines individual client preferences with aggregated global knowledge, allowing for
a flexible trade-off between personalisation and collective learning. We propose a phase-
based elimination algorithm that achieves sublinear regret with logarithmic communication
overhead, making it well-suited for federated settings. Theoretical analysis and empirical
evaluations demonstrate the effectiveness of our approach compared to existing methods.
Potential applications of this work span various domains, including healthcare, smart home
devices, and e-commerce, where balancing personalisation with global insights is crucial.
1",TMLR
"In online convex optimization, some efficient algorithms have been designed for each of the
individual classes of objective functions, e.g., convex, strongly convex, and exp-concave.
However, existing regret analyses, including those of universal algorithms, are limited to
cases in which the objective functions in all rounds belong to the same class and cannot be
applied to cases in which the property of objective functions may change in each time step.
This paper introduces a novel approach to address such cases, proposing a new regime we
term ascontaminated online convex optimization. For the contaminated case, we demon-
strate that the regret is lower bounded by â„¦(logT+âˆš
k). Here,ksignifies the level of
contamination in the objective functions. We also demonstrate that the regret is bounded
byO(logT+âˆšklogT)when universal algorithms are used. When our proposed algorithms
with additional information are employed, the regret is bounded by O(logT+âˆš
k), which
matches the lower bound. These are intermediate bounds between a convex case and a
strongly convex or exp-concave case.
1",TMLR
"Hawkes processes are point process models that have been used to capture self-excitatory
behaviour in social interactions, neural activity, earthquakes and viral epidemics. They can
model the occurrence of the times and locations of events. We develop a new class of spa-
tiotemporal Hawkes processes that can capture both triggering and clustering behaviour and
we provide an efficient method for performing inference. We use a log-Gaussian Cox process
(LGCP)aspriorforthebackgroundrateoftheHawkesprocesswhichgivesarbitraryflexibil-
ity to capture a wide range of underlying background effects (for infectious diseases these are
called endemic effects). The Hawkes process and LGCP are computationally expensive due
to the former having a likelihood with quadratic complexity in the number of observations
and the latter involving inversion of the precision matrix which is cubic in observations. We
propose a novel approach to perform MCMC sampling for our Hawkes process with LGCP
background, using pre-trained Gaussian process generators which provide direct and cheap
access to samples during inference. We show the efficacy and flexibility of our approach in
experiments on simulated data and use our methods to uncover the trends in a dataset of
reported crimes in the US.
Keywords: Gaussian process, self-excitation, clustering, Bayesian inference
1 Published in Transactions on Machine Learning Research (05/2023)
1",TMLR
"Estimating the Lipschitz constant of a function, also known as Lipschitz learning, is a funda-
mental problem with broad applications in fields such as control and global optimization. In
this paper, we study the Lipschitz learning problem with minimal parametric assumptions
on the target function. As a first theoretical contribution, we derive novel lower bounds
on the sample complexity of this problem for both noise-free and noisy settings under mild
assumptions. Moreover, we propose a simple Lipschitz learning algorithm called Lipschitz
Constant Estimation by Least Squares Regression (referred to as LCLS). We show that
LCLS is asymptotically consistent for general noise assumptions and offers finite sample
guarantees that can be translated to new upper bounds on the sample complexity of the
Lipschitz learning problem. Our analysis shows that the sample complexity rates derived in
this paper are optimal in both the noise-free setting and in the noisy setting when the noise
is assumed to follow a Gaussian distribution and that LCLS is a sample-optimal algorithm
in both cases. Finally, we show that by design, the LCLS algorithm is computationally
faster than existing theoretically consistent methods, and can be readily adapted to various
noise assumptions with little to no prior knowledge of the target function properties or noise
distribution.
1",TMLR
"Do vision-language models (VLMs) pre-trained to caption an image of a durianlearn visual
concepts such as brown(color) and spiky(texture) at the same time? We aim to answer
this question as visual concepts learned â€œfor freeâ€ would enable wide applications such as
neuro-symbolic reasoning or human-interpretable object classification. We assume that the
visual concepts, if captured by pre-trained VLMs, can be extracted by their vision-language
interface with text-based concept prompts. We observe that recent works prompting VLMs
with concepts often differ in their strategies to define and evaluate the visual concepts,
leading to conflicting conclusions. We propose a new concept definition strategy based on
two observations: First, certain concept prompts include shortcuts that recognize correct
concepts for wrong reasons; Second, multimodal information (e.g. visual discriminativeness,
and textual knowledge) should be leveraged when selecting the concepts. Our proposed
concept discovery and learning (CDL) framework is thus designed to identify a diverse list
of generic visual concepts (e.g. spikyas opposed to spiky durian ), which are ranked and
selected based on visual and language mutual information. We carefully design quantitative
andhumanevaluationsofthediscoveredconceptsonninediversevisualrecognitiondatasets,
which confirm that pre-trained VLMs do learn visual concepts that provide accurate and
thorough descriptions for the recognized objects. Code and models are publicly released.1
1",TMLR
"Expanding reinforcement learning (RL) to offline domains generates promising prospects,
particularly in sectors where data collection poses substantial challenges or risks. Pivotal
to the success of transferring RL offline is mitigating overestimation bias in value estimates
for state-action pairs absent from data. Whilst numerous approaches have been proposed in
recentyears,thesetendtofocusprimarilyoncontinuousorsmall-scalediscreteactionspaces.
Factorised discrete action spaces, on the other hand, have received relatively little attention,
despite many real-world problems naturally having factorisable actions. In this work, we
undertake a formative investigation into offline reinforcement learning in factorisable action
spaces. Using value-decomposition as formulated in DecQN as a foundation, we present the
caseforafactorisedapproachandconductanextensiveempiricalevaluationofseveraloffline
techniques adapted to the factorised setting. In the absence of established benchmarks, we
introduce a suite of our own comprising datasets of varying quality and task complexity.
Advocating for reproducible research and innovation, we make all datasets available for
public use alongside our code base.
1",TMLR
"Recent research on time-series self-supervised models shows great promise in learning se-
mantic representations. However, it has been limited to small-scale datasets, e.g., thou-
sands of temporal sequences. In this work, we make key technical contributions that are
tailored to the numerical properties of time-series data and allow the model to scale to
large datasets, e.g., millions of temporal sequences. We adopt the Transformer architec-
ture by ï¬rst partitioning the input into non-overlapping windows. Each window is then
characterized by its normalized shape and two scalar values denoting the mean and stan-
dard deviation within each window. To embed scalar values that may possess arbitrary
numerical amplitudes in a high-dimensional space, we propose a numerically multi-scaled
embedding module enumerating all possible numerical scales for the scalars. The model
undergoes pretraining with a simple contrastive objective on a large-scale dataset over a
million sequences collected by merging existing public data. We study its transfer perfor-
mance on a number of univariate and multivariate classiï¬cation tasks, few shot learning,
unsupervised clustering and anomaly detection benchmarks. Our method exhibits remark-
able improvement against previous pretraining approaches and establishes the new state of
the art, even compared with domain-speciï¬c non-learning-based methods. Code is available
at:https://github.com/chenguolin/NuTime .
ÃºThis work was done during an internship at Microsoft. â€ indicates equal ï¬rst author contribution.
1 Published in Transactions on Machine Learning Research (07/2024)
Figure 1: (a) Numerical scales of three temporal sequences from three datasets dier signiï¬cantly. (b) Even
a single sequence may contain multiple scales of numerical variations. The zoom-in view shows the local
structure of small variations. Note that sequences are shifted above the x-axis and presented in a logarithmic
scale for better visualizations.
1",TMLR
"Curvature in form of the Hessian or its generalized Gauss-Newton ( GGN) approximation
is valuable for algorithms that rely on a local model for the loss to train, compress, or
explain deep networks. Existing methods based on implicit multiplication via automatic
differentiation or Kronecker-factored block diagonal approximations do not consider noise in
the mini-batch. We present ViViT, a curvature model that leverages the GGNâ€™s low-rank
structure without further approximations. It allows for efficient computation of eigenvalues,
eigenvectors, as well as per-sample first- and second-order directional derivatives. The
representation is computed in parallel with gradients in one backward pass and offers a fine-
grained cost-accuracy trade-off, which allows it to scale. We demonstrate this by conducting
performance benchmarks and substantiate ViViTâ€™s usefulness by studying the impact of
noise on the GGNâ€™s structural properties during neural network training.
1",TMLR
"Unsupervised graph representation learning has recently gained interest in several applica-
tion domains such as neuroscience, where modeling the diverse morphology of cell types
in the brain is one of the key challenges. It is currently unknown how many excitatory
cortical cell types exist and what their defining morphological features are. Here we present
GraphDINO , a purely data-driven approach to learn low-dimensional representations of
3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel
transformer-based representation learning method for spatially-embedded graphs. To enable
self-supervised learning on transformers, we (1) developed data augmentation strategies for
spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel
attention mechanism, AC-Attention , which combines attention-based global interaction
between nodes and classic graph convolutional processing. We show, in two different species
and across multiple brain areas, that this method yields morphological cell type cluster-
ings that are on par with manual feature-based classification by experts, but without using
prior knowledge about the structural features of neurons. Moreover, it outperforms pre-
vious approaches on quantitative benchmarks predicting expert labels. Our method could
potentially enable data-driven discovery of novel morphological features and cell types in
large-scale datasets. It is applicable beyond neuroscience in settings where samples in a
dataset are graphs and graph-level embeddings are desired.
1",TMLR
"We study the problem of clustering data points in a streaming setting when one has access
to the geometry of the space only via approximate nearest neighbour (ANN) oracles. In
this setting, we present algorithms for streaming O(1)-approximate k-median clustering and
its (streaming) coreset construction. In certain domains of interest, such as spaces with
constant expansion, our algorithms improve upon the best-known runtime of both these
problems. Furthermore, our results extend to cost functions satisfying the approximate
triangle inequality, which subsumes k-means clustering and M-estimators. Finally, we run
experiments on Census1990 dataset wherein the results empirically support our theory.
1",TMLR
"Previous works on Treatment Effect Estimation (TEE) can be limited to carefully examined
use cases because they are predominantly theoretical, where strong parametric assumptions
are made but untractable for practical application. Recent works use Multilayer Perceptron
(MLP) for modeling casual relationships, however, MLPâ€™s lag far behind recent advances in
ML methodology in handling unstructured, high-dimensional, non-tabular data, which limits
their applicability and generalizability. To extend beyond the single domain formulation and
towards more realistic learning scenarios, we explore model design spaces beyond MLPs, i.e.,
transformer backbones, which provide flexibility where attention layers govern interactions
among treatments and covariates to exploit structural similarities of potential outcomes
for confounding control. Through careful model design, Transformers as TreatmentEffect
Estimators (TransTEE) is proposed. We show empirically that TransTEE can: (1) serve as
a general-purpose treatment effect estimator which significantly outperforms competitive
baselines on a variety of challenging TEE problems (e.g., discrete, continuous, structured,
or dosage-associated treatments.) and is applicable to both when covariates are tabular
and when they consist of structural data (e.g., texts, graphs); (2) yield multiple advantages:
compatibility with propensity score modeling, parameter efficiency, robustness to continuous
treatment value distribution shifts, explainability in covariate adjustment, and real-world
utility in auditing fair predictions of pre-trained language models.
1",TMLR
"Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1k
dataset. In this work, given the excellent scalability of web data, we consider self-supervised
pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark
study of representative self-supervised pre-training methods on large-scale web data in a
like-for-like setting. We compare a range of methods, including single-modal ones that use
masked training objectives and multi-modal ones that use image-text constrastive train-
ing. We observe that existing multi-modal methods do not outperform their single-modal
counterparts on vision transfer learning tasks. We derive an information-theoretical view
to explain these benchmark results, which provides insight into how to design a novel vi-
sion learner. Inspired by this insight, we present a new visual representation pre-training
method, MUlti-modal Generator (MUG), that learns from scalable web sourced image-text
data. MUG achieves state-of-the-art transfer performance on a variety of tasks and demon-
strates promising scaling properties.
1",TMLR
"Sequence-to-sequence models based on LSTM and GRU are a most popular choice for fore-
casting time series data reaching state-of-the-art performance. Training such models can be
delicate though. The two most common training strategies within this context are teacher
forcing (TF) and free running (FR). TF can be used to help the model to converge faster
but may provoke an exposure bias issue due to a discrepancy between training and infer-
ence phase. FR helps to avoid this but does not necessarily lead to better results, since
it tends to make the training slow and unstable instead. Scheduled sampling was the ï¬rst
approach tackling these issues by picking the best from both worlds and combining it into a
curriculum learning (CL) strategy. Although scheduled sampling seems to be a convincing
alternative to FR and TF, we found that, even if parametrized carefully, scheduled sampling
may lead to premature termination of the training when applied for time series forecast-
ing. To mitigate the problems of the above approaches we formalize CL strategies along
the training as well as the training iteration scale. We propose several new curricula, and
systematically evaluate their performance in two experimental sets. For our experiments,
we utilize six datasets generated from prominent chaotic systems. We found that the newly
proposed increasing training scale curricula with a probabilistic iteration scale curriculum
consistently outperforms previous training strategies yielding an NRMSE improvement of
up to 81% over FR or TF training. For some datasets we additionally observe a reduced
number of training iterations. We observed that all models trained with the new curricula
yield higher prediction stability allowing for longer prediction horizons.
1",TMLR
"This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient track-
ing algorithm ( DoCoM) for communication-efficient decentralized optimization. The algo-
rithm features two main ingredients to achieve a near-optimal sample complexity while
allowing for communication compression. First, the algorithm tracks both the averaged
iterate and stochastic gradient using compressed gossiping consensus. Second, a momen-
tum step is incorporated for adaptive variance reduction with the local gradient estimates.
We show that DoCoMfinds a near-stationary solution at all participating agents satisfying
E[âˆ¥âˆ‡f(Î¸)âˆ¥2] =O(1/T2/3)inTiterations, where f(Î¸)is a smooth (possibly non-convex) ob-
jective function. Notice that the proof is achieved via analytically designing a new potential
function that tightly tracks the one-iteration progress of DoCoM. As a corollary, our analysis
also established the linear convergence of DoCoMto a global optimal solution for objective
functions with the Polyak-Åojasiewicz condition. Numerical experiments demonstrate that
our algorithm outperforms several state-of-the-art algorithms in practice.
1",TMLR
"A challenge in reinforcement learning lies in effectively deploying trained policies to handle
out-of-distribution data and environmental variations. Agents observing pixel-based image
data are generally sensitive to background distractions and color changes. Commonly, color
generalization is achieved through data augmentation. In contrast, we propose a color-
invariant neural network layer that adopts distinct color symmetries in a self-supervised
fashion. This allows for color sensitivity while achieving generalization. Our approach is
based on dynamic-mode decomposition, which also accommodates spatial and temporal
symmetries; we discuss the controlled breaking of the latter. We empirically evaluate our
method in the Minigrid, Procgen, and DeepMind Control suites and find improved color
sensitivity and generalisation.
1",TMLR
"Deep reinforcement learning has shown lots of success in closed, well-defined domains such
as games (Chess, Go, StarCraft). The next frontier is real-world scenarios, where setups are
numerous and varied. For this, agents need to learn the underlying environment dynamics,
so as to robustly generalise to conditions that differ from those they were trained on. Model-
basedreinforcementlearningalgorithms, suchasMuZeroorDreamer, aimtoaccomplishthis
bylearningaworldmodel. However, leveragingaworldmodelhasnotyetconsistentlyshown
greater generalisation capabilities compared to model-free alternatives. In this work, we
propose improving the data efficiency and generalisation capabilities of MuZero by explicitly
incorporating the symmetries of the environment in its world-model architecture. We prove
that,solongastheneuralnetworksusedbyMuZeroareequivarianttoaparticularsymmetry
group acting on the environment, the entirety of MuZeroâ€™s action-selection algorithm will
also be equivariant to that group. As such, Equivariant MuZero is guaranteed to behave
symmetrically in symmetrically-transformed states, and will hence be more data-efficient
whenlearningitsworldmodels. WeevaluateEquivariantMuZeroonprocedurally-generated
MiniPacman and on Chaser from the ProcGen suite: training on a set of mazes, and then
testing on unseen rotated versions, demonstrating the benefits of equivariance. We verify
that our improvements hold even when only some of the components of Equivariant MuZero
obey strict equivariance, which highlights the robustness of our construction.
1",TMLR
"Real-world natural language processing systems need to be robust to human adversaries.
Collecting examples of human adversaries for training is an eï¬€ective but expensive solution.
On the other hand, training on synthetic attacks with small perturbationsâ€”such as word-
substitutionâ€”does not actually improve robustness to human adversaries. In this paper, we
propose an adversarial training framework that uses limited human adversarial examples to
generate more useful adversarial examples at scale. We demonstrate the advantages of this
system on the ANLI and hate speech detection benchmark datasetsâ€”both collected via an
iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only
on observed human attacks, also training on our synthetic adversarial examples improves
model robustness to future rounds. In ANLI, we see accuracy gains on the current set
of attacks (44.1% â†’50.1%) and on two future unseen rounds of human generated attacks
(32.5%â†’43.4%, and 29.4% â†’40.2%). In hate speech detection, we see AUC gains on current
attacks (0.76â†’0.84) and a future round (0.77 â†’0.79). Attacks from methods that do not
learn the distribution of existing human adversaries, meanwhile, degrade robustness.
1",TMLR
"Model merging aims to cheaply combine individual task-specific models into a single multi-
task model. In this work, we view past merging methods as leveraging different notions of a
â€œtask parameter subspaceâ€ in which models are matched before being merged. We connect
the task parameter subspace of a given model to its loss landscape and formalize how this
approach to model merging can be seen as solving a linear system of equations. While
past work has generally been limited to linear systems that have a closed-form solution, we
consider using the conjugate gradient method to find a solution. We show that using the
conjugate gradient method can outperform closed-form solutions, enables merging via lin-
ear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide
variety of initializations and estimates for the â€œtask parameter subspaceâ€. We ultimately
demonstrate that our merging framework called â€œ Matching Models in their Task Parameter
Subspaceâ€ ( MaTS) achieves state-of-the-art results in multitask and intermediate-task model
merging. We release all of the code and checkpoints used in our work.1
1",TMLR
"The development of language models have moved from encoder-decoder to decoder-only designs.
In addition, we observe that the two most popular multimodal tasks, the generative and contrastive
tasks, are nontrivial to accommodate in one architecture, and further need adaptations for down-
stream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal
tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks.
This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a
text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass
approach on the text decoder. We demonstrate that joint learning of these diverse objectives is sim-
ple, effective, and maximizes the weight-sharing of the model across these tasks. Furthermore, the
same architecture enables straightforward extensions to open-vocabulary object detection and video-
language tasks. The model tackles a diverse range of tasks, while being modest in capacity. Our
model achieves the state of the art on image-text and text-image retrieval, video question answer-
ing and open-vocabulary detection tasks, outperforming much larger and more extensively trained
foundational models. It shows very competitive results on VQA and Video Captioning, especially
considering its capacity. Ablations conï¬rm the ï¬‚exibility and advantages of our approach.
1",TMLR
"Federated Learning has become a widely-used framework which allows learning a global model on
decentralized local datasets under the condition of protecting local data privacy. However, federated
learning faces severe optimization difficulty when training samples are not independently and iden-
tically distributed (non-i.i.d.). In this paper, we point out that the client sampling practice plays a
decisive role in the aforementioned optimization difficulty. We find that the negative client sampling
will cause the merged data distribution of currently sampled clients heavily inconsistent with that
of all available clients, and further make the aggregated gradient unreliable. To address this issue,
we propose a novel learning rate adaptation mechanism to adaptively adjust the server learning rate
for the aggregated gradient in each round, according to the consistency between the merged data
distribution of currently sampled clients and that of all available clients. Specifically, we make the-
oretical deductions to find a meaningful and robust indicator that is positively related to the optimal
server learning rate, which is supposed to minimize the Euclidean distance between the aggregated
gradient given currently sampled clients and that if all clients could participate in the current round.
We show that our proposed indicator can effectively reflect the merged data distribution of sampled
clients, thus we utilize it for the server learning rate adaptation. Extensive experiments on multiple
image and text classification tasks validate the great effectiveness of our method in various settings.
Our code is available at https://github.com/lancopku/FedGLAD .
âˆ—Part of the work was done while Yankai Lin and Peng Li were at Tencent.
â€ Part of the work was done while Guangxiang Zhao was at Peking University.
1 Published in Transactions on Machine Learning Research (05/2023)
Client 1 + Client2
uploaded local gradientaveraged server gradientideal server gradient!!!!!!Sample Client1and Client2
Sample Client 1and Client N
Client 2
Client NClient 1
Client 1 + ClientN
Figure 1: An illustration of the impact of client sampling through a binary (cats v.s. dogs) image classification task.
(Right Top) : If Client 1 and Client 2 are selected, their merged local samples are almost all cat images. Then the
averaged gradient deviates far away from the ideal gradient that would be averaged by all clientsâ€™ local gradients.
(Right Bottom) : If Client 1 and Client N are selected, their merged data distribution matches well with the global data
distribution merged by all clientsâ€™ data, and the averaged gradient has a more reliable direction.
1",TMLR
"Interpretable models are gaining increasing attention in the machine learning community,
and significant progress is being made to develop simple, interpretable, yet powerful deep
learning approaches. Generalized Additive Models (GAM) and Neural Additive Models
(NAM) are prime examples. Despite these methodsâ€™ great potential and popularity in crit-
ical applications, e.g., medical applications, they fail to generalize to distributions with
more than one mode (multimodal1). The main reason behind this limitation is that these
""all-fit-one"" models collapse multiple relationships by being forced to fit the data uni-
modally. We address this critical limitation by proposing interpretable multimodal net-
work frameworks capable of learning a Mixture of Neural Additive Models (MNAM). The
proposed MNAM learns relationships between input features and outputs in a multimodal
fashion and assigns a probability to each mode. The proposed method shares similari-
ties with Mixture Density Networks (MDN) while keeping the interpretability that char-
acterizes GAM and NAM. We demonstrate how the proposed MNAM balances between
rich representations and interpretability with numerous empirical observations and peda-
gogical studies. We present and discuss different training alternatives and provided ex-
tensive practical evaluation to assess the proposed framework. The code is available at
https://github.com/youngkyungkim93/MNAM.
1",TMLR
"Vision Transformers (ViT) have been shown to attain highly competitive performance
for a wide range of vision applications, such as image classiï¬cation, object detection and
semantic image segmentation. In comparison to convolutional neural networks, the Vision
Transformerâ€™s weaker inductive bias is generally found to cause an increased reliance on
model regularization or data augmentation (â€œAugRegâ€ for short) when training on smaller
training datasets. We conduct a systematic empirical study in order to better understand the
interplay between the amount of training data, AugReg, model size and compute budget.1
As one result of this study we ï¬nd that the combination of increased compute and AugReg
can yield models with the same performance as models trained on an order of magnitude
more training data: we train ViT models of various sizes on the public ImageNet-21k dataset
which either match or outperform their counterparts trained on the larger, but not publicly
available JFT-300M dataset.
1",TMLR
"Machine learning models deployed in the wild naturally encounter unlabeled samples from
both known and novel classes. Challenges arise in learning from both the labeled and
unlabeled data, in an open-world semi-supervised manner. In this paper, we introduce a
new learning framework, open-world contrastive learning (OpenCon). OpenCon tackles
the challenges of learning compact representations for both known and novel classes , and
facilitates novelty discovery along the way. We demonstrate the effectiveness of OpenCon on
challenging benchmark datasets and establish competitive performance. On the ImageNet
dataset, OpenCon significantly outperforms the current best method by 11.9% and 7.4%
on novel and overall classification accuracy, respectively. Theoretically, OpenCon can be
rigorously interpreted from an EM algorithm perspectiveâ€”minimizing our contrastive loss
partially maximizes the likelihood by clustering similar samples in the embedding space. The
code is available at https://github.com/deeplearning-wisc/opencon .
1",TMLR
"This paper presents a simple and effective visual prompting method for adapting pre-trained
models to downstream recognition tasks. Our approach is underpinned by two key designs.
First, rather than directly adding together the prompt and the image, we treat the prompt
as an extra and independent learnable entity. We show that the strategy of reconciling the
promptandtheimagematters, andfindthatwarpingthepromptaroundaproperlyshrinked
imageempiricallyworksthebest. Second, were-introducetwoâ€œoldtricksâ€commonlyusedin
building transferable adversarial examples, i.e., input diversity and gradient normalization,
into the realm of visual prompting. These techniques improve optimization and enable the
prompt to generalize better. We provide extensive experimental results to demonstrate the
effectiveness of our method. Using a CLIP model, our prompting method registers a new
record of 82.5%average accuracy across 12 popular classification datasets, substantially
surpassing the prior art by +5.2%. It is worth noting that such performance not only
surpasses linear probing by +2.2%, but, in certain datasets, is on par with the results
from fully fine-tuning. Additionally, our prompting method shows competitive performance
across different data scales and against distribution shifts.
1",TMLR
"We study a localized version of kernel ridge regression that can continuously, smoothly
interpolate the underlying function values which are highly non-linear with observed data
points. This new method can deal with the data of which (a) local density is highly uneven
and (b) the function values change dramatically in certain small but unknown regions. By
introducing a new rank-based interpolation scheme, which can be interpreted as a variable
bandwidth Nadaraya-Watson Kernel Regression, the interpolated values provided by our
local method can be proven to continuously vary with query points. Our method is scalable
by avoiding the full matrix inverse, compared with traditional kernel ridge regression.
1",TMLR
"Federated learning is a framework for training machine learning models from clients with
multiple local data sets without access to the data in its aggregate. Instead, a shared model
is jointly learned through an interactive process between a centralized server that combines
locally learned model gradients or weights from the client. However, the lack of data trans-
parency naturally raises concerns about model security. Recently, several state-of-the-art
backdoor attacks have been proposed, which achieve high attack success rates while simul-
taneously being difficult to detect, leading to compromised federated learning models. In
this paper, motivated by differences in the logits of models trained with and without the
presence of backdoor attacks, we propose a defense method that can prevent backdoor at-
tacks from influencing the model while maintaining the accuracy of the original classification
task. TAG leverages a small validation data set to estimate the most considerable change
a benign clientâ€™s local training can make to the shared model, which can be used to filter
clients from updating the shared model. Experimental results on multiple data sets show
that TAG defends against backdoor attacks even when 40 percent of user submissions to
update the shared model are malicious.
1",TMLR
"Multi-source unsupervised domain adaptation (MUDA) is a framework to address the chal-
lenge of annotated data scarcity in a target domain via transferring knowledge from multiple
annotated source domains. When the source domains are distributed, data privacy and se-
curity can become significant concerns and protocols may limit data sharing, yet existing
MUDA methods overlook these constraints. We develop an algorithm to address MUDA
when source domain data cannot be shared with the target or across the source domains.
Our method is based on aligning the distributions of source and target domains indirectly
via estimating the source feature embeddings and predicting over a confidence based combi-
nation of domain specific model predictions. We provide theoretical analysis to support our
approach and conduct empirical experiments to demonstrate that our algorithm is effective.
1",TMLR
"State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful
in leveraging a mix of labeled and unlabeled data, often via self-training or pseudo-labeling.
During pseudo-labeling, the modelâ€™s predictions on unlabeled data are used for training and
may result in confirmation bias where the model reinforces its own mistakes. In this work, we
show that SOTA SSL methods often suffer from confirmation bias and demonstrate that this
is often a result of using a poorly calibrated classifier for pseudo labeling. We introduce BaM-
SSL, an efficient Bayesian Model averaging technique that improves uncertainty quantification
in SSL methods with limited computational or memory overhead. We demonstrate that BaM-
SSL mitigates confirmation bias in SOTA SSL methods across standard vision benchmarks of
CIFAR-10, CIFAR-100 and ImageNet, giving up to 16% improvement in test accuracy on the
CIFAR-100 with 400 labels benchmark. Furthermore, we also demonstrate their effectiveness
in additional realistic and challenging problems, such as class-imbalanced datasets and in
photonics science.
1 Published in Transactions on Machine Learning Research (08/2023)
1",TMLR
"Prior work has observed that the test error of state-of-the-art deep neural networks often
continues to decrease with increasing over-parameterization, a phenomenon referred to as
doubledescent. Thisallowsdeeplearningengineerstoinstantiatelargemodelswithouthaving
to worry about over-fitting. Despite its benefits, however, prior work has shown that over-
parameterizationcanexacerbatebiasagainstminoritysubgroups. Severalfairness-constrained
DNN training methods have been proposed to address this concern. Here, we critically
examine MinDiff, a fairness-constrained training procedure implemented within TensorFlowâ€™s
Responsible AI Toolkit, that aims to achieve Equality of Opportunity. We show that although
MinDiff improves fairness for under-parameterized models, it is likely to be ineffective in
the over-parameterized regime. This is because an overfit model with zero training loss is
trivially group-wise fair on training data, creating an â€œillusion of fairness,â€ thus turning off
the MinDiff optimization (this will apply to any disparity-based measures which care about
errors or accuracy; while it wonâ€™t apply to demographic parity). We find that within specified
fairness constraints, under-parameterized MinDiff models can even have lower error compared
to their over-parameterized counterparts (despite baseline over-parameterized models having
lower error compared to their under-parameterized counterparts). We further show that
MinDiff optimization is very sensitive to choice of batch size in the under-parameterized
regime. Thus, fair model training using MinDiff requires time-consuming hyper-parameter
searches. Finally, we suggest using previously proposed regularization techniques, viz. L2,
early stopping and flooding in conjunction with MinDiff to train fair over-parameterized
models. In our results, over-parameterized models trained using MinDiff+regularization with
standard batch sizes are fairer than their under-parameterized counterparts, suggesting that
at the very least, regularizers should be integrated into fair deep learning flows, like MinDiff.
âˆ—Equal Contribution
1 Published in Transactions on Machine Learning Research (05/2023)
1",TMLR
"Graph attention networks estimate the relational importance of node neighbors to aggregate
relevant information over local neighborhoods for a prediction task. However, the inferred
attentionsarevulnerabletospuriouscorrelationsandconnectivityinthetrainingdata, ham-
pering the generalizability of models. We introduce CAR, a general-purpose regularization
framework for graph attention networks. Embodying a causal inference approach based on
invariance prediction, CAR aligns the attention mechanism with the causal effects of active
interventions on graph connectivity in a scalable manner. CAR is compatible with a variety
of graph attention architectures, and we show that it systematically improves generalizabil-
ity on various node classification tasks. Our ablation studies indicate that CAR hones in on
the aspects of graph structure most pertinent to the prediction (e.g., homophily), and does
so more effectively than alternative approaches. Finally, we also show that CAR enhances
interpretability of attention coefficients by accentuating node-neighbor relations that point
to causal hypotheses.
1",TMLR
"Industrial anomaly detection is crucial for quality control and predictive maintenance, but it
presents challenges due to limited training data, diverse anomaly types, and external factors
that alter object appearances. Existing methods commonly detect structural anomalies,
such as dents and scratches, by leveraging multi-scale features from image patches extracted
through deep pre-trained networks. However, significant memory and computational demands
often limit their practical application. Additionally, detecting logical anomaliesâ€”such as im-
ages with missing or excess elementsâ€”requires an understanding of spatial relationships that
traditional patch-based methods fail to capture. In this work, we address these limitations by
focusing on Deep Feature Reconstruction (DFR), a memory- and compute-efficient approach
for detecting structural anomalies. We further enhance DFR into a unified framework, called
ULSAD, which is capable of detecting both structural and logical anomalies. Specifically, we
refine the DFR training objective to improve performance in structural anomaly detection,
while introducing an attention-based loss mechanism using a global autoencoder-like network
to handle logical anomaly detection. Our empirical evaluation across five benchmark datasets
demonstrates the performance of ULSAD in detecting and localizing both structural and
logical anomalies, outperforming eight state-of-the-art methods. An extensive ablation study
further highlights the contribution of each component to the overall performance improvement.
Our code is available at https://github.com/sukanyapatra1997/ULSAD-2024.git.
1",TMLR
"Learning Markov decision processes (MDP) in an adversarial environment has been a chal-
lenging problem. The problem becomes even more challenging with function approximation
since the underlying structure of the loss function and transition kernel are especially hard
to estimate in a varying environment. In fact, the state-of-the-art results for linear adver-
sarial MDP achieve a regret of ËœO(K6/7)(Kdenotes the number of episodes), which admits
a large room for improvement. In this paper, we propose a novel explore-exploit algorithm
framework and investigate the problem with a new view, which reduces linear MDP into
linear optimization by subtly setting the feature maps of the bandit arms of linear opti-
mization. This new technique, under an exploratory assumption, yields an improved bound
ofËœO(K4/5)for linear adversarial MDP without access to a transition simulator. The new
view could be of independent interest for solving other MDP problems that possess a linear
structure.
1",TMLR
"Recent years have witnessed a surge of successful applications of machine reading compre-
hension. Of central importance to these tasks is the availability of massive amount of labeled
data, which facilitates training of large-scale neural networks. However, in many real-world
problems, annotated data are expensive to gather not only because of time cost and bud-
get, but also of certain domain-speciï¬c restrictions such as privacy for healthcare data. In
this regard, we propose an uncertainty-based active learning algorithm for reading compre-
hension, which interleaves data annotation and model updating to mitigate the demand of
labeling. Our key techniques are two-fold: 1) an unsupervised uncertainty-based sampling
schemethatqueriesthelabelsofthemostinformativeinstanceswithrespecttothecurrently
learned model; and 2) an adaptive loss minimization paradigm that simultaneously ï¬ts the
data and controls the degree of model updating. We demonstrate on benchmark datasets
that 25% less labeled samples suï¬ƒce to guarantee comparable, or even improved perfor-
mance. Our results show strong evidence that for label-demanding scenarios, the proposed
approach oï¬€ers a practical guide on data collection and model training.
1",TMLR
"The celebrated message-passing updates for graph neural networks allow representing large-
scale graphs with local and computationally tractable updates. However, the updates suffer
from backtracking, i.e., a message flowing through the same edge twice and revisiting the
previously visited node. Since the number of message flows increases exponentially with
the number of updates, the redundancy in local updates prevents the graph neural network
from accurately recognizing a particular message flow relevant for downstream tasks. In
this work, we propose to resolve such a redundancy issue via the non-backtracking graph
neural network (NBA-GNN) that updates a message without incorporating the message
from the previously visited node. We theoretically investigate how NBA-GNN alleviates the
over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive
performance of non-backtracking updates for stochastic block model recovery. Furthermore,
we empirically verify the effectiveness of our NBA-GNN on the long-range graph benchmark
and transductive node classification problems.
1",TMLR
"We establish a connection between stochastic optimal control and generative models based
on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic
models. In particular, we derive a Hamiltonâ€“Jacobiâ€“Bellman equation that governs the
evolution of the log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we show that
the evidence lower bound is a direct consequence of the well-known verification theorem
from control theory. Further, we can formulate diffusion-based generative modeling as a
minimization of the Kullbackâ€“Leibler divergence between suitable measures in path space.
Finally, we develop a novel diffusion-based method for sampling from unnormalized densities
â€“ a problem frequently occurring in statistics and computational sciences. We demonstrate
that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling
approaches on multiple numerical examples.
1",TMLR
"Continual segmentation has not yet tackled the challenge of improving open-vocabulary
segmentation models with training data for accurate segmentation across large, continually
expanding vocabularies. We discover that traditional continual training results in severe
catastrophic forgetting, failing to outperform a zero-shot segmentation baseline. We intro-
duceanoveltraining-freestrategy, kNN-CLIP,whichaugmentsthemodelwithadatabaseof
instance embeddings for semantic and panoptic segmentation that achieves zero forgetting.
We demonstrate that kNN-CLIP can adapt to continually growing vocabularies without the
need for retraining or large memory costs. kNN-CLIP enables open-vocabulary segmenta-
tion methods to expand their vocabularies on any domain with a single pass through the
data, while only storing compact embeddings. This approach minimizes both compute and
memory costs. kNN-CLIP achieves state-of-the-art performance across large-vocabulary se-
mantic and panoptic segmentation datasets. We hope kNN-CLIP represents a significant
step forward in enabling more efficient and adaptable continual segmentation, paving the
way for advances in real-world large-vocabulary continual segmentation methods.
Figure 1: We propose kNN-CLIP to continually expand the vocabulary space of segmentation models. Our
approach adapts to concept customization and identifying long-tailed concepts, a known challenge for CLIP
models (Udandarao et al., 2024). For concept customization, we build the supporting database for each long-
tailed concept efficiently using C2C (Prabhu et al., 2023c). We use EntitySeg (Qi et al., 2022) to generate
class-agnostic masks for entities in the database and kNN-CLIP to label these masks. At inference time, we
filter the masks from EntitySeg based on confidence thresholds for both the mask and class predictions.
âˆ—Equal advising; correspondence to Shuyang Sun ( kevinsun@robots.ox.ac.uk ) and Ameya Prabhu ( ameya@prabhu.be )
1 Published in Transactions on Machine Learning Research (07/2024)
1",TMLR
"Federated learning (FL) is an emerging paradigm in machine learning, where a shared model
is collaboratively learned using data from multiple devices to mitigate the risk of data leak-
age. While recent studies posit that Vision Transformer (ViT) outperforms Convolutional
Neural Networks (CNNs) in addressing data heterogeneity in FL, the specific architectural
components that underpin this advantage have yet to be elucidated. In this paper, we
systematically investigate the impact of different architectural elements, such as activation
functions and normalization layers, on the performance within heterogeneous FL. Through
rigorous empirical analyses, we are able to offer the first-of-its-kind general guidance on
micro-architecture design principles for heterogeneous FL.
Intriguingly, ourfindingsindicatethatwithstrategicarchitecturalmodifications, pureCNNs
can achieve a level of robustness that either matches or even exceeds that of ViTs when
handling heterogeneous data clients in FL. Additionally, our approach is compatible with
existing FL techniques and delivers state-of-the-art solutions across a broad spectrum of FL
benchmarks. The code is publicly available at https://github.com/UCSC-VLAA/FedConv .
1",TMLR
"One of the most striking findings in modern research on large language models (LLMs) is
that scaling up compute during training leads to better results. However, less attention
has been given to the benefits of scaling compute during inference. This survey focuses
on these inference-time approaches. We explore three areas under a unified mathemati-
cal formalism: token-level generation algorithms, meta-generation algorithms, and efficient
generation. Token-level generation algorithms, often called decoding algorithms, operate
by sampling a single token at a time or constructing a token-level search space and then
selecting an output. These methods typically assume access to a language modelâ€™s logits,
next-token distributions, or probability scores. Meta-generation algorithms work on partial
or full sequences, incorporating domain knowledge, enabling backtracking, and integrating
external information. Efficient generation methods aim to reduce token costs and improve
the speed of generation. Our survey unifies perspectives from three research communities:
traditional natural language processing, modern LLMs, and machine learning systems.
1 Contents
1",TMLR
"A crucial challenge in reinforcement learning is to reduce the number of interactions
with the environment that an agent requires to master a given task. Transfer learn-
ing proposes to address this issue by re-using knowledge from previously learned tasks.
However, determining which source task qualifies as the most appropriate for knowledge
extraction, as well as the choice regarding which algorithm components to transfer, rep-
resent severe obstacles to its application in reinforcement learning. The goal of this paper
is to address these issues with modular multi-source transfer learning techniques. The
proposed techniques automatically learn how to extract useful information from source
tasks, regardless of the difference in state-action space and reward function. We support
our claims with extensive and challenging cross-domain experiments for visual control.
1",TMLR
"Detecting out-of-distribution (OOD) samples is a critical task for reliable machine learning.
However, it becomes particularly challenging when the models are trained on long-tailed
datasets, as the models often struggle to distinguish tail-class in-distribution samples from
OOD samples. We examine the main challenges in this problem by identifying the trade-offs
between OOD detection and in-distribution (ID) classification, faced by existing methods.
We then introduce our method, called Representation Norm Amplification (RNA), which
solves this challenge by decoupling the two problems. The main idea is to use the norm of
the representation as a new dimension for OOD detection, and to develop a training method
that generates a noticeable discrepancy in the representation norm between ID and OOD
data, while not perturbing the feature learning for ID classification. Our experiments show
that RNA achieves superior performance in both OOD detection and classification compared
to the state-of-the-art methods, by 1.70% and 9.46% in FPR95 and 2.43% and 6.87% in
classification accuracy on CIFAR10-LT and ImageNet-LT, respectively. The code for this
work is available at https://github.com/dgshin21/RNA .
1",TMLR
"Searching the vast chemical space for drug-like molecules that bind with a protein pocket
is a challenging task in drug discovery. Recently, structure-based generative models have
been introduced which promise to be more efficient by learning to generate molecules for
any given protein structure. However, since they learn the distribution of a limited protein-
ligand complex dataset, structure-based methods do not yet outperform optimization-based
methods that generate binding molecules for just one pocket. To overcome limitations on
data while leveraging learning across protein targets, we choose to model the reward distri-
bution conditioned on pocket structure, instead of the training data distribution. We design
TacoGFN , a novel GFlowNet-based approach for structure-based drug design, which can
generate molecules conditioned on any protein pocket structure with probabilities propor-
tional to its affinity and property rewards. In the generative setting for CrossDocked2020
benchmark, TacoGFN attains a state-of-the-art success rate of 56.0%andâˆ’8.44kcal/mol
in median Vina Dock score while improving the generation time by multiple orders of mag-
nitude. Fine-tuning TacoGFN further improves the median Vina Dock score to âˆ’10.93
kcal/mol and the success rate to 88.8%, outperforming all optimization-based methods.
1",TMLR
"Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across
several mobile vision tasks, including classification and detection. Though these models have
fewer parameters, they have high latency as compared to convolutional neural network-based
models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention
(MHA) in transformers, which requires O(k2)time complexity with respect to the number of
tokens (or patches) k. Moreover, MHA requires costly operations (e.g., batch-wise matrix
multiplication) for computing self-attention, impacting latency on resource-constrained
devices. This paper introduces a separable self-attention method with linear complexity, i.e.
O(k). A simple yet effective characteristic of the proposed method is that it uses element-wise
operations for computing self-attention, making it a good choice for resource-constrained
devices. The improved model, MobileViTv2 , is state-of-the-art on several mobile vision
tasks, including ImageNet object classification and MS-COCO object detection. With about
three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet
dataset, outperforming MobileViT by about 1% while running 3.2Ã—faster on a mobile device.
Our source code is available at: https://github.com/apple/ml-cvnets .
1",TMLR
"A big challenge in branch-and-bound lies in identifying the optimal node within the search
tree from which to proceed. Current state-of-the-art selectors utilize either hand-crafted
ensembles that automatically switch between naive subnode selectors, or learned node
selectors that rely on individual node data. In contrast to existing approaches that only
consider isolated nodes, we propose a novel simulation technique that uses reinforcement
learning (RL) while considering the entire tree state. To achieve this, we train a graph neural
network that produces a probability distribution based on the path from the modelâ€™s root to
its â€œto-be-selectedâ€ leaves. Representing node-selection as a probability distribution allows
us to train a decision-making policy using state-of-the-art RL techniques that capture both
intrinsic node-quality and node-evaluation costs. Our method induces a high quality node
selection policy on a set of varied and complex problem sets, despite only being trained
on specially designed synthetic traveling salesmen problem (TSP) instances. Using such a
fixed pretrained policy shows significant improvements on several benchmarks in optimality
gap reductions and per-node efficiency under a short time limit of 45 sand demonstrates
generalization to a significantly longer 5mintime limit.
1",TMLR
"We present Cross-Client Label Propagation (XCLP) , a new method for transductive and
semi-supervised federated learning. XCLP estimates a data graph jointly from the data of
multiple clients and computes labels for the unlabeled data by propagating label information
across the graph. To avoid clients having to share their data with anyone, XCLP employs
two cryptographically secure protocols: secure Hamming distance computation andsecure
summation . We demonstrate two distinct applications of XCLP within federated learning. In
the ï¬rst, we use it in a one-shot way to predict labels for unseen test points. In the second,
we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised
setting. Experiments on both real federated and standard benchmark datasets show that in
both applications XCLP achieves higher classiï¬cation accuracy than alternative approaches.
1",TMLR
"Decentralized learning algorithms enable the training of deep learning models over large
distributed datasets, without the need for a central server. The current state-of-the-art de-
centralized algorithms mostly assume the data distributions to be Independent and Identi-
cally Distributed (IID). In practical scenarios, the distributed datasets can have significantly
different data distributions across the agents. This paper focuses on improving decentral-
ized learning on non-IID data with minimal compute and memory overheads. We propose
Neighborhood Gradient Mean (NGM) , a novel decentralized learning algorithm that modifies
the local gradients of each agent using self- and cross-gradient information. In particular,
the proposed method averages the local gradients with model-variant or data-variant cross-
gradients based on the communication budget. Model-variant cross-gradients are derivatives
of the received neighborsâ€™ model parameters with respect to the local dataset. Data-variant
cross-gradient derivatives of the local model with respect to its neighborsâ€™ datasets. The
data-variant cross-gradients are aggregated through an additional communication round.
We theoretically analyze the convergence characteristics of NGMand demonstrate its ef-
ficiency on non-IID data sampled from various vision and language datasets. Our exper-
iments demonstrate that the proposed method either remains competitive or outperforms
(by0âˆ’6%) the existing state-of-the-art (SoTA) decentralized learning algorithm on non-IID
data with significantly less compute and memory requirements. Further, we show that the
model-variant cross-gradient information available locally at each agent can improve the
performance on non-IID data by 2âˆ’20%without additional communication cost.
1",TMLR
"Deep neural network based object detectors are continuously evolving and are used in a
multitude of applications, each having its own set of requirements. While safety-critical
applications need high accuracy and reliability, low-latency tasks need resource and energy-
efficient networks. Real-time detection networks, which are a necessity in high-impact real-
world applications, are continuously proposed but they overemphasize the improvements in
accuracy and speed while other capabilities such as versatility, robustness, resource, and en-
ergy efficiency are omitted. A reference benchmark for existing networks does not exist nor
does a standard evaluation guideline for designing new networks, which results in ambiguous
and inconsistent comparisons. We, therefore, conduct a comprehensive study on multiple
real-time detection networks (anchor-based, keypoint-based, and transformer-based) on a
wide range of datasets and report results on an extensive set of metrics. We also study the
impact of variables such as image size, anchor dimensions, confidence thresholds, and archi-
tecture layers on the overall performance. We analyze the robustness of detection networks
against distribution shift, natural corruptions, and adversarial attacks. Also, we provide the
calibration analysis to gauge the reliability of the predictions. Finally, to highlight the real-
world impact, we conduct two unique case studies, on autonomous driving and healthcare
application. To further gauge the capability of networks in critical real-time applications,
we report the performance after deploying the detection networks on edge devices. Our
extensive empirical study can act as a guideline for the industrial community to make an
informed choice on the existing networks. We also hope to inspire the research community
towards a new direction of design and evaluation of networks that focuses on the bigger and
holistic overview for a far-reaching impact.
1",TMLR
"Graph Neural Networks (GNNs) with numerical node features and graph structure as inputs
have demonstrated superior performance on various semi-supervised learning tasks with
graph data. However, the numerical node features utilized by GNNs are commonly extracted
from raw data which is of text or tabular (numeric/categorical) type in most real-world
applications. The best models for such data types in most standard supervised learning
settings with IID (non-graph) data are not simple neural network layers and thus are not
easily incorporated into a GNN. Here we propose a robust stacking framework that fuses
graph-aware propagation with arbitrary models intended for IID data, which are ensembled
and stacked in multiple layers. Our layer-wise framework leverages bagging and stacking
strategies to enjoy strong generalization, in a manner which effectively mitigates label leakage
and overfitting. Across a variety of graph datasets with tabular/text node features, our
method achieves comparable or superior performance relative to both tabular/text and graph
neural network models, as well as existing state-of-the-art hybrid strategies that combine the
two.
1",TMLR
"Prevailing supervised deep graph learning models often suffer from the issue of label scarcity,
leading to performance degradation in the face of limited annotated data. Although numerous
graph few-shot learning (GFL) methods have been developed to mitigate this problem, they
tend to rely excessively on labeled data. This over-reliance on labeled data can result
in impaired generalization ability in the test phase due to the existence of a distribution
gap. Moreover, existing GFL methods lack a general purpose as their designs are coupled
with task or data-specific characteristics. To address these shortcomings, we propose a
novel Self-Distilled GraphFew-shot Learning framework (SDGFL) that is both general
and effective. SDGFL leverages a self-distilled contrastive learning procedure to boost
GFL. Specifically, our model first pre-trains a graph encoder with contrastive learning using
unlabeled data. Later, the trained encoder is frozen as a teacher model to distill a student
model with a contrastive loss. The distilled model is then fed to GFL. By learning data
representation in a self-supervised manner, SDGFL effectively mitigates the distribution
gap and enhances generalization ability. Furthermore, our proposed framework is task and
data-independent, making it a versatile tool for general graph mining purposes. To evaluate
the effectiveness of our proposed framework, we introduce an information-based measurement
that quantifies its capability. Through comprehensive experiments, we demonstrate that
SDGFL outperforms state-of-the-art baselines on various graph mining tasks across multiple
datasets in the few-shot scenario. We also provide a quantitative measurement of SDGFLâ€™s
superior performance in comparison to existing methods.
1",TMLR
"The ability to leverage heterogeneous robotic experience from different robots and tasks
to quickly master novel skills and embodiments has the potential to transform robot learn-
ing. Inspired by recent advances in foundation models for vision and language, we propose a
multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named
RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-
labelled visual experience. This data spans a large repertoire of motor control skills from
simulated and real robotic arms with varying sets of observations and actions. With Robo-
Cat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well
as through adaptation using only 100â€“1000 examples for the target task. We also show how
a trained model itself can be used to generate data for subsequent training iterations, thus
providing a basic building block for an autonomous improvement loop. We investigate the
agentâ€™s capabilities, with large-scale evaluations both in simulation and on three different
real robot embodiments. We find that as we grow and diversify its training data, RoboCat
not only shows signs of cross-task transfer, but also becomes more efficient at adapting to
new tasks.
1",TMLR
"Concept-basedinterpretabilityaddressestheopacityofdeepneuralnetworksbyconstructing
an explanation for a modelâ€™s prediction using high-level units of information referred to as
concepts. Research in this area, however, has been mainly focused on image and graph-
structured data, leaving high-stakes tasks whose data is tabular out of reach of existing
methods. In this paper, we address this gap by introducing the first definition of what a
high-level concept may entail in tabular data. We use this definition to propose Tabular
Concept Bottleneck Models (TabCBMs), a family of interpretable self-explaining neural
architectures capable of learning high-level concept explanations for tabular tasks. As our
method produces concept-based explanations both when partial concept supervision or no
concept supervision is available at training time, it is adaptable to settings where concept
annotations are missing. We evaluate our method in both synthetic and real-world tabular
tasks and show that TabCBM outperforms or performs competitively compared to state-of-
the-art methods, while providing a high level of interpretability as measured by its ability to
discover known high-level concepts. Finally, we show that TabCBM can discover important
high-level concepts in synthetic datasets inspired by critical tabular tasks (e.g., single-cell
RNAseq) and allows for human-in-the-loop concept interventions in which an expert can
identify and correct mispredicted concepts to boost the modelâ€™s performance.
1",TMLR
"Generative neural models hold great promise in enhancing programming education by syn-
thesizing new content. We seek to design neural models that can automatically generate
programming tasks for a given specification in the context of visual programming domains.
Despite the recent successes of large generative models like GPT-4, our initial results show
that these models are ineffective in synthesizing visual programming tasks and struggle with
logicalandspatialreasoning. Weproposeanovelneuro-symbolictechnique, NeurTaskSyn ,
that can synthesize programming tasks for a specification given in the form of desired
programming concepts exercised by its solution code and constraints on the visual task.
NeurTaskSyn has two components: the first component is trained via imitation learn-
ing procedure to generate possible solution codes, and the second component is trained via
reinforcement learning procedure to guide an underlying symbolic execution engine that
generates visual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn
through an extensive empirical evaluation and a qualitative study on reference tasks taken
from the Hour of Code: Classic Maze challenge by Code.org and the Intro to Programming
with Karel course by CodeHS.com.
1",TMLR
"Several recent studies have demonstrated that deep-learning based image generation models,
such as GANs, can be uniquely identified, and possibly even reverse-engineered, by the
fingerprints they leave on their output images. We extend this research to single image
super-resolution (SISR) networks. Compared to previously studied models, SISR networks
areauniquelychallengingclassofimagegenerationmodelfromwhichtoextractandanalyze
fingerprints, as they can often generate images that closely match the corresponding ground
truth and thus likely leave little flexibility to embed signatures. We take SISR models as
examples to investigate if the findings from the previous work on fingerprints of GAN-based
networks are valid for general image generation models. We show that SISR networks with
a high upscaling factor or trained using adversarial loss leave highly distinctive fingerprints,
and that under certain conditions, some SISR network hyperparameters can be reverse-
engineered from these fingerprints.
1",TMLR
"Policy iteration (PI) is a fundamental policy search algorithm in standard reinforcement
learning (RL) setting, which can be shown to converge to an optimal policy by policy
improvement theorems. However, the standard PI relies on Bellmanâ€™s Principle of Optimality,
which might be violated by some specifications of objectives (also known as time-inconsistent
(TIC) objectives), suchas non-exponentiallydiscountedreward functions. Theuse of standard
PI under TIC objectives has thus been marked with questions regarding the convergence of its
policy improvement scheme and the optimality of its termination policy, often leading to its
avoidance. In this paper, we consider an infinite-horizon TIC RL setting and formally present
an alternative type of optimality drawn from game theory, i.e., subgame perfect equilibrium
(SPE), that attempts to resolve the aforementioned questions. We first analyze standard PI
under the SPE type of optimality, revealing its merits and insufficiencies. Drawing on these
observations, we propose backward Q-learning (bwdQ), a new algorithm in the approximate
PI family that targets SPE policy under non-exponentially discounted reward functions.
Finally, with two TIC gridworld environments, we demonstrate the implications of our
theoretical findings on the behavior of bwdQ and other approximate PI variants.
1",TMLR
"A challenge in fair algorithm design is that, while there are compelling notions of individual
fairness, these notions typically do not satisfy desirable composition properties, and down-
stream applications based on fair classiï¬ers might not preserve fairness. To study fairness
under composition, Dwork & Ilvento (2019) introduced an archetypal problem called fair-
cohort-selection problem , where a single fair classiï¬er is composed with itself to select a
group of candidates of a given size, and proposed a solution to this problem. In this work
we design algorithms for selecting cohorts that not only preserve fairness, but also maximize
the utility of the selected cohort under two notions of utility that we introduce and motivate.
We give optimal (or approximately optimal) polynomial-time algorithms for this problem
in both an oï¬„ine setting, and an online setting where candidates arrive one at a time and
are classiï¬ed as they arrive.
1",TMLR
"Reinforcement learning (RL) allows an agent interacting sequentially with an environment
to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm,
the agent goes beyond the limit of the expected value, to capture the underlying probabil-
ity distribution of the return across all time steps . The set of DistrRL algorithms has led
to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully
understood, especially in the control case. In this paper, we present the simpler one-step
distributional reinforcement learning (OS-DistrRL) framework encompassing only the ran-
domness induced by the one-step dynamics of the environment. Contrary to DistrRL, we
show that our approach comes with a uniï¬ed theory for both policy evaluation and control.
Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure con-
vergence analysis. The proposed approach compares favorably with categorical DistrRL on
various environments.
1",TMLR
"EnsemblingcanimprovetheperformanceofNeuralNetworks, butexistingapproachesstrug-
gle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore,
existing methods construct equally weighted ensembles, and this is likely to be vulnera-
ble to the failure modes of the weaker architectures. By viewing ensembling as approxi-
mately marginalising over architectures we construct ensembles using the tools of Bayesian
Quadrature â€“ tools which are well suited to the exploration of likelihood surfaces with dis-
persed, narrowpeaks. Additionally, theresultingensemblesconsistofarchitecturesweighted
commensurate with their performance. We show empirically â€“ in terms of test likelihood,
accuracy, and expected calibration error â€“ that our method outperforms state-of-the-art
baselines, and verify via ablation studies that its components do so independently.
1",TMLR
"State-of-the-art (SOTA) approaches to deep network (DN) training overparametrize the
model and then prune a posteriori to obtain a â€œwinning ticketâ€ subnetwork that can achieve
high accuracy. Using a recently developed spline interpretation of DNs, we obtain novel
insights into how DN pruning affects its mapping. In particular, under the realm of spline
operators, we are able to pinpoint the impact of pruning onto the DNâ€™s underlying input
space partition and per-region affine mappings, opening new avenues in understanding why
and when are pruned DNs able to maintain high performance. We also discover that a
DNâ€™s spline mapping exhibits an early-bird (EB) phenomenon whereby the splineâ€™s partition
converges at early training stages, bridging the recently developed DN spline theory and
lottery ticket hypothesis of DNs. We finally leverage this new insight to develop a principled
and efficient pruning strategy whose goal is to prune isolated groups of nodes that have a
redundant contribution in the forming of the spline partition. Extensive experiments on four
networksandthreedatasetsvalidatethatournewspline-basedDNpruningapproachreduces
training FLOPs by up to 3.5Ã—while achieving similar or even better accuracy than current
state-of-the-art methods. Code is available at https://github.com/RICE-EIC/Spline-EB .
1",TMLR
"A common way of partitioning graphs is through minimum cuts. One drawback of classi-
cal minimum cut methods is that they tend to produce small groups, which is why more
balanced variants such as normalized and ratio cuts have seen more success. However, we
believe that with these variants, the balance constraints can be too restrictive for some ap-
plications like for clustering of imbalanced datasets, while not being restrictive enough for
when searching for perfectly balanced partitions. Here, we propose a new graph cut algo-
rithm for partitioning graphs under arbitrary size constraints. We formulate the graph cut
problem as a Gromov-Wasserstein with a concave regularizer problem. We then propose to
solve it using an accelerated proximal GD algorithm which guarantees global convergence to
a critical point, results in sparse solutions and only incurs an additional ratio of O(log(n))
compared to the classical spectral clustering algorithm but was seen to be more efficient.
1",TMLR
"Combining reinforcement learning with language grounding is challenging as the agent needs
to explore the environment while simultaneously learning multiple language-conditioned
tasks. To address this, we introduce a novel method: the compositionally-enabled rein-
forcement learning language agent (CERLLA). Our method reduces the sample complexity
of tasks specified with language by leveraging compositional policy representations and a
semantic parser trained using reinforcement learning and in-context learning. We evaluate
our approach in an environment requiring function approximation and demonstrate com-
positional generalization to novel tasks. Our method significantly outperforms the previous
best non-compositional baseline in terms of sample complexity on 162tasks designed to
test compositional generalization. Our model attains a higher success rate and learns in
fewer steps than the non-compositional baseline. It reaches a success rate equal to an oracle
policyâ€™s upper-bound performance of 92%. With the same number of environment steps,
the baseline only reaches a success rate of 80%.
1",TMLR
"Neurally-parameterized Structural Causal Models in the Pearlian notion to causality, re-
ferred to as NCM, were recently introduced as a step towards next-generation learning
systems. However, said NCM are only concerned with the learning aspect of causal infer-
ence and totally miss out on the architecture aspect. That is, actual causal inference within
NCM is intractable in that the NCM wonâ€™t return an answer to a query in polynomial
time. This insight follows as corollary to the more general statement on the intractability
of arbitrary structural causal model (SCM) parameterizations, which we prove in this work
through classical 3-SAT reduction. Since future learning algorithms will be required to deal
with both high dimensional data and highly complex mechanisms governing the data, we
ultimately believe work on tractable inference for causality to be decisive. We also show
that not all â€œcausalâ€ models are created equal. More specifically, there are models capable
of answering causal queries that are not SCM, which we refer to as partially causal models
(PCM). We provide a tabular taxonomy in terms of tractability properties for all of the dif-
ferent model families, namely correlation-based, PCM and SCM. To conclude our work, we
also provide some initial ideas on how to overcome parts of the intractability of causal infer-
ence with SCM by showing an example of how parameterizing an SCM with SPN modules
can at least allow for tractable mechanisms.
Withthisworkwe hopethatourinsightscanraiseawarenessforthisnovelresearchdirection
since achieving success with causality in real world downstream tasks will not only depend
on learning correct models but also require having the practical ability to gain access to
model inferences.
1",TMLR
"Monge map refers to the optimal transport map between two probability distributions and
provides a principled approach to transform one distribution to another. Neural network-
based optimal transport map solver has gained great attention in recent years. Along this
line, we present a scalable algorithm for computing the neural Monge map between two
probability distributions. Our algorithm is based on a weak form of the optimal transport
problem, thus it only requires samples from the marginals instead of their analytic expres-
sions, and can be applied in large-scale settings. Furthermore, using the duality gap we
prove rigorously a posteriori error analysis for the method. Our algorithm is suitable for
general cost functions, compared with other existing methods for estimating Monge maps
using samples, which are usually for quadratic costs. The performance of our algorithms
is demonstrated through a series of experiments with both synthetic and realistic data,
including text-to-image generation, class-preserving map, and image inpainting tasks.
1",TMLR
"Multimodal Variational Autoencoders (VAEs) represent a promising group of generative
models that facilitate the construction of a tractable posterior within the latent space given
multiple modalities. Previous studies have shown that as the number of modalities increases,
the generative quality of each modality declines. In this study, we explore an alternative
approach to enhance the generative performance of multimodal VAEs by jointly modeling the
latent space of independently trained unimodal VAEs using score-based models (SBMs). The
role of the SBM is to enforce multimodal coherence by learning the correlation among the
latent variables. Consequently, our model combines a better generative quality of unimodal
VAEs with coherent integration across different modalities using the latent score-based model.
In addition, our approach provides the best unconditional coherence. The code can be found
at https://github.com/rooshenasgroup/sbmae
1",TMLR
"Decentralized learning algorithms empower interconnected devices to share data and computa-
tional resources to collaboratively train a machine learning model without the aid of a central
coordinator. In the case of heterogeneous data distributions at the network nodes, collabora-
tion can yield predictors with unsatisfactory performance for a subset of the devices. For this
reason, in this work, we consider the formulation of a distributionally robust decentralized
learning task and we propose a decentralized single loop gradient descent/ascent algorithm
(AD-GDA) to directly solve the underlying minimax optimization problem. We render our
algorithm communication-efficient by employing a compressed consensus scheme and we
provide convergence guarantees for smooth convex and non-convex loss functions. Finally, we
corroborate the theoretical findings with empirical results that highlight AD-GDAâ€™s ability
to provide unbiased predictors and to greatly improve communication efficiency compared to
existing distributionally robust algorithms.
1",TMLR
"Deep neural networks (DNNs) have greatly impacted numerous fields over the past
decade. Yet despite exhibiting superb performance over many problems, their black-
box nature still poses a significant challenge with respect to explainability. Indeed,
explainable artificial intelligence (XAI) is crucial in several fields, wherein the an-
swer aloneâ€”sans a reasoning of how said answer was derivedâ€”is of little value.
This paper uncovers a troubling property of explanation methods for image-based
DNNs: by making small visual changes to the input imageâ€”hardly influencing the
networkâ€™s outputâ€”we demonstrate how explanations may be arbitrarily manipulated
through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-and-
data XAI-agnostic, adversarial attack on XAI algorithms, only requires access to the
output logits of a classifier and to the explanation map; these weak assumptions
render our approach highly useful where real-world models and data are concerned.
We compare our methodâ€™s performance on two benchmark datasetsâ€”CIFAR100 and
ImageNetâ€”using four different pretrained deep-learning models: VGG16-CIFAR100,
VGG16-ImageNet, MobileNet-CIFAR100, and Inception-v3-ImageNet. We find that
the XAI methods can be manipulated without the use of gradients or other model
internals. AttaXAI successfully manipulates an image such that several XAI meth-
ods output a specific explanation map. To our knowledge, this is the first such
method in a black-box setting, and we believe it has significant value where ex-
plainability is desired, required, or legally mandatory. The code is available at
https://github.com/razla/Foiling-Explanations-in-Deep-Neural-Networks .
Keywords: deep learning, computer vision, adversarial attack, evolutionary algo-
rithm, explainable artificial intelligence
1",TMLR
"Clinical deep learning systems often generate population-based and opaque medical diag-
noses. This is in contrast to how primary care physicians make decisions, often adapting
population-based protocols to the unique patient under consideration. Inspired by the work-
flow of such physicians, we develop a framework for learning embeddings, referred to as pa-
tient cardiac prototypes (PCPs), which capture information that is unique to an individual
patientâ€™s electrocardiogram (ECG) data. Through rigorous evaluation on three publicly-
available ECG datasets, we show that PCPs allow researchers to inspect why a particular
diagnosis was made. We also demonstrate that PCPs are effective dataset distillers, where
they can be used to train a model in lieu of a dataset orders of magnitude larger to achieve
comparable performance. We show that PCPs can also be exploited to retrieve similar
patient data across clinical databases. Our framework contributes to the development of
transparent and patient-specific clinical deep learning systems.
1",TMLR
"Vision-based imitation learning has shown promising capabilities of endowing robots with
various motion skills given visual observation. However, current visuomotor policies fail
to adapt to drastic changes in their visual observations. We present Perception Stitching
that enables strong zero-shot adaptation to large visual changes by directly stitching novel
combinations of visual encoders. Our key idea is to enforce modularity of visual encoders
by aligning the latent visual features among different visuomotor policies. Our method
disentangles the perceptual knowledge with the downstream motion skills and allows the
reuse of the visual encoders by directly stitching them to a policy network trained with
partially different visual conditions. We evaluate our method in various simulated and real-
world manipulation tasks. While baseline methods failed at all attempts, our method could
achieve zero-shot success in real-world visuomotor tasks. Our quantitative and qualitative
analysis of the learned features of the policy network provides more insights into the high
performance of our proposed method.
Figure 1: Perception Stitching : â€œPolicy Aâ€ was trained with an in-hand camera and a front-view camera. â€œPolicy Bâ€ was
trained with a close-up camera and a side-view camera. Perception Stitching enables zero-shot stitching of the original Policy
A and B by reusing their relevant components for each sensing configuration to form a â€œPolicy Câ€. â€œPolicy Câ€ can maintain
strong zero-shot transfer performance with an in-hand camera and a side-view camera.
1",TMLR
"Receiver operating characteristic (ROC) curves are a popular method of summarising the
performance of classifiers. The ROC curve describes the separability of the distributions of
predictions from a two-class classifier. There are a variety of situations in which an analyst
seeks to aggregate multiple ROC curves into a single representative example. A number
of methods of doing so are available; however, there is a degree of subtlety that is often
overlooked when selecting the appropriate one. An important component of this relates to
the interpretation of the decision process for which the classifier will be used. This paper
summarises a number of methods of aggregation and carefully delineates the interpretations
of each in order to inform their correct usage. A toy example is provided that highlights
how an injudicious choice of aggregation method can lead to erroneous conclusions.
1",TMLR
"Gradient scarcity emerges when learning graphs by minimizing a loss on a subset of nodes
under the semi-supervised setting. It consists in edges between unlabeled nodes that are far
from the labeled ones receiving zero gradients. The phenomenon was ï¬rst described when
jointly optimizing the graph and the parameters of a shallow Graph Neural Network (GNN)
using a single loss function. In this work, we give a precise mathematical characterization
of this phenomenon, and prove that it also emerges in bileveloptimization. While for GNNs
gradient scarcity occurs due to their ï¬nite receptive ï¬eld, we show that it also occurs with
the Laplacian regularization as gradients decrease exponentially in amplitude with distance
to labeled nodes, despite the inï¬nite receptive ï¬eld of this model. We study several solutions
to this issue including latent graph learning using a Graph-to-Graph model (G2G), graph
regularization to impose a prior structure on the graph, and reducing the graph diameter
by optimizing for a larger set of edges. Our empirical results validate our analysis and
show that this issue also occurs with the Approximate Personalized Propagation of Neural
Predictions (APPNP), which approximates a model of inï¬nite receptive ï¬eld.
1",TMLR
"Neural controlled differential equations (Neural CDEs) are a continuous-time extension
of recurrent neural networks (RNNs), achieving state-of-the-art (SOTA) performance at
modelling functions of irregular time series. In order to interpret discrete data in continuous
time, current implementations rely on non-causal interpolations of the data. This is fine when
the whole time series is observed in advance, but means that Neural CDEs are not suitable
for use in online prediction tasks , where predictions need to be made in real-time: a major
use case for recurrent networks. Here, we show how this limitation may be rectified. First,
we identify several theoretical conditions that control paths for Neural CDEs should satisfy,
such as boundedness and uniqueness. Second, we use these to motivate the",TMLR
"Federated learning (FL) has been proposed as a privacy-preserving approach in distributed
machinelearning. Afederatedlearningarchitectureconsistsofacentralserverandanumber
of clients that have access to private, potentially sensitive data. Clients are able to keep
their data in their local machines and only share their locally trained modelâ€™s parameters
with a central server that manages the collaborative learning process. FL has delivered
promising results in real-life scenarios, such as healthcare, energy, and finance. However,
when the number of participating clients is large, the overhead of managing the clients slows
down the learning. Thus, client selection has been introduced as an approach to limit the
number of communicating parties at every step of the process. Since the early naÃ¯ve random
selection of clients, several client selection methods have been proposed in the literature.
Unfortunately, given that this is an emergent field, there is a lack of a taxonomy of client
selection methods, making it hard to compare approaches. In this paper, we propose a
taxonomy of client selection in Federated Learning that enables us to shed light on current
progress in the field and identify potential areas of future research in this promising area of
machine learning.
1",TMLR
"Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in
deep learning to provide rigorously defined privacy, which requires gradient clipping to
bound the maximum norm of individual gradients and additive isotropic Gaussian noise.
With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that
randomly sparsifying gradients before clipping and noisification adjusts a trade-off between
internal components of the convergence bound and leads to a smaller upper bound when the
noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that
the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling
noisification or gradient clipping eliminates the trade-off in the bound. This observation
is indicative, as it implies DP-SGD has special inherent room for (even simply random)
gradient compression. To verify the observation an utilize it, we propose an efficient and
lightweight extension using random sparsification (RS) to strengthen DP-SGD. Experiments
with various DP-SGD frameworks show that RS can improve performance. Additionally,
the produced sparse gradients of RS exhibit advantages in reducing communication cost and
strengthening privacy against reconstruction attacks, which are also key problems in private
machine learning.
1",TMLR
"Hamiltonian mechanics is one of the cornerstones of the natural sciences. Recently there
has been significant interest in learning Hamiltonian systems in a free-form way directly
from trajectory data. Previous methods have tackled the problem of learning from many
short, low-noise trajectories, but learning from a small number of long, noisy trajectories,
whilst accounting for model uncertainty has not been addressed. In this work, we present a
Gaussian process model for Hamiltonian systems with efficient decoupled parameterisation,
and introduce an energy-conserving shooting method that allows robust inference from both
short and long trajectories. We demonstrate the methodâ€™s success in learning Hamiltonian
systems in various data settings.
1",TMLR
"Developing meta-learning algorithms that are un-biased toward a subset of training tasks
often requires hand-designed criteria to weight tasks, potentially resulting in sub-optimal
solutions. In this paper, we introduce a new principled and fully-automated task-weighting
algorithm for meta-learning methods. Specifically, we frame the task-weighting problem
as a trajectory optimization problem, where the weights of tasks within a mini-batch are
treated as an action, and the meta-parameter of interest is viewed as the system state. Such
a modelling allows us to employ the iterative linear quadratic regulator to determine the
optimal task weights. We theoretically show that the proposed algorithm converges to an
Ïµ0-stationary point, and empirically demonstrate that the proposed approach out-performs
common hand-engineering weighting methods on two few-shot learning benchmarks.
1",TMLR
"Weconsideraregularizedexpectedrewardoptimizationprobleminthenon-oblivioussetting
that covers many existing problems in reinforcement learning (RL). In order to solve such
an optimization problem, we apply and analyze the classical stochastic proximal gradient
method. In particular, the method has shown to admit an O(Ïµâˆ’4)sample complexity to an
Ïµ-stationary point, under standard conditions. Since the variance of the classical stochastic
gradient estimator is typically large, which slows down the convergence, we also apply an
efficient stochastic variance-reduce proximal gradient method with an importance sampling
based ProbAbilistic Gradient Estimator (PAGE). Our analysis shows that the sample com-
plexity can be improved from O(Ïµâˆ’4)toO(Ïµâˆ’3)under additional conditions. Our results on
the stochastic (variance-reduced) proximal gradient method match the sample complexity of
their most competitive counterparts for discounted Markov decision processes under similar
settings. To the best of our knowledge, the proposed methods represent a novel approach
in addressing the general regularized reward optimization problem.
1",TMLR
"Inanomalydetection, thedegreeofirregularityisoftensummarizedasareal-valuedanomaly
score. We address the problem of attributing such anomaly scores to input features for inter-
preting the results of anomaly detection. We particularly investigate the use of the Shapley
value for attributing anomaly scores of semi-supervised detection methods. We propose a
characteristic function speciï¬cally designed for attributing anomaly scores. The idea is to
approximate the absence of some features by locally minimizing the anomaly score with
regard to the to-be-absent features. We examine the applicability of the proposed charac-
teristic function and other general approaches for interpreting anomaly scores on multiple
datasets and multiple anomaly detection methods. The results indicate the potential utility
of the attribution methods including the proposed one.
1",TMLR
"Modern deep neural networks are highly over-parameterized compared to the data on which
they are trained, yet they often generalize remarkably well. A ï¬‚urry of recent work has asked:
why do deep networks not overï¬t to their training data? In this work, we make a series of
empirical observations that investigate and extend the hypothesis that deeper networks are
inductively biased to ï¬nd solutions with lower eï¬€ective rank embeddings. We conjecture that
this bias exists because the volume of functions that maps to low eï¬€ective rank embedding
increases with depth. We show empirically that our claim holds true on ï¬nite width linear
and non-linear models on practical learning paradigms and show that on natural data, these
are often the solutions that generalize well. We then show that the simplicity bias exists
at both initialization and after training and is resilient to hyper-parameters and learning
methods. We further demonstrate how linear over-parameterization of deep non-linear models
can be used to induce low-rank bias, improving generalization performance on CIFAR and
ImageNet without changing the modeling capacity.
1",TMLR
"Data augmentation is widely used in scenarios where one needs to train a neural network given
little labeled data. A common practice of augmentation training is applying a composition
of multiple transformations sequentially to the data. Existing augmentation methods such
as RandAugment rely on domain expertise to select a list of transformations, while other
methods such as AutoAugment formulate an optimization problem over a search space of size
kd, which is the number of sequences of length d, given a list of ktransformation functions.
In this paper, we focus on designing efficient algorithms whose running time complexity is
much faster than the worst-case complexity of O(kd), provably. We propose a new algorithm
to search for a binary tree-structured composition of ktransformations, where each tree node
corresponds to one transformation. The binary tree generalizes sequential augmentations,
such as the one constructed by SimCLR. Using a top-down, recursive search procedure, our
algorithm achieves a runtime complexity of O(2dk), which is much faster than O(kd)ask
increases above 2. We apply the algorithm to tackle data distributions with heterogeneous
subpopulations, by searching for one tree in each subpopulation, and then learn a weighted
combination, leading to a forestof the trees.
We validate the proposed algorithms on numerous graph and image data sets, including
a multi-label graph classification data set we collected. The data set exhibits significant
variations in the sizes of graphs and their average degrees, making it ideal for studying data
augmentation. We show that our approach can reduce the computation cost (measured by
GPU hours) by 43% over existing augmentation search methods while improving performance
by 4.3%. Extensive experiments on contrastive learning also validate the benefit of our
approach. The tree structures can be used to interpret the relative importance of each
transformation, such as identifying the important transformations on small vs. large graphs.
1",TMLR
"Optimal transport (OT) is a powerful framework to compare probability measures, a funda-
mental task in many statistical and machine learning problems. Substantial advances have
been made in designing OT variants which are either computationally and statistically more
efficient or robust. Among them, sliced OT distances have been extensively used to mitigate
optimal transportâ€™s cubic algorithmic complexity and curse of dimensionality. In parallel,
unbalanced OT was designed to allow comparisons of more general positive measures, while
being more robust to outliers. In this paper, we bridge the gap between those two concepts
and develop a general framework for efficiently comparing positive measures. We notably
formulate two different versions of sliced unbalanced OT, and study the associated topol-
ogy and statistical properties. We then develop a GPU-friendly Frank-Wolfe like algorithm
to compute the corresponding loss functions, and show that the resulting methodology is
modular as it encompasses and extends prior related work. We finally conduct an empiri-
cal analysis of our loss functions and methodology on both synthetic and real datasets, to
illustrate their computational efficiency, relevance and applicability to real-world scenarios
including geophysical data.
1",TMLR
"Capturing the correct tail behavior is difficult, yet essential for a faithful generative model.
In this work, we provide an improved framework for training flows-based models with robust
capabilities to capture the tail behavior of mixed-tail data. We propose a combination of a
tail-flexible base distribution and a robust training algorithm to enable the flow to model
heterogeneous tail behavior in the target distribution. We support our claim with extensive
experiments on synthetic and real world data.
1",TMLR
"State-of-the-art self-supervised representation learning methods for Graphs are typically
based on contrastive learning (CL) principles. These CL objective functions can be posed
as a supervised discriminative task using â€˜hardâ€™labels that consider any minor augmented
pairs of graphs as â€˜equally positiveâ€™. However, such a notion of â€˜equalâ€™ pairs is incorrect
for graphs as even a smaller â€˜discreteâ€™ perturbation may lead to large semantic changes
that should be carefully encapsulated within the learned representations. This paper pro-
poses a novel CL framework for GNNs, called Teacher-guided Graph Contrastive Learning
(TGCL), that incorporates â€˜softâ€™ pseudo-labels to facilitate a more regularized discrimi-
nation. In particular, we propose a teacher-student framework where the student learns
the representation by distilling the teacherâ€™s perception. Our TGCL framework can be
adapted to existing CL methods to enhance their performance. Our empirical findings val-
idate these claims on both inductive and transductive settings across diverse downstream
tasks, including molecular graphs and social networks. Our experiments on benchmark
datasets demonstrate that our framework consistently improves the average AUROC scores
for moleculesâ€™ property prediction and social network link prediction. Our code is available
at https://github.com/jayjaynandy/TGCL.
1",TMLR
"Compressed sensing combines the power of convex optimization techniques with a sparsity-
inducing prior on the signal space to solve an underdetermined system of equations. For many
problems, the sparsifying dictionary is not directly given, nor its existence can be assumed.
Besides, the sensing matrix can change across different scenarios. Addressing these issues
requires solving a sparse representation learning problem, namely dictionary learning, taking
into account the epistemic uncertainty of the learned dictionaries and, finally, jointly learning
sparse representations and reconstructions under varying sensing matrix conditions. We
address both concerns by proposing a variant of the LISTA architecture. First, we introduce
Augmented Dictionary Learning ISTA ( A-DLISTA ), which incorporates an augmentation
module to adapt parameters to the current measurement setup. Then, we propose to learn
a distribution over dictionaries via a variational approach, dubbed Variational Learning
ISTA (VLISTA).VLISTA exploits A-DLISTA as the likelihood model and approximates
a posterior distribution over the dictionaries as part of an unfolded LISTA-based recovery
algorithm. As a result, VLISTA provides a probabilistic way to jointly learn the dictionary
distribution and the reconstruction algorithm with varying sensing matrices. We provide
theoretical and experimental support for our architecture and show that our model learns
calibrated uncertainties.
1",TMLR
"Contrastive Learning (CL) aims to create effective embedding for input data by minimizing
the distance between positive pairs, i.e., different augmentations or views of the same sample.
To avoid degeneracy, CL also employs auxiliary loss to maximize the discrepancy between
negative pairs formed with views of distinct samples. As a self-supervised learning strategy,
CLinherentlyattemptstoclusterinputdataintonaturalgroups. However, theoftenimproper
trade-off between the attractive and repulsive forces, respectively induced by positive and
negative pairs, can lead to deformed clustering, particularly when the number of clusters k
is unknown. To address this, we propose NRCC, a CL-based deep clustering framework that
generates cluster-friendly embeddings. NRCC repurposes Stochastic Gradient Hamiltonian
Monte Carlo sampling as an approximately invariant data augmentation, to curate hard
negative pairs that judiciously enhance and balance the two adversarial forces through
a regularizer. By preserving the cluster structure in the CL embedding, NRCC retains
local density landscapes in lower dimensions through neighborhood-conserving projections.
This enables the application of mode-seeking clustering algorithms, typically hindered by
high-dimensional CL feature spaces, to achieve exceptional accuracy without needing a
predetermined k. NRCCâ€™s superiority is demonstrated across various datasets with different
scales and cluster structures, outperforming 20 state-of-the-art methods.
1",TMLR
"ThismanuscripttakesacriticallookattheinteractionsbetweenKoopmantheoryandrepro-
ducing kernel Hilbert spaces with an eye towards giving a tighter theoretical foundation for
Koopman based dynamic mode decomposition (DMD), a data driven method for modeling
a nonlinear dynamical system from snapshots. In particular, this paper explores the various
necessary conditions imposed on the dynamics when a Koopman operator is bounded or
compact over a reproducing kernel Hilbert space.
Ultimately, it is determined that for many RKHSs, the imposition of compactness or bound-
edness on a Koopman operator forces the dynamics to be aï¬ƒne.
However, a numerical method is still recovered in more general cases through the consid-
eration of the Koopman operator as a closed and densely deï¬ned operator, which requires
a closer examination of the connection between the Koopman operator and a RKHS. By
abandoning the feature representation of RKHSs, the tools of function theory are brought to
bear, and a simpler algorithm is obtained for DMD than what was introduced in Williams
et al (2016). This algorithm is also generalized to utilize vector valued RKHSs.
1",TMLR
"Tabular data is arguably one of the most ubiquitous data structures in application domains
such as science, healthcare, finance and manufacturing. Given the recent success of deep
learning (DL), there has been a surge of new DL models for tabular learning. However,
despitetheefforts, tabularDLmodelsstillclearlytrailbehindtree-basedapproaches. Inthis
work, we propose DisTab, a novel framework for tabular learning based on the transformer
architecture. Our method leverages model distillation to mimic the favorable inductive
biases of tree-based models, and incorporates language guidance for more expressive feature
embeddings. Empirically, DisTab outperforms existing tabular DL models and is highly
competitive against tree-based models across diverse datasets, effectively closing the gap
with these methods.
1",TMLR
"Imaging is a canonical inverse problem, where the task of reconstructing a ground truth
from a noisy measurement is typically ill-conditioned or ill-posed. Recent state-of-the-
art approaches for imaging use deep learning, spearheaded by unrolled and end-to-end
models and trained on various image datasets. However, such methods typically require
the availability of ground truth data, which may be unavailable or expensive, leading to
a fundamental barrier that can not be addressed by choice of architecture. Unsupervised
learning presents a powerful alternative paradigm that bypasses this requirement by allowing
to learn directly from noisy measurement data without the need for any ground truth.
A principled statistical approach to unsupervised learning is to maximize the marginal
likelihood of the model parameters with respect to the given noisy measurements. This paper
proposes an unsupervised learning approach that leverages maximum marginal likelihood
estimation and stochastic approximation computation in order to train a convex neural
network-based image regularization term directly on noisy measurements, improving upon
previous work in both model expressiveness and dataset size. Experiments demonstrate
that the proposed method produces image priors that are comparable in performance to the
analogous supervised models for various image corruption operators, maintaining significantly
better generalization properties when compared to end-to-end methods. Moreover, we provide
a detailed theoretical analysis of the convergence properties of our proposed algorithm.
1 Published in Transactions on Machine Learning Research (12/2024)
1",TMLR
"Variational inference with Gaussian mixture models (GMMs) enables learning of highly
tractableyetmulti-modalapproximationsofintractabletargetdistributionswithuptoafew
hundred dimensions. The two currently most effective methods for GMM-based variational
inference, VIPS and iBayes-GMM, both employ independent natural gradient updates for
the individual components and their weights. We show for the first time, that their derived
updates are equivalent, although their practical implementations and theoretical guarantees
differ. We identify several design choices that distinguish both approaches, namely with
respect to sample selection, natural gradient estimation, stepsize adaptation, and whether
trust regions are enforced or the number of components adapted. We argue that for both
approaches, the quality of the learned approximations can heavily suffer from the respective
design choices: By updating the individual components using samples from the mixture
model, iBayes-GMM often fails to produce meaningful updates to low-weight components,
and by using a zero-order method for estimating the natural gradient, VIPS scales badly
to higher-dimensional problems. Furthermore, we show that information-geometric trust-
regions (used by VIPS) are effective even when using first-order natural gradient estimates,
and often outperform the improved Bayesian learning rule (iBLR) update used by iBayes-
GMM. We systematically evaluate the effects of design choices and show that a hybrid
approach significantly outperforms both prior works. Along with this work, we publish our
highly modular and efficient implementation for natural gradient variational inference with
Gaussian mixture models, which supports 432different combinations of design choices, facil-
itates the reproduction of all our experiments, and may prove valuable for the practitioner.
1 Published in Transactions on Machine Learning Research (07/2023)
1",TMLR
"Systematic generalization is a crucial aspect of intelligence, which refers to the ability to
generalize to novel tasks by combining known subtasks and concepts. One critical factor
that has been shown to influence systematic generalization is the diversity of training data.
However, diversity can be defined in various ways, as data have many factors of variation.
A more granular understanding of how different aspects of data diversity affect systematic
generalization is lacking. We present new evidence in the problem of Visual Question
Answering (VQA) that reveals that the diversity of simple tasks (i.e. tasks formed by a
few subtasks and concepts) plays a key role in achieving systematic generalization. This
implies that it may not be essential to gather a large and varied number of complex tasks,
which could be costly to obtain. We demonstrate that this result is independent of the
similarity between the training and testing data and applies to well-known families of neural
network architectures for VQA (i.e. monolithic architectures and neural module networks).
Additionally, we observe that neural module networks leverage all forms of data diversity
we evaluated, while monolithic architectures require more extensive amounts of data to do
so. These findings provide a first step towards understanding the interactions between data
diversity design, neural network architectures, and systematic generalization capabilities.
1",TMLR
"Efficient exploration is a crucial challenge in deep reinforcement learning. Several methods,
such as behavioral priors, are able to leverage offline data in order to efficiently accelerate
reinforcement learning on complex tasks. However, if the task at hand deviates excessively
from the demonstrated task, the effectiveness of such methods is limited. In our work, we
propose to learn features from offline data that are shared by a more diverse range of tasks,
such as correlation between actions and directedness. Therefore, we introduce state-free
priors, which directly model temporal consistency in demonstrated trajectories, and are
capable of driving exploration in complex tasks, even when trained on data collected on
simpler tasks. Furthermore, we introduce a novel integration scheme for action priors in off-
policy reinforcement learning by dynamically sampling actions from a probabilistic mixture
of policy and action prior. We compare our approach against strong baselines and provide
empirical evidence that it can accelerate reinforcement learning in long-horizon continuous
control tasks under sparse reward settings.
1",TMLR
"Automated machine learning (AutoML) usually involves several crucial components, such
as Data Augmentation (DA) policy, Hyper-Parameter Optimization (HPO), and Neural
Architecture Search (NAS). Although many strategies have been developed for automating
these components in separation, joint optimization of these components remains challenging
due to the largely increased search dimension and the variant input types of each component.
In parallel to this, the common practice of searching for the optimal architecture ï¬rst and
then retraining it before deployment in NAS often suers from low performance correlation
between the searching and retraining stages. An end-to-end solution that integrates the
AutoML components and returns a ready-to-use model at the end of the search is desirable.
In view of these, we propose DHA , which achieves joint optimization of Data augmentation
policy, Hyper-parameter and Architecture. Speciï¬cally, end-to-end NAS is achieved in a
dierentiable manner by optimizing a compressed lower-dimensional feature space, while DA
policy and HPO are regarded as dynamic schedulers, which adapt themselves to the update
of network parameters and network architecture at the same time. Experiments show that
DHA achieves state-of-the-art (SOTA) results on various datasets and search spaces. To the
best of our knowledge, we are the ï¬rst to eciently and jointly optimize DA policy, NAS,
and HPO in an end-to-end manner without retraining.
1",TMLR
"We study the dual problem of convexiï¬ed convolutional neural networks (DCCNNs). First, we
introduce a primal learning problem motivated by convexiï¬ed convolutional neural networks
(CCNNs), and then construct the dual convex training program through careful analysis of
the Karush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach reduces
the computational overhead of constructing a large kernel matrix and more importantly,
eliminates the ambiguity of factorizing the matrix. Due to the low-rank structure in CCNNs
and the related subdierential of nuclear norms, there is no closed-form expression to recover
the primal solution from the dual solution. To overcome this, we propose a highly novel
weight recovery algorithm, which takes the dual solution and the kernel information as the
input, and recovers the linear weight and the output of convolutional layer, instead of weight
parameter. Furthermore, our recovery algorithm exploits the low-rank structure and imposes
a small number of ï¬lters indirectly, which reduces the parameter size. As a result, DCCNNs
inherit all the statistical beneï¬ts of CCNNs, while enjoying a more formal and ecient
workï¬‚ow.
1",TMLR
"Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate
rendition of specific elements like distinct characters or unique styles in generated images.
Nonetheless, existing methods face challenges in effectively composing multiple LoRAs,
especially as the number of LoRAs to be integrated grows, thus hindering the creation of
compleximagery. Inthispaper, westudymulti-LoRAcompositionthroughadecoding-centric
perspective. Wepresenttwotraining-freemethods: LoRA Switch , whichalternatesbetween
different LoRAs at each denoising step, and LoRA Composite , which simultaneously
incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed
approaches, we establish ComposLoRA , a new comprehensive testbed as part of this research.
It features a diverse range of LoRA categories with 480 composition sets. Utilizing an
evaluation framework based on GPT-4V, our findings demonstrate a clear improvement
in performance with our methods over the prevalent baseline, particularly evident when
increasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights,
and all evaluation details are available on our project website.
1",TMLR
"Direct policy optimization in reinforcement learning is usually solved with policy-gradient
algorithms, which optimize policy parameters via stochastic gradient ascent. This paper
provides a new theoretical interpretation and justification of these algorithms. First, we
formulate direct policy optimization in the optimization by continuation framework. The
latter is a framework for optimizing nonconvex functions where a sequence of surrogate
objective functions, called continuations, are locally optimized. Second, we show that op-
timizing affine Gaussian policies and performing entropy regularization can be interpreted
as implicitly optimizing deterministic policies by continuation. Based on these theoretical
results, we argue that exploration in policy-gradient algorithms consists in computing a
continuation of the return of the policy at hand, and that the variance of policies should
be history-dependent functions adapted to avoid local extrema rather than to maximize the
return of the policy.
1",TMLR
"Adjustable hyperparameters of machine learning models typically impact various key
trade-offs such as accuracy, fairness, robustness, or inference cost. Our goal in this paper
is to find a configuration that adheres to user-specified limits on certain risks while being
useful with respect to other conflicting metrics. We solve this by combining Bayesian
Optimization (BO) with rigorous risk-controlling procedures, where our core idea is to steer
BO towards an efficient testing strategy. Our BO method identifies a set of Pareto optimal
configurations residing in a designated region of interest. The resulting candidates are
statistically verified, and the best-performing configuration is selected with guaranteed risk
levels. We demonstrate the effectiveness of our approach on a range of tasks with multiple
desiderata, including low error rates, equitable predictions, handling spurious correlations,
managing rate and distortion in generative models, and reducing computational costs.1
1",TMLR
"Combining the strengths of many existing predictors to obtain a Mixture of Experts which is
superior to its individual components is an effective way to improve the performance without
having to develop new architectures or train a model from scratch. However, surprisingly, we
find that naÃ¯vely combining off-the-shelf object detectors in a similar way to Deep Ensembles,
can often lead to degraded performance. We identify that the primary cause of this issue is
that the predictions of the experts do not match their performance, a term referred to as
miscalibration. Consequently, the most confident detector dominates the final predictions,
preventing the mixture from leveraging all the predictions from the experts appropriately.
To address this, when constructing the Mixture of Experts for object detection, we propose
to combine their predictions in a manner which reflects the individual performance of
the experts; an objective we achieve by first calibrating the predictions before filtering
and refining them. We term this approach the Mixture of Calibrated Experts (M oCaE)
and demonstrate its effectiveness through extensive experiments on 5 different detection
tasks, showing that it: (i) improves object detectors on COCO and instance segmentation
methods on LVIS by up toâˆ¼2.5AP; (ii) reaches state-of-the-art on COCO test-dev
with 65.1AP and on DOTA with 82.62AP50; (iii) outperforms single models consistently
on recent detection tasks such as Open Vocabulary Object Detection. Code is available at:
https://github.com/fiveai/MoCaE .
1",TMLR
"In recent years, continual learning (CL) techniques have made significant progress in learning from
streaming data while preserving knowledge across sequential tasks, particularly in the realm of
euclidean data. To foster fair evaluation and recognize challenges in CL settings, several evaluation
frameworks have been proposed, focusing mainly on the single- and multi-label classification
task on euclidean data. However, these evaluation frameworks are not trivially applicable when
the input data is graph-structured, as they do not consider the topological structure inherent in
graphs. Existing continual graph learning (CGL) evaluation frameworks have predominantly fo-
cused on single-label scenarios in the node classification (NC) task. This focus has overlooked the
complexities of multi-label scenarios, where nodes may exhibit affiliations with multiple labels,
simultaneously participating in multiple tasks. We develop a graph-aware evaluation ( AGALE )
framework that accommodates both single-labeled and multi-labeled nodes, addressing the limi-
tations of previous evaluation frameworks. In particular, we define new incremental settings and
devise data partitioning algorithms tailored to CGL datasets. We perform extensive experiments
comparing methods from the domains of continual learning, continual graph learning, and dy-
namic graph learning (DGL). We theoretically analyze AGALE and provide new insights about
the role of homophily in the performance of compared methods. We release our framework at
https://github.com/Tianqi-py/AGALE .
1",TMLR
"We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and
3D point clouds through systematically rearranging points without destroying their overall
structure ( distribution normalization ). LJL simulates a dissipative process of repulsive and
weakly attractive interactions between individual points by considering the nearest neighbor
of each point at a given moment in time. This pushes the particles into a potential valley,
reaching a well-defined stable configuration that approximates an equidistant sampling after
the stabilization process. We apply LJLs to redistribute randomly generated point clouds
into a randomized uniform distribution. Moreover, LJLs are embedded in the generation
process of point cloud networks by adding them at later stages of the inference process. The
improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and
quantitatively. Finally, we apply LJLs to improve the point distribution of a score-based
3D point cloud denoising network. In general, we demonstrate that LJLs are effective for
distribution normalization which can be applied at negligible cost without retraining the
given neural network.
Source Code: Upon request, we are happy to share the source code to generate the results
presented in this paper. Please contact the first or the last author of this manuscript.
Keywords: Distribution Normalization, Generative Modeling, Lennard-Jones Potential,
Particle Simulation, Point Clouds.
1 Published in Transactions on Machine Learning Research (09/2024)
LJL
 LJL
| |
Figure 1: An overview of the integration of LJLs into the inference process of well-trained generative models.
A well-trained model generates a meaningful point cloud from random noise in a sequential way (bottom
row). LJLs are inserted after certain intermediate-generation steps with damping step size âˆ†tnand LJ
potential parameters ÏµandÏƒ(top row). Embedding LJLs in generative models can improve the generation
results by normalizing the point distribution.
1",TMLR
"Training generative models that capture rich semantics of the data and interpreting the
latent representations encoded by such models are very important problems in un-/self-
supervised learning. In this work, we provide a simple algorithm that relies on perturbation
experiments on latent codes of a pre-trained generative autoencoder to uncover an attribute
graph that is implied by the generative model. We perform perturbation experiments to
check for inï¬‚uence of a given latent variable on a subset of attributes. Given this, we
show that one can ï¬t an eective graphical model that models a structural equation model
between latent codes taken as exogenous variables and attributes taken as observed variables.
One interesting aspect is that a single latent variable controls multiple overlapping subsets
of attributes unlike conventional approaches that try to impose full independence. Using
a pre-trained generative autoencoder trained on a large dataset of small molecules, we
demonstrate that the graphical model between various molecular attributes and latent codes
learned by our algorithm can be used to predict a speciï¬c property for molecules which are
drawn from a dierent distribution. We compare prediction models trained on various
feature subsets chosen by simple baselines, as well as existing causal discovery and sparse
learning/feature selection methods, with the ones in the derived Markov blanket from our
method. Results show empirically that the predictor that relies on our Markov blanket
attributes is robust to distribution shifts when transferred or ï¬ne-tuned with a few samples
from the new distribution, especially when training data is limited.
1",TMLR
"Controllable text-to-image (T2I) diffusion models generate images conditioned on both text
prompts and semantic inputs of other modalities like edge maps. Nevertheless, current
controllable T2I methods commonly face challenges related to efficiency and faithfulness,
especially when conditioning on multiple inputs from either the same or diverse modalities. In
this paper, we propose a novel FlexibleandEfficient method, FlexEControl, for controllable
T2I generation. At the core of FlexEControl is a unique weight decomposition strategy, which
allows for streamlined integration of various input types. This approach not only enhances
the faithfulness of the generated image to the control, but also significantly reduces the
computational overhead typically associated with multimodal conditioning. Our approach
achieves a reduction of 41% in trainable parameters and 30% in memory usage compared
with Uni-ControlNet. Moreover, it doubles data efficiency and can flexibly generate images
under the guidance of multiple input conditions of various modalities.
1",TMLR
"Clustering is an essential task for the purpose that data points can be classiï¬ed in an unsu-
pervised manner. Most deep clustering algorithms are very eï¬€ective when given the number
of clustersK. However, when Kis unknown, ï¬nding the appropriate Kfor these algorithms
can be computationally expensive via model-selection criteria, and applying algorithms with
an inaccurate Kcan hardly achieve the state-of-the-art performance. This paper proposes a
plug-and-play clustering module to automatically adjust the number of clusters, which can
be easily embedded into existing deep parametric clustering methods. By analyzing the goal
of clustering, a split-and-merge framework is introduced to reduce the intra-class diversity
and increase the inter-class diï¬€erence, which leverages the entropy between diï¬€erent clus-
ters. Speciï¬cally, given an initial clustering number, clusters can be split into sub-clusters
or merged into super-clusters and converge to a stable number of Kclusters at the end
of training. Experiments on benchmark datasets demonstrate that the proposed method
can achieve comparable performance with the state-of-the-art works without requiring the
number of clusters.
1",TMLR
"The development of Large Language Models (LLMs) often confronts challenges stemming
from the heavy reliance on human annotators in the reinforcement learning with human
feedback (RLHF) framework, or the frequent and costly external queries tied to the self-
instructparadigm. Inthiswork,wepivottoReinforcementLearning(RL)â€”butwithatwist.
DivergingfromthetypicalRLHF,whichrefinesLLMsfollowinginstructiondatatraining, we
use RL to directly generate the foundational instruction dataset that alone suffices for fine-
tuning. Our method, TeaMs-RL , uses a suite of textual operations and rules, prioritizing
the diversification of training datasets. It facilitates the generation of high-quality data
without excessive reliance on external advanced models, paving the way for a single fine-
tuning step and negating the need for subsequent RLHF stages. Our findings highlight
key advantages of our approach: reduced need for human involvement and fewer model
queries (only 5.73%of the strong baselineâ€™s total ), along with enhanced capabilities of
LLMs in crafting and comprehending complex instructions compared to strong baselines,
and substantially improved model privacy protection. Code is available at the link: https:
//github.com/SafeRL-Lab/TeaMs-RL
1",TMLR
"While deep learning (DL) performance is exceptional for many applications, there is no
consensus on whether DL or gradient boosted decision trees (GBDTs) are superior for tab-
ular data. We compare TabNet (a DL model for tabular data), two simple neural networks
inspired by ResNet (a DL model) and Catboost (a GBDT model) on a large UK insurer
dataset for the task of claim reserving. This dataset is of particular interest for its large
amount of informative missing values which are not missing completely at random, high-
lighting the impact of missing value handling on accuracy. Under certain missing value
schemes a carefully optimised simple neural network performed comparably to Catboost
with default settings. However, using less-than-minimum imputation, Catboost with de-
fault settings substantially outperformed carefully optimised DL models - achieving the
best overall accuracy. We conclude that handling missing values is an important, yet often
overlooked, step when comparing DL to GBDT algorithms for tabular data.
1",TMLR
"While Reinforcement Learning ( RL) has made great strides towards solving increasingly
complicated problems, many algorithms are still brittle to even slight environmental changes.
Contextual Reinforcement Learning ( cRL) provides a framework to model such changes in
a principled manner, thereby enabling ï¬‚exible, precise and interpretable task speciï¬cation
and generation. Our goal is to show how the framework of cRLcontributes to improving
zero-shot generalization in RL through meaningful benchmarks and structured reasoning
about generalization tasks. We conï¬rm the insight that optimal behavior in cRL requires
context information, as in other related areas of partial observability. To empirically validate
this in the cRL framework, we provide various context-extended versions of common RL
environments. They are part of the ï¬rst benchmark library, CARL, designed for generalization
based on cRLextensions of popular benchmarks, which we propose as a testbed to further
study general agents. We show that in the contextual setting, even simple RL environments
become challenging - and that naive solutions are not enough to generalize across complex
context spaces.
âˆ—Equal Contribution
1 Published in Transactions on Machine Learning Research (06/2023)
(a) Example of a contextual extension of Braxâ€™
Fetch (Freeman et al., 2021) as part of CARL
(b) Variations in target distances
(c) Ground friction simulating grass, concrete and ice
Figure 1: CARLallows to conï¬gure and modify existing environments through the use of context. This context
can be made visible to agents through context features to inform them directly about the current instantiation
of the context (see a). Speciï¬c instances of CARLFetch with variations in goal distance (see b) and with
diï¬€erent ground frictions (grass, concrete and ice, see c).
1",TMLR
"As machine learning (ML) algorithms are used in applications that involve humans, concerns
have arisen that these algorithms may be biased against certain social groups. Counterfac-
tual fairness (CF) is a fairness notion proposed in Kusner et al. (2017) that measures the
unfairness of ML predictions; it requires that the prediction perceived by an individual in
the real world has the same marginal distribution as it would be in a counterfactual world, in
which the individual belongs to a different group. Although CF ensures fair ML predictions,
it fails to consider the downstream effects of ML predictions on individuals. Since humans
are strategic and often adapt their behaviors in response to the ML system, predictions
that satisfy CF may not lead to a fair future outcome for the individuals. In this paper,
we introduce lookahead counterfactual fairness (LCF), a fairness notion accounting for the
downstream effects of ML models which requires the individual future status to be counter-
factually fair. We theoretically identify conditions under which LCF can be satisfied and
propose an algorithm based on the theorems. We also extend the concept to path-dependent
fairness. Experiments on both synthetic and real data validate the proposed method1.
1",TMLR
"The causal bandit problem setting is a sequential decision-making framework where actions
of interest correspond to interventions on variables in a system assumed to be governed by
a causal model. The underlying causality may be exploited when investigating actions in
the interest of optimizing the yield of the reward variable. Most existing approaches assume
prior knowledge of the underlying causal graph, which is in practice restrictive and often
unrealistic. In this paper, we develop a novel Bayesian framework for tackling causal bandit
problems that does not rely on possession of the causal graph, but rather simultaneously
learns the causal graph while exploiting causal inferences to optimize the reward. Our meth-
ods efficiently utilize joint inferences from interventional and observational data in a unified
Bayesian model constructed with intervention calculus and causal graph learning. For the
implementationofourproposedmethodologyinthediscretedistributionalsetting, wederive
an approximation of the sampling variance of the backdoor adjustment estimator. In the
Gaussian setting, we characterize the interventional variance with intervention calculus and
propose a simple graphical criterion to share information between arms. We validate our
proposed methodology in an extensive empirical study, demonstrating compelling cumula-
tive regret performance against state-of-the-art standard algorithms as well as optimistic
implementations of their causal variants that assume strong prior knowledge of the causal
structure.
1",TMLR
"Recent efforts have augmented large language models (LLMs) with external resources (e.g.,
the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding
or reasoning, leading to a new class of language agents . While these agents have achieved
substantial empirical success, we lack a framework to organize existing agents and plan future
developments. In this paper, we draw on the rich history of cognitive science and symbolic
artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA).
CoALA describes a language agent with modular memory components, a structured action
space to interact with internal memory and external environments, and a generalized decision-
making process to choose actions. We use CoALA to retrospectively survey and organize
a large body of recent work, and prospectively identify actionable directions towards more
capable agents. Taken together, CoALA contextualizes todayâ€™s language agents within the
broader history of AI and outlines a path towards language-based general intelligence.
1",TMLR
"Estimating the individual-level continuous treatment effect holds significant practical impor-
tance in various decision-making domains, such as personalized healthcare and customized
marketing. However, most current methods for individual treatment effect estimation are
limited to discrete treatments and struggle to precisely adjust for selection bias under con-
tinuous settings, leading to inaccurate estimation. To address these challenges, we propose
a novel Disentangled Representation Network (DTRNet) to estimate the individualized
dose-response function (IDRF), which learns disentangled representations and precisely
adjusts for selection bias. To the best of our knowledge, our work is the first attempt to
precisely adjust for selection bias in continuous settings. Extensive results on synthetic and
semi-synthetic datasets demonstrate that our DTRNet outperforms most state-of-the-art
methods. Our code is available at DTRNet.
1",TMLR
"Machine learning (ML) models are costly to train as they can require a significant amount
of data, computational resources and technical expertise. Thus, they constitute valuable
intellectual property that needs protection from adversaries wanting to steal them. Owner-
ship verification techniques allow the victims of model stealing attacks to demonstrate that
a suspect model was in fact stolen from theirs.
Although a number of ownership verification techniques based on watermarking or finger-
printing have been proposed, most of them fall short either in terms of security guarantees
(well-equipped adversaries can evade verification) or computational cost. A fingerprinting
technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency
than prior methods.
The authors of DIprovided a correctness proof for linear (suspect) models. However, in a
subspace of the same setting, we prove that DIsuffers from high false positives (FPs) â€“ it can
incorrectly identify an independent model trained with non-overlapping data from the same
distribution as stolen. We further prove that DIalso triggers FPs in realistic, non-linear
suspect models. We then confirm empirically that DIin the black-box setting leads to FPs,
with high confidence.
Second, we show that DIalso suffers from false negatives (FNs) â€“ an adversary can fool
DI(at the cost of incurring some accuracy loss) by regularising a stolen modelâ€™s decision
boundariesusingadversarialtraining, therebyleadingtoanFN.Tothisend, wedemonstrate
that black-box DIfails to identify a model adversarially trained from a stolen dataset â€“ the
setting where DIis the hardest to evade.
Finally, we discuss the implications of our findings, the viability of fingerprinting-based
ownership verification in general, and suggest directions for future work.
1",TMLR
"Demonstrations provide insight into relevant state or action space regions, bearing great
potential to boost the efficiency and practicality of reinforcement learning agents. In this
work, we propose to leverage demonstration datasets by combining skill learning and sequence
modeling. Starting with a learned joint latent space, we separately train a generative model
of demonstration sequences and an accompanying low-level policy. The sequence model
forms a latent space prior over plausible demonstration behaviors to accelerate learning of
high-level policies. We show how to acquire such priors from state-only motion capture
demonstrations and explore several methods for integrating them into policy learning on
transfer tasks. Our experimental results confirm that latent space priors provide significant
gains in learning speed and final performance. We benchmark our approach on a set of
challenging sparse-reward environments with a complex, simulated humanoid, and on offline
RL benchmarks for navigation and object manipulation1
1",TMLR
"Despite progress in adversarial training (AT), there is a substantial gap between the top-
performing and worst-performing classes in many datasets. For example, on CIFAR10, the
accuracies for the best and worst classes are 74% and 23%, respectively. We argue that this
gap can be reduced by explicitly optimizing for the worst-performing class, resulting in a
min-max-max optimization formulation. Our method, called class focused online learning
(CFOL), includes high probability convergence guarantees for the worst class loss and can
be easily integrated into existing training setups with minimal computational overhead.
We demonstrate an improvement to 32% in the worst class accuracy on CIFAR10, and
we observe consistent behavior across CIFAR100 and STL10. Our study highlights the
importance of moving beyond average accuracy, which is particularly important in safety-
critical applications.
1",TMLR
"In the past few years, off-policy reinforcement learning methods have shown promising
results in their application to robot control. Q-learning based methods, however, still suffer
from poor data-efficiency and are susceptible to stochasticity or noise in the immediate
reward, whichislimitingwithregardtoreal-worldapplications. Wealleviatethisproblemby
proposing two novel off-policy Temporal-Difference formulations: (1) Truncated Q-functions
which represent the return for the first nsteps of a target-policy rollout with respect to
the full action-value and (2) Shifted Q-functions, acting as the farsighted return after this
truncated rollout. This decomposition allows us to optimize both parts with their individual
learning rates, achieving significant learning speedup and robustness to variance in the
reward signal, leading to the Composite Q-learning algorithm. We show the efficacy of
Composite Q-learning in the tabular case and furthermore employ Composite Q-learning
within TD3. We compare Composite TD3 with TD3 and TD3( âˆ†), which we introduce as
an off-policy variant of TD( âˆ†). Moreover, we show that Composite TD3 outperforms TD3
as well as TD3( âˆ†) significantly in terms of data-efficiency in multiple simulated robot tasks
and that Composite Q-learning is robust to stochastic immediate rewards.
1",TMLR
"In this paper, we introduce LInK, a novel framework that integrates contrastive learning
of performance and design space with optimization techniques for solving complex inverse
problems in engineering design with discrete and continuous variables. We focus on the
path synthesis problem for planar linkage mechanisms. By leveraging a multimodal and
transformation-invariant contrastive learning framework, LInK learns a joint representa-
tion that captures complex physics and design representations of mechanisms, enabling
rapid retrieval from a vast dataset of over 10 million mechanisms. This approach improves
precision through the warm start of a hierarchical unconstrained nonlinear optimization
algorithm, combining the robustness of traditional optimization with the speed and adapt-
ability of modern deep learning methods. Our results on an existing benchmark demon-
strate that LInK outperforms existing methods with 28 times less error compared to a
state-of-the-art approach while taking 20 times less time on an existing benchmark. More-
over, we introduce a significantly more challenging benchmark, named LINK-ABC, which
involves synthesizing linkages that trace the trajectories of English capital alphabetsâ€”an
inverse design benchmark task that existing methods struggle with due to large nonlinear-
ities and tiny feasible space. Our results demonstrate that LInK not only advances the
field of mechanism design but also broadens the applicability of contrastive learning and
1 Published in Transactions on Machine Learning Research (10/2024)
optimization to other areas of engineering. The code and data are publicly available at
https://github.com/ahnobari/LInK .
1",TMLR
"Data-driven Deep Learning (DL) models have revolutionized autonomous systems, but en-
suring their safety and reliability necessitates the assessment of predictive confidence or
uncertainty. Bayesian DL provides a principled approach to quantify uncertainty via prob-
ability density functions defined over model parameters. However, the exact solution is
intractable for most DL models, and the approximation methods, often based on heuristics,
suffer from scalability issues and stringent distribution assumptions and may lack theoretical
guarantees. This work develops a Sequential Importance Sampling framework that approx-
imates the posterior probability density function through weighted samples (or particles),
which can be used to find the mean, variance, or higher-order moments of the posterior dis-
tribution. We demonstrate that propagating particles, which capture information about the
higher-order moments, through the layers of the DL model results in increased robustness to
natural and malicious noise (adversarial attacks). The variance computed from these par-
ticles effectively quantifies the modelâ€™s decision uncertainty, demonstrating well-calibrated
and accurate predictive confidence.
1",TMLR
"Recently, program synthesis driven by large language models (LLMs) has become increas-
ingly popular. However, program synthesis for machine learning (ML) tasks still poses
significant challenges. This paper explores a novel form of program synthesis, targeting
ML programs, by combining LLMs and automated machine learning (autoML). Specifically,
our goal is to fully automate the generation and optimization of the code of the entire ML
workflow, from data preparation to modeling and post-processing, utilizing only textual
descriptions of the ML tasks. To manage the length and diversity of ML programs, we
propose to break each ML program into smaller, manageable parts. Each part is gener-
ated separately by the LLM, with careful consideration of their compatibilities. To ensure
compatibilities, we design a testing technique for ML programs. Unlike traditional program
synthesis, which typically relies on binary evaluations (i.e., correct or incorrect), evaluat-
ing ML programs necessitates more than just binary judgments. Our approach automates
the numerical evaluation and optimization of these programs, selecting the best candidates
through autoML techniques. In experiments across various ML tasks, our method outper-
forms existing methods in 10 out of 12 tasks for generating ML programs. In addition,
autoML significantly improves the performance of the generated ML programs. In experi-
ments, given the textual task description, our method, Text-to-ML , generates the complete
and optimized ML program in a fully autonomous process. The implementation of our
method is available at https://github.com/JLX0/llm-automl .
1 Published in Transactions on Machine Learning Research (09/2024)
1",TMLR
"Large models represent a groundbreaking advancement in multiple application fields, en-
abling remarkable achievements across various tasks. However, their unprecedented scale
comes with significant computational costs. These models, often consisting of billions of
parameters, require vast amounts of computational resources for execution. Especially, the
expansive scale and computational demands pose considerable challenges when customizing
them for particular downstream tasks, particularly over the hardware platforms constrained
by computational capabilities.
Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjust-
ing the large models over the various downstream tasks. In particular, PEFT refers to
the process of adjusting the parameters of a pre-trained large model to adapt it to a spe-
cific task or domain while minimizing the number of additional parameters introduced or
computational resources required. This approach is particularly important when dealing
with large-scale language models with high parameter counts, as fine-tuning these models
from scratch can be computationally expensive and resource-intensive, posing considerable
challenges in the supporting system platform design.
In this survey, we present comprehensive studies of various PEFT algorithms, examining
their performance and computational overhead. Moreover, we provide an overview of ap-
plications developed using different PEFT algorithms and discuss common techniques em-
ployed to mitigate PEFT computation costs. In addition to providing an extensive survey
from an algorithmic standpoint, we also examine various real-world system designs to in-
vestigate the implementation costs associated with different PEFT approaches. This survey
serves as a valuable resource for researchers aiming to understand both the PEFT algo-
rithm and its system implementation, offering detailed insights into recent advancements
and practical applications.
ËšCorresponding author
1 Published in Transactions on Machine Learning Research (10/2024)
Background 
Computational 
flow for LLM PEFT 
Taxonomy 
Selective 
PEFT Additive 
PEFT System Design 
Challenge 
System Design 
for PEFT 
Centralized PEFT 
Serving System PEFT for 
LLMs 
Distributed PEFT 
Training System 
Hybrid 
PEFT PEFT 
overview Reparameterized 
PEFT Efficient PEFT 
Design 
KV-cache 
Management for 
PEFT Efficiency 
PEFT Pruning 
PEFT 
Quantization 
Memory-efficient 
PEFT Parallel PEFT 
Training System Apply PEFT for 
other Applications 
PEFT for 
ViTs
PEFT for 
VLAs 
PEFT for 
Diffusion Models Downstream 
tasks Section 2 Section 3 Section 4 Section 5 Section 6 
2.1
2.2
2.33.1
3.2
3.3
3.44.1
4.2
4.3
4.45.1
5.2
5.3
5.46.1
6.2
6.3
6.4
Figure 1: A content overview covered in the survey.
1",TMLR
"Recent research shows that large language models are susceptible to privacy attacks that
infer aspects of the training data. However, it is unclear if simpler generative models, like
topic models, share similar vulnerabilities. In this work, we propose an attack against
topic models that can confidently identify members of the training data in Latent Dirichlet
Allocation. Our results suggest that the privacy risks associated with generative modeling
are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we
explore differentially private (DP) topic modeling. We propose a framework for private topic
modeling that incorporates DP vocabulary selection as a pre-processing step, and show that
it improves privacy while having limited effects on practical utility.
1",TMLR
"Personalised federated learning (FL) approaches assume that raw data of all clients are
defined in a common space i.e.all clients store their data according to the same schema. For
real-world applications, this assumption is restrictive as clients, having their own systems to
collect and then store data, may use heterogeneous data representations. To bridge the gap
between the assumption of a shared subspace and the more realistic situation of client-specific
spaces, we propose a general framework coined FLICthat maps clientâ€™s data onto a common
feature space via local embedding functions, in a federated manner. Preservation of class
information in the latent space is ensured by a distribution alignment with respect to a learned
reference distribution. We provide the algorithmic details of FLICas well as theoretical
insights supporting the relevance of our methodology. We compare its performances against
FL benchmarks involving heterogeneous input features spaces. Notably, we are the first to
present a successful application of FL to Brain-Computer Interface signals acquired on a
different number of sensors.
1",TMLR
"Dataset distillation methods reduce large-scale datasets to smaller sets of synthetic data,
preserving sufficient information to quickly train a new model from scratch. However,
prior work on dataset distillation has focused exclusively on image classification datasets,
whereas modern large-scale datasets are primarily vision-language datasets. In this work, we
design the first vision-language dataset distillation method, building on the idea of trajectory
matching. A key challenge is that vision-language datasets do not have a set of discrete classes.
To overcome this, our proposed method jointly distills image-text pairs in a contrastive
formulation. Further, we leverage Low-Rank Adaptation (LoRA) matching to enable more
efficient and effective trajectory matching in complex modern vision-language models. Since
there are no existing baselines, we compare our distillation approach with three adapted
vision-language coreset selection methods. We demonstrate significant improvements on the
challenging Flickr30K and COCO retrieval benchmarks: for example, on Flickr30K, the
best coreset selection method selecting 1000 image-text pairs for training achieves only 5.6%
image-to-text retrieval accuracy (i.e., recall@1); in contrast, our dataset distillation almost
doubles that to 9.9% with just 100 training pairs, an order of magnitude fewer.
1",TMLR
"In-context learning allows adapting a model to new tasks given a task description at test time.
In this paper, we present IMProv - a generative model that is able to in-context learn visual
tasks from multimodal prompts. Given a textual description of a visual task (e.g. â€œLeft:
input image, Right: foreground segmentationâ€), a few input-output visual examples, or both,
the model in-context learns to solve it for a new test input. We train a masked generative
transformer on a new dataset of figures from computer vision papers and their associated
captions, together with a captioned large-scale image-text dataset. During inference time, we
prompt the model with text and/or image task example(s) and have the model inpaint the
corresponding output. We show that training our model with text conditioning and scaling
the dataset size improves in-context learning for computer vision tasks by over +10%AP for
Foreground Segmentation, over +5%gains in AP for Single Object Detection, and almost
20%lower LPIPS in Colorization. Our emperical results suggest that vision and language
prompts are complementary and it is advantageous to use both to achieve better in-context
learning performance.
1",TMLR
"Low-precision training has emerged as a promising low-cost technique to enhance the train-
ing efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian
counterpart can further provide uncertainty quantification and improved generalization ac-
curacy. This paper investigates low-precision sampling via Stochastic Gradient Hamilto-
nian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for
both strongly log-concave and non-log-concave distributions. Theoretically, our results show
that to achieve Ïµ-error in the 2-Wasserstein distance for non-log-concave distributions, low-
precision SGHMC achieves quadratic improvement ( ËœO/parenleftbig
Ïµâˆ’2Âµâˆ—âˆ’2log2/parenleftbig
Ïµâˆ’1/parenrightbig/parenrightbig
) compared to
the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD)
(ËœO/parenleftbig
Ïµâˆ’4Î»âˆ—âˆ’1log5/parenleftbig
Ïµâˆ’1/parenrightbig/parenrightbig
). Moreover, we prove that low-precision SGHMC is more robust
to the quantization error compared to low-precision SGLD due to the robustness of the
momentum-based update w.r.t. gradient noise. Empirically, we conduct experiments on
synthetic data, and MNIST, CIFAR-10 & CIFAR-100 datasets, which validate our theoreti-
cal findings. Our study highlights the potential of low-precision SGHMC as an efficient and
accurate sampling method for large-scale and resource-limited machine learning.
1",TMLR
"Reinforcement Learning (RL) algorithms have shown tremendous success in simulation
environments, but their application to real-world problems faces significant challenges, with
safety being a major concern. In particular, enforcing state-wise constraints is essential
for many challenging tasks such as autonomous driving and robot manipulation. However,
existing safe RL algorithms under the framework of Constrained Markov decision process
(CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise
Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm
for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise
constraint satisfaction in expectation. In particular, we introduce the framework of Maximum
Markov decision process, and prove that the worst-case safety violation is bounded under
SCPO. We demonstrate the effectiveness of our approach on training neural network policies
for extensive robot locomotion tasks, where the agent must satisfy a variety of state-wise
safety constraints. Our results show that SCPO significantly outperforms existing methods
and can handle state-wise constraints in high-dimensional robotics tasks.
1",TMLR
"Capsule networks are a class of neural networks that aim at solving some limiting factors
of Convolutional Neural Networks. However, baseline capsule networks have failed to reach
state-of-the-art results on more complex datasets due to the high computation and memory
requirements. We tackle this problem by proposing a new network architecture, called Mo-
mentum Capsule Network (MoCapsNet). MoCapsNets are inspired by Momentum ResNets,
a type of network that applies reversible residual building blocks. Reversible networks al-
low for recalculating activations of the forward pass in the backpropagation algorithm, so
those memory requirements can be drastically reduced. In this paper, we provide a frame-
work on how invertible residual building blocks can be applied to capsule networks. We
will show that MoCapsNet beats the accuracy of baseline capsule networks on MNIST,
SVHN, CIFAR-10 and CIFAR-100 while using considerably less memory. The source code
is available on https://github.com/moejoe95/MoCapsNet .
1",TMLR
"Sparse representations of images are useful in many computer vision applications. Sparse
coding with an l1penalty and a learned linear dictionary requires regularization of the dic-
tionary to prevent a collapse in the l1norms of the codes. Typically, this regularization
entails bounding the Euclidean norms of the dictionaryâ€™s elements. In this work, we propose
a novel sparse coding protocol which prevents a collapse in the codes without the need to
regularize the decoder. Our method regularizes the codes directly so that each latent code
component has variance greater than a fixed threshold over a set of sparse representations
for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding
systems with multi-layer decoders since they can model more complex relationships than
linear dictionaries. In our experiments with MNIST and natural image patches, we show
that decoders learned with our approach have interpretable features both in the linear and
multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders
trained using our variance regularization method produce higher quality reconstructions
with sparser representations when compared to autoencoders with linear dictionaries. Ad-
ditionally, sparse representations obtained with our variance regularization approach are
useful in the downstream tasks of denoising and classification in the low-data regime.
1",TMLR
"Scaling up language models has been shown to predictably improve performance and sample
eï¬ƒciency on a wide range of downstream tasks. This paper instead discusses an unpredictable
phenomenon that we refer to as emergent abilities of large language models. We consider an
ability to be emergent if it is not present in smaller models but is present in larger models.
Thus, emergent abilities cannot be predicted simply by extrapolating the performance of
smaller models. The existence of such emergence raises the question of whether additional
scaling could potentially further expand the range of capabilities of language models.
1",TMLR
"Theoretical results from discrete geometry suggest that normed spaces can abstractly embed
finite metric spaces with surprisingly low theoretical bounds on distortion in low dimensions.
Inspired by this theoretical insight, we highlight in this paper normed spaces as a more flexible
and computationally efficient alternative to several popular Riemannian manifolds for learning
graph embeddings. Normed space embeddings significantly outperform several popular
manifolds on a large range of synthetic and real-world graph reconstruction benchmark
datasets while requiring significantly fewer computational resources. We also empirically
verify the superiority of normed space embeddings on growing families of graphs associated
with negative, zero, and positive curvature, further reinforcing the flexibility of normed
spaces in capturing diverse graph structures as graph sizes increase. Lastly, we demonstrate
the utility of normed space embeddings on two applied graph embedding tasks, namely, link
prediction and recommender systems. Our work highlights the potential of normed spaces for
geometric graph representation learning, raises new research questions, and offers a valuable
tool for experimental mathematics in the field of finite metric space embeddings. We make
our code and data publically available1.
1",TMLR
Abstract not found,TMLR
"Machine learning (ml) methods have the potential to automate high-stakes decisions, such as
bail admissions or credit lending, by analyzing and learning from historical data. But these
algorithmic decisions may be unfair: in learning from historical data, they may replicate
discriminatory practices from the past. In this paper, we propose two algorithms that adjust
ï¬tted ML predictors to produce decisions that are fair. Our methods provide post-hoc
adjustments to the predictors, without requiring that they be retrained. We consider a causal
model of the ML decisions, deï¬ne fairness through counterfactual decisions within the model,
and then form algorithmic decisions that capture the historical data as well as possible, but
are provably fair. In particular, we consider two deï¬nitions of fairness. The ï¬rst is â€œequal
counterfactual opportunity,â€ where the counterfactual distribution of the decision is the same
regardless of the protected attribute; the second is counterfactual fairness. We evaluate the
algorithms, and the trade-o between accuracy and fairness, on datasets about admissions,
income, credit, and recidivism.
1",TMLR
"Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can
improve data efficiency by repeatedly using the previously gathered data. However, off-policy
learning becomes challenging when the discrepancy between the underlying distributions
of the agentâ€™s policy and collected data increases. Although the well-studied importance
sampling and off-policy policy gradient techniques were proposed to compensate for this
discrepancy, they usually require a collection of long trajectories and induce additional
problems such as vanishing/exploding gradients or discarding many useful experiences, which
eventually increases the computational complexity. Moreover, their generalization to either
continuous action domains or policies approximated by deterministic deep neural networks is
strictly limited. To overcome these limitations, we introduce a novel policy similarity measure
to mitigate the effects of such discrepancy in continuous control. Our method offers an
adequate single-step off-policy correction that is applicable to deterministic policy networks.
Theoretical and empirical studies demonstrate that it can achieve a â€œsafeâ€ off-policy learning
and substantially improve the state-of-the-art by attaining higher returns in fewer steps than
the competing methods through an effective schedule of the learning rate in Q-learning and
policy optimization.
1",TMLR
"Mixed-integer linear programming (MILP) stands as a notable NP-hard problem pivotal
to numerous crucial industrial applications. The development of effective algorithms, the
tuning of solvers, and the training of machine learning models for MILP resolution all hinge
on access to extensive, diverse, and representative data. Yet compared to the abundant
naturally occurring data in image and text realms, MILP is markedly data deficient, under-
scoring the vital role of synthetic MILP generation. We present DIG-MILP, a deep genera-
tive framework adept at extracting deep-level structural features from highly limited MILP
data and producing instances that closely mirror the target data. Notably, by leveraging
the MILP duality, DIG-MILP guarantees a correct and complete generation space as well
as ensures the boundedness and feasibility of the generated instances. Our empirical study
highlights the novelty and quality of the instances generated by DIG-MILP through two dis-
tinct downstream tasks: (S1) We identify Data Sharing , which involves MILP solver tuning
without directly sharing the original data. Experiments show that solver solution time on
instances generated by Dig-MILP correlate highly positively with the original data, offering
a potential solution direction.; (S2) Data Augmentation , wherein the DIG-MILP-generated
instances bolster the generalization performance of machine learning models tasked with
resolving MILP problems.
1",TMLR
"Multi-class support vector machine (SVM) models are typically built using all possible
pairs of binary SVM in a one-against-one fashion. This requires too much computation
for datasets with hundreds or thousands of classes, which motivates the search for multi-
class models that do not use all pairwise SVM. Our models correspond to the choice of
the model graph, whose vertices correspond to classes and edges represent which pairwise
SVMs are trained. We conduct experiments to uncover metrical and topological properties
that impact the accuracy of a multi-class SVM model. Based on their results we propose a
way to construct intermediate multi-class SVM models. The key insight is that for model
graphs of diameter two, we can estimate missing pairwise probabilities from the known
ones thus transforming the computation of posteriors to the usual complete (maximal) case.
Our proposed algorithm allows one to reduce computational effort by 50-80% while keeping
accuracy near, or even above that of a softmax classifier. In our work we use convolutional
data sets, which have multiple advantages for benchmarking multi-class SVM models.
1",TMLR
"The rapid advancement of Generative Adversarial Networks (GANs) necessitates the need to ro-
bustly evaluate these models. Among the established evaluation criteria, the FrÃ©chet Inception Dis-
tance (FID) has been widely adopted due to its conceptual simplicity, fast computation time, and
strong correlation with human perception. However, FID has inherent limitations, mainly stemming
from its assumption that feature embeddings follow a Gaussian distribution, and therefore can be
defined by their first two moments. As this does not hold in practice, in this paper we explore the
importance of third-moments in image feature data and use this information to define a new measure,
which we call the Skew Inception Distance (SID). We prove that SID is a pseudometric on prob-
ability distributions, show how it extends FID, and present a practical method for its computation.
Our numerical experiments support that SID either tracks with FID or, in some cases, aligns more
closely with human perception when evaluating image features of ImageNet data. Our work also
shows that principal component analysis can be used to speed up the computation time of both FID
and SID. Although we focus on using SID on image features for GAN evaluation, SID is applicable
much more generally, including for the evaluation of other generative models.
1",TMLR
"Network embedding (NE) approaches have emerged as a predominant technique to repre-
sent complex networks and have benefited numerous tasks. However, most NE approaches
rely on a homophily assumption to learn embeddings with the guidance of supervisory sig-
nals, leaving the unsupervised heterophilous scenario relatively unexplored. This problem
becomes especially relevant in fields where a scarcity of labels exists. Here, we formulate
the unsupervised NE task as an r-ego network discrimination problem and develop the SE-
LENE framework for learning on networks with homophily and heterophily. Specifically,
we design a dual-channel feature embedding pipeline to discriminate r-ego networks us-
ing node attributes and structural information separately. We employ heterophily adapted
self-supervised learning objective functions to optimise the framework to learn intrinsic node
embeddings. We show that SELENEâ€™s components improve the quality of node embeddings,
facilitating the discrimination of connected heterophilous nodes. Comprehensive empirical
evaluations on both synthetic and real-world datasets with varying homophily ratios vali-
date the effectiveness of SELENE in homophilous and heterophilous settings showing an up
to12.52%clustering accuracy gain.
1",TMLR
"We address the question of binary classification when no labels are available and the input
features are categorical. The lack of labels means supervised approaches canâ€™t be used, and
the lack of a natural distance measure means that most unsupervised methods do poorly.
For such problems, where the alternatives might be a) do nothing or b) heuristic rules-based
approaches, we offer a third alternative: a classifier that approximates Naive Bayes. Our
primary scenarios are those that involve distinguishing scripted, or bot, web traffic from
that of legitimate users.
Our main assumption is the existence of some attribute xâˆ—more prevalent in the benign
than the scripted traffic; i.e., P(xâˆ—|bot) =KÂ·P(xâˆ—|bot),forK > 1.We show that any
such disparity yields a lower bound on P(bot|xj)even when we have no prior estimates of
P(xâˆ—|bot), P(xâˆ—|bot)orK(except that K > 1). We show that when at least one bin of
at least one feature receives no attack traffic then we under-estimate the actual conditional
probability by a factor of 1âˆ’1/K.Thus, any attribute with a large disparity between
prevalence in benign and abuse traffic (i.e., Kis large), allows good approximation of the
Naive Bayes classifier without the benefit of labels.
The approach is particularly suited to problems where Kis high and thus the approximation
is very accurate. Example problems (and relevant attributes) might be: password-guessing,
if login attempts from legitimate users succeed at a much higher rate than those from
password-guessing attackers; Credit Card Verification Value (CVV) guessing, if an attacker
exhaustively tries all possible 3 or 4-digit values and fails at a higher rate than legitimate
users; account registration, if legitimate users use email addresses from services that do not
allowfeeanonymousaccounts(e.g., .edu)atamuchhigherratethanattackers; click-fraudif
legitimate users visit pages and services that contain no ads at a higher rate than click-fraud
bots.
1",TMLR
"Imitation learning from observations (IfO) constrains the classic imitation learning setting
to cases where expert observations are easy to obtain, but no expert actions are available.
Most existing IfO methods require access to task-specific cost functions or many interactions
with the target environment. Learning a forward dynamics model in combination with a
latent policy has been shown to solve these issues. However, the limited supervision in the
IfOscenariocanleadtomodecollapsewhenlearningthegenerativeforwarddynamicsmodel
and the corresponding latent policy. In this paper, we analyze the mode collapse problem
in this setting and show that it is caused by a combination of deterministic expert data
and bad initialization of the models. Under the assumption of piecewise continuous system
dynamics, we propose ILPO-MP, a method to prevent the mode collapse using clustering
of expert transitions to impose a mode prior on the generative model and the latent policy.
We show that ILPO-MP prevents mode collapse and improves performance in a variety of
environments.
1",TMLR
"Learning in multi-agent systems is highly challenging due to several factors including
the non-stationarity introduced by agentsâ€™ interactions and the combinatorial nature of
their state and action spaces. In particular, we consider the Mean-Field Control (MFC)
problem which assumes an asymptotically infinite population of identical agents that
aim to collaboratively maximize the collective reward. In many cases, solutions of an
MFC problem are good approximations for large systems, hence, efficient learning for
MFC is valuable for the analogous discrete agent setting with many agents. Specifically,
we focus on the case of unknown system dynamics where the goal is to simultaneously
optimize for the rewards and learn from experience. We propose an efficient model-based
reinforcement learning algorithm, M3â€“UCRL, that runs in episodes, balances between
exploration and exploitation during policy learning, and provably solves this problem.
Our main theoretical contributions are the first general regret bounds for model-based
reinforcement learning for MFC, obtained via a novel mean-field type analysis. To learn
the systemâ€™s dynamics, M3â€“UCRL can be instantiated with various statistical models, e.g.,
neural networks or Gaussian Processes. Moreover, we provide a practical parametrization of
the core optimization problem that facilitates gradient-based optimization techniques when
combined with differentiable dynamics approximation methods such as neural networks.
1",TMLR
"Many machine learning applications, such as feature selection, recommendation, and social
advertising, require the joint optimization of the global utilityand therepresentativeness
for different groups of items or users. To meet such requirements, we propose a novel multi-
objective combinatorial optimization problem called Submodular Maximization with Fair
Representation (SMFR), which selects subsets from a ground set, subject to a knapsack
or matroid constraint, to maximize a submodular ( utility) functionfas well as a set of d
submodular ( representativeness ) functions g1,...,gd. We show that the maximization of f
might conflict with the maximization of g1,...,gd, so that no single solution can optimize
all these objectives at the same time. Therefore, we propose a Pareto optimization approach
toSMFR, which finds a set of solutions to approximate all Pareto-optimal solutions with
different trade-offs between the objectives. Our method converts an instance of SMFRinto
several submodular cover instances by adjusting the weights of the objective functions; then
it computes a set of solutions by running the greedy algorithm on each submodular cover
instance. We prove that our method provides approximation guarantees for SMFRunder
knapsack or matroid constraints. Finally, we demonstrate the effectiveness of SMFRand
ourproposedapproachintworeal-worldproblems: maximum coverage andrecommendation .
1",TMLR
"In this paper, we address the challenge of performing counterfactual inference with ob-
servational data via Bayesian nonparametric regression adjustment, with a focus on high-
dimensional settings featuring multiple actions and multiple correlated outcomes. We present
a general class of counterfactual multi-task deep kernels models that estimate causal effects
and learn policies proficiently thanks to their sample efficiency gains, while scaling well with
high dimensions. In the first part of the work, we rely on Structural Causal Models (SCM) to
formally introduce the setup and the problem of identifying counterfactual quantities under
observed confounding. We then discuss the benefits of tackling the task of causal effects
estimation via stacked coregionalized Gaussian Processes and Deep Kernels. Finally, we
demonstrate the use of the proposed methods on simulated experiments that span individual
causal effects estimation, off-policy evaluation and optimization.
1",TMLR
"Differential Privacy (DP) is the de facto standard for reasoning about the privacy of a
training algorithm. Yet, learning with DP often yields poor performance unless one trains
on a large dataset. In this paper, we instead outline how training on lessdata can be
beneficial when we are only interested in defending against specific attacks; we take the
canonical example of defending against membership inference. To arrive at this result, we
first derive (tight) bounds on the success of all membership inference attacks. These bounds
donotreplaceDP,rathertheyintroduceacomplementaryinterpretationofaDPalgorithmâ€™s
ability to defend against membership inference specifically. Because our bound more tightly
captures the effect of how training data was selected, we can show that decreasing the
sampling rate when constructing the training dataset has a disparate effect on the bound
when compared to strengthening the DP guarantee. Thus, when the privacy protection
we care about is defending against membership inference, training on less data can yield
more advantageous trade-offs between preventing membership inference and utility than
strengthening the DP guarantee. We empirically illustrate this on MNIST, CIFAR10 and
SVHN-extended.
1",TMLR
"In this article, we propose a new algorithm for unsupervised anomaly detection in univariate
time series, based on topological data analysis. It relies on delay embeddings and on the
extraction of persistent cycles from the 1-dimensional persistent homology constructed from
the distance to measure Rips filtration. This filtration makes it possible to identify 1-cycles
(i.e. loops) corresponding to recurrent patterns by leveraging density information. Points
in those cycles are considered as normal, and the algorithm can then assign an anomaly
score to any point which is its distance to the normal set. In this paper, we describe the
algorithm, make a theoretical study, and test it on several real-world and synthetic datasets,
showing that it is competitive with state-of-the-art anomaly detection methods.
1",TMLR
"Temporal point process (TPP) is commonly used to model the asynchronous event sequence
featuring occurrence timestamps and revealed by probabilistic models conditioned on histori-
cal impacts. While lots of previous works have focused on â€˜goodness-of-ï¬tâ€™ of TPP models
by maximizing the likelihood, their predictive performance is unsatisfactory, which means
the timestamps generated by models are far apart from true observations. Recently, deep
generative models such as denoising diï¬€usion and score matching models have achieved
great progress in image generating tasks by demonstrating their capability of generating
samples of high quality. However, there are no complete and uniï¬ed works exploring and
studying the potential of generative models in the context of event occurence modeling for
TPP. In this work, we try to ï¬ll the gap by designing a uniï¬ed generative framework for
neuraltemporalpointprocess ( GNTPP ) model to explore their feasibility and eï¬€ectiveness,
and further improve modelsâ€™ predictive performance. Besides, in terms of measuring the
historical impacts, we revise the attentive models which summarize inï¬‚uence from historical
events with an adaptive reweighting term considering eventsâ€™ type relation and time intervals.
Extensive experiments have been conducted to illustrate the improved predictive capability
ofGNTPP with a line of generative probabilistic decoders, and performance gain from
the revised attention. To the best of our knowledge, this is the ï¬rst work that adapts
generative models in a complete uniï¬ed framework and studies their eï¬€ectiveness in the
context of TPP. Our codebase including all the methods given in Section. 5.1.1 is open in
https://github.com/BIRD-TAO/GNTPP . We hope the code framework can facilitate future
research in Neural TPPs.
1",TMLR
"There has been significant recent progress in training differentially private (DP) models which
achieve accuracy that approaches the best non-private models. These DP models are typically
pretrained on large public datasets and then fine-tuned on private downstream datasets that
are relatively large and similar in distribution to the pretraining data. However, in many
applications including personalization and federated learning, it is crucial to perform well
(i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic;
and (ii) on datasets from a wide variety of domains for use in various specialist settings. To
understand under which conditions few-shot DP can be effective, we perform an exhaustive
set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP
image classification models are affected as the number of shots per class, privacy level, model
architecture, downstream dataset, and subset of learnable parameters in the model vary. We
show that to achieve DP accuracy on par with non-private models, the shots per class must
be increased as the privacy level increases. We also show that learning parameter-efficient
FiLM adapters under DP is competitive with learning just the final classifier layer or learning
all of the network parameters. Finally, we evaluate DP federated learning systems and
establish state-of-the-art performance on the challenging FLAIR benchmark.
âˆ—These authors contributed equally
1 Published in Transactions on Machine Learning Research (12/2023)
1",TMLR
"The â€œNeural Tangent Kernelâ€ (NTK) (Jacot et al., 2018), and its empirical variants have
been proposed as a proxy to capture certain behaviors of real neural networks. In this
work, we study NTKs through the lens of scaling laws, and demonstrate that they fall
short of explaining important aspects of neural network generalization. In particular, we
demonstrate realistic settings where finite-width neural networks have significantly better
data scaling exponents as compared to their corresponding empirical and infinite NTKs at
initialization. This reveals a more fundamental difference between the real networks and
NTKs, beyond just a few percentage points of test accuracy. Further, we show that even if
the empirical NTK is allowed to be pre-trained on a constant number of samples, the kernel
scaling does not catch up to the neural network scaling. Finally, we show that the empirical
NTK continues to evolve throughout most of the training, in contrast with prior work which
suggests that it stabilizes after a few epochs of training. Altogether, our work establishes
concrete limitations of the NTK approach in understanding scaling laws of real networks on
natural datasets.
1",TMLR
"Efficient inference of Deep Neural Networks (DNNs) on resource-constrained edge devices
is essential. Quantization and sparsity are key techniques that translate to repetition
and sparsity respectively within tensors at the hardware-software interface. This paper
introduces the concept of repetition-sparsity trade-off that helps explain computational
efficiency during inference. We propose PLUM, a unified co-design framework that inte-
grates DNN inference systems and quantization (forward and backward pass) to leverage
repetition-sparsity trade-off to improve inference efficiency. Our results demonstrate
that PLUMâ€™s quantization method is more accurate than binary quantization with the
same number of non-zero weights. Detailed analysis indicates that signed binarization
generates a smaller distribution of effectual (non-zero) parameters nested within a larger
distribution of total parameters of latent full-precision weights for a DNN block. Finally,
the proposed PLUM framework achieves a 26% speedup on real hardware, doubles energy
efficiency, and reduces density by 2.8Ã—compared to binary methods while retaining top-1
accuracy when compared to prior-art methods for ResNets on ImageNet (by achieving
66.2% top-1 accuracy), presenting an alternative solution for deploying efficient models in
resource-limited environments. Code available at https://github.com/sachitkuhar/PLUM
1",TMLR
"Persistent homology (PH) is a method for generating topology-inspired representations of
data. Empirical studies that investigate the properties of PH, such as its sensitivity to
perturbations or ability to detect a feature of interest, commonly rely on training and testing
an additional model on the basis of the PH representation. To gain more intrinsic insights
about PH, independently of the choice of such a model, we propose a novel methodology
based on the pull-back geometry that a PH encoding induces on the data manifold. The
spectrum and eigenvectors of the induced metric help to identify the most and least significant
information captured by PH. Furthermore, the pull-back norm of tangent vectors provides
insights about the sensitivity of PH to a given perturbation, or its potential to detect a
given feature of interest, and in turn its ability to solve a given classification or regression
problem. Experimentally, the insights gained through our methodology align well with the
existing knowledge about PH. Moreover, we show that the pull-back norm correlates with
the performance on downstream tasks, and can therefore guide the choice of a suitable PH
encoding.
Keywords: Persistent homology, data representation, Jacobian spectrum, pull-back geometry,
sensitivity analysis
1",TMLR
"Extracting Implicit Neural Representations (INRs) on video data poses unique challenges
duetotheadditionaltemporaldimension. Inthecontextofvideos,INRshavepredominantly
relied on a frame-only parameterization, which sacrifices the spatiotemporal continuity ob-
served in pixel-level (spatial) representations. To mitigate this, we introduce Polynomial
NeuralRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR for
videos that preserves spatiotemporal continuity. PNeRV leverages the modeling capabilities
of Polynomial Neural Networks to perform the modulation of a continuous spatial (patch)
signal with a continuous time (frame) signal. We further propose a custom Hierarchical
Patch-wise Spatial Sampling Scheme that ensures spatial continuity while retaining param-
eter efficiency. We also employ a carefully designed Positional Embedding methodology to
further enhance PNeRVâ€™s performance. Our extensive experimentation demonstrates that
PNeRV outperforms the baselines in conventional Implicit Neural Representation tasks like
compression along with downstream applications that require spatiotemporal continuity in
the underlying representation. PNeRV not only addresses the challenges posed by video
data in the realm of INRs but also opens new avenues for advanced video processing and
analysis.
1",TMLR
"Confounding is a significant obstacle to unbiased estimation of causal effects from observa-
tional data. For settings with high-dimensional covariatesâ€”such as text data, genomics, or
the behavioral social sciencesâ€”researchers have proposed methods to adjust for confounding
by adapting machine learning methods to the goal of causal estimation. However, empirical
evaluation of these adjustment methods has been challenging and limited. In this work, we
build on a promising empirical evaluation strategy that simplifies evaluation design and uses
real data: subsampling randomized controlled trials (RCTs) to create confounded observa-
tional datasets while using the average causal effects from the RCTs as ground-truth. We
contribute a new sampling algorithm, which we call RCT rejection sampling , and provide
theoretical guarantees that causal identification holds in the observational data to allow for
valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm
indeed results in low bias when oracle estimators are evaluated on the confounded sam-
ples, which is not always the case for a previously proposed algorithm. In addition to this
identification result, we highlight several finite data considerations for evaluation designers
who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we
implement an example evaluation pipeline and walk through these finite data considerations
with a novel, real-world RCTâ€”which we release publiclyâ€”consisting of approximately 70k
observations and text data as high-dimensional covariates. Together, these contributions
build towards a broader agenda of improved empirical evaluation for causal estimation.
1",TMLR
"The ï¬nite-time convergence of oï¬€-policy temporal diï¬€erence (TD) learning has been compre-
hensively studied recently. However, such a type of convergence has not been established for
oï¬€-policy TD learning in the multi-agent setting, which covers broader reinforcement learning
applications and is fundamentally more challenging. This work develops a decentralized TD
with correction (TDC) algorithm for multi-agent oï¬€-policy TD learning under Markovian
sampling. In particular, our algorithm avoids sharing the actions, policies and rewards of the
agents, and adopts mini-batch sampling to reduce the sampling variance and communication
frequency. Under Markovian sampling and linear function approximation, we proved that
the ï¬nite-time sample complexity of our algorithm for achieving an /epsilon1-accurate solution is in
the order ofO/parenleftbigMln/epsilon1âˆ’1
/epsilon1(1âˆ’Ïƒ2)2/parenrightbig
, whereMdenotes the total number of agents and Ïƒ2is a network
parameter. This matches the sample complexity of the centralized TDC. Moreover, our
algorithm achieves the optimal communication complexity O/parenleftbigâˆš
Mln/epsilon1âˆ’1
1âˆ’Ïƒ2/parenrightbig
for synchronizing
the value function parameters, which is order-wise lower than the communication complexity
of the existing decentralized TD(0). Numerical simulations corroborate our theoretical
ï¬ndings.
1",TMLR
"The growing reproducibility crisis in machine learning has brought forward a need for care-
ful examination of research findings. This paper investigates the claims made by Lei et
al. (2023) regarding their proposed method, LICO, for enhancing post-hoc interpretabil-
ity techniques and improving image classification performance. LICO leverages natural
language supervision from a vision-language model to enrich feature representations and
guide the learning process. We conduct a comprehensive reproducibility study, employing
(Wide) ResNets and established interpretability methods like Grad-CAM and RISE. We
were mostly unable to reproduce the authorsâ€™ results. In particular, we did not find that
LICO consistently led to improved classification performance or improvements in quantita-
tiveandqualitativemeasuresofinterpretability. Thus, ourfindingshighlighttheimportance
of rigorous evaluation and transparent reporting in interpretability research.
1",TMLR
"Minimizing functionals in the space of probability distributions can be done with Wasser-
stein gradient flows. To solve them numerically, a possible approach is to rely on the
Jordanâ€“Kinderlehrerâ€“Otto (JKO) scheme which is analogous to the proximal scheme in
Euclidean spaces. However, it requires solving a nested optimization problem at each it-
eration, and is known for its computational challenges, especially in high dimension. To
alleviate it, very recent works propose to approximate the JKO scheme leveraging Brenierâ€™s
theorem, and using gradients of Input Convex Neural Networks to parameterize the density
(JKO-ICNN). However, this method comes with a high computational cost and stability is-
sues. Instead, this work proposes to use gradient flows in the space of probability measures
endowed with the sliced-Wasserstein (SW) distance. We argue that this method is more flex-
ible than JKO-ICNN, since SW enjoys a closed-form differentiable approximation. Thus,
the density at each step can be parameterized by any generative model which alleviates the
computational burden and makes it tractable in higher dimensions.
1",TMLR
"Symmetries of input and latent vectors have provided valuable insights for disentanglement
learning in VAEs. However, only a few works were proposed as an unsupervised method,
and even these works require known factor information in the training data. We propose a
novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated
into VAEs for learning symmetry-based disentanglement in unsupervised learning without
any knowledge of the dataset factor information. CFASL incorporates three novel features
for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent
vector dimensions to factor-aligned symmetries within an explicit learnable symmetry code-
book 2) Learning a composite symmetry to express unknown factors change between two
random samples by learning factor-aligned symmetries within the codebook 3) Inducing a
group equivariant encoder and decoder in training VAEs with the two conditions. In ad-
dition, we propose an extended evaluation metric for multi-factor changes in comparison
to disentanglement evaluation in VAEs. In quantitative and in-depth qualitative analysis,
CFASL demonstrates a significant improvement of disentanglement in single-factor change,
and multi-factor change conditions compared to state-of-the-art methods.
1",TMLR
"Off-policy evaluation and learning are concerned with assessing a given policy and learning
an optimal policy from offline data without direct interaction with the environment. Often,
the environment in which the data are collected differs from the environment in which the
learned policy is applied. To account for the effect of different environments during learning
and execution, distributionally robust optimization (DRO) methods have been developed
that compute worst-case bounds on the policy values assuming that the distribution of the
new environment lies within an uncertainty set. Typically, this uncertainty set is defined
based on the KL divergence around the empirical distribution computed from the logging
dataset. However, the KL uncertainty set fails to encompass distributions with varying
support and lacks awareness of the geometry of the distribution support. As a result, KL
approachesfallshortinaddressingpracticalenvironmentmismatchesandleadtoover-fitting
to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach
that employs the Wasserstein distance instead. While Wasserstein DRO is generally com-
putationally more expensive compared to KL DRO, we present a regularized method and a
practical (biased) stochastic gradient descent method to optimize the policy efficiently. We
also provide a theoretical analysis of the finite sample complexity and iteration complexity
for our proposed method. We further validate our approach using a public dataset that was
recorded in a randomized stoke trial.
1",TMLR
"Thispaperpresentsaconvergenceanalysisandtrajectorycomparisonofthegradientdescent
(GD) method for overparameterized deep linear neural networks with different random ini-
tializations, demonstrating that the GD trajectory for these networks closely matches that
of the corresponding convex optimization problem. This study touches upon one major
open theoretical problem in machine learningâ€“why deep neural networks trained with GD
methods are efficient in many practical applications? While the solution of this problem is
still beyond the reach of general nonlinear deep neural networks, extensive efforts have been
invested in studying relevant questions for deep linear neural networks, and many interest-
ing results have been reported to date. For example, recent results on loss landscape show
that even though the loss function of deep linear neural networks is non-convex, every local
minimizer is also a global minimizer. We focus on the trajectory of GD when applied to deep
linear networks and demonstrate that, with appropriate initialization and sufficient width
of the hidden layers, the GD trajectory closely matches that of the corresponding convex
optimization problem. This result holds regardless of the depth of the network, providing
insight into the efficiency of GD in the training of deep neural networks. Furthermore, we
show that the GD trajectory for an overparameterized deep linear network automatically
avoids bad saddle points.
1",TMLR
"For many practical problems, it is important to measure similarity between graphs. This
can be done via graph kernels . One particular application where the choice of a graph kernel
is essential is assessing the quality of graph generative models. However, despite the vast
number of graph kernels available in the literature, only basic kernels are usually considered
for generative model evaluation. In this paper, we fill this gap and analyze how different
graph kernels perform as an ingredient in the pipeline of generative model performance
evaluation. To conduct a detailed analysis, we propose a framework for comparing graph
kernels in terms of which high-level structural properties they are sensitive to: heterogeneity
of degree distribution, the presence of community structure, the presence of latent geometry,
and others. For this, we design continuous transitions between random graph models that
affect a particular property and measure which graph kernel is sensitive to the corresponding
change. Weshowthatusingsuchdiversemodelswiththecorrespondingtransitionsiscrucial
for evaluation: many kernels can successfully capture some properties and fail on others.
We also found some well-known kernels that show good performance in our experiments but
have been previously overlooked in the literature on evaluating graph generative models.
1",TMLR
"Federated learning (FL) is a distributed learning paradigm that allows multiple clients to
collaboratively train a shared model via communications to a central server. However, optimal
models of different clients often differ due to heterogeneity of data across clients. In this
paper, we address the dichotomy between heterogeneous models and simultaneous training
in FL via a clustering structure among the clients. The clustering framework is one way to
allow for high heterogeneity level between clients, while clients with similar data can still train
a shared model. We define a new clustering framework for FL based on the (optimal) local
models of the clients: two clients belong to the same cluster if their local models are close. We
propose an algorithm, Successive Refine Federated Clustering Algorithm (SR-FCA), that treats
each client as a singleton cluster as an initialization, and then successively refine the cluster
estimation via exploiting similarity with other clients. In any intermediate step, SR-FCAuses
anerror-tolerant federated learning algorithm within each cluster to exploit simultaneous
training and to correct clustering errors. Unlike some prominent prior works SR-FCAdoes not
requireany goodinitialization(orwarmstart),bothintheoryandpractice. Weshowthatwith
proper choice of learning rate, SR-FCAincurs arbitrarily small clustering error. Additionally,
SR-FCAdoes not require the knowledge of the number of clusters apriori like some prior works.
We validate the performance of SR-FCAon real-world FL datasets including FEMNIST and
Shakespeare in non-convex problems and show the benefits of SR-FCAover several baselines.
1",TMLR
"Defining and measuring decision-making styles, also known as playstyles, is crucial in gam-
ing, where these styles reflect a broad spectrum of individuality and diversity. However,
finding a universally applicable measure for these styles poses a challenge. Building on
Playstyle Distance , the first unsupervised metric to measure playstyle similarity based on
game screens and raw actions by identifying comparable states with discrete representations
for computing policy distance, we introduce three enhancements to increase accuracy: mul-
tiscale analysis with varied state granularity, a perceptual kernel rooted in psychology, and
the utilization of the intersection-over-union method for efficient evaluation. These innova-
tions not only advance measurement precision but also offer insights into human cognition
of similarity. Across two racing games and seven Atari games, our techniques significantly
improve the precision of zero-shot playstyle classification, achieving an accuracy exceeding
90% with fewer than 512 observation-action pairsâ€”less than half an episode of these games.
Furthermore, our experiments with 2048andGodemonstrate the potential of discrete
playstyle measures in puzzle and board games. We also develop an algorithm for assessing
decision-making diversity using these measures. Our findings improve the measurement of
end-to-end game analysis and the evolution of artificial intelligence for diverse playstyles.
1",TMLR
"The issue of bias in Machine Learning (ML) models is a significant challenge for the machine
learning community. Real-world biases can be embedded in the data used to train models,
and prior studies have shown that ML models can learn and even amplify these biases.
This can result in unfair treatment of individuals based on their inherent characteristics
or sensitive attributes such as gender, race, or age. Ensuring fairness is crucial with the
increasing use of ML models in high-stakes scenarios and has gained significant attention
from researchers in recent years. However, the challenge of ensuring fairness becomes much
greater when the assumption of full access to sensitive attributes does not hold. The settings
where the hypothesis does not hold include cases where (1) only limited or noisy demographic
information is available or (2) demographic information is entirely unobserved due to privacy
restrictions. This survey reviews recent research efforts to enforce fairness when sensitive
attributes are missing. We propose a taxonomy of existing works and, more importantly,
highlight current challenges and future research directions to stimulate research in ML
fairness in the setting of missing sensitive attributes.
1",TMLR
"In order for artificial agents to successfully perform tasks in changing environments, they
must be able to both detect and adapt to novelty. However, visual novelty detection research
often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object
classification, where images focus on one distinct, well-centered object. New benchmarks
are needed to represent the challenges of navigating the complex scenes of an open world.
Our new NovelCraft1dataset contains multimodal episodic data of the images and symbolic
world-states seen by an agent completing a pogo stick assembly task within a modified
Minecraft environment. In some episodes, we insert novel objects of varying size within the
complex 3D scene that may impact gameplay. Our visual novelty detection benchmark finds
that methods that rank best on popular area-under-the-curve metrics may be outperformed
by simpler alternatives when controlling false positives matters most. Further multimodal
novelty detection experiments suggest that methods that fuse both visual and symbolic
information can improve time until detection as well as overall discrimination. Finally, our
evaluation of recent generalized category discovery methods suggests that adapting to new
imbalanced categories in complex scenes remains an exciting open problem.
Gameplay:Fence Gameplay:Tree Beacon Cactus Glowstone Pumpkin Web
Figure 1: Example images for a subset of novelties in NovelCraft. See Appendix Fig. A.1 for images of all 53 novelties.
1",TMLR
"Adam (Kingma & Ba, 2014) and Adam-family (Loshchilov & Hutter, 2017; Tieleman &
Hinton, 2012) of adaptive gradient methods have become indispensable for optimizing neural
networks, particularly in conjunction with Transformers (Vaswani et al., 2017; Dosovitskiy
et al., 2020). In this paper, we present a novel optimization anomaly called the Slingshot
Effect, which manifests during extremely late stages of training. We identify a distinctive
characteristic of this phenomenon through cyclic phase transitions between stable and
unstable training regimes, as evidenced by the cyclic behavior of the norm of the last
layerâ€™s weights. Although the Slingshot Effect can be easily reproduced in more general
settings, it does not align with any known optimization theories, emphasizing the need
for in-depth examination. Moreover, we make a noteworthy observation that Grokking, as
reported by Power et al. (2021), occurs predominantly during the onset of the Slingshot
Effects and is absent without it, even in the absence of explicit regularization. This finding
suggests a surprising inductive bias of adaptive gradient optimizers at late training stages,
urging a revised theoretical analysis of their origin. Our study sheds light on an intriguing
optimization behavior that has significant implications for understanding the inner workings
of Adam-family of gradient methods.
1",TMLR
"Deep learning with noisy labels is a challenging task, which has received much attention
from the machine learning and computer vision communities. Recent prominent methods
that build on a specific sample selection (SS) strategy and a specific semi-supervised learning
(SSL) model achieved state-of-the-art performance. Intuitively, better performance could
be achieved if stronger SS strategies and SSL models are employed. Following this intuition,
one might easily derive various effective noisy-label learning methods using different combi-
nations of SS strategies and SSL models, which is, however, simply reinventing the wheel in
essence. To prevent this problem, we propose SemiNLL , a versatile framework that investi-
gates how to naturally combine different SS and SSL components based on their effects and
efficiencies. We conduct a systematic and detailed analysis of the combinations of possible
components based on our framework. Our framework can absorb various SS strategies and
SSL backbones, utilizing their power to achieve promising performance. The instantiations
of our framework demonstrate substantial improvements over state-of-the-art methods on
benchmark-simulated and real-world datasets with noisy labels.
1",TMLR
"Graph neural networks are powerful tools that enable deep learning on non-Euclidean data
structures like graphs, point clouds, and meshes. They leverage the connectivity of data
points and can even benefit learning tasks on data, which is not naturally graph-structured
â€“like point clouds. In these cases, the graph structure needs to be determined from the
dataset, which adds a significant challenge to the learning process. This opens up a multi-
tude of design choices for creating suitable graph structures, which have a substantial impact
on the success of the graph learning task. However, so far no concrete guidance for choosing
the most appropriate graph construction is available, not only due to the large variety of
methods out there but also because of its strong connection to the dataset at hand. In
medicine, for example, a large variety of different data types complicates the selection of
graph construction methods even more. We therefore summarise the current state-of-the-art
graph construction methods, especially for medical data. In this work, we introduce a cat-
egorisation scheme for graph types and graph construction methods. We identify two main
strands of graph construction: static and adaptive methods, discuss their advantages and
disadvantages, and formulate recommendations for choosing a suitable graph construction
method. We furthermore discuss how a created graph structure can be assessed and to what
degree it supports graph learning. We hope to support medical research with graph deep
learning with this work by elucidating the wide variety of graph construction methods.8
1",TMLR
"Dimensionality reduction methods are unsupervised approaches which learn low-dimensional
spaces where some properties of the initial space, typically the notion of â€œneighborhoodâ€, are
preserved. Such methods usually require propagation on large k-NN graphs or complicated
optimizationsolvers. Ontheotherhand, self-supervised learning approaches, typicallyusedto
learn representations from scratch, rely on simple and more scalable frameworks for learning.
In this paper, we propose TLDR, a dimensionality reduction method for generic input spaces
thatisportingtherecentself-supervisedlearningframeworkofZbontaretal.(2021)tothespe-
ciï¬c task of dimensionality reduction, over arbitrary representations. We propose to use near-
est neighbors to build pairs from a training set and a redundancy reduction loss to learn an en-
coderthatproducesrepresentationsinvariantacrosssuchpairs. TLDRisamethodthatissim-
ple, easy to train, and of broad applicability; it consists of an oï¬„ine nearest neighbor computa-
tion step that can be highly approximated, and a straightforward learning process. Aiming for
scalability, we focus on improving lineardimensionality reduction, and show consistent gains
onimageanddocumentretrievaltasks, e.g.gaining+4%mAPoverPCAon ROxfordforGeM-
AP, improving the performance of DINO on ImageNet or retaining it with a 10Ã—compression.
Code available at: https://github.com/naver/tldr
1 Published in Transactions on Machine Learning Research (06/2022)
1",TMLR
"We propose a new object-centric video prediction algorithm based on the deep latent
particle (DLP) representation of Daniel & Tamar (2022a). In comparison to existing slot- or
patch-based representations, DLPs model the scene using a set of keypoints with learned
parameters for properties such as position and size, and are both efficient and interpretable.
Our method, deep dynamic latent particles (DDLP), yields state-of-the-art object-centric
video prediction results on several challenging datasets. The interpretable nature of DDLP
allows us to perform â€œwhat-ifâ€ generation â€“ predict the consequence of changing properties
of objects in the initial frames, and DLPâ€™s compact structure enables efficient diffusion-
based unconditional video generation. Videos, code and pre-trained models are available:
https://taldatech.github.io/ddlp-web/ .
Figure 1: Video prediction on OBJ3D. Top: ground-truth video overlaid with DDLPâ€™s inferred (posterior)
trajectory for particle on green ball (blue). Middle: DDLPâ€™s generated video conditioned on the first 4 frames,
andpredicted trajectory for particle on green ball (blue). Bottom: a modification in the latent particle
space (red arrow) according to the ""what if...?"" question, and the resulting DDLP video. Note the different
trajectory resulting from different collisions due to the modification.
1 Published in Transactions on Machine Learning Research (02/2024)
1",TMLR
"A natural way of estimating heteroscedastic label noise in regression is to model the observed
(potentially noisy) target as a sample from a normal distribution, whose parameters can
be learned by minimizing the negative log-likelihood. This formulation has desirable loss
attenuation properties, as it reduces the contribution of high-error examples. Intuitively, this
behavior can improve robustness against label noise by reducing overfitting. We propose
an extension of this simple and probabilistic approach to classification that has the same
desirable loss attenuation properties. Furthermore, we discuss and address some practical
challenges of this extension. We evaluate the effectiveness of the method by measuring
its robustness against label noise in classification. We perform enlightening experiments
exploring the inner workings of the method, including sensitivity to hyperparameters, ablation
studies, and other insightful analyses.
1",TMLR
"Few-shot learning with Graph Neural Networks (GNNs) is an important challenge in ex-
panding the remarkable success that GNNs have achieved. In the transductive node clas-
sification scenario, conventional supervised training methods for GNNs fail when only few
labeled nodes are available. Self-training, wherein the GNN is trained in stages by aug-
menting the training data with a subset of the unlabeled data and the predictions of the
GNN on this data (pseudolabels), has emerged as a promising approach to few-shot trans-
ductive learning. However, multi-stage self-training significantly increases the computa-
tional demands of GNN training. In addition, while the training set evolves considerably
across the stages of self-training, the GNN architecture, graph topology and training hy-
perparameters are kept constant, adversely affecting the accuracy of the resulting model
as well as the computational efficiency of training. To address this challenge, we propose
FASTRAIN-GNN, a framework for efficient and accurate self-training of GNNs with few
labeled nodes. FASTRAIN-GNN performs four main optimizations in each stage of self-
training: (1) Sampling-based Pseudolabel Filtering removes nodes whose pseudolabels are
likely to be incorrect from the enlarged training set. (2,3) Dynamic Sizing and Dynamic
Regularization find the optimal network architecture and amount of training regularization
in each stage of self-training, respectively, and (4) Progressive Graph Pruning removes se-
lected edges between nodes in the training set to reduce the impact of over-smoothing. On
few-shot node classification tasks using different GNN architectures, FASTRAIN-GNN pro-
duces models that are consistently more accurate (by up to 4.4%), while also substantially
reducing the self-training time (by up to 2.1Ã—) over the current state-of-the-art methods.
Code is available at https://github.com/amrnag/FASTRAIN-GNN .
1",TMLR
"With the ever-increasing complexity of large-scale pre-trained models coupled with a short-
age of labeled data for downstream training, transfer learning has become the primary
approach in many fields, including natural language processing, computer vision, and multi-
modal learning. Despite recent progress, the fine-tuning process for large-scale pre-trained
models in vision still mostly relies on trial and error. This work investigates the relation-
ship between neural collapse (NC) and transfer learning for classification problems. NC
is an intriguing while prevalent phenomenon that has been recently discovered in terms of
the final-layer features and linear classifiers of trained neural networks. Specifically, dur-
ing the terminal phase of training, NC implies that the variability of the features within
each class diminishes to zero, while the means of features between classes are maximally
and equally distanced. In this work, we examine the NC attributes of pre-trained models
on both downstream and training data for transfer learning, and we find strong correla-
tion between feature collapse and downstream performance. In particular, we discovered
a systematic pattern that emerges when linear probing pre-trained models on downstream
training data: the more feature collapse of pre-trained models on downstream data, the
higher the transfer accuracy. Additionally, we also studied the relationship between NC and
transfer accuracy on the training data. Moreover, these findings allow us to develop a prin-
cipled, parameter-efficient fine-tuning method that employs skip-connection to induce the
last-layer feature collapse on downstream data. Our proposed fine-tuning methods deliver
âˆ—The first two authors contributed equally to the work.
1 Published in Transactions on Machine Learning Research (05/2024)
good performances while reducing fine-tuning parameters by at least 90% and mitigating
overfitting in situations especially when the downstream data is scarce.
1",TMLR
"Large pretrained visual models exhibit remarkable generalization across diverse recognition
tasks. Yet, real-world applications often demand compact models tailored to specific prob-
lems. Variants of knowledge distillation have been devised for such a purpose, enabling
task-specific compact models (the students) to learn from a generic large pretrained one
(the teacher). In this paper, we show that the excellent robustness and versatility of recent
pretrained models challenge common practices established in the literature, calling for a
new set of optimal guidelines for task-specific distillation. To address the lack of samples in
downstream tasks, we also show that a variant of Mixup based on stable diffusion comple-
ments standard data augmentation. This strategy eliminates the need for engineered text
prompts and improves distillation of generic models into streamlined specialized networks.1
1",TMLR
"Interval analysis (or interval bound propagation, IBP) is a popular technique for verifying and
training provably robust deep neural networks, a fundamental challenge in the area of reliable
machine learning. However, despite substantial eï¬€orts, progress on addressing this key chal-
lenge has stagnated, calling into question whether interval analysis is a viable path forward.
In this paper we present a fundamental result on the limitation of neural networks for interval
analyzable robust classiï¬cation. Our main theorem shows that non-invertible functions
can not be built such that interval analysis is precise everywhere. Given this, we derive a
paradox: while every dataset can be robustly classiï¬ed, there are simple datasets that can
not be provably robustly classiï¬ed with interval analysis.
1",TMLR
"In real-world multi-agent reinforcement learning (MARL) applications, agents may not have
perfect state information (e.g., due to inaccurate measurement or malicious attacks), which
challenges the robustness of agentsâ€™ policies. Though robustness is getting important in
MARL deployment, little prior work has studied state uncertainties in MARL, neither in
problem formulation nor algorithm design. Motivated by this robustness issue and the
lack of corresponding studies, we study the problem of MARL with state uncertainty in
this work. We provide the first attempt to the theoretical and empirical analysis of this
challenging problem. We first model the problem as a Markov Game with state perturbation
adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov
Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA.
We conduct a fundamental analysis regarding MG-SPA such as giving conditions under
which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning
(RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle
high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC)
algorithm based on an analytical expression of the policy gradient derived in the paper.
Our experiments show that the proposed RMAQ algorithm converges to the optimal value
function; our RMAAC algorithm outperforms several MARL and robust MARL methods in
multiple multi-agent environments when state uncertainty is present. The source code is
public on https://github.com/sihongho/robust_marl_with_state_uncertainty .
1 Published in Transactions on Machine Learning Research (06/2023)
1",TMLR
"Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-
image diffusion models. It operates by combining the conditional and unconditional pre-
dictions using a fixed weight. However, recent works vary the weights throughout the dif-
fusion process, reporting superior results but without providing any rationale or analysis.
By conducting comprehensive experiments, this paper provides insights into CFG weight
schedulers. Our findings suggest that simple, monotonically increasing weight schedulers
consistently lead to improved performances, requiring merely a single line of code. In addi-
tion, more complex parametrized schedulers can be optimized for further improvement, but
do not generalize across different models and tasks.
1",TMLR
"Image generation using diffusion can be controlled in multiple ways. In this paper, we
systematically analyze the equations of modern generative diffusion networks to propose
a framework, called MDP, that explains the design space of suitable manipulations. We
identify 5 different manipulations, including intermediate latent, conditional embedding,
cross attention maps, guidance, and predicted noise. We analyze the corresponding param-
eters of these manipulations and the manipulation schedule. We show that some previous
editing methods fit nicely into our framework. Particularly, we identified one specific con-
figuration as a new type of control by manipulating the predicted noise, which can perform
higher-quality edits than previous work for a variety of local and global edits.
1",TMLR
"Unsupervised Domain Adaptation (UDA) aims at classifying unlabeled target images lever-
aging source labeled ones. In the case of an extreme label shift scenario between the source
and target domains, where we have extra source classes not present in the target domain, the
UDA problem becomes a harder problem called Partial Domain Adaptation (PDA). While
differentmethodshavebeendevelopedtosolvethePDAproblem, mostsuccessfulalgorithms
use model selection strategies that rely on target labels to find the best hyper-parameters
and/or models along training. These strategies violate the main assumption in PDA: only
unlabeled target domain samples are available. In addition, there are also experimental in-
consistencies between developed methods - different architectures, hyper-parameter tuning,
number of runs - yielding unfair comparisons. The main goal of this work is to provide a
realistic evaluation of PDA methods under different model selection strategies and a con-
sistent evaluation protocol. We evaluate 6 state-of-the-art PDA algorithms on 2 different
real-world datasets using 7 different model selection strategies. Our two main findings are:
(i)without target labels for model selection, the accuracy of the methods decreases up
to 30 percentage points; (ii)only one method and model selection pair performs well on
both datasets. Experiments were performed with our PyTorch framework, BenchmarkPDA,
which we open source.
1",TMLR
"We propose the first study of adversarial attacks on online learning to rank. The goal of
the attacker is to misguide the online learning to rank algorithm to place the target item
on top of the ranking list linear times to time horizon Twith a sublinear attack cost. We
propose generalized list poisoning attacks that perturb the ranking list presented to the
user. This strategy can efficiently attack any no-regret ranker in general stochastic click
models. Furthermore, we propose a click poisoning-based strategy named attack-then-quit
that can efficiently attack two representative OLTR algorithms for stochastic click models.
We theoretically analyze the success and cost upper bound of the two proposed methods.
Experimental results based on synthetic and real-world data further validate the effectiveness
and cost-efficiency of the proposed attack strategies.
1",TMLR
"Regression is a powerful tool to accurately predict the outcome metric of a system given a
set of parameters, but has traditionally been restricted to methods which are only appli-
cable to a specific task. In this paper, we propose OmniPred , a framework for training
language models as universal end-to-end regressors over (x,y)data from arbitrary formats.
Using data sourced from Google Vizier, one of the largest proprietary blackbox optimization
databases in the world, our extensive experiments demonstrate that language models are
capable of very precise numerical regression using only textual representations of mathe-
matical parameters and values, and if given the opportunity to train at scale over multiple
tasks, can significantly outperform traditional regression models.
1",TMLR
"Counterfactuals answer questions of what would have been observed under altered circum-
stances and can therefore offer valuable insights. Whereas the classical interventional in-
terpretation of counterfactuals has been studied extensively, backtracking constitutes a less
studied alternative where all causal laws are kept intact. In the present work, we introduce
a practical method called deep backtracking counterfactuals (DeepBC) for computing back-
tracking counterfactuals in structural causal models that consist of deep generative compo-
nents. We propose two distinct versions of our methodâ€”one utilizing Langevin Monte Carlo
sampling and the other employing constrained optimizationâ€”to generate counterfactuals for
high-dimensional data. As a special case, our formulation reduces to methods in the field of
counterfactual explanations. Compared to these, our approach represents a causally com-
pliant, versatile and modular alternative. We demonstrate these properties experimentally
on a modified version of MNIST and CelebA.
1",TMLR
"Many approaches have been proposed to use diusion models to augment training datasets
for downstream tasks, such as classiï¬cation. However, diusion models are themselves
trained on large datasets, often with noisy annotations, and it remains an open question to
which extent these models contribute to downstream classiï¬cation performance. In particu-
lar, it remains unclear if they generalize enough to improve over directly using the additional
data of their pre-training process for augmentation. We systematically evaluate a range of
existing methods to generate images from diusion models and study new extensions to
assess their beneï¬t for data augmentation. Personalizing diusion models towards the tar-
get data outperforms simpler prompting strategies. However, using the pre-training data of
the diusion model alone, via a simple nearest-neighbor retrieval procedure, leads to even
stronger downstream performance. Our study explores the potential of diusion models in
generating new training data, and surprisingly ï¬nds that these sophisticated models are not
yet able to beat a simple and strong image retrieval baseline on simple downstream vision
tasks.
1",TMLR
"Mechanism design in resource allocation studies dividing limited resources among self-
interested agents whose satisfaction with the allocation depends on privately held utilities.
We consider the problem in a payment-free setting, with the aim of maximizing social
welfare while enforcing incentive compatibility (IC), i.e., agents cannot inflate allocations by
misreporting their utilities. The well-known proportional fairness (PF) mechanism achieves
the maximum possible social welfare but incurs an undesirably high exploitability (the
maximum unilateral inflation in utility from misreport and a measure of deviation from
IC). In fact, it is known that no mechanism can achieve the maximum social welfare and
exact incentive compatibility (IC) simultaneously without the use of monetary incentives
(Cole et al., 2013). Motivated by this fact, we propose learning an approximate mechanism
that desirably trades off the competing objectives. Our main contribution is to design an
innovative neural network architecture tailored to the resource allocation problem, which
we name Regularized Proportional Fairness Network ( RPF-Net).RPF-Net regularizes the
output of the PF mechanism by a learned function approximator of the most exploitable
allocation, with the aim of reducing the incentive for any agent to misreport. We derive
generalization bounds that guarantee the mechanism performance when trained under finite
and out-of-distribution samples and experimentally demonstrate the merits of the proposed
mechanism compared to the state-of-the-art.
The PF mechanism acts as an important benchmark for comparing the social welfare of any
mechanism. However, there exists no established way of computing its exploitability. The
challenge here is that we need to find the maximizer of an optimization problem for which
the gradient is only implicitly defined. We for the first time provide a systematic method
for finding such (sub)gradients, which enables the evaluation of the exploitability of the PF
mechanism through iterative (sub)gradient ascent.
1",TMLR
"Inrecentyearstherehasbeenincreasedinterestinunderstandingtheinterplaybetweendeep
generative models (DGMs) and the manifold hypothesis. Research in this area focuses on
understanding the reasons why commonly-used DGMs succeed or fail at learning distribu-
tions supported on unknown low-dimensional manifolds, as well as developing new models
explicitly designed to account for manifold-supported data. This manifold lens provides
both clarity as to why some DGMs (e.g. diffusion models and some generative adversarial
networks) empirically surpass others (e.g. likelihood-based models such as variational au-
toencoders, normalizing flows, or energy-based models) at sample generation, and guidance
for devising more performant DGMs. We carry out the first survey of DGMs viewed through
thislens, makingtwonovelcontributionsalongtheway. First, weformallyestablishthatnu-
merical instability of likelihoods in high ambient dimensions is unavoidable when modelling
data with low intrinsic dimension. We then show that DGMs on learned representations
of autoencoders can be interpreted as approximately minimizing Wasserstein distance: this
result, which applies to latent diffusion models, helps justify their outstanding empirical
results. The manifold lens provides a rich perspective from which to understand DGMs,
and we aim to make this perspective more accessible and widespread.
1 Published in Transactions on Machine Learning Research (09/2024)
Contents
1",TMLR
"Importance weighting is a fundamental procedure in statistics and machine learning that
weights the objective function or probability distribution based on the importance of the
instance in some sense. The simplicity and usefulness of the idea has led to many applica-
tions of importance weighting. For example, it is known that supervised learning under an
assumption about the difference between the training and test distributions, called distribu-
tion shift, can guarantee statistically desirable properties through importance weighting by
their density ratio. This survey summarizes the broad applications of importance weighting
in machine learning and related research.
1",TMLR
"Predicting and anticipating future outcomes or reasoning about missing information in
a sequence are critical skills for agents to be able to make intelligent decisions. This
requires strong, temporally coherent generative capabilities. Diffusion models have shown
remarkable success in several generative tasks, but have not been extensively explored in
the video domain. We present Random-Mask Video Diffusion (RaMViD), which extends
image diffusion models to videos using 3D convolutions, and introduces a new conditioning
technique during training. By varying the mask we condition on, the model is able to
perform video prediction, infilling, and upsampling. Due to our simple conditioning scheme,
we can utilize the same architecture as used for unconditional training, which allows us
to train the model in a conditional and unconditional fashion at the same time. We
evaluate RaMViD on two benchmark datasets for video prediction, on which we achieve
state-of-the-art results, and one for video generation. High-resolution videos are provided at
https://sites.google.com/view/video-diffusion-prediction .
1",TMLR
"Transformers have shown remarkable success in natural language processing and computer
vision, serving as the foundation of large language and multimodal models. These networks
can capture nuanced context sensitivity across high-dimensional language tokens or image
pixels, but it remains unclear how highly structured behavior and systematic generalization
can arise in these systems. Here, we explore the solution process a causal transformer
discovers as it learns to solve a set of algorithmic tasks involving copying, sorting, and
hierarchical compositions of these operations. We search for the minimal layer and head
configuration sufficient to solve these tasks and unpack the roles of the attention heads,
as well as how token representations are reweighted across layers to complement these
roles. Our results provide new insights into how attention layers in transformers support
structured computation within and across tasks: 1) Replacing fixed position labels with
labels sampled from a larger set enables strong length generalization and faster learning. The
learnable embeddings of these labels develop different representations, capturing sequence
order if necessary, depending on task demand. 2) Two-layer transformers can learn reliable
solutions to the multi-level problems we explore. The first layer tends to transform the input
representation to allow the second layer to share computation across repeated components
within a task or across related tasks. 3) We introduce an analysis pipeline that quantifies
how the representation space in a given layer prioritizes different aspects of each item. We
show that these representations prioritize information needed to guide attention relative to
information that only requires downstream readout.
1",TMLR
"Deep learning has dramatically improved performance in various image analysis applications
in the last few years. However, recent deep learning architectures can be very large, with up
to hundreds of layers and millions or even billions of model parameters that are impossible to
ï¬t into commodity graphics processing units. We propose a novel approach for compressing
high-dimensional activation maps, the most memory-consuming part when training modern
deep learning architectures. The proposed method can be used to compress the feature
maps of a single layer, multiple layers, or the entire network according to speciï¬c needs.
To this end, we also evaluated three diï¬€erent methods to compress the activation maps:
Wavelet Transform, Discrete Cosine Transform, and Simple Thresholding. We performed
experiments in two classiï¬cation tasks for natural images and two semantic segmentation
tasks for medical images. Using the proposed method, we could reduce the memory usage
for activation maps by up to 95%. Additionally, we show that the proposed method
induces a regularization eï¬€ect that acts on the layer weight gradients. Code is available at
https://github.com/vuhoangminh/Compressing-the-Activation-Maps-in-DNNs .
1",TMLR
"We present the workflow of Online Iterative Reinforcement Learning from Human Feed-
back (RLHF) in this technical report, which is widely reported to outperform its offline
counterpart by a large margin in the recent large language model (LLM) literature. How-
ever, existing open-source RLHF projects are still largely confined to the offline learning
setting. In this technical report, we aim to fill in this gap and provide a detailed recipe
that is easy to reproduce for online iterative RLHF. In particular, since online human feed-
back is usually infeasible for open-source communities with limited resources, we start by
constructing preference models using a diverse set of open-source datasets and use the con-
structed proxy preference model to approximate human feedback. Then, we discuss the
theoretical insights and algorithmic principles behind online iterative RLHF, followed by a
detailed practical implementation. Our trained LLM achieves impressive performance on
LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well
as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that
supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance
with fully open-source datasets. Further, we have made our models, curated datasets, and
comprehensive step-by-step code guidebooks publicly available.
1",TMLR
"Practical Bayesian learning often requires (1) online inference, (2) dynamic models, and (3)
ensembling over multiple different models. Recent advances have shown how to use random
feature approximations to achieve scalable, online ensembling of Gaussian processes with
desirable theoretical properties and fruitful applications. One key to these methodsâ€™ success
is the inclusion of a random walk on the model parameters, which makes models dynamic.
We show that these methods can be generalized easily to any basis expansion model and
that using alternative basis expansions, such as Hilbert space Gaussian processes, often
results in better performance. To simplify the process of choosing a specific basis expansion,
our methodâ€™s generality also allows the ensembling of several entirely different models, for
example, a Gaussian process and polynomial regression. Finally, we propose a novel method
to ensemble static and dynamic models together.
1",TMLR
"The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the
dataset at training time, which can significantly impair the performance of a machine learning
model in practical settings due to a lack of knowledge about the dataâ€™s distribution at test
time. To address this problem, we postulate that real-world distributions are composed of
latentInvariant Elementary Distributions (I.E.D) across different domains. This assumption
implies an invariant structure in the solution space that enables knowledge transfer to unseen
domains. To exploit this property for domain generalization, we introduce a modular neural
network layer consisting of Gated Domain Units (GDUs) that learn a representation for each
latent elementary distribution. During inference, a weighted ensemble of learning machines
can be created by comparing new observations with the representations of each elementary
distribution. Our flexible framework also accommodates scenarios where explicit domain
information is not present. Extensive experiments on image, text, and graph data show
consistent performance improvement on out-of-training target domains. These findings
support the practicality of the I.E.D assumption and the effectiveness of GDUs for domain
generalisation.
âˆ—SF and AD contributed equally to this paper.
â€ EE and SLC contributed equally to this paper.
1 Published in Transactions on Machine Learning Research (09/2023)
1",TMLR
"Many practical applications require optimization of multiple, computationally expensive, and
possibly competing objectives that are well-suited for multi-objective Bayesian optimization
(MOBO). However, for many types of biomedical data, measures of data analysis workflow
success are often heuristic and therefore it is not known a prioriwhich objectives are useful.
Thus, MOBO methods that return the full Pareto front may be suboptimal in these cases.
Here we propose a novel MOBO method that adaptively updates the scalarization function
using properties of the posterior of a multi-output Gaussian process surrogate function. This
approach selects useful objectives based on a flexible set of desirable criteria, allowing the
functional form of each objective to guide optimization. We demonstrate the qualitative
behaviour of our method on toy data and perform proof-of-concept analyses of single-cell
RNA sequencing and highly multiplexed imaging datasets for univariate input optimization.
1",TMLR
"Mixup is a popular regularization technique for training deep neural networks that improves
generalization and increases robustness to certain distribution shifts. It perturbs input
training data in the direction of other randomly-chosen instances in the training set. To
better leverage the structure of the data, we extend mixup in a simple, broadly applicable
way tok-mixup, which perturbs k-batches of training points in the direction of other k-
batches. The perturbation is done with displacement interpolation, i.e. interpolation under
the Wasserstein metric. We demonstrate theoretically and in simulations that k-mixup
preserves cluster and manifold structures, and we extend theory studying the efficacy of
standard mixup to the k-mixup case. Our empirical results show that training with k-mixup
further improves generalization and robustness across several network architectures and
benchmark datasets of differing modalities. For the wide variety of real datasets considered,
the performance gains of k-mixup over standard mixup are similar to or larger than the
gains of mixup itself over standard ERM after hyperparameter optimization. In several
instances, in fact, k-mixup achieves gains in settings where standard mixup has negligible to
zero improvement over ERM.
1",TMLR
"Recent work on permutation-based model merging has shown impressive low- or zero-barrier mode
connectivity between models from completely different initializations. However, this line of work
has not yet extended to the Transformer architecture, despite its dominant popularity in the language
domain. Therefore, in this work, we investigate the extent to which separate Transformer minima
learn similar features, and propose a model merging technique to investigate the relationship between
these minima in the loss landscape. The specifics of the architecture, like its residual connections,
multi-headed attention, and discrete, sequential input, require specific interventions in order to
compute model permutations that remain within the same functional equivalence class. In merging
these models with our method, we consistently find lower loss barriers between minima compared
to model averaging, across models trained on a masked-language modeling task or fine-tuned on a
language understanding benchmark. Our results show that the minima of these models are less sharp
and isolated than previously understood, and provide a basis for future work on merging separately
trained Transformer models.
1",TMLR
"Unsupervised domain adaptation transfers knowledge from a fully labeled source domain to a
different target domain, where no labeled data are available. Some researchers have proposed
upper bounds for the target error when transferring knowledge. For example, Ben-David
et al. (2010) established a theory based on minimizing the source error and distance between
marginal distributions simultaneously. However, in most research, the joint error is ignored
because of its intractability. In this research, we argue that joint errors are essential for
domain adaptation problems, particularly when the domain gap is large. To address this
problem, we propose a novel objective related to the upper bound of the joint error. Moreover,
we adopt a source/pseudo-target label-induced hypothesis space that can reduce the search
space to further tighten this bound. To measure the dissimilarity between hypotheses, we
define a novel cross-margin discrepancy to alleviate instability during adversarial learning.
In addition, we present extensive empirical evidence showing that the proposed method
boosts the performance of image classification accuracy on standard domain adaptation
benchmarks.
1",TMLR
"Transfer learning is a powerful technique for knowledge-sharing between different tasks.
Recent work has found that the representations of models with certain invariances, such
as to adversarial input perturbations, achieve higher performance on downstream tasks.
These findings suggest that invariance may be an important property in the context of
transfer learning. However, the relationship of invariance with transfer performance is not
fully understood yet and a number of questions remain. For instance, how important is
invariance compared to other factors of the pretraining task? How transferable is learned
invariance? In this work, we systematically investigate the importance of representational
invariance for transfer learning, as well as how it interacts with other parameters during
pretraining. To do so, we introduce a family of synthetic datasets that allow us to precisely
control factors of variation both in training and test data. Using these datasets, we a) show
that for learning representations with high transfer performance, invariance to the right
transformations is as, or often more, important than most other factors such as the number
of training samples, the model architecture and the identity of the pretraining classes, b)
show conditions under which invariance can harm the ability to transfer representations and
c) explore how transferable invariance is between tasks. The code is available at https:
//github.com/tillspeicher/representation-invariance-transfer .
1",TMLR
"Foundationmodeldevelopmentattractsarapidlyexpandingbodyofcontributors,scien-
tists, and applications. To help shape responsible development practices , we introduce the
Foundation Model Development Cheatsheet: a growing collection of 250+ tools and re-
sources spanning text, vision, and speech modalities. We draw on a large body of prior
worktosurveyresources( e.g.software,documentation, frameworks, guides,andpractical
tools) that support informed data selection, processing, and understanding, precise and
limitation-aware artifact documentation, efficient model training, advance awareness of the
environmental impact from training, careful model evaluation of capabilities, risks, and
claims,aswellasresponsiblemodelrelease,licensinganddeploymentpractices. Theprocess
of curating this list, enabled us to review the AI development ecosystem, revealing what
tools are critically missing, misused, or over-used in existing practices. We find that (i) tools
for data sourcing, model evaluation, and monitoring are critically under-serving ethical and
real-world needs,(ii) evaluationsfor modelsafety, capabilities,and environmentalimpact
all lack reproducibility and transparency, (iii) text and particularly English-centric analyses
continuetodominateovermultilingualandmulti-modalanalyses,and(iv)evaluationof
systems, rather than just models, is needed for capabilities to be assessed in context.
âˆ—Equal contribution. Correspondence: slongpre@mit.edu.1 Published in Transactions on Machine Learning Research (12/2024)
1",TMLR
"Large Language Models (LLMs) trained on massive datasets may inadvertently acquire sensi-
tive information such as personal details and potentially harmful content. This risk is further
heightened in multimodal LLMs (aka MLLMs) as they integrate information from multiple
modalities (image and text). Adversaries can exploit this stored knowledge by crafting
inputs across modalities to extract sensitive details. Evaluating how effectively MLLMs can
forget such information (targeted unlearning) necessitates the creation of high-quality, well-
annotated image-text pairs. While significant research has addressed the creation of datasets
for unlearning within LLMs, it has primarily concentrated on text modality. Creation of anal-
ogous datasets for multimodal data and models remain an understudied area. To address this
gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Out-
side Knowledge VQA), as well as an â€œattack-and-defenseâ€ framework to evaluate methods for
deleting specific multimodal knowledge from MLLMs. Our dataset generation process involves
an automated pipeline to create samples of varied proximity levels to the target data point
for evaluation of generalization and specificity, followed by manual filtering to retain only the
high-quality data points. We use this process to extend a visual question-answering dataset
for evaluating multimodal information deletion. Next, we present a comprehensive unlearning
evaluation involving an attack-and-defense framework consisting of four whitebox and three
blackbox attacks against six unlearning defense objectives. We also design a whitebox attack
based on the interpretability of hidden states in LLMs motivated by past work. Our experi-
mental results demonstrate that multimodal extraction attacks (with an attack success rate of
45.5%) are more successful than either image-only ( 32%) or text-only attacks ( 39%). The best
1 Published in Transactions on Machine Learning Research (12/2024)
overall defense mechanism, which removes answer information from internal model hidden
states, reduces the success rate of multimodal attack to 15.7%. Furthermore, our findings
suggest that larger models exhibit greater resilience to attacks after being edited for deletion,
implying that scaling models could be a valuable strategy for enhancing robustness and devel-
oping safer systems. UnLOK-VQA thus facilitates a comprehensive evaluation of unlearning
in MLLMs and serves as a challenging benchmark for future research in unlearning.1.
1",TMLR
"Bayesian Optimization (BO) is an effective method for finding the global optimum of expen-
sive black-box functions. However, it is well known that applying BO to high-dimensional
optimization problems is challenging. To address this issue, a promising solution is to use
a local search strategy that partitions the search domain into local regions with high likeli-
hood of containing the global optimum, and then use BO to optimize the objective function
within these regions. In this paper, we propose a novel technique for defining the local
regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA
to learn a search distribution that can estimate the probabilities of data points being the
global optimum of the objective function. Based on this search distribution, we then define
the local regions consisting of data points with high probabilities of being the global op-
timum. Our approach serves as a meta-algorithm as it can incorporate existing black-box
BO optimizers, such as BO,TuRBO(Eriksson et al., 2019), and BAxUS(Papenmeier et al.,
2022), to find the global optimum of the objective function within our derived local regions.
We evaluate our proposed method on various benchmark synthetic and real-world problems.
The results demonstrate that our method outperforms existing state-of-the-art techniques.
1",TMLR
"Parkinsonâ€™s Disease (PD) is the second most common neurodegenerative disease in humans.
PD is characterized by the gradual loss of dopaminergic neurons in the Substantia Nigra
(SN, a part of the mid-brain). Counting the number of dopaminergic neurons in the SN is
one of the most important indexes in evaluating drug efficacy in PD animal models. Cur-
rently, analyzing and quantifying dopaminergic neurons is conducted manually by experts
through analysis of digital pathology images which is laborious, time-consuming, and highly
subjective. As such, a reliable and unbiased automated system is demanded for the quan-
tification of dopaminergic neurons in digital pathology images. Recent years have seen a
surge in adopting deep learning solutions in medical image processing. However, developing
high-performing deep learning models hinges on the availability of large-scale, high-quality
annotated data, which can be expensive to acquire, especially in applications like digital
pathology image analysis. To this end, we propose an end-to-end deep learning framework
based on self-supervised learning for the segmentation and quantification of dopaminergic
neurons in PD animal models. To the best of our knowledge, this is the first deep learning
model that detects the cell body of dopaminergic neurons, counts the number of dopaminer-
gic neurons, and provides characteristics of individual dopaminergic neurons as a numerical
output. Extensive experiments demonstrate the effectiveness of our model in quantifying
neurons with high precision, which can provide a faster turnaround for drug efficacy studies,
âˆ—Corresponding authors
1 Published in Transactions on Machine Learning Research (10/2023)
better understanding of dopaminergic neuronal health status, and unbiased results in PD
pre-clinical research. As part of our contributions, we also provide the firstpublicly avail-
able dataset of histology digital images along with expert annotations for the segmentation
of TH-positive DA neuronal soma.
1",TMLR
"Causal structure learning methods are vital for unveiling causal relationships embedded
into observed data. However, the state of the art suffers a major limitation: it assumes
that causal interactions occur only at the frequency at which data is observed. To address
this limitation, this paper proposes a method that allows structural learning of linear causal
relationships occurring at different time scales. Specifically, we explicitly take into account
instantaneous and lagged inter-relations between multiple time series, represented at differ-
ent scales, hinging on wavelet transform. We cast the problem as the learning of a multiscale
causal graph havingsparsestructure and dagnessconstraints, enforcing causality through
directed and acyclic topology. To solve the resulting (non-convex) formulation, we propose
an algorithm termed MS-CASTLE, which exhibits consistent performance across differ-
ent noise distributions and wavelet choices. We also propose a single-scale version of our
algorithm, SS-CASTLE, which outperforms existing methods in computational efficiency,
performance, and robustness on synthetic data. Finally, we apply the proposed approach to
learn the multiscale causal structure of the risk of 15 global equity markets, during covid-19
pandemic, illustrating the importance of multiscale analysis to reveal useful interactions at
different time resolutions. Financial investors can leverage our approach to manage risk
within equity portfolios from a causal perspective, tailored to their investment horizon.
1",TMLR
"Deep image prior (DIP) and its variants have shown remarkable potential to solve inverse
problemsincomputationalimaging, needing no separate training data . PracticalDIPmodels
are often substantially overparameterized. During the learning process, these models first
learn the desired visual content and then pick up potential modeling and observational
noise, i.e., performing early learning then overfitting (ELTO). Thus, the practicality of DIP
hinges on early stopping (ES) that can capture the transition period. In this regard, most
previous DIP works for computational imaging tasks only demonstrate the potential of the
models, reporting peak performance against ground truth, but providing no clue about how
to operationally obtain near-peak performance without access to ground truth . In this paper,
we set out to break this practicality barrier of DIP and propose an effective ES strategy that
consistently detects near-peak performance in various computational imaging tasks and DIP
variants. Simply based on the running variance of DIP intermediate reconstructions, our ES
method not only outpaces the existing onesâ€”which only work in very narrow regimes, but
also remains effective when combined with methods that try to mitigate overfitting. The
code to reproduce our experimental results is available at https://github.com/sun-umn/
Early_Stopping_for_DIP .
1",TMLR
"Cellularautomata(CAs)arenotablecomputationalmodelsexhibitingrichdynamicsemerging
from the local interaction of cells arranged in a regular lattice. Graph CAs (GCAs) generalise
standard CAs by allowing for arbitrary graphs rather than regular lattices, similar to how
Graph Neural Networks (GNNs) generalise Convolutional NNs. Recently, Graph Neural
CAs (GNCAs) have been proposed as models built on top of standard GNNs that can be
trained to approximate the transition rule of any arbitrary GCA. We note that existing
GNCAs can violate the locality principle of CAs by leveraging global information and,
furthermore, are anisotropic in the sense that their transition rules are not equivariant to
isometries of the nodesâ€™ spatial locations. However, it is desirable for instances related
by such transformations to be treated identically by the model. By replacing standard
graph convolutions with E( n)-equivariant ones, we avoid anisotropy by design and propose
a class of isotropic automata that we call E( n)-GNCAs. These models are lightweight,
but can nevertheless handle large graphs, capture complex dynamics and exhibit emergent
self-organising behaviours. We showcase the broad and successful applicability of E( n)-
GNCAs on three different tasks: (i) isotropic pattern formation, (ii) graph auto-encoding,
and (iii) simulation of E( n)-equivariant dynamical systems.
1",TMLR
"Generative models have recently achieved remarkable success and widespread adoption in
society, yet they often struggle to generate realistic and accurate outputs. This challenge
extends beyond language and vision into fields like engineering design, where safety-critical
engineering standards and non-negotiable physical laws tightly constrain what outputs are
considered acceptable. In this work, we introduce a novel training method to guide a
generative model toward constraint-satisfying outputs using â€˜negative dataâ€™ â€“ examples of
what to avoid. Our negative-data generative model (NDGM) formulation easily outperforms
classic models, generating 1/6 as many constraint-violating samples using 1/8 as much data
in certain problems. It also consistently outperforms other baselines, achieving a balance
between constraint satisfaction and distributional similarity that is unsurpassed by any other
model in 12 of the 14 problems tested. This widespread superiority is rigorously demonstrated
across numerous synthetic tests and real engineering problems, such as ship hull synthesis
with hydrodynamic constraints and vehicle design with impact safety constraints. Our
benchmarks showcase both the best-in-class performance of our new NDGM formulation
and the overall dominance of NDGMs versus classic generative models. We publicly release
the code and benchmarks at https://github.com/Lyleregenwetter/NDGMs .
1",TMLR
"Vector Gaussian processes are becoming increasingly important in machine learning and
statistics, with applications to many branches of applied sciences. Recent efforts have al-
lowed to understand smoothness in scalar Gaussian processes defined over manifolds as well
as over product spaces involving manifolds.
Under assumptions of Gaussianity and mean-square continuity, the smoothness of a zero-
mean scalar process is in one-to-one correspondence with the smoothness of the covariance
kernel. Unfortunately, such a result is not available for vector-valued random fields, as the
way each component in the covariance kernel contributes to the smoothness of the vector
field is unclear.
This paper challenges the problem of quantifying smoothness of matrix-valued continuous
kernels that are associated with mean-square continuous vector Gaussian processes defined
over non-Euclidean product manifolds. After noting that a constructive RKHS approach
is unsuitable for this specific task, we proceed through the analysis of spectral properties.
Specifically, wefindaspectralrepresentationtoquantifysmoothnessthroughSobolevspaces
that are adapted to certain measure spaces of product measures obtained through the ten-
sor product of Haar measures with multivariate Gaussian measures. Our results allow to
measure smoothness in a simple way, and open for the study of foundational properties of
certain machine learning techniques over product spaces.
1",TMLR
"The sensitivity to adversarial attacks and noise is a significant drawback of neural net-
works, and understanding and certifying their robustness has attracted much attention.
Studies have attempted to bridge two extreme analyses of robustness; one is the worst-case
analysis, which often gives too pessimistic certification, and the other is the average-case
analysis, which often fails to give a tight guarantee of robustness. Among them, Randomized
Smoothing became prominent by certifying a worst-case region of a classifier under input
noise. However, the method still suffers from several limitations, probably due to the lack of
a larger underlying framework to locate it. Here, inspired by the Smoothed Analysis of al-
gorithmic complexity, which bridges the worst-case and average-case analyses of algorithms,
we provide a theoretical framework for robustness analyses of classifiers, which contains
Randomized Smoothing as a special case. Using the framework, we also propose a novel
robustness analysis that works even in the small noise regime and thus provides a more con-
fident robustness certification than Randomized Smoothing . To validate the approach, we
evaluate the robustness of fully connected and convolutional neural networks on the MNIST
and CIFAR-10 datasets, respectively, and find that it indeed improves both adversarial and
noise robustness.
1",TMLR
"With the increasing popularity of graph neural networks (GNNs) in several sensitive applica-
tions like healthcare and medicine, concerns have been raised over the privacy aspects of
trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership
inference attacks, even if only black-box access to the trained model is granted. We propose
PrivGnn , a privacy-preserving framework for releasing GNN models in a centralized setting.
Assuming an access to a public unlabeled graph, PrivGnn provides a framework to release
GNN models trained explicitly on public data along with knowledge obtained from the
private data in a privacy preserving manner. PrivGnn combines the knowledge-distillation
framework with the two noise mechanisms, random subsampling, and noisy labeling, to
ensure rigorous privacy guarantees. We theoretically analyze our approach in the RÃ¨nyi
differential privacy framework. Besides, we show the solid experimental performance of
our method compared to several baselines adapted for graph-structured data. Our code is
available at https://github.com/iyempissy/privGnn .
1",TMLR
"Personalized federated learning, as a variant of federated learning, trains customized mod-
els for clients using their heterogeneously distributed data. However, it is still inconclusive
about how to design personalized models with better representation of shared global knowl-
edge and personalized pattern. To bridge the gap, we in this paper explore personalized
models with low-rank and sparse decomposition. Specifically, we employ proper regulariza-
tion to extract a low-rank global knowledge representation (GKR), so as to distill global
knowledge into a compact representation. Subsequently, we employ a sparse component
over the obtained GKR to fuse the personalized pattern into the global knowledge. As a
solution, we propose a two-stage proximal-based algorithm named Federated learning with
mixedSparse and Low-Rank representation (FedSLR) to efficiently search for the mixed
models. Theoretically, under proper assumptions, we show that the GKR trained by Fed-
SLR can at least sub-linearly converge to a stationary point of the regularized problem, and
that the sparse component being fused can converge to its stationary point under proper
settings. Extensive experiments also demonstrate the superior empirical performance of
FedSLR. Moreover, FedSLR reduces the number of parameters, and lowers the down-link
communication complexity, which are all desirable for federated learning algorithms. Source
code is available in https://github.com/huangtiansheng/fedslr .
1",TMLR
"The utility of reinforcement learning is limited by the alignment of reward functions with the
interests of human stakeholders. One promising method for alignment is to learn the reward
function from human-generated preferences between pairs of trajectory segments, a type of
reinforcement learning from human feedback (RLHF). These human preferences are typically
assumedtobeinformedsolelybypartialreturn,thesumofrewardsalongeachsegment. Wefind
this assumption to be flawed and propose modeling human preferences instead as informed by
eachsegmentâ€™sregret,ameasureofasegmentâ€™sdeviationfromoptimaldecision-making. Given
infinitelymanypreferencesgeneratedaccordingtoregret,weprovethatwecanidentifyareward
function equivalent to the reward function that generated those preferences, and we prove that
the previous partial return model lacks this identifiability property in multiple contexts. We
empirically show that our proposed regret preference model outperforms the partial return
preference model with finite training data in otherwise the same setting. Additionally, we find
that our proposed regret preference model better predicts real humanpreferences and also
learns reward functions from these preferences that lead to policies that are better human-
aligned. Overall,thisworkestablishesthatthechoiceofpreferencemodelisimpactful,andour
proposed regret preference model provides an improvement upon a core assumption of recent
research. We have open sourced our experimental code, the human preferences dataset we
gathered, and our training and preference elicitation interfaces for gathering a such a dataset.
1",TMLR
"In peer review, reviewers are usually asked to provide scores for the papers. The scores are
then used by Area Chairs or Program Chairs in various ways in the decision-making process.
The scores are usually elicited in a quantized form to accommodate the limited cognitive
ability of humans to describe their opinions in numerical values. It has been found that the
quantized scores suffer from a large number of ties, thereby leading to a significant loss of
information. To mitigate this issue, conferences have started to ask reviewers to additionally
provide a ranking of the papers they have reviewed. There are however two key challenges.
First, thereisnostandardprocedureforusingthisrankinginformationandAreaChairsmay
use it in different ways (including simply ignoring them), thereby leading to arbitrariness
in the peer-review process. Second, there are no suitable interfaces for judicious use of this
data nor methods to incorporate it in existing workflows, thereby leading to inefficiencies.
We take a principled approach to integrate the ranking information into the scores. The
output of our method is an updated score pertaining to each review that also incorporates
the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring
that rankings are incorporated into the updated scores in the same manner for all papers,
thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and
workflows designed for scores. We empirically evaluate our method on synthetic datasets as
well as on peer reviews from the ICLR 2017 conference, and find that it reduces the error
by approximately 30%as compared to the best performing baseline on the ICLR 2017 data.
1",TMLR
"Feature removal is a central building block for eXplainable AI (XAI), both for occlusion-
basedexplanations(Shapleyvalues)aswellastheirevaluation(pixelflipping, PF).However,
occlusion strategies can vary significantly from simple mean replacement up to inpainting
with state-of-the-art diffusion models. This ambiguity limits the usefulness of occlusion-
based approaches. For example, PF benchmarks lead to contradicting rankings. This is
amplified by competing PF measures: Features are either removed starting with most influ-
ential first (MIF) or least influential first (LIF).
This study proposes two complementary perspectives to resolve this disagreement problem.
Firstly, we address the common criticism of occlusion-based XAI, that artificial samples lead
to unreliable model evaluations. We propose to measure the reliability by the R(eference)-
Out-of-Model-Scope (OMS) score. The R-OMS score enables a systematic comparison of
occlusion strategies and resolves the disagreement problem by grouping consistent PF rank-
ings. Secondly, we show that the insightfulness of MIF and LIF is conversely dependent
on the R-OMS score. To leverage this, we combine the MIF and LIF measures into the
symmetric relevance gain (SRG) measure. This breaks the inherent connection to the un-
derlying occlusion strategy and leads to consistent rankings. This resolves the disagreement
problem of PF benchmarks, which we verify for a set of 40 different occlusion strategies.
1",TMLR
"Largelanguagemodels(LLMs)aregainingincreasingattentionfortheircapabilitytoprocess
graphs with rich text attributes, especially in a zero-shot fashion. Recent studies demon-
strate that LLMs obtain decent text classification performance on common text-rich graph
benchmarks, and the performance can be improved by appending encoded structural infor-
mation as natural languages into prompts. We aim to understand why the incorporation
of structural information inherent in graph data can improve the prediction performance
of LLMs. First, we rule out the concern of data leakage by curating a novel leakage-free
dataset and conducting a comparative analysis alongside a previously widely-used dataset.
Second, as past work usually encodes the ego-graph by describing the graph structure in
natural language, we ask the question: do LLMs understand the prompts in graph struc-
tures? Third, we investigate why LLMs can improve their performance after incorporating
structural information. Our exploration of these questions reveals that (i) there is no sub-
stantial evidence that the performance of LLMs is significantly attributed to data leakage;
(ii) instead of understanding prompts as graph structures, LLMs tend to process prompts
more as contextual paragraphs and (iii) the most efficient elements of the local neighborhood
included in the prompt are phrases that are pertinent to the node label, rather than the
graph structure1.
1",TMLR
"Theincreasingdepthofparametricdomainknowledgeinlargelanguagemodels(LLMs)isfu-
eling their rapid deployment in real-world applications. Understanding model vulnerabilities
in high-stakes and knowledge-intensive tasks is essential to quantifying the trustworthiness
of model predictions and regulating model use. The recent discovery of named entities as
adversarial examples (i.e. adversarial entities) in natural language processing tasks raises
questions about their potential impact on the knowledge robustness of pre-trained and fine-
tuned LLMs in high-stakes and specialized domains. We examined the use of type-consistent
entity substitution as a template for collecting adversarial entities for medium-sized billion-
parameterLLMswithbiomedicalknowledge. Tothisend,wedevelopedanembeddingspace,
gradient-free attack based on powerscaled distance-weighted sampling for robustness evalu-
ation, which has a low query budget and controllable coverage. Our method has favorable
query efficiency and scaling over alternative approaches based on blackbox gradient-guided
search, which we demonstrated for adversarial distractor generation in biomedical question
answering. Subsequent failure mode analysis uncovered two regimes of adversarial entities
on the attack surface with distinct characteristics. We also showed that entity substitution
attacks can manipulate token-wise Shapley value explanations, which become deceptive in
this setting. Our approach complements standard evaluations for high-capacity models and
the results highlight the brittleness of domain knowledge in LLMs1.
1",TMLR
"We equip a smaller Language Model to generalise to answering challenging compositional
questions that have not been seen in training. To do so we propose a combination of multi-
task supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities,
and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments.
Recent progress in question-answering has been achieved either through prompting methods
against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning
smaller models, sometimes in conjunction with information retrieval. We focus on the less
explored question of the extent to which zero-shot generalisation can be enabled in smaller
models with retrieval against a corpus within which sufficient information to answer a par-
ticular question may not exist. We establish strong baselines in this setting for diverse
evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA),
and show that performance can be significantly improved by adding retrieval-augmented
training datasets which are designed to expose our models to a variety of heuristic reasoning
strategies such as weighing partial evidence or ignoring an irrelevant context.
1",TMLR
"Composed image retrieval aims to ï¬nd an image that best matches a given multi-modal user
query consisting of a reference image and text pair. Existing methods commonly pre-compute
image embeddings over the entire corpus and compare these to a reference image embedding
modiï¬ed by the query text at test time. Such a pipeline is very eï¬ƒcient at test time since
fast vector distances can be used to evaluate candidates, but modifying the reference image
embedding guided only by a short textual description can be diï¬ƒcult, especially independent
of potential candidates. An alternative approach is to allow interactions between the query
and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from
the entire set. Though this approach is more discriminative, for large-scale datasets the
computational cost is prohibitive since pre-computation of candidate embeddings is no
longer possible. We propose to combine the merits of both schemes using a two-stage
model. Our ï¬rst stage adopts the conventional vector distancing metric and performs a fast
pruning among candidates. Meanwhile, our second stage employs a dual-encoder architecture,
which eï¬€ectively attends to the input triplet of reference-text-candidate and re-ranks the
candidates. Both stages utilize a vision-and-language pre-trained network, which has proven
beneï¬cial for various downstream tasks. Our method consistently outperforms state-of-the-
art approaches on standard benchmarks for the task. Our implementation is available at
https://github.com/Cuberick-Orion/Candidate-Reranking-CIR .
1",TMLR
"The Bayesian transformed Gaussian (BTG) model, proposed by Kedem and Oliviera in
1997, was developed as a Bayesian approach to trans-Kriging in the spatial statistics com-
munity. In this paper, we revisit BTG in the context of modern Gaussian process litera-
ture by framing it as a fully Bayesian counterpart to the Warped Gaussian process that
marginalizes out a joint prior over input warping and kernel hyperparameters. As with
any other fully Bayesian approach, this treatment introduces prohibitively expensive com-
putational overhead; unsurprisingly, the BTG posterior predictive distribution, itself esti-
mated through high-dimensional integration, must be inverted in order to perform model
prediction. To address these challenges, we introduce principled numerical techniques for
computing with BTG efficiently using a combination of doubly sparse quadrature rules,
tight quantile bounds, and rank-one matrix algebra to enable both fast model prediction
and model selection. These efficient methods allow us to compute with higher-dimensional
datasets and apply BTG with layered transformations that greatly improve its expressibil-
ity. We demonstrate that BTG achieves superior empirical performance over MLE-based
models in the low-data regime â€”situations in which MLE tends to overfit.
1",TMLR
"Neural network calibration is an essential task in deep learning to ensure consistency be-
tween the conï¬dence of model prediction and the true correctness likelihood. In this paper,
we propose a new post-processing calibration method called Neural Clamping ,w h i c he m -
ploys a simple joint input-output transformation on a pre-trained classiï¬er via a learnable
universal input perturbation and an output temperature scaling parameter. Moreover, we
provide theoretical explanations on why Neural Clamping is provably better than temper-
ature scaling. Evaluated on BloodMNIST, CIFAR-100, and ImageNet image recognition
datasets and a variety of deep neural network models, our empirical results show that Neu-
ral Clamping signiï¬cantly outperforms state-of-the-art post-processing calibration methods.
The code is available at github.com/yungchentang/NCToolkit, and the demo is available at
huggingface.co/spaces/TrustSafeAI/NCTV.
1",TMLR
"Diffusion Probabilistic Models stand as a critical tool in generative modelling, enabling the
generation of complex data distributions. This family of generative models yields record-
breaking performance in tasks such as image synthesis, video generation, and molecule
design. Despite their capabilities, their efficiency, especially in the reverse process, remains
a challenge due to slow convergence rates and high computational costs. In this paper,
we introduce an approach that leverages continuous dynamical systems to design a novel
denoising network for diffusion models that is more parameter-efficient, exhibits faster
convergence, and demonstrates increased noise robustness. Experimenting with Denoising
Diffusion Probabilistic Models (DDPMs), our framework operates with approximately a
quarter of the parameters, and âˆ¼30% of the Floating Point Operations (FLOPs) compared
to standard U-Nets in DDPMs. Furthermore, our model is notably faster in inference than
the baseline when measured in fair and equal conditions. We also provide a mathematical
intuition as to why our proposed reverse process is faster as well as a mathematical discussion
of the empirical tradeoffs in the denoising downstream task. Finally, we argue that our
method is compatible with existing performance enhancement techniques, enabling further
improvements in efficiency, quality, and speed.
1",TMLR
"Diagrams matter. Unfortunately, the deep learning community has no standard method for
diagramming architectures. The current combination of linear algebra notation and ad-hoc
diagrams fails to offer the necessary precision to understand architectures in all their detail.
However, this detail is critical for faithful implementation, mathematical analysis, further
innovation, and ethical assurances. I present neural circuit diagrams, a graphical language
tailored to the needs of communicating deep learning architectures. Neural circuit diagrams
naturally keep track of the changing arrangement of data, precisely show how operations
are broadcast over axes, and display the critical parallel behavior of linear operations. A
lingering issue with existing diagramming methods is the inability to simultaneously express
the detail of axes and the free arrangement of data, which neural circuit diagrams solve.
Their compositional structure is analogous to code, creating a close correspondence between
diagrams and implementation.
In this work, I introduce neural circuit diagrams for an audience of machine learning re-
searchers. After introducing neural circuit diagrams, I cover a host of architectures to show
their utility and breed familiarity. This includes the transformer architecture, convolution
(and its difficult-to-explain extensions), residual networks, the U-Net, and the vision trans-
former. I include a Jupyter notebook that provides evidence for the close correspondence
between diagrams and code. Finally, I examine backpropagation using neural circuit di-
agrams. I show their utility in providing mathematical insight and analyzing algorithmsâ€™
time and space complexities.
1",TMLR
"Replay methods are known to be successful at mitigating catastrophic forgetting in con-
tinual learning scenarios despite having limited access to historical data. However, storing
historical data is cheap in many real-world settings, yet replaying all historical data is often
prohibited due to processing time constraints. In such settings, we propose that continual
learning systems should learn the time to learn and schedule which tasks to replay at dif-
ferent time steps. We first demonstrate the benefits of our proposal by using Monte Carlo
tree search to find a proper replay schedule, and show that the found replay schedules can
outperform fixed scheduling policies when combined with various replay methods in differ-
ent continual learning settings. Additionally, we propose a framework for learning replay
scheduling policies with reinforcement learning. We show that the learned policies can gen-
eralize better in new continual learning scenarios compared to equally replaying all seen
tasks, without added computational cost. Our study reveals the importance of learning the
time to learn in continual learning, which brings current research closer to real-world needs.
1",TMLR
"Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized
clients to collaboratively learn a common model without sharing local data. Although
local data is not exposed directly, privacy concerns nonetheless exist as clientsâ€™ sensitive
information can be inferred from intermediate computations. Moreover, such information
leakage accumulates substantially over time as the same data is repeatedly used during
the iterative learning process. As a result, it can be particularly difficult to balance the
privacy-accuracy trade-off when designing privacy-preserving FL algorithms. This paper
introduces Upcycled-FL , a simple yet effective strategy that applies first-order approximation
at every even round of model update. Under this strategy, half of the FL updates incur no
information leakage and require much less computational and transmission costs. We first
conduct the theoretical analysis on the convergence (rate) of Upcycled-FL and then apply
two perturbation mechanisms to preserve privacy. Extensive experiments on both synthetic
and real-world data show that the Upcycled-FL strategy can be adapted to many existing
FL frameworks and consistently improve the privacy-accuracy trade-off.1
1",TMLR
"Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally,
for optimization, it is of interest to understand the convergence properties of gradient descent
using fractional derivatives. Convergence analysis of fractional gradient descent is currently
limited both in the methods analyzed and the settings analyzed. This paper aims to fill
in these gaps by analyzing variations of fractional gradient descent in smooth and convex,
smooth and strongly convex, and smooth and non-convex settings. First, novel bounds
will be established bridging fractional and integer derivatives. Then, these bounds will be
applied to the aforementioned settings to prove linear convergence for smooth and strongly
convex functions and O(1/T)convergence for smooth and convex functions. Additionally, we
proveO(1/T)convergence for smooth and non-convex functions using an extended notion of
smoothness - HÃ¶lder smoothness - that is more natural for fractional derivatives. Finally,
empirical results will be presented on the potential speed up of fractional gradient descent
over standard gradient descent as well as some preliminary theoretical results explaining this
speed up.
1",TMLR
"We focus on the problem of imitation learning from visual observations, where the learning
agent has access to videos of experts as its sole learning source. The challenges of this frame-
work include the absence of expert actions and the partial observability of the environment,
as the ground-truth states can only be inferred from pixels. To tackle this problem, we first
conduct a theoretical analysis of imitation learning in partially observable environments.
We establish upper bounds on the suboptimality of the learning agent with respect to the
divergence between the expert and the agent latent state-transition distributions. Moti-
vated by this analysis, we introduce an algorithm called Latent Adversarial Imitation from
Observations, which combines off-policy adversarial imitation techniques with a learned
latent representation of the agentâ€™s state from sequences of observations. In experiments
on high-dimensional continuous robotic tasks, we show that our model-free approach in
latent space matches state-of-the-art performance. Additionally, we show how our method
can be used to improve the efficiency of reinforcement learning from pixels by leveraging
expert videos. To ensure reproducibility, we provide free access to all the learning curves
and open-source our code.
1",TMLR
"Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose
performance can be enhanced by variance reduction methods. Recently, multiple works have
sought to fuse TD learning with Stochastic Variance Reduced Gradient (SVRG) method
to achieve a geometric rate of convergence. However, the resulting convergence rate is
significantly weaker than what is achieved by SVRG in the setting of convex optimization.
In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient
of an appropriately chosen function, thus simplifying the algorithm and fusing TD with
SVRG. Our main result is a geometric convergence bound with predetermined learning rate
of1/8, which is identical to the convergence bound available for SVRG in the convex setting.
Our theoretical findings are supported by a set of experiments.
1",TMLR
"Link prediction is central to many real-world applications, but its performance may be
hampered when the graph of interest is sparse. To alleviate issues caused by sparsity,
we investigate a previously overlooked phenomenon: in many cases, a densely connected,
complementary graph can be found for the original graph. The denser graph may share nodes
with the original graph, which offers a natural bridge for transferring selective, meaningful
knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL),
which is motivated by practical applications in e-commerce or academic co-authorship
predictions. We develop a framework to effectively leverage the structural prior in this
setting. We first create an intersection subgraph using the shared nodes between the two
graphs, then transfer knowledge from the source-enriched intersection subgraph to the full
target graph. In the second step, we consider two approaches: a modified label propagation,
and a multi-layer perceptron (MLP) model in a teacher-student regime. Experimental results
on proprietary e-commerce datasets and open-source citation graphs show that the proposed
workflow outperforms existing transfer learning baselines that do not explicitly utilize the
intersection structure.
1",TMLR
"We propose an efficient approach to train large diffusion models with masked transformers.
While masked transformers have been extensively explored for representation learning, their
application to generative learning is less explored in the vision domain. Our work is the
first to exploit masked training to reduce the training cost of diffusion models significantly.
Specifically, we randomly mask out a high proportion ( e.g., 50%) of patches in diffused input
images during training. For masked training, we introduce an asymmetric encoder-decoder
architectureconsistingofatransformerencoderthatoperatesonlyonunmaskedpatchesand
a lightweight transformer decoder on full patches. To promote a long-range understanding of
fullpatches, weaddanauxiliarytaskofreconstructingmaskedpatchestothedenoisingscore
matching objective that learns the score of unmasked patches. Experiments on ImageNet-
256Ã—256 and ImageNet-512 Ã—512 show that our approach achieves competitive and even
better generative performance than the state-of-the-art Diffusion Transformer (DiT) model,
using only around 30% of its original training time. Thus, our method shows a promising
way of efficiently training large transformer-based diffusion models without sacrificing the
generative performance. Ourcodeis availableat https://github.com/Anima-Lab/MaskDiT.
1",TMLR
"In this reproducibility study, we present our results and experience while replicating the
paper titled CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition (Ahn
et al., 2023). Traditional datasets used in image recognition, such as ImageNet, are often
synthetically balanced, meaning each class has an equal number of samples. In practical
scenarios, datasets frequently exhibit significant class imbalances, with certain classes hav-
ing a disproportionately larger number of samples than others. This discrepancy poses a
challenge for traditional image recognition models as they tend to favor classes with larger
sample sizes, leading to poor performance in minority classes. CUDA proposes a class-wise
data augmentation technique that can be used over any existing model to improve the accu-
racy for LTR: Long-Tailed Recognition. We successfully replicated a substantial portion of
the results pertaining to the long-tailed CIFAR-100-LT dataset and extended our analysis
to provide deeper insights into how CUDA efficiently tackles class imbalance.
1",TMLR
"Assessment of model fitness is a key part of machine learning. The standard paradigm of
model evaluation is analysis of the average loss over future data. This is often explicit in
model fitting, where we select models that minimize the average loss over training data as
a surrogate, but comes with limited theoretical guarantees. In this paper, we consider the
problem of characterizing a batch of out-of-sample losses of a model using a calibration data
set. We provide finite-sample limits on the out-of-sample losses that are statistically valid
under quite general conditions and propose a diagonistic tool that is simple to compute and
interpret. Several numerical experiments are presented to show how the proposed method
quantifies the impact of distribution shifts, aids the analysis of regression, and enables model
selection as well as hyperparameter tuning.
1",TMLR
"Shuï¬„e model of diï¬€erential privacy is a novel distributed privacy model based on a
combination of local privacy mechanisms and a secure shuï¬„er. It has been shown that
the additional randomisation provided by the shuï¬„er improves privacy bounds compared
to the purely local mechanisms. Accounting tight bounds, however, is complicated by
the complexity brought by the shuï¬„er. The recently proposed numerical techniques for
evaluating (Îµ,Î´)-diï¬€erential privacy guarantees have been shown to give tighter bounds than
commonly used methods for compositions of various complex mechanisms. In this paper,
we show how to utilise these numerical accountants for adaptive compositions of general
Îµ-LDP shuï¬„ers and for shuï¬„ers of k-randomised response mechanisms, including their
subsampled variants. This is enabled by an approximation that speeds up the evaluation of
the corresponding privacy loss distribution from O(n2)toO(n), wherenis the number of
users, without noticeable change in the resulting Î´(Îµ)-upper bounds. We also demonstrate
looseness of the existing bounds and methods found in the literature, improving previous
composition results for shuï¬„ers signiï¬cantly.
1",TMLR
"We propose the use of U-statistics to reduce variance for gradient estimation in importance-
weighted variational inference. The key observation is that, given a base gradient estimator
that requires m > 1samples and a total of n > msamples to be used for estimation,
lower variance is achieved by averaging the base estimator on overlapping batches of size m
than disjoint batches, as currently done. We use classical U-statistic theory to analyze the
variance reduction, and propose novel approximations with theoretical guarantees to ensure
computational eï¬ƒciency. We ï¬nd empirically that U-statistic variance reduction can lead
to modest to signiï¬cant improvements in inference performance on a range of models, with
little computational cost.
1",TMLR
"Deep neural networks (DNNs) are often trained on the premise that the complete training
data set is provided ahead of time. However, in real-world scenarios, data often arrive
in chunks over time. This leads to important considerations about the optimal strategy
for training DNNs, such as whether to fine-tune them with each chunk of incoming data
(warm-start) or to retrain them from scratch with the entire corpus of data whenever a new
chunk is available. While employing the latter for training can be resource-intensive, recent
work has pointed out the lack of generalization in warm-start models. Therefore, to strike
a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn
(LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning
phase, which selectively forgets the undesirable information in the model through weight
reinitialization in a data-dependent manner, and the relearning phase, which emphasizes
learning on generalizable features. We show that our training paradigm provides consistent
performance gains across datasets in both classification and few-shot settings. We further
show that it leads to more robust and well-calibrated models.1
1",TMLR
"A common assumption in meta-learning is that meta-training and meta-test tasks are drawn
from the same distribution. However, this assumption is often not fulï¬lled. Under such task
shift, standard meta-learning algorithms do not work as desired since their unbiasedness
is no longer maintained. In this paper, we propose a new meta-learning method called
Importance Weighted Meta-Learning (IWML), which preserves unbiasedness even under
task shift. Our approach uses both labeled meta-training datasets and unlabeled datasets
in tasks obtained from the meta-test task distribution to assign weights to each meta-
training task. These weights are determined by the ratio of meta-test and meta-training
task densities. Our method enables the model to focus more on the meta-training tasks that
closely align with meta-test tasks during the meta-training process. We meta-learn neural
network-based models by minimizing the expected weighted meta-training error, which is
an unbiased estimator of the expected error over meta-test tasks. The task density ratio is
estimated using kernel density estimation, where the distance between tasks is measured by
the maximum mean discrepancy. Our empirical evaluation of few-shot classiï¬cation datasets
demonstrates a signiï¬cant improvement of IWML over existing approaches.
1",TMLR
"We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks.
It consists in a streaming encoder-decoder architecture with quantized latent space trained
in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale
spectrogram adversary that efficiently reduces artifacts and produce high-quality samples.
We introduce a novel loss balancer mechanism to stabilize training: the weightof a loss
now defines the fraction of the overall gradient it should represent, thus decoupling the
choice of this hyper-parameter from the typical scale of the loss. Finally, we study how
lightweight Transformer models can be used to further compress the obtained representation
by up to 40%, while staying faster than real time. We provide a detailed description of
the key design choices of the proposed model including: training objective, architectural
changes and a study of various perceptual loss functions. We present an extensive subjective
evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths
and audio domains, including speech, noisy-reverberant speech, and music. Our approach
is superior to the baselines methods across all evaluated settings, considering both 24
kHz monophonic and 48 kHz stereophonic audio. Code and samples are available under
github.com/facebookresearch/encodec.
1",TMLR
"This paper empirically studies commonly observed training difficulties of Physics-Informed
Neural Networks (PINNs) on dynamical systems. Our results indicate that fixed points
which are inherent to these systems play a key role in the optimization of the in PINNs
embedded physics loss function. We observe that the loss landscape exhibits local optima
that are shaped by the presence of fixed points. We find that these local optima contribute
to the complexity of the physics loss optimization which can explain common training diffi-
culties and resulting nonphysical predictions. Under certain settings, e.g., initial conditions
close to fixed points or long simulations times, we show that those optima can even become
better than that of the desired solution.
1",TMLR
"Ordinary differential equations (ODEs) allow interpretation of phenomena in various scien-
tific fields. They have mostly been applied to numerical data observed at regular intervals,
but not to irregularly observed discrete events, also known as point processes. In this study,
we introduce an ODE modeling of such events by combining ODEs with log-Gaussian Cox
processes (MÃ¸ller et al., 1998). In the experiments with different types of ODEs regard-
ing infectious disease, predator-prey interaction, and competition among participants, our
method outperformed existing baseline methods assuming regularly observed continuous
data with respect to the accuracy of recovering the latent parameters of ODEs. Through
both synthetic and actual examples, we also showed the ability of our method to extrapo-
late, model latent events that cannot be observed, and offer interpretability of phenomena
from the viewpoint of the estimated parameters of ODE.
1",TMLR
"We study robustfederated learning (FL) within a game theoretic framework to alleviate
the server vulnerabilities to even an informed adversary who can tailor training-time at-
tacks (Fang et al., 2020; Xie et al., 2020a; Ozfatura et al., 2022; RodrÃ­guez-Barroso et al.,
2023). Specifically, we introduce RobustTailor , a simulation-based framework that prevents
the adversary from being omniscient and derives its convergence guarantees. RobustTailor
improves robustness to training-time attacks significantly with a minor trade-off of privacy.
Empirical results under challenging attacks show that RobustTailor performs close to an
upper bound with perfect knowledge of honest clients.
1",TMLR
"Federated learning is a paradigm of distributed machine learning in which multiple clients
coordinate with a central server to learn a model, without sharing their own training data.
Standard federated optimization methods such as Federated Averaging (FedAvg) ensure bal-
ance among the clients by using the same stepsize for local updates on all clients. However,
this means that all clients need to respect the global geometry of the function which could
yield slow convergence. In this work, we propose locally adaptive federated learning algo-
rithms, that leverage the local geometric information for each client function. We show that
such locally adaptive methods with uncoordinated stepsizes across all clients can be particu-
larly efficient in interpolated (overparameterized) settings, and analyze their convergence in
the presence of heterogeneous data for convex and strongly convex settings. We validate our
theoretical claims by performing illustrative experiments for both i.i.d. non-i.i.d. cases. Our
proposed algorithms match the optimization performance of tuned FedAvg in the convex
setting, outperform FedAvg as well as state-of-the-art adaptive federated algorithms like
FedAMS for non-convex experiments, and come with superior generalization performance.
1",TMLR
"The inference stage of diffusion models can be seen as running a reverse-time diffusion
stochastic differential equation, where samples from a Gaussian latent distribution are trans-
formed into samples from a target distribution that usually reside on a low-dimensional
manifold, e.g., an image manifold. The intermediate values between the initial latent space
and the image manifold can be interpreted as noisy images, with the amount of noise de-
termined by the forward diffusion process noise schedule. We utilize this interpretation to
present Boomerang, an approach for local sampling of image manifolds exploiting the re-
versediffusionprocessdynamics. Asimpliedbyitsname, Boomeranglocalsamplinginvolves
adding noise to an input image, moving it closer to the latent space, and then mapping it
back to the image manifold through a partial reverse diffusion process. Thus, Boomerang
generates images on the manifold that are â€œsimilar,â€ but nonidentical, to the original input
image. We can control the proximity of the generated images to the original by adjusting
the amount of noise added. Furthermore, due to the stochastic nature of the partial reverse
diffusion process in Boomerang, the generated images display a certain degree of stochastic-
ity, allowing us to obtain ample local samples from the manifold without encountering any
duplicates. Boomerang offers the flexibility to work seamlessly with any pretrained diffu-
sion model, such as Stable Diffusion, without necessitating any adjustments to the reverse
diffusion process. We present three applications for local sampling using Boomerang. First,
we provide a framework for constructing privacy-preserving datasets having controllable de-
grees of anonymity. Second, we show that using Boomerang for data augmentation increases
generalization performance and outperforms state-of-the-art synthetic data augmentation.
Lastly, we introduce a perceptual image enhancement framework powered by Boomerang,
which enables resolution enhancement.
1",TMLR
"We introduce a hybrid active learning method that simultaneously considers uncertainty
and diversity for sample selection. Our method consists of two key steps: computing a novel
uncertainty-weighted embedding, then applying distance-based sampling for sample selec-
tion. Our proposed uncertainty-weighted embedding is computed by weighting a sampleâ€™s
feature representation by an uncertainty measure. We show how this embedding generalizes
the gradient embedding of BADGE so it can be used with arbitrary loss functions and be
computed more efficiently, especially for dense prediction tasks and network architectures
with large numbers of parameters in the final layer. We extensively evaluate the proposed
hybrid active learning method on image classification, semantic segmentation and object
detection tasks, and demonstrate that it achieves state-of-the-art performance.
1",TMLR
"This work identifies 18foundational challenges in assuring the alignment and safety
of large language models (LLMs). These challenges are organized into three different
categories: scientific understanding of LLMs ,development and deployment methods ,
andsociotechnical challenges . Based on the identified challenges, we pose 200 +concrete
research questions.
Corresponding author: Usman Anwar Â« usmananwar391@gmail.com Â»
1 Contents
1",TMLR
"Phase retrieval is the problem of reconstructing images from magnitude-only measurements.
In many real-world applications the problem is underdetermined. When training data is
available, generative models allow optimization in a lower-dimensional latent space, hereby
constraining the solution set to those images that can be synthesized by the generative
model. However, not all possible solutions are within the range of the generator. Instead,
they are represented with some error. To reduce this representation error in the context of
phase retrieval, we first leverage a novel variation of intermediate layer optimization (ILO)
to extend the range of the generator while still producing images consistent with the training
data. Second, we introduce new initialization schemes that further improve the quality
of the reconstruction. With extensive experiments on the Fourier phase retrieval problem
and thorough ablation studies, we can show the benefits of our modified ILO and the new
initialization schemes. Additionally, we analyze the performance of our approach on the
Gaussian phase retrieval problem.
1",TMLR
"Machine learning systems have been widely used to make decisions about individuals who
may behave strategically to receive favorable outcomes, e.g., they may genuinely improve
the true labels or manipulate observable features directly to game the system without chang-
ing labels. Although both behaviors have been studied (often as two separate problems)
in the literature, most works assume individuals can (i) perfectly foresee the outcomes of
their behaviors when they best respond; (ii) change their features arbitrarily as long as it
is affordable, and the costs they need to pay are deterministic functions of feature changes.
In this paper, we consider a different setting and focus on imitative strategic behaviors with
unforeseeable outcomes, i.e., individuals manipulate/improve by imitating the features of
those with positive labels, but the induced feature changes are unforeseeable. We first pro-
pose a Stackelberg game to model the interplay between individuals and the decision-maker,
under which we examine how the decision-makerâ€™s ability to anticipate individual behavior
affects its objective function and the individualâ€™s best response. We show that the objective
difference between the two can be decomposed into three interpretable terms, with each
representing the decision-makerâ€™s preference for a certain behavior. By exploring the roles
of each term, we theoretically illustrate how a decision-maker with adjusted preferences may
simultaneously disincentivize manipulation, incentivize improvement, and promote fairness.
Such theoretical results provide a guideline for decision-makers to inform better and socially
responsible decisions in practice.
1",TMLR
"Any representation of data involves arbitrary investigator choices. Because those choices are
externaltothedata-generatingprocess,eachchoiceleadstoanexactsymmetry,corresponding
to the group of transformations that takes one possible representation to another. These
are thepassive symmetries ; they include coordinate freedom, gauge symmetry, and units
covariance, all of which have led to important results in physics. In machine learning, the
most visible passive symmetry is the relabeling or permutation symmetry of graphs. The
active symmetries are those that must be established by observation and experiment. They
include, for instance, translations invariances or rotation invariances of physical law. These
symmetries are the subject of most of the equivariant machine learning literature. Our goal,
in this conceptual contribution, is to understand the implications for machine learning of the
many passive and active symmetries in play. We discuss dos and donâ€™ts for machine learning
practice if passive symmetries are to be respected. We discuss links to causal modeling and
argue that the implementation of passive symmetries is particularly valuable when the goal
of the learning problem is to generalize out of sample. We conjecture that the implementation
of passive symmetries might help machine learning in the same ways that it transformed
physics in the twentieth century.
1",TMLR
"In deep learning, leveraging transfer learning has recently been shown to be an eï¬€ective
strategy for training large high performance models with Diï¬€erential Privacy (DP). Moreover,
somewhat surprisingly, recent works have found that privately training just the last layer of
a pre-trained model provides the best utility with DP. While past studies largely rely on
using ï¬rst-order diï¬€erentially private training algorithms like DP-SGD for training large
models, in the speciï¬c case of privately learning from features, we observe that computational
burden is often low enough to allow for more sophisticated optimization schemes, including
second-order methods. To that end, we systematically explore the eï¬€ect of design parameters
such as loss function and optimization algorithm. We ï¬nd that, while commonly used logistic
regression performs better than linear regression in the non-private setting, the situation is
reversed in the private setting. We ï¬nd that least-squares linear regression is much more
eï¬€ective than logistic regression from both privacy and computational standpoint, especially
at stricter epsilon values ( Îµ<1). On the optimization side, we also explore using Newtonâ€™s
method, and ï¬nd that second-order information is quite helpful even with privacy, although
the beneï¬t signiï¬cantly diminishes with stricter privacy guarantees. While both methods
use second-order information, least squares is more eï¬€ective at lower epsilon values while
Newtonâ€™s method is more eï¬€ective at larger epsilon values. To combine the beneï¬ts of both
methods, we propose a novel optimization algorithm called DP-FC, which leverages feature
covariance instead of the Hessian of the logistic regression loss and performs well across all Îµ
values we tried. With this, we obtain new SOTA results on ImageNet-1k, CIFAR-100 and
CIFAR-10 across all values of Îµtypically considered. Most remarkably, on ImageNet-1K, we
obtain top-1 accuracy of 88% under DP guarantee of (8, 8âˆ—10âˆ’7) and 84.3% under (0.1,
8âˆ—10âˆ’7).
1",TMLR
"Responsibleuseofmachinelearningrequiresmodelstobeauditedforundesirableproperties.
Whileabodyofworkhasproposedusingexplanationsforauditing, howtodosoandwhyhas
remained relatively ill-understood. This work formalizes the role of explanations in auditing
using inspirations from active learning and investigates if and how model explanations can
help audits. As an instantiation of our framework, we look at â€˜feature sensitivityâ€™ and
propose explanation-based algorithms for auditing linear classifiers and decision trees for
this property. Our results illustrate that Counterfactual explanations are extremely helpful
for auditing feature sensitivity, even in the worst-case. While Anchor explanations and
decision paths may not be as beneficial in the worst-case, in the average-case they do aid
significantly as demonstrated by our experiments.
1",TMLR
"Binary neural networks (BNNs) are a promising approach for compressing and accelerating
deep learning models, especially in resource-constrained environments. However, the opti-
mization gap between BNNs and their full-precision counterparts has long been an open
problem limiting their performance. In this work, we propose a novel optimization pipeline
to enhance the performance of BNNs. The main approach includes three key components:
(1) BNext, a strong binary baseline based on an optimization-friendly basic block design,
(2) knowledge complexity, a simple yet effective teacher-selection metric taking the capacity
gap between teachers and binary students under consideration, (3) consecutive knowledge
distillation (CKD), a novel multi-round optimization technique to transfer high-confidence
knowledge from strong teachers to low-capacity BNNs. We empirically validate the su-
periority of the method on several vision classification tasks CIFAR-10/100 & ImageNet.
For instance, the BNext family outperforms previous BNNs under different capacity levels
and contributes the first binary neural network to reach the state-of-the-art 80.57% Top-1
accuracy on ImageNet with 0.82 GOPS, which verifies the potential of BNNs and already
contributes a strong baseline for future research on high-accuracy BNNs. The code is pub-
licly available at https://github.com/hpi-xnor/BNext .
1",TMLR
"Federated Learning (FL) is a distributed machine learning approach to learn models on
decentralized heterogeneous data, without the need for clients to share their data. Many
existing FL approaches assume that all clients have equal importance and construct a global
objective based on all clients. We consider a version of FL we call Prioritized FL, where
the goal is to learn a weighted mean objective of a subset of clients, designated as priority
clients. An important question arises: How do we choose well-aligned non-priority clients to
participate in the federation, while discarding misaligned clients? We present FedALIGN
(Federated Adaptive Learning with Inclusion of Global Needs) to address this challenge.
The algorithm employs a matching strategy that chooses non-priority clients based on how
similar the modelâ€™s loss is on their data compared to the global data, thereby ensuring
the use of non-priority client gradients only when it is beneï¬cial for priority clients. This
approach ensures mutual beneï¬ts as non-priority clients are motivated to join when the
model performs satisfactorily on their data, and priority clients can utilize their updates
and computational resources when their goals align. We present a convergence analysis that
quantiï¬es the trade-o between client selection and speed of convergence. Our algorithm
shows faster convergence and higher test accuracy than baselines for various synthetic and
benchmark datasets.
1",TMLR
"In environments with sparse rewards, ï¬nding a good inductive bias for exploration is crucial
to the agentâ€™s success. However, there are two competing goals: novelty search and systematic
exploration. While existing approaches such as curiosity-driven exploration ï¬nd novelty, they
sometimes do not systematically explore the whole state space, akin to depth-ï¬rst-search vs
breadth-ï¬rst-search. In this paper, we propose a new intrinsic reward that is cyclophobic,
i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting
the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the
agentâ€™s cropped observations we are able to achieve excellent results in the MiniGrid and
MiniHack environments. Both are particularly hard, as they require complex interactions
with diï¬€erent objects in order to be solved. Detailed comparisons with previous approaches
and thorough ablation studies show that our newly proposed cyclophobic reinforcement
learning is more sample eï¬ƒcient than other state of the art methods in a variety of tasks.
1",TMLR
"Conditional generative models became a very powerful tool to sample from
Bayesian inverse problem posteriors. It is well-known in classical Bayesian lit-
erature that posterior measures are quite robust with respect to perturbations of
both the prior measure and the negative log-likelihood, which includes perturba-
tions of the observations. However, to the best of our knowledge, the robustness
of conditional generative models with respect to perturbations of the observations
has not been investigated yet. In this paper, we prove for the first time that ap-
propriately learned conditional generative models provide robust results for single
observations.
1",TMLR
"While PAC-Bayes is now an established learning framework for light-tailed losses ( e.g., sub-
gaussian or subexponential), its extension to the case of heavy-tailed losses remains largely
uncharted and has attracted a growing interest in recent years. We contribute PAC-Bayes
generalisation bounds for heavy-tailed losses under the sole assumption of bounded variance
of the loss function. Under that assumption, we extend previous results from Kuzborskij and
SzepesvÃ¡ri (2019). Our key technical contribution is exploiting an extention of Markovâ€™s
inequality for supermartingales. Our proof technique uniï¬es and extends diï¬€erent PAC-
Bayesian frameworks by providing bounds for unbounded martingales as well as bounds for
batch and online learning with heavy-tailed losses.
1",TMLR
"The resilience of complex networks refers to their ability to maintain functionality in the face
of structural attacks. This ability can be improved by performing minimal modifications to
thenetworkstructureviadegree-preservingedgerewiring-basedmethods. Existinglearning-
free edge rewiring methods, although effective, are limited in their ability to generalize to
different graphs. Such a limitation cannot be trivially addressed by existing graph neural
networks (GNNs)-based learning approaches since there is no rich initial node features for
GNNs to learn meaningful representations. In this work, inspired by persistent homology,
we specifically design a variant of GNN called FireGNN to learn meaningful node repre-
sentations solely from graph structures. We then develop an end-to-end inductive method
called ResiNet, which aims to discover resilientnetwork topologies while balancing network
utility. ResiNet reformulates the optimization of network resilience as a Markov decision
process equipped with edge rewiring action space. It learns to sequentially select the ap-
propriate edges to rewire for maximizing resilience. Extensive experiments demonstrate
that ResiNet outperforms existing approaches and achieves near-optimal resilience gains on
various graphs while balancing network utility.
1",TMLR
"We present a framework for the unsupervised learning of neurosymbolic encoders, which are
encoders obtained by composing neural networks with symbolic programs from a domain-
specific language. Our framework naturally incorporates symbolic expert knowledge into
the learning process, which leads to more interpretable and factorized latent representations
compared to fully neural encoders. We integrate modern program synthesis techniques with
the variational autoencoding (VAE) framework, in order to learn a neurosymbolic encoder in
conjunction with a standard decoder. The programmatic descriptions from our encoders can
benefitmanyanalysisworkflows, suchasinbehaviormodelingwhereinterpretingagentactions
and movements is important. We evaluate our method on learning latent representations
for real-world trajectory data from animal biology and sports analytics. We show that our
approach offers significantly better separation of meaningful categories than standard VAEs
and leads to practical gains on downstream analysis tasks, such as for behavior classification.
Code can be found at https://github.com/ezhan94/neurosymbolic-encoders .
1",TMLR
"Reinforcement learning in sparse-reward navigation environments with expensive and limited
interactions is challenging and poses a need for effective exploration. Motivated by complex
navigation tasks that require real-world training (when cheap simulators are not available),
we consider an agent that faces an unknown distribution of environments and must decide
on an exploration strategy. It may leverage a series of training environments to improve
its policy before it is evaluated in a test environment drawn from the same environment
distribution. Most existing approaches focus on fixed exploration strategies, while the few
that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient
exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches
over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety
of levers â€” the locations of the subgoals, the length of each episode, and the number of
replications per trial â€” in order to overcome the challenges of sparse rewards, expensive
interactions, and noise. An experimental evaluation demonstrates that the new approach
outperforms existing baselines across a number of problem domains. We also provide a
theoretical foundation and prove that the method asymptotically identifies a near-optimal
subgoal design.
1",TMLR
"Calibration of neural networks is a topical problem that is becoming more and more important
as neural networks increasingly underpin real-world applications. The problem is especially
noticeable when using modern neural networks, for which there is a significant difference
between the confidence of the model and the probability of correct prediction. Various
strategies have been proposed to improve calibration, yet accurate calibration remains
challenging. We propose a novel framework with two contributions: introducing a new
differentiable surrogate for expected calibration error (DECE) that allows calibration quality
to be directly optimised, and a meta-learning framework that uses DECE to optimise for
validation set calibration with respect to model hyper-parameters. The results show that we
achieve competitive performance with existing calibration approaches. Our framework opens
up a new avenue and toolset for tackling calibration, which we believe will inspire further
work on this important challenge.
1",TMLR
"Learning representations for pixel-based control has garnered signiï¬cant attention recently in re-
inforcement learning. A wide range of methods have been proposed to enable efï¬cient learning,
leading to sample complexities similar to those in the full state setting. However, moving beyond
carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains
challenging. In this paper, we adopt a more difï¬cult setting, incorporating background distractors,
as a ï¬rst step towards addressing this challenge. We start by exploring a simple baseline approach
that does not use metric-based learning, data augmentations, world-model learning, or contrastive
learning. We then analyze when and why previously proposed methods are likely to fail or reduce
to the same performance as the baseline in this harder setting and why we should think carefully
about extending such methods beyond the well curated environments. Our results show that ï¬ner
categorization of benchmarks on the basis of characteristics like density of reward, planning horizon
of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms.
Based on these observations, we propose different metrics to consider when evaluating an algorithm
on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink repre-
sentation learning when investigating how to best apply RL to real-world tasks. Code available:
https://github.com/UtkarshMishra04/pixel-representations-RL
1",TMLR
"This work investigates the reproducibility of the paper "" Explaining RL decisions with
trajectories â€œ by Deshmukh et al. (2023). The original paper introduces a novel approach in
explainable reinforcement learning based on the attribution decisions of an agent to specific
clusters of trajectories encountered during training. We verify the main claims from the
paper, which state that (i) training on less trajectories induces a lower initial state value, (ii)
trajectories in a cluster present similar high-level patterns, (iii) distant trajectories influence
the decision of an agent, and (iv) humans correctly identify the attributed trajectories
to the decision of the agent. We recover the environments used by the authors based
on the partial original code they provided for one of the environments ( Grid-World ), and
implemented the remaining from scratch ( Seaquest andHalfCheetah ,Breakout ,Q*Bert).
While we confirm that (i), (ii), and (iii) partially hold, we extend on the largely qualitative
experiments from the authors by introducing a quantitative metric to further support (iii),
and new experiments and visual results for (i). Moreover, we investigate the use of different
clusteringalgorithmsandencoderarchitecturestofurthersupport(ii). Wecouldnotsupport
(iv), given the limited extent of the original experiments. We conclude that, while some of
the claims can be supported, further investigations and experiments could be of interest.
We recognize the novelty of the work from the authors and hope that our work paves the
way for clearer and more transparent approaches.
1",TMLR
"Prior work on Private Inference (PI)â€”inferences performed directly on encrypted inputâ€”has
focused on minimizing a networkâ€™s ReLUs, which have been assumed to dominate PI latency
rather than FLOPs. Recent work has shown that FLOPs for PI can no longer be ignored
and incur high latency penalties. In this paper, we develop DeepReShape, a technique
that optimizes neural network architectures under PIâ€™s constraints, optimizing for both
ReLUsandFLOPs for the first time. The key insight is strategically allocating channels to
position the networkâ€™s ReLUs in order of their criticality to network accuracy, simultaneously
optimizes ReLU and FLOPs efficiency. DeepReShape automates network development with
an efficient process, and we call generated networks HybReNets. We evaluate DeepReShape
using standard PI benchmarks and demonstrate a 2.1% accuracy gain with a 5.2 Ã—runtime
improvement at iso-ReLU on CIFAR-100 and an 8.7 Ã—runtime improvement at iso-accuracy
on TinyImageNet. Furthermore, we investigate the significance of network selection in prior
ReLU optimizations and shed light on the key network attributes for superior PI performance.
1",TMLR
"Selecting a well-performing algorithm for a given task or dataset can be time-consuming and
tedious, but is crucial for the successful day-to-day business of developing new AI & ML
applications. Algorithm Selection (AS) mitigates this through a meta-model leveraging
meta-information about previous tasks. However, most of the available AS methods are
error-prone because they characterize a task by either cheap-to-compute properties of the
dataset or evaluations of cheap proxy algorithms, called landmarks. In this work, we extend
the classical AS data setup to include multi-fidelity information and empirically demonstrate
how meta-learning on algorithmsâ€™ learning behaviour allows us to exploit cheap test-time
evidence effectively and combat myopia significantly. We further postulate a budget-regret
trade-off w.r.t. the selection process. Our new selector MASIF is able to jointly interpret
online evidence on a task in form of varying-length learning curves without any parametric
assumption by leveraging a transformer-based encoder. This opens up new possibilities for
guided rapid prototyping in data science on cheaply observed partial learning curves.
1",TMLR
"Characters do not convey meaning, but sequences of characters do. We propose an unsuper-
vised distributional method to learn the abstract meaningful units in a sequence of characters.
Rather than segmenting the sequence, our Dynamic Capacity Slot Attention model discovers
continuous representations of the objectsin the sequence, extending an architecture for
object discovery in images. We train our model on different languages and evaluate the
quality of the obtained representations with forward and reverse probing classifiers. These
experiments show that our model succeeds in discovering units which are similar to those
proposed previously in form, content and level of abstraction, and which show promise for
capturing meaningful information at a higher level of abstraction.
1",TMLR
"Vector quantization (VQ) is a technique to deterministically learn features with discrete
codebookrepresentations. Itiscommonlyperformedwithavariationalautoencodingmodel,
VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity
reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the
codebook/layer collapse issue, where the codebook is not efficiently used to express the data,
and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel
unified framework to stochastically learn hierarchical discrete representation on the basis
of the variational Bayes framework, called hierarchically quantized variational autoencoder
(HQ-VAE).HQ-VAEnaturallygeneralizesthehierarchicalvariantsofVQ-VAE,suchasVQ-
VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training
scheme. Our comprehensive experiments on image datasets show that HQ-VAE enhances
codebook usage and improves reconstruction performance. We also validated HQ-VAE in
terms of its applicability to a different modality with an audio dataset.
1",TMLR
"When trained on diverse labelled data, machine learning models have proven themselves to
be a powerful tool in all facets of society. However, due to budget limitations, deliberate or
non-deliberate censorship, and other problems during data collection, certain groups may
be under-represented in the labelled training set. We investigate a scenario in which the
absence of certain data is linked to the second level of a two-level hierarchy in the data.
Inspiredbytheideaofprotectedattributesfromalgorithmicfairness, weconsidergeneralised
secondary â€œattributesâ€ which subdivide the classes into smaller partitions. We refer to the
partitions defined by the combination of an attribute and a class label, or leaf nodes in
aforementioned hierarchy, as groups. To characterise the problem, we introduce the concept
of classes with incomplete attribute support . The representational bias in the training set
can give rise to spurious correlations between the classes and the attributes which cause
standard classification models to generalise poorly to unseen groups. To overcome this bias,
we make use of an additional, diverse but unlabelled dataset, called the deployment set ,
to learn a representation that is invariant to the attributes. This is done by adversarially
matching the support of the training and deployment sets in representation space using a set
discriminator operating on sets, or bags, of samples. In order to learn the desired invariance,
it is paramount that the bags are balanced by class; this is easily achieved for the training
set, but requires using semi-supervised clustering for the deployment set. We demonstrate
the effectiveness of our method on several datasets and realisations of the problem. Code
for the paper is publicly available at https://github.com/wearepal/support-matching .
1",TMLR
"Feature selection that selects an informative subset of variables from data not only enhances
themodelinterpretabilityandperformancebutalsoalleviatestheresourcedemands. Recently,
there has been growing attention on feature selection using neural networks. However, exist-
ing methods usually suffer from high computational costs when applied to high-dimensional
datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient
supervised feature selection method using sparse neural networks, named â€œNeuroFSâ€. By
gradually pruning the uninformative features from the input layer of a sparse neural network
trainedfromscratch, NeuroFSderivesaninformativesubsetoffeaturesefficiently. Byperform-
ing several experiments on 11low and high-dimensional real-world benchmarks of different
types, we demonstrate that NeuroFS achieves the highest ranking-based score among the con-
sidered state-of-the-art supervised feature selection models. The code is available on GitHub1.
1",TMLR
"Graph neural networks ( Gnns) drive several real-world applications including drug-
discovery, recommendationengines, andchipdesigning. Unfortunately, Gnnsareablack-box
since they do not allow human-intelligible explanations of their predictions. Counterfactual
reasoning is an effort to overcome this limitation. Specifically, the objective is to minimally
perturb the input graph to a Gnn, so that its prediction changes. While several algo-
rithms have been proposed towards counterfactual explanations of Gnns, majority suffer
from three key limitations: (1)they only consider perturbations in the form of deletions of
existing edges, (2)they perform an inefficient exploration of the combinatorial search space,
(3)the counterfactual explanation model is transductive in nature, i.e., they do not gener-
alize tounseendata. In this work, we propose an inductive algorithm called InduCE , that
overcomes these limitations. Through extensive experiments on graph datasets, we show
that incorporating edge additions, and modelling marginal effect of perturbations aid in
generating better counterfactuals among available recourse. Furthermore, inductive mod-
eling enables InduCE to directly predictcounterfactual perturbations without requiring
instance-specific training. This leads to significant computational speed-up over baselines
and allows counterfactual analyses for Gnns at scale.
1",TMLR
"To date, most directed acyclic graphs (DAGs) structure learning approaches require data to
be stored in a central server. However, due to the consideration of privacy protection, data
owners gradually refuse to share their personalized raw data to avoid private information
leakage, making this task more troublesome by cutting off the first step. Thus, a puzzle
arises:how do we discover the underlying DAG structure from decentralized data? In this
paper, focusing on the additive noise models (ANMs) assumption of data generation, we
take the first step in developing a gradient-based learning framework named FedDAG, which
can learn the DAG structure without directly touching the local data and also can naturally
handle the data heterogeneity. Our method benefits from a two-level structure of each
local model. The first level structure learns the edges and directions of the graph and
communicates with the server to get the model information from other clients during the
learning procedure, while the second level structure approximates the mechanisms among
variables and personally updates on its own data to accommodate the data heterogeneity.
Moreover, FedDAG formulates the overall learning task as a continuous optimization problem
by taking advantage of an equality acyclicity constraint, which can be solved by gradient
descent methods to boost the searching efficiency. Extensive experiments on both synthetic
and real-world datasets verify the efficacy of the proposed method.
1",TMLR
"Deepactive learning (AL) seeks to reduce the annotation costs required for training deep
neural networks (DNNs). Often, deep AL strategies focus on instances where the predic-
tive uncertainty of a DNN is high. Furthermore, Bayesian concepts to model uncertainty
are frequently adopted. Despite considerable research, a detailed analysis of the role of
uncertainty in deep AL is still missing, especially regarding aleatoric and epistemic uncer-
tainty, both related to predictive uncertainty. This article provides an in-depth empirical
study analyzing the interplay of uncertainty and deep AL in image classification. Our
study investigates four hypotheses that provide an intuitive understanding of the effects
of accurately estimating aleatoric and epistemic uncertainty on existing uncertainty-based
AL strategies but also, in the opposite direction, the impact of uncertainty-based AL on
the quality of uncertainty estimates that are needed in many applications. By analyzing
these hypotheses on synthetic and real-world data, we find that accurate aleatoric estimates
can even impair instance selection, while accurate epistemic estimates have negligible ef-
fects. Moreover, we provide a publicly available toolbox for deep AL with various models
and strategies to facilitate further research and practical applications. Code is available at
https://github.com/dhuseljic/dal-toolbox .
1 Published in Transactions on Machine Learning Research (05/2024)
1",TMLR
"The success of modern neural networks has prompted study of the connection between
memorisation andgeneralisation : overparameterised models generalise well, despite being
able to perfectly ï¬t (â€œmemoriseâ€) completely random labels. To carefully study this issue,
Feldman (2019) proposed a metric to quantify the degree of memorisation of individual
training examples, and empirically computed the corresponding memorisation proï¬le of a
ResNet on image classiï¬cation benchmarks. While an exciting ï¬rst glimpse into what real-
world models memorise, this leaves open a fundamental question: do larger neural models
memorise more? This aligns with the common practice of training models of diï¬€erent sizes,
each oï¬€ering diï¬€erent cost-quality trade-oï¬€s: while larger models are typically observed to
have higher quality, it is of interest to understand whether this is merely a consequence
of them memorising larger numbers of input-output patterns. We present a comprehensive
empirical analysis of this question on image classiï¬cation benchmarks. We ï¬nd that training
examplesexhibitanunexpectedlydiversesetofmemorisationtrajectoriesacrossmodelsizes:
most samples experience decreased memorisation under larger models, while the rest exhibit
cap-shaped orincreasing memorisation. We show that various proxies for the Feldman
(2019) memorisation score fail to capture these fundamental trends. Lastly, we ï¬nd that
knowledge distillation â€” an eï¬€ective and popular model compression technique â€” tends
to inhibit memorisation, while also improving generalisation. Speciï¬cally, memorisation is
mostly inhibited on examples with increasing memorisation trajectories, thus pointing at
how distillation improves generalisation.
1",TMLR
"Federated learning provides a framework to address the challenges of distributed computing,
data ownership, and privacy over a large number of distributed clients with low computational
and communication capabilities. In this paper, we study the problem of learning the
exact support of sparse linear regression in the federated learning setup. We provide a
simple communication ecient algorithm that only needs one-shot communication with the
centralized server to compute the exact support by majority voting. Our method does not
require the clients to solve any optimization problem and thus, can be run on devices with
low computational capabilities. Our method is naturally robust to the problems of client
failure, model poisoning, and straggling clients. We formally prove that our method requires
a number of samples per client that is polynomial with respect to the support size, but
independent of the dimension of the problem. We require the number of distributed clients to
be logarithmic in the dimension of the problem. For certain classes of predictor variables (e.g.
mutually independent, correlated Gaussian, etc.), the overall sample complexity matches
the optimal sample complexity of the non-federated centralized setting. Furthermore, our
method is easy to implement and has an overall polynomial time complexity.
1",TMLR
"In recent times machine learning methods have made significant advances in becoming a
useful tool for analyzing physical systems. A particularly active area in this theme has been
â€œphysics-informed machine learningâ€ which focuses on using neural nets for numerically
solving differential equations. In this work, we aim to advance the theory of measuring out-
of-sample error while training DeepONets â€“ which is among the most versatile ways to solve
P.D.E systems in one-shot. Firstly, for a class of DeepONets, we prove a bound on their
Rademacher complexity which does not explicitly scale with the width of the nets involved.
Secondly, we use this to show how the Huber loss can be chosen so that for these DeepONet
classes generalization error bounds can be obtained that have no explicit dependence on the
size of the nets. The effective capacity measure for DeepONets that we thus derive is also
shown to correlate with the behavior of generalization error in experiments.
1",TMLR
"Inspired by progress in large-scale language modeling, we apply a similar approach towards
building a single generalist agent beyond the realm of text outputs. The agent, which we
refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy.
The same network with the same weights can play Atari, caption images, chat, stack blocks
with a real robot arm and much more, deciding based on its context whether to output text,
joint torques, button presses, or other tokens. In this report we describe the model and the
data, and document the current capabilities of Gato.
A man surfing in the ocean 
as the sun sets
G
G
What is the capital of 
France?
Paris.
Can you write me a poem?
I donâ€™t know exactly what 
to write. Thereâ€™s just so 
much to answer. 
three giraffes some brown 
and green grass some bushes
A cat that is sitting next 
to a brick wall
Gato
Figure 1: A generalist agent. Gato can sense and act with different embodiments across a wide range of
environments using a single neural network with the same set of weights. Gato was trained on 604 distinct tasks
with varying modalities, observations and action specifications.
1 Published in Transactions on Machine Learning Research (11/2022)
Text
Images, proprioception 
and continuous actions
I'm going to London
Gato
Q: Whatâ€™s in the picture?
A: Itâ€™s a cute cat
Images and
questionsText
Images, proprioception 
and continuous actions
Images and
questions Atari images 
and discrete actions
0
1
0
1 Atari images 
and discrete actions Atari images 
and discrete actions
Images, proprioception 
and continuous actions
Images and
questionsBatched input
Text
Continuous actions
Discrete actionsImageProprioceptionBatched and masked
shifted targets
Figure 2: Training phase of Gato . Data from different tasks and modalities is serialized into a flat sequence of
tokens, batched, and processed by a transformer neural network akin to a large language model. Masking is used
such that the loss function is applied only to target outputs, i.e. text and various actions.
1",TMLR
"For faster sampling and higher sample quality, we propose DiNof ( Diffusion with
Normalizing flow priors), a technique that makes use of normalizing flows and diffusion
models. We use normalizing flows to parameterize the noisy data at any arbitrary step of
the diffusion process and utilize it as the prior in the reverse diffusion process. More specif-
ically, the forward noising process turns a data distribution into partially noisy data, which
are subsequently transformed into a Gaussian distribution by a nonlinear process. The
backward denoising procedure begins with a prior created by sampling from the Gaussian
distribution and applying the invertible normalizing flow transformations deterministically.
To generate the data distribution, the prior then undergoes the remaining diffusion stochas-
tic denoising procedure. Through the reduction of the number of total diffusion steps, we are
able to speed up both the forward and backward processes. More importantly, we improve
the expressive power of diffusion models by employing both deterministic and stochastic
mappings. Experiments on standard image generation datasets demonstrate the advantage
of the proposed method over existing approaches. On the unconditional CIFAR10 dataset,
for example, we achieve an FID of 2.01 and an Inception score of 9.96. Our method also
demonstratescompetitiveperformanceonCelebA-HQ-256datasetasitobtainsanFIDscore
of 7.11. Code is available at https://github.com/MohsenZand/DiNof.
1",TMLR
"Contrastive learning (CL) methods eï¬€ectively learn data representations in a self-supervision
manner, where the encoder contrasts each positive sample over multiple negative samples via
a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image
data, recent CL methods have achieved promising results when pretrained on large-scale
datasets, such as ImageNet. However, most of them consider the augmented views from
the same instance are positive pairs, while views from other instances are negative ones.
Such binary partition insuï¬ƒciently considers the relation between samples and tends to
yield worse performance when generalized on images in the wild. In this paper, to further
improve the performance of CL and enhance its robustness on various datasets, we propose a
â‹†The ï¬rst two authors share equal contribution
1 Published in Transactions on Machine Learning Research (07/2023)
doubly CL strategy that separately compares positive and negative samples within their own
groups, and then proceeds with a contrast between positive and negative groups. We realize
this strategy with contrastive attraction and contrastive repulsion (CACR), which makes
the query not only exert a greater force to attract more distant positive samples but also
do so to repel closer negative samples. Theoretical analysis reveals that CACR generalizes
CLâ€™s behavior by positive attraction and negative repulsion, and it further considers the
intra-contrastive relation within the positive and negative pairs to narrow the gap between the
sampled and true distribution, which is important when datasets are less curated. With our
extensive experiments, CACR not only demonstrates good performance on CL benchmarks,
but also shows better robustness when generalized on imbalanced image datasets. Code and
pre-trained checkpoints are available at https://github.com/JegZheng/CACR-SSL .
1",TMLR
"We present a full implementation and simulation of a novel quantum reinforcement learning
method. Our work is a detailed and formal proof of concept for how quantum algorithms
can be used to solve reinforcement learning problems and shows that, given access to error-
free, efficient quantum realizations of the agent and environment, quantum methods can
yield provable improvements over classical Monte-Carlo based methods in terms of sample
complexity. Our approach shows in detail how to combine amplitude estimation and Grover
search into a policy evaluation and improvement scheme. We first develop quantum policy
evaluation (QPE) which is quadratically more efficient compared to an analogous classi-
cal Monte Carlo estimation and is based on a quantum mechanical realization of a finite
Markov decision process (MDP). Building on QPE, we derive a quantum policy iteration
that repeatedly improves an initial policy using Grover search until the optimum is reached.
Finally, we present an implementation of our algorithm for a two-armed bandit MDP which
we then simulate.
1",TMLR
"Spiking neural networks (SNNs) are energy-efficient neural networks because of their spik-
ing nature. However, as the spike firing rate of SNNs increases, the energy consumption
does as well, and thus, the advantage of SNNs diminishes. Here, we tackle this problem by
introducing a novel penalty term for the spiking activity into the objective function in the
training phase. Our method is designed so as to optimize the energy consumption metric
directly without modifying the network architecture. Therefore, the proposed method can
reduce the energy consumption more than other methods while maintaining the accuracy.
We conducted experiments for image classification tasks, and the results indicate the ef-
fectiveness of the proposed method, which mitigates the dilemma of the energyâ€“accuracy
trade-off.
1",TMLR
"We study the problem of building reasoning agents that are able to generalize in an effective
manner. Towards this goal, we propose an end-to-end approach for building model-based
reinforcement learning agents that dynamically focus their reasoning to the relevant aspects
of the environment: after automatically identifying the distinct aspects of the environment,
theseagentsdynamicallyfilterouttherelevantonesandthenpassthemtotheirsimulatorto
perform partial reasoning. Unlike existing approaches, our approach works with pixel-based
inputs and it allows for interpreting the focal points of the agent. Our quantitative analyses
show that the proposed approach allows for effective generalization in high-dimensional
domains with raw observational inputs. We also perform ablation analyses to validate our
design choices. Finally, we demonstrate through qualitative analyses that our approach
actually allows for building agents that focus their reasoning on the relevant aspects of the
environment.
1",TMLR
"In the real world, the data samples often follow a long-tailed distribution, which poses a great
challenge for Federated Learning (FL). That is, when the data is decentralized and long-tailed, FL
may produce a poorly-behaved global model that is severely biased towards the head classes with the
majority of the training samples. To settle this issue, decoupled training has recently been introduced
to FL. Decoupled training aims to re-balance the biased classifier after the normal instance-balanced
training, and has achieved promising results in centralized long-tailed learning. The current study
directly adopts the decoupled training idea on the server side by re-training the classifier on a set
of pseudo features, due to the unavailability of a global balanced dataset in FL. Unfortunately, this
practice restricts the capacity of decoupled training in federated long-tailed learning as the low-
quality pseudo features lead to a sub-optimal classifier. In this work, motivated by the distributed
characteristic of FL, we propose a decentralized decoupled training mechanism by leveraging the
abundant real data stored in the local. Specifically, we integrate the local real data with the global
gradient prototypes to form the local balanced datasets, and thus re-balance the classifier during the
local training. Furthermore, we introduce a supplementary classifier in the training phase to help
model the global data distribution, which addresses the problem of contradictory optimization goals
caused by performing classifier re-balancing locally. Extensive experiments show that our method
consistently outperforms the existing state-of-the-art methods in various settings. Our code is available
athttps://github.com/keven980716/Federated_Learning_Experiments .
1 Published in Transactions on Machine Learning Research (04/2024)
1",TMLR
"Patch foraging is one of the most heavily studied behavioral optimization challenges in
biology. However, despite its importance to biological intelligence, this behavioral optimiza-
tion problem is understudied in artificial intelligence research. Patch foraging is especially
amenable to study given that it has a known optimal solution, which may be difficult to
discover given current techniques in deep reinforcement learning. Here, we investigate deep
reinforcement learning agents in an ecological patch foraging task. For the first time, we
show that machine learning agents can learn to patch forage adaptively in patterns similar
to biological foragers, and approach optimal patch foraging behavior when accounting for
temporal discounting. Finally, we show emergent internal dynamics in these agents that
resemble single-cell recordings from foraging non-human primates, which complements ex-
perimental and theoretical work on the neural mechanisms of biological foraging. This work
suggests that agents interacting in complex environments with ecologically valid pressures
arrive at common solutions, suggesting the emergence of foundational computations behind
adaptive, intelligent behavior in both biological and artificial agents.
1",TMLR
"In this paper, we consider variational autoencoders (VAE) for general state space mod-
els. We consider a backward factorization of the variational distributions to analyze the
excess risk associated with VAE. Such backward factorizations were recently proposed to
perform online variational learning Campbell et al. (2021) and to obtain upper bounds on
the variational estimation error Chagneux et al. (2022). When independent trajectories of
sequences are observed and under strong mixing assumptions on the state space model and
on the variational distribution, we provide an oracle inequality explicit in the number of
samples and in the length of the observation sequences. We then derive consequences of
this theoretical result. In particular, when the data distribution is given by a state space
model, we provide an upper bound for the Kullback-Leibler divergence between the data
distribution and its estimator and between the variational posterior and the estimated state
space posterior distributions. Under classical assumptions, we prove that our results can be
applied to Gaussian backward kernels built with dense and recurrent neural networks.
1",TMLR
"Among attempts at giving a theoretical account of the success of deep neural networks, a
recent line of work has identified a so-called â€˜lazyâ€™ training regime in which the network
can be well approximated by its linearization around initialization. Here we investigate
the comparative effect of the lazy (linear) and feature learning (non-linear) regimes on
subgroups of examples based on their difficulty. Specifically, we show that easier examples
are given more weight in feature learning mode, resulting in faster training compared to more
difficult ones. In other words, the non-linear dynamics tends to sequentialize the learning
of examples of increasing difficulty. We illustrate this phenomenon across different ways to
quantify example difficulty, including c-score, label noise, and in the presence of easy-to-learn
spurious correlations. Our results reveal a new understanding of how deep networks prioritize
resources across example difficulty.
1",TMLR
"While Contrastive Learning (CL) achieves great success in many downstream tasks, its good
performance heavily relies on a large model capacity. As previous methods focus on scaling
dense models, training and inference costs increase rapidly with model sizes, leading to
large resource consumption. In this paper, we explore CL with an efficient scaling method,
Mixture of Experts (MoE), to obtain a large but sparse model. We start by plugging in the
state-of-the-art CL method to MoE. However, this naive combination fails to visibly improve
performance despite a much larger capacity. A closer look reveals that the naive MoE+CL
model has a strong tendency to route two augmented views of the same image token to
different subsets of experts: such â€œcross-view instability"" breaks the weight-sharing nature
in CL and misleads the invariant feature learning. To address this issue, we introduce a new
regularization mechanism, by enforcing expert-routing similarity between different views of
the same image (or its overlapped patch tokens), while promoting expert-routing diversity
of patches from different images. The resultant method, called CR-MoE, improves by 1.7
pointsintermsof1%semi-supervisedlearningaccuracyonImageNet, comparedtothenaive
combinationbaseline. Itfurthersurpassesthestate-of-the-artCLmethodsonImageNetpre-
training of Vision Transformer (ViT) by 2.8 points, at the same computational cost. Our
findings validate CR-MoE as an effective and efficient image representation learner. Code
is available at https://github.com/VITA-Group/CRMoE .
1",TMLR
"Bayesian optimization is a methodology for global optimization of unknown and expensive
objectives. It combines a surrogate Bayesian regression model with an acquisition function
to decide where to evaluate the objective. Typical regression models are given by Gaussian
processes with stationary covariance functions. However, these functions are unable to ex-
press prior input-dependent information, including possible locations of the optimum. The
ubiquity of stationary models has led to the common practice of exploiting prior information
via informative mean functions. In this paper, we highlight that these models can perform
poorly, especially in high dimensions. We propose novel informative covariance functions
for optimization, leveraging nonstationarity to encode preferences for certain regions of the
search space and adaptively promote local exploration during optimization. We demonstrate
that the proposed functions can increase the sample efficiency of Bayesian optimization in
high dimensions, even under weak prior information.
1",TMLR
"An increasingly important building block of large scale machine learning systems is based
on returning slates; an ordered lists of items given a query. Applications of this technology
include: search, information retrieval and recommender systems. When the action space
is large, decision systems are restricted to a particular structure to complete online queries
quickly. This paper addresses the optimization of these large scale decision systems given an
arbitrary reward function. We cast this learning problem in a policy optimization framework
and propose a new class of policies, born from a novel relaxation of decision functions. This
results in a simple, yet efficient learning algorithm that scales to massive action spaces. We
compare our method to the commonly adopted Plackett-Luce policy class and demonstrate
the effectiveness of our approach on problems with action space sizes in the order of millions.
1",TMLR
"Experience replay (ER) is a crucial component of many deep reinforcement learning (RL)
systems. However, uniform sampling from an ER buffer can lead to slow convergence and
unstable asymptotic behaviors. This paper introduces Stratified Sampling from Event Ta-
bles (SSET), which partitions an ER buffer into Event Tables, each capturing important
subsequences of optimal behavior. We prove a theoretical advantage over the traditional
monolithicbufferapproachandcombineSSETwithanexistingprioritizedsamplingstrategy
to further improve learning speed and stability. Empirical results in challenging MiniGrid
domains, benchmark RL environments, and a high-fidelity car racing simulator demonstrate
the advantages and versatility of SSET over existing ER buffer sampling approaches.
1",TMLR
"Real-world applications of machine learning tools in high-stakes domains are often regulated to
be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with
respect to a protected attribute. However, the exact tradeoff between fairness and accuracy with a
real-valued target is not entirely clear. In this paper, we characterize the inherent tradeoff between
statistical parity and accuracy in the regression setting by providing a lower bound on the error of
any attribute-blind fair regressor. Our lower bound is sharp, algorithm-independent, and admits a
simple interpretation: when the moments of the target differ between groups, any fair algorithm has
to make an error on at least one of the groups. We further extend this result to give a lower bound on
the joint error of any (approximately) fair algorithm, using the Wasserstein distance to measure the
quality of the approximation. With our novel lower bound, we also show that the price paid by a fair
regressor that does not take the protected attribute as input is less than that of a fair regressor with
explicit access to the protected attribute. On the upside, we establish the ï¬rst connection between
individual fairness, accuracy parity, and the Wasserstein distance by showing that if a regressor is
individually fair, it also approximately veriï¬es the accuracy parity, where the gap is again given by
the Wasserstein distance between the two groups. Inspired by our theoretical results, we develop
a practical algorithm for fair regression through the lens of representation learning, and conduct
experiments on a real-world dataset to corroborate our ï¬ndings.
1",TMLR
"Traditional deep learning models are trained and tested on relatively low-resolution images
(<300px), and cannot be directly operated on large-scale images due to compute and
memory constraints. We propose Patch Gradient Descent (PatchGD), an effective learning
strategy that allows us to train the existing CNN and transformer architectures (hereby
referred to as deep learning models) on large-scale images in an end-to-end manner. PatchGD
is based on the hypothesis that instead of performing gradient-based updates on an entire
image at once, it should be possible to achieve a good solution by performing model updates
on only small parts of the image at a time, ensuring that the majority of it is covered over the
course of iterations. PatchGD thus extensively enjoys better memory and compute efficiency
when training models on large-scale images. PatchGD is thoroughly evaluated on PANDA,
UltraMNIST, TCGA, and ImageNet datasets with ResNet50, MobileNetV2, ConvNeXtV2,
and DeiT models under different memory constraints. Our evaluation clearly shows that
PatchGD is much more stable and efficient than the standard gradient-descent method in
handling large images, especially when the compute memory is limited. Code is available at
https://github.com/nyunAI/PatchGD .
1",TMLR
"Robust reinforcement learning (RL) aims to find a policy that optimizes worst-case per-
formance in the face of uncertainties. In this paper, we focus on action robust RL with
the probabilistic policy execution uncertainty, in which, instead of always carrying out the
action specified by the policy, the agent will take the action specified by the policy with
probability 1âˆ’Ïand an alternative adversarial action with probability Ï. We show the
existence of an optimal policy on the action robust MDPs with probabilistic policy execu-
tion uncertainty and provide the action robust Bellman optimality equation for its solution.
Based on that, we develop Action Robust Reinforcement Learning with Certificates (AR-
RLC) algorithm that achieves minimax optimal regret and sample complexity. Our results
highlight that action-robust RL shares the same sample complexity barriers as standard RL,
ensuring robust performance without additional complexity costs. Furthermore, we conduct
numerical experiments to validate our approachâ€™s robustness, demonstrating that ARRLC
outperforms non-robust RL algorithms and converges faster than the other action robust
RL algorithms in the presence of action perturbations.
1",TMLR
"We show that in a cooperative N-agent network, one can design locally executable policies
for the agents such that the resulting discounted sum of average rewards (value) well ap-
proximates the optimal value computed over all (including non-local) policies. Specifically,
we prove that, if |X|,|U|denote the size of state, and action spaces of individual agents,
then for sufficiently small discount factor, the approximation error is given by O(e)where
eâ‰œ1âˆš
N/bracketleftï£¬ig/radicalbig
|X|+/radicalbig
|U|/bracketrightï£¬ig
. Moreover, in a special case where the reward and state transition
functions are independent of the action distribution of the population, the error improves
toO(e)whereeâ‰œ1âˆš
N/radicalbig
|X|. Finally, we also devise an algorithm to explicitly construct a
local policy. With the help of our approximation results, we further establish that the con-
structed local policy is within O(max{e,Ïµ})distance of the optimal policy, and the sample
complexity to achieve such a local policy is O(Ïµâˆ’3), for anyÏµ>0.
1",TMLR
"Class-incremental learning (CIL) suffers from the notorious dilemma between learning newly
added classes and preserving previously learned class knowledge. That catastrophic forgetting
issue could be mitigated by storing historical data for replay, which yet would cause memory
overheads as well as imbalanced prediction updates. To address this dilemma, we propose
to leverage â€œfreeâ€ external unlabeled data querying in continual learning. We first present
a CIL with Queried Unlabeled Data ( CIL-QUD ) scheme, where we only store a handful
of past training samples as anchors and use them to query relevant unlabeled examples
each time. Along with new and past stored data, the queried unlabeled are effectively
utilized, through learning-without-forgetting (LwF) regularizers and class-balance training.
Besides preserving model generalization over past and current tasks, we next study the
problem of adversarial robustness for CIL-QUD. Inspired by the recent success of learning
robust models with unlabeled data, we explore a new robustness-aware CIL setting, where
the learned adversarial robustness has to resist forgetting and be transferred as new tasks
come in continually. While existing options easily fail, we show queried unlabeled data can
continue to benefit, and seamlessly extend CIL-QUD into its robustified versions, RCIL-
QUD. Extensive experiments demonstrate that CIL-QUD achieves substantial accuracy
gains on CIFAR-10 and CIFAR-100, compared to previous state-of-the-art CIL approaches.
Moreover, RCIL-QUD establishes the first strong milestone for robustness-aware CIL. Codes
are available in https://github.com/VITA-Group/CIL-QUD .
1",TMLR
"Over the past decade, neural networks have been successful at making predictions from
biological sequences. As in other fields of deep learning, tools have been devised to extract
features such as sequence motifs that can explain the predictions made by a trained net-
work. Here we intend to go beyond explainable machine learning and introduce SEISM, a
selective inference procedure to test the association between these extracted features and
the predicted phenotype. In particular, we discuss how training a one-layer convolutional
network is formally equivalent to selecting motifs maximizing some association score. We
adapt existing sampling-based selective inference procedures by quantizing this selection
over an infinite set to a large but finite grid. Finally, we show that sampling under a specific
choice of parameters is sufficient to characterize the composite null hypothesis typically used
for selective inferenceâ€”a result that goes well beyond our particular framework. We illus-
trate the behavior of our method in terms of calibration, power and speed and discuss its
power/speed trade-off with a simpler data-split strategy. SEISM paves the way to an easier
analysis of neural networks used in regulatory genomics, and to more powerful methods for
genome wide association studies (GWAS).
1",TMLR
"We propose a methodology for planting watermarks in text from an autoregressive language
model that are robust to perturbations without changing the distribution over text up to a
certain maximum generation budget. We generate watermarked text by mapping a sequence
of random numbersâ€”which we compute using a randomized watermark keyâ€”to a sample
from the language model. To detect watermarked text, any party who knows the key can
align the text to the random number sequence. We instantiate our watermark method-
ology with two sampling schemes: inverse transform sampling and exponential minimum
sampling. We apply these watermarks to three language modelsâ€”OPT-1.3B, LLaMA-7B
and Alpaca-7Bâ€”to experimentally validate their statistical power and robustness to various
paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find
we can reliably detect watermarked text ( pâ‰¤0.01) from 35tokens even after corrupting
between 40-50% of the tokens via random edits (i.e., substitutions, insertions or deletions).
For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking re-
sponses to typical user instructions. Due to the lower entropy of the responses, detection is
more difficult: around 25%of the responsesâ€”whose median length is around 100tokensâ€”
are detectable with pâ‰¤0.01, and the watermark is also less robust to certain automated
paraphrasing attacks we implement.1
1",TMLR
"Biological sequence analysis relies on the ability to denoise the imprecise output of sequenc-
ing platforms. We consider a common setting where a short sequence is read out repeatedly
using a high-throughput long-read platform to generate multiple subreads, or noisy obser-
vations of the same sequence. Denoising these subreads with alignment-based approaches
often fails when too few subreads are available or error rates are too high. In this paper, we
propose a novel method for blindly denoising sets of sequences without directly observing
clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers
subreads together in an embedding space and estimates a single set embedding as the mid-
point of the subreads in both the latent and sequence spaces. This set embedding represents
the â€œaverageâ€ of the subreads and can be decoded into a prediction of the clean sequence.
In experiments on simulated long-read DNA data, SSSL methods denoise small reads of
â‰¤6subreads with 17% fewer errors and large reads of >6subreads with 8% fewer errors
compared to the best baseline. On a real dataset of antibody sequences, SSSL improves
over baselines on two self-supervised metrics, with a significant improvement on difficult
small reads that comprise over 60% of the test set. By accurately denoising these reads,
SSSL promises to better realize the potential of high-throughput DNA sequencing data for
downstream scientific applications.
1",TMLR
"Federated learning (FL) has become a popular tool for solving traditional Reinforcement
Learning (RL) tasks, particularly when individual agents need to collaborate due to low
sample efficiency but are concerned about data privacy. The multi-agent structure addresses
the major concern of data-hungry in traditional RL, while the federated mechanism protects
the data privacy of individual agents. Despite the advantage FL brings to RL, Federated
Reinforcement Learning (FRL) is inherently susceptible to poisoning, as both FL and RL
are vulnerable to such training-time attacks; however, the vulnerability of FRL has not
been well-studied before. In this work, we propose a general framework to characterize FRL
poisoning as an optimization problem and design a poisoning protocol that can be applied
to policy-based FRL. Our framework is versatile, catering to FRL scenarios employing both
policy-gradient local RL and actor-critic local RL. In the context of actor-critic configurations,
we conduct training for a pair of critics, one private and one public, aimed at maximizing
the potency of poisoning. We provably show that our method can strictly hurt the global
objective. We verify the effectiveness of our poisoning approach through comprehensive
experiments, supported by mainstream RL algorithms, across various RL OpenAI Gym
environments covering a wide range of difficulty levels. Within these experiments, we assess
our proposed attack by comparing it to various baselines, including standard, poisoned,
and robust FRL methods. The results demonstrate the power of the proposed protocol in
effectively poisoning FRL systems â€“ It consistently diminishes performance across diverse
environments, proving to be more effective than baseline methods. Our work provides
new insights into the training-time vulnerability of FL in RL and poses new challenges for
designing secure FRL algorithms.
1",TMLR
"Code-mixing and script-mixing are prevalent across online social networks and multilingual
societies. However, a userâ€™s preference toward code-mixing depends on the socioeconomic
status, demographics of the user, and the local context, which existing generative models
tend to ignore while generating code-mixed texts. In this work, we make a pioneering at-
tempt to develop a persona-aware generative model to generate texts resembling real-life
code-mixed texts of individuals. We propose PARADOX, a persona-aware generative model
for code-mixed text generation, which is a novel Transformer-based encoder-decoder model
that encodes an utterance conditioned on a userâ€™s persona and generates code-mixed texts
withoutmonolingual reference data. We propose an alignment module that re-calibrates the
generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed
texts that are semantically more meaningful and linguistically more valid. To evaluate the
personification capabilities of PARADOX, we propose four new metrics â€“ CM BLEU, CM
Rouge-1, CM Rouge-L and CM KS. On average, PARADOX achieves 1.6%better CM BLEU,
57%better perplexity and 32%better semantic coherence than the non-persona-based coun-
terparts. The source code is available at: https://github.com/victor7246/PARADOX .
1",TMLR
"To obtain state-of-the-art performance, many deeper artificial intelligence models sacrifice
human explainability in their decision-making. One solution proposed for achieving top per-
formance and retaining explainability is the Post-Hoc Concept Bottleneck Model (PCBM)
(Yuksekgonul et al., 2023), which can convert the embeddings of any deep neural network
into a set of human-interpretable concept weights. In this work, we reproduce and expand
upon the findings of Yuksekgonul et al. (2023), showing that while their claims and results
do generally hold, some of them could not be sufficiently replicated. Specifically, the claims
relating to PCBM performance preservation and its non-requirement of labeled concept
datasets were generally reproduced, whereas the one claiming its model editing capabili-
ties was not. Beyond these results, our contributions to their work include evidence that
PCBMs may work for audio classification problems, verification of the interpretability of
their methods, and updates to their code for missing implementations. The code for our
implementations can be found in https://github.com/dgcnz/FACT.
1",TMLR
"ion and Reasoning Corpus: Successes,
Failures, andtheImportanceofObject-basedRepresentations
Yudong Xu wil.xu@mail.utoronto.ca
Department of Mechanical & Industrial Engineering, University of Toronto
Wenhao Li chriswenhao.li@mail.utoronto.ca
Department of Mechanical & Industrial Engineering, University of Toronto
Pashootan Vaezipoor pashootan@cs.toronto.edu
Department of Computer Science, University of Toronto
Vector Institute for Artificial Intelligence
Scott Sanner ssanner@mie.utoronto.ca
Department of Mechanical & Industrial Engineering, University of Toronto
Elias B. Khalil khalil@mie.utoronto.ca
Department of Mechanical & Industrial Engineering, University of Toronto
Scale AI Research Chair in Data-Driven Algorithms for Modern Supply Chains
Reviewed on OpenReview: https: // openreview. net/ forum? id= E8m8oySvPJ
Abstract
Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore
this broad question through a systematic analysis of GPT on the Abstraction and Reasoning
Corpus (ARC) (Chollet, 2019), a representative benchmark of abstract reasoning ability
from limited examples in which solutions require some â€œcore knowledgeâ€ of concepts such
as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the
most straightforward ARC tasks when using textual encodings for their two-dimensional
input-output grids. Our failure analysis reveals that GPT-4â€™s capacity to identify objects
and reason about them is significantly influenced by the sequential nature of the text that
represents an object within a text encoding of a task. To test this hypothesis, we design
a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that
are more conducive to GPT-based reasoning, and where it indeed performs better than on
the (2D) ARC. To alleviate this issue, we propose an object-based representation that is
obtained through an external tool, resulting in nearly doubling the performance on solved
ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art
GPT-4 is unable to â€œreasonâ€ perfectly within non-language domains such as the 1D-ARC
or a simple ARC subset, our study reveals that the use of object-based representations can
significantly improve its reasoning ability. Visualizations, GPT logs, and data are available
athttps://khalil-research.github.io/LLM4ARC .
1",TMLR
"Pointcloudsemanticsegmentationfromprojectedviews,suchasrange-view(RV)andbirdâ€™s-
eye-view(BEV),hasbeenintensivelyinvestigated. Differentviewscapturedifferentinforma-
tion of point clouds and thus are complementary to each other. However, recent projection-
based methods for point cloud semantic segmentation usually utilize a vanilla late fusion
strategy for the predictions of different views, failing to explore the complementary informa-
tion from a geometric perspective during the representation learning. In this paper, we in-
troduce a geometric flow network (GFNet) to explore the geometric correspondence between
different views in an align-before-fuse manner. Specifically, we devise a novel geometric flow
module(GFM)tobidirectionallyalignandpropagatethecomplementaryinformationacross
different views according to geometric relationships under the end-to-end learning scheme.
We perform extensive experiments on two widely used benchmark datasets, SemanticKITTI
and nuScenes, to demonstrate the effectiveness of our GFNet for project-based point cloud
semantic segmentation. Concretely, GFNet not only significantly boosts the performance
of each individual view but also achieves state-of-the-art results over all existing projection-
based models. Code is available at https://github.com/haibo-qiu/GFNet .
1",TMLR
"Graph-structured data is prevalent in numerous fields, but the scarcity of labeled instances
often limits the effective application of deep learning techniques. Traditional unsupervised
domain adaptation (UDA) strategies for graphs typically rely on adversarial learning and
pseudo-labeling. However, these methods often fail to leverage the discriminative features
of graphs, resulting in class mismatches and unreliable label quality. To overcome these
challenges, we developed the Denoising and Nuclear-Norm Wasserstein Adaptation Net-
work (DNAN). DNAN utilizes the Nuclear-Norm Wasserstein Discrepancy (NWD), which
simultaneously achieves domain alignment and class distinction. The NWD is integrated
with a denoising mechanism using a variational graph autoencoder, with a theoretical anal-
ysis provided for the denoising process. This denoising mechanism aims to address domain
shifts in structural patterns between the source and target domains. Our comprehensive
experiments demonstrate that DNAN outperforms state-of-the-art methods on standard
UDA benchmarks for graph classification, highlighting its effectiveness and robustness. Our
implementation is available at: https://github.com/WMX567/GraphHarmony .
1",TMLR
"Latent linear dynamical systems with Bernoulli observations provide a powerful modeling
framework for identifying the temporal dynamics underlying binary time series data, which
arise in a variety of contexts such as binary decision-making and discrete stochastic processes
(e.g., binned neural spike trains). Here we develop a spectral learning method for fast,
efficient fitting of probit-Bernoulli latent linear dynamical system (LDS) models. Our
approach extends traditional subspace identification methods to the Bernoulli setting via a
transformation of the first and second sample moments. This results in a robust, fixed-cost
estimator that avoids the hazards of local optima and the long computation time of iterative
fitting procedures like the expectation-maximization (EM) algorithm. In regimes where
data is limited or assumptions about the statistical structure of the data are not met, we
demonstrate that the spectral estimate provides a good initialization for Laplace-EM fitting.
Finally, we show that the estimator provides substantial benefits to real world settings by
analyzing data from mice performing a sensory decision-making task.
1",TMLR
"In Multi-Task Learning (MTL), Kdistinct tasks are jointly optimized. With the vary-
ing nature and complexities of tasks, few tasks might dominate learning. For other tasks,
their respective performances may get compromised due to a negative transfer from domi-
nant tasks. We propose a Dropped-Scheduled Task (DST) algorithm, which probabilistically
â€œdropsâ€ specific tasks during joint optimization while scheduling others to reduce negative
transfer. For each task, a scheduling probability is decided based on four different met-
rics: (i) task depth, (ii) number of ground-truth samples per task, (iii) amount of training
completed, and (iv) task stagnancy. Based on the scheduling probability, specific tasks get
joint computation cycles while others are â€œ droppedâ€. To demonstrate the effectiveness of
the proposed DST algorithm, we perform multi-task learning on three applications and two
architectures. Across unilateral (single input) and bilateral (multiple input) multi-task net-
works, the chosen applications are (a) face (AFLW), (b) fingerprint (IIITD MOLF, MUST,
and NIST SD27), and (c) character recognition (Omniglot) applications. Experimental re-
sults show that the proposed DST algorithm has the minimum negative transfer and overall
least errors across different state-of-the-art algorithms and tasks.
1",TMLR
"The estimation of cumulative distribution functions ( CDF) is an important learning task
with a great variety of downstream applications, such as risk assessments in predictions
and decision making. In this paper, we study functional regression of contextual CDFs
where each data point is sampled from a linear combination of context dependent CDFbasis
functions. We propose functional ridge-regression-based estimation methods that estimate
CDFs accurately everywhere. In particular, given nsamples with dbasis functions, we show
estimation error upper bounds of rOpa
d{nqfor fixed design, random design, and adversarial
context cases. We also derive matching information theoretic lower bounds, establishing
minimax optimality for CDFfunctional regression. Furthermore, we remove the burn-in time
in the random design setting using an alternative penalized estimator. Then, we consider
agnostic settings where there is a mismatch in the data generation process. We characterize
the error of the proposed estimators in terms of the mismatched error, and show that the
estimators are well-behaved under model mismatch. Moreover, to complete our study, we
formalize infinite dimensional models where the parameter space is an infinite dimensional
Hilbert space, and establish a self-normalized estimation error upper bound for this setting.
Notably, the upper bound reduces to the rOpa
d{nqbound when the parameter space is
constrained to be d-dimensional. Our comprehensive numerical experiments validate the
efficacy of our estimation methods in both synthetic and practical settings.
1",TMLR
"The imputation of missing values represents a significant obstacle for many real-world data
analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation
model that relies on two emerging technologies, (conditional) diffusion models as state-of-
the-art generative models and structured state space models as internal model architecture,
which are particularly suited to capture long-term dependencies in time series data. We
demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation
and forecasting performance on a broad range of data sets and different missingness scenarios,
including the challenging blackout-missing scenarios, where prior approaches failed to provide
meaningful results.
1",TMLR
"Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate a source
image to a target image domain with the style of a target image exemplar, without ground-
truth input-translation pairs. Existing UEI2I methods represent style using one vector per
image or rely on semantic supervision to define one style vector per object. Here, in contrast,
we propose to represent style as a dense feature map, allowing for a finer-grained transfer
to the source image without requiring any external semantic information. We then rely on
perceptual and adversarial losses to disentangle our dense style and content representations.
To stylize the source content with the exemplar style, we extract unsupervised cross-domain
semantic correspondences and warp the exemplar style to the source content. We demon-
strate the effectiveness of our method on four datasets using standard metrics together with
a localized style metric we propose, which measures style similarity in a class-wise man-
ner. Our results show that the translations produced by our approach are more diverse,
preserve the source content better, and are closer to the exemplars when compared to the
state-of-the-art methods. Project page: https://github.com/IVRL/dsi2i
1",TMLR
"While current deep learning algorithms have been successful for a wide variety of ar-
tificial intelligence (AI) tasks, including those involving structured image data, they
present deep neurophysiological conceptual issues due to their reliance on the gradi-
ents that are computed by backpropagation of errors (backprop). Gradients are re-
quired to obtain synaptic weight adjustments but require knowledge of feed-forward
activities in order to conduct backward propagation, a biologically implausible pro-
cess. This is known as the â€œweight transport problemâ€. Therefore, in this work, we
present a more biologically plausible approach towards solving the weight transport
problem for image data. This approach, which we name the error-kernel driven ac-
tivation alignment (EKDAA) algorithm, accomplishes through the",TMLR
"Denoising diusion models are a novel class of generative algorithms that achieve state-of-the-
art performance across a range of domains, including image generation and text-to-image tasks.
Building on this success, diusion models have recently been extended to the Riemannian
manifold setting, broadening their applicability to a range of problems from the natural and
engineering sciences. However, these Riemannian diusion models are built on the assumption
that their forward and backward processes are well-deï¬ned for all times, preventing them
from being applied to an important set of tasks that consider manifolds deï¬ned via a set of
inequality constraints. In this work, we introduce a principled framework to bridge this gap.
We present two distinct noising processes based on (i)thelogarithmic barrier metric and
(ii)thereï¬‚ected Brownian motion induced by the constraints. As existing diusion model
techniques cannot be applied in this setting, we derive new tools to deï¬ne such models in
our framework. We then demonstrate the practical utility of our methods on a number of
synthetic and real-world tasks, including applications from robotics and protein design.
1",TMLR
"There is a recent surge in the development of spatio-temporal forecasting models in the
transportation domain. Long-range traffic forecasting, however, remains a challenging task
due to the intricate and extensive spatio-temporal correlations observed in traffic networks.
Current works primarily rely on road networks with graph structures and learn representa-
tions using graph neural networks (GNNs), but this approach suffers from over-smoothing
problem in deep architectures. To tackle this problem, recent methods introduced the com-
bination of GNNs with residual connections or neural ordinary differential equations (ODE).
However, current graph ODE models face two key limitations in feature extraction: (1) they
lean towards global temporal patterns, overlooking local patterns that are important for un-
expected events; and (2) they lack dynamic semantic edges in their architectural design. In
this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks
(GRAM-ODE ) which is designed with multiple connective ODE-GNN modules to learn bet-
ter representations by capturing different views of complex local and global dynamic spatio-
temporal dependencies. We also add some techniques like shared weights and divergence
constraints into the intermediate layers of distinct ODE-GNN modules to further improve
their communication towards the forecasting task. Our extensive set of experiments con-
ductedonsixreal-worlddatasetsdemonstratethesuperiorperformanceof GRAM-ODE com-
pared with state-of-the-art baselines as well as the contribution of different components to
the overall performance. The code is available at https://github.com/zbliu98/GRAM-ODE
1",TMLR
"Deep neural networks are the most commonly used function approximators in oï¬„ine re-
inforcement learning. Prior works have shown that neural nets trained with TD-learning
and gradient descent can exhibit implicit regularization that can be characterized by under-
parameterization of these networks. Speciï¬cally, the rank of the penultimate feature layer,
also called eï¬€ective rank , has been observed to drastically collapse during the training. In
turn, this collapse has been argued to reduce the modelâ€™s ability to further adapt in later
stages of learning, leading to the diminished ï¬nal performance. Such an association between
the eï¬€ective rank and performance makes eï¬€ective rank compelling for oï¬„ine RL, primarily
for oï¬„ine policy evaluation. In this work, we conduct a careful empirical study on the re-
lation between eï¬€ective rank and performance on three oï¬„ine RL datasets : bsuite, Atari,
and DeepMind lab. We observe that a direct association exists only in restricted settings
and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify
three phases of learning that explain the impact of implicit regularization on the learning
dynamics and found that bootstrapping alone is insuï¬ƒcient to explain the collapse of the
eï¬€ective rank. Further, we show that several other factors could confound the relationship
between eï¬€ective rank and performance and conclude that studying this association under
simplistic assumptions could be highly misleading.
1",TMLR
"Deep reinforcement learning has successfully allowed agents to learn complex behaviors
for many tasks. However, a key limitation of current learning approaches is the sample-
inefficiency problem, which limits performance of the learning agent. This paper considers
how agents can benefit from improved learning via teachersâ€™ advice. In particular, we
consider the setting with multiple sub-optimal teachers, as opposed to having a single near-
optimal teacher. We propose a flexible two-level actor-critic algorithm where the high-level
network learns to choose the best teacher in the current situation while the low-level network
learns the control policy.
1",TMLR
"Understanding how different classes are distributed in an unlabeled data set is important
for the calibration of probabilistic classifiers and uncertainty quantification. Methods like
adjusted classify and count, black-box shift estimators, and invariant ratio estimators use an
auxiliaryandpotentiallybiasedblack-boxclassifiertrainedonadifferentdatasettoestimate
the class distribution on the current data set and yield asymptotic guarantees under weak
assumptions. We demonstrate that these algorithms are closely related to the inference in a
particularprobabilisticgraphicalmodelapproximatingtheassumedground-truthgenerative
process, and we propose a Bayesian estimator. Then, we discuss an efficient Markov chain
Monte Carlo sampling scheme for the introduced model and show an asymptotic consistency
guarantee in the large-data limit. We compare the introduced model against the established
point estimators in a variety of scenarios, and show it is competitive, and in some cases
superior, with the non-Bayesian alternatives.
1",TMLR
"Unsupervised semantic segmentation aims to automatically partition images into semanti-
cally meaningful regions by identifying global semantic categories within an image corpus
without any form of annotation. Building upon recent advances in self-supervised rep-
resentation learning, we focus on how to leverage these large pre-trained models for the
downstream task of unsupervised segmentation. We present PriMaPs â€“ Principal Mask
Proposals â€“ decomposing images into semantically meaningful masks based on their feature
representation. This allows us to realize unsupervised semantic segmentation by ï¬tting class
prototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM.
Despite its conceptual simplicity, PriMaPs-EM leads to competitive results across various
pre-trained backbone models, including DINO and DINOv2, and across diï¬€erent datasets,
such as Cityscapes, COCO-Stuï¬€, and Potsdam-3. Importantly, PriMaPs-EM is able to
boost results when applied orthogonally to current state-of-the-art unsupervised semantic
segmentation pipelines. Code is available at https://github.com/visinf/primaps .
1",TMLR
"For ï¬‚exible yet safe imitation learning (IL), we propose theory and a modular method,
with a safety layer that enables a closed-form probability density/gradient of the safe gen-
erative continuous policy, end-to-end generative adversarial training, and worst-case safety
guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-
of-variables formula plus additivity of measures for the density. The set of safe actions is
inferred by ï¬rst checking safety of a ï¬nite sample of actions via adversarial reachability
analysis of fallback maneuvers, and then concluding on the safety of these actionsâ€™ neigh-
borhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the
robustness advantage of using the safety layer already during training (imitation error lin-
ear in the horizon) compared to only using it at test time (up to quadratic error). In an
experiment on real-world driver interaction data, we empirically demonstrate tractability,
safety and imitation performance of our approach.
1",TMLR
"Separating signals from an additive mixture may be an unnecessarily hard problem when
one is only interested in specific properties of a given signal. In this work, we tackle simpler
â€œstatistical component separationâ€ problems that focus on recovering a predefined set of
statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of
the noise process, we investigate a method devised to match the statistics of the solution
candidate corrupted by noise samples with those of the observed mixture. We first analyze
the behavior of this method using simple examples with analytically tractable calculations.
Then, we apply it in an image denoising context employing 1) wavelet-based descriptors,
2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show
that our method better recovers the descriptors of the target data than a standard denoising
method in most situations. Additionally, despite not constructed for this purpose, it performs
surprisingly well in terms of peak signal-to-noise ratio on full signal reconstruction. In
comparison, representation 2) appears less suitable for image denoising. Finally, we extend
this method by introducing a diffusive stepwise algorithm which gives a new perspective
to the initial method and leads to promising results for image denoising under specific
circumstances.
1",TMLR
"Decentralized Actor-Critic (AC) algorithms have been widely utilized for multi-agent rein-
forcement learning (MARL) and have achieved remarkable success. Apart from its empirical
success, the theoretical convergence property of decentralized AC algorithms is largely un-
explored. Most of the existing finite-time convergence results are derived based on either
double-loop update or two-timescale step sizes rule, and this is the case even for centralized
AC algorithm under a single-agent setting. In practice, the single-timescale update is widely
utilized, where actor and critic are updated in an alternating manner with step sizes being
of the same order. In this work, we study a decentralized single-timescale AC algorithm.
Theoretically, using linear approximation for value and reward estimation, we show that
the algorithm has sample complexity of ËœO(Îµâˆ’2)under Markovian sampling, which matches
the optimal complexity with a double-loop implementation (here, ËœOhides a logarithmic
term). When we reduce to the single-agent setting, our result yields new sample complexity
for centralized AC using a single-timescale update scheme. The central to establishing
our complexity results is the hidden smoothness of the optimal critic variable we revealed.
We also provide a local action privacy-preserving version of our algorithm and its analysis.
Finally, we conduct experiments to show the superiority of our algorithm over the existing
decentralized AC algorithms.
1",TMLR
"The federated learning paradigm has motivated the development of methods for aggregating
multiple client updates into a global server model, without sharing client data. Many fed-
erated learning algorithms, including the canonical Federated Averaging ( FedAvg ), take a
direct (possibly weighted) average of the client parameter updates, motivated by results in
distributed optimization. In this work, we adopt a function space perspective and propose
a new algorithm, FedFish , that aggregates local approximations to the functions learned
by clients, using an estimate based on their Fisher information. We evaluate FedFish on
realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suï¬€er
as client models drift further apart, we demonstrate that FedFish is more robust to longer
local training. Our evaluation across several settings in image and language benchmarks
shows that FedFish outperforms FedAvg as local training epochs increase. Further, Fed-
Fishresults in global networks that are more amenable to eï¬ƒcient personalization via local
ï¬ne-tuning on the same or shifted data distributions. For instance, federated pretraining
on the C4 dataset, followed by few-shot personalization on Stack Overï¬‚ow, results in a 7%
improvement in next-token prediction by FedFish overFedAvg .
1",TMLR
"Large language models (LLMs) have shown remarkable performance in various tasks and
have been extensively utilized by the public. However, the increasing concerns regarding
the misuse of LLMs, such as plagiarism and spamming, have led to the development of
multiple detectors, including fine-tuned classifiers and statistical methods. In this study,
we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate
the vulnerability of these detectors. We propose a novel Substitution-based In-Context
example Optimization method (SICO) to automatically construct prompts for evading the
detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited
number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has
been constructed, it can be universally used against a wide range of detectors. Extensive
experiments across three real-world tasks demonstrate that SICO significantly outperforms
the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing
their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation show that
the SICO-generated text achieves human-level readability and task completion rates, while
preserving high imperceptibility. Finally, we propose an ensemble approach to enhance the
robustness of detectors against SICO attack.1
1The code is publicly available at https://github.com/ColinLu50/Evade-GPT-Detector .
1 Published in Transactions on Machine Learning Research (05/2024)
1",TMLR
"In advancing the understanding of natural decision-making processes, inverse reinforcement
learning (IRL) methods have proven instrumental in reconstructing animalâ€™s intentions un-
derlying complex behaviors. Given the recent development of a continuous-time multi-
intention IRL framework, there has been persistent inquiry into inferring discrete time-
varying rewards with IRL. To address this challenge, we introduce the class of hierarchical
inverse Q-learning (HIQL) algorithms. Through an unsupervised learning process, HIQL
divides expert trajectories into multiple intention segments, and solves the IRL problem
independently for each. Applying HIQL to simulated experiments and several real animal
behavior datasets, our approach outperforms current benchmarks in behavior prediction
and produces interpretable reward functions. Our results suggest that the intention tran-
sition dynamics underlying complex decision-making behavior is better modeled by a step
function instead of a smoothly varying function. This advancement holds promise for neu-
roscience and cognitive science, contributing to a deeper understanding of decision-making
and uncovering underlying brain mechanisms.
1",TMLR
"Visual reasoning is dominated by end-to-end neural networks scaled to billions of model
parameters and training examples. However, even the largest models struggle with composi-
tional reasoning, generalization, ï¬ne-grained spatial and temporal reasoning, and counting.
Visual reasoning with large language models (LLMs) as controllers can, in principle, address
these limitations by decomposing the task and solving subtasks by orchestrating a set of (vi-
sual) tools. Recently, these models achieved great performance on tasks such as compositional
visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in
their current form, these models heavily rely on human engineering of in-context examples
in the prompt, which are often dataset- and task-speciï¬c and require signiï¬cant labor by
highly skilled programmers. In this work, we present a framework that mitigates these
issues by introducing spatially and temporally abstract routines and by leveraging a small
number of labeled examples to automatically generate in-context examples, thereby avoiding
human-created in-context examples. On a number of visual reasoning tasks, we show that
our framework leads to consistent gains in performance, makes LLMs as controllers setup
more robust, and removes the need for human engineering of in-context examples.
1",TMLR
"ed Observations
Rolf A. N. Starre r.a.n.starre@tudelft.nl
Delft University of Technology
Marco Loog marco.loog@ru.nl
Radboud University
Elena Congeduti e.congeduti@tudelft.nl
Delft University of Technology
Frans A. Oliehoek f.a.oliehoek@tudelft.nl
Delft University of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= YQWOzzSMPp
Abstract
Many methods for Model-based Reinforcement learning (MBRL) in Markov decision pro-
cesses (MDPs) provide guarantees for both the accuracy of the model they can deliver and
the learning efficiency. At the same time, state abstraction techniques allow for a reduc-
tion of the size of an MDP while maintaining a bounded loss with respect to the original
problem. Therefore, it may come as a surprise that no such guarantees are available when
combining both techniques, i.e., where MBRL merely observes abstract states. Our theoret-
ical analysis shows that abstraction can introduce a dependence between samples collected
online (e.g., in the real world). That means that, without taking this dependence into
account, results for MBRL do not directly extend to this setting. Our result shows that
we can use concentration inequalities for martingales to overcome this problem. This re-
sult makes it possible to extend the guarantees of existing MBRL algorithms to the setting
with abstraction. We illustrate this by combining R-MAX, a prototypical MBRL algorithm,
with abstraction, thus producing the first performance guarantees for model-based â€˜RL from
Abstracted Observationsâ€™: model-based reinforcement learning with an abstract model.
1",TMLR
"Attribute-based representations help machine learning models perform tasks based on hu-
man understandable concepts, allowing a closer human-machine collaboration. However,
learning attributes that accurately reï¬‚ect the content of an image is not always straightfor-
ward, as per-image ground truth attributes are often not available. We propose applying
the Multiple Instance Learning (MIL) paradigm to attribute learning (AMIL) while only
using class-level labels. We allow the model to under-predict the positive attributes, which
may be missing in a particular image due to occlusions or unfavorable pose, but not to
over-predict the negative ones, which are almost certainly not present. We evaluate it in
the zero-shot learning (ZSL) setting, where training and test classes are disjoint, and show
that this also allows to proï¬t from knowledge about the semantic relatedness of attributes.
In addition, we apply the MIL assumption to ZSL classiï¬cation and propose MIL-DAP,
an attribute-based zero-shot classiï¬cation method, based on Direct Attribute Prediction
(DAP), to evaluate attribute prediction methods when no image-level data is available for
evaluation. Experiments on CUB-200-2011, SUN Attributes and AwA2 show improvements
on attribute detection, attribute-based zero-shot classiï¬cation and weakly supervised part
localization.
1",TMLR
"Most current domain adaptation methods address either covariate shift or label shift, but
are not applicable where they occur simultaneously and are confounded with each other.
Domain adaptation approaches which do account for such confounding are designed to adapt
covariates to optimally predict a particular label whose shift is confounded with covariate
shift. In this paper, we instead seek to achieve general-purpose data backwards compatibility.
This would allow the adapted covariates to be used for a variety of downstream problems,
including on pre-existing prediction models and on data analytics tasks. To do this we
consider a modification of generalized label shift (GLS), which we call confounded shift . We
present a novel framework for this problem, based on minimizing the expected divergence
between the source and target conditional distributions, conditioning on possible confounders.
Within this framework, we provide concrete implementations using the Gaussian reverse
Kullback-Leibler divergence and the maximum mean discrepancy. Finally, we demonstrate
our approach on synthetic and real datasets.
1",TMLR
"We present GPS ++, a hybrid Message Passing Neural Network / Graph Transformer model
for molecular property prediction. Our model integrates a well-tuned local message passing
component and biased global attention with other key ideas from prior literature to achieve
state-of-the-art results on large-scale molecular dataset PCQM4Mv2. Through a thorough
ablation study we highlight the impact of individual components and find that nearly all of
the modelâ€™s performance can be maintained without any use of global self-attention, showing
that message passing is still a competitive approach for 3D molecular property prediction
despite the recent dominance of graph transformers. We also find that our approach is
significantly more accurate than prior art when 3D positional information is not available.
Table 1: Comparison of model size and accuracy on large-scale molecular property prediction dataset
PCQM4Mv2.
Model # ParamsModel Validation
Type MAE (meV) â†“
GIN-virtual (Hu et al., 2021) 6.7M MPNN 108.3
GPS (RampÃ¡Å¡ek et al., 2022) 19.4M Hybrid 85.8
GEM-2 (Liu et al., 2022a) 32.1M Transformer 79.3
Global-ViSNet (Wang et al., 2022b) 78.5M Transformer 78.4
Transformer-M (Luo et al., 2022) 69.0M Transformer 77.2
GPS ++[MPNN only] 40.0M MPNN 77.2
GPS ++ 44.3M Hybrid 76.6
1",TMLR
"Equivariance of linear neural network layers is well studied. In this work, we relax the
equivariance condition to only be true in a projective sense. Hereby, we introduce the
topic of projective equivariance to the machine learning audience. We theoretically study
the relation of projectively and linearly equivariant linear layers. We ï¬nd that in some
important cases, surprisingly, the two types of layers coincide. We also propose a way
to construct a projectively equivariant neural network, which boils down to building a
standard equivariant network where the linear group representations acting on each in-
termediate feature space are lifts of projective group representations. Projective equivari-
ance is showcased in two simple experiments. Code for the experiments is provided at
github.com/usinedepain/projectively_equivariant_deep_nets
1",TMLR
"A peculiarity of conversational search systems is that they involve mixed-initiatives such
as system-generated query clarifying questions. Evaluating those systems at a large scale
on the end task of IR is very challenging, requiring adequate datasets containing such
interactions. However, current datasets only focus on either traditional ad-hoc IR tasks
or query clarification tasks, the latter being usually seen as a reformulation task from the
initial query. Only a few datasets are known to contain both document relevance judgments
and the associated clarification interactions such as Qulac and ClariQ. Both are based on
the TREC Web Track 2009-12 collection, but cover a very limited number of topics (237
topics), far from being enough for training and testing conversational IR models. To fill
the gap, we propose a methodology to automatically build large-scale conversational IR
datasets from ad-hoc IR datasets in order to facilitate explorations on conversational IR.
Our methodology is based on two processes: 1) generating query clarification interactions
through query clarification and answer generators, and 2) augmenting ad-hoc IR datasets
with simulated interactions. In this paper, we focus on MsMarco and augment it with query
clarification and answer simulations. We perform a thorough evaluation showing the quality
and the relevance of the generated interactions for each initial query. This paper shows the
feasibility and utility of augmenting ad-hoc IR datasets for conversational IR.
1",TMLR
"Self-supervised learning of deep neural networks has become a prevalent paradigm for
learning representations that transfer to a variety of downstream tasks. Similar to proposed
models of the ventral stream of biological vision, it is observed that these networks lead to a
separation of category manifolds in the representations of the penultimate layer. Although
this observation matches the manifold hypothesis of representation learning, current self-
supervised approaches are limited in their ability to explicitly model this manifold. Indeed,
current approaches often only apply a pre-specified set of augmentations for â€œpositive pairsâ€
during learning. In this work, we propose a contrastive learning approach that directly models
the latent manifold using Lie group operators parameterized by coefficients with a sparsity-
promoting prior. A variational distribution over these coefficients provides a generative model
of the manifold, with samples which provide feature augmentations applicable both during
contrastive training and downstream tasks. Additionally, learned coefficient distributions
provide a quantification of which transformations are most likely at each point on the manifold
while preserving identity. We demonstrate benefits in self-supervised benchmarks for image
datasets, as well as a downstream semi-supervised task. In the former case, we demonstrate
that the proposed methods can effectively apply manifold feature augmentations and improve
learning both with and without a projection head. In the latter case, we demonstrate that
feature augmentations sampled from learned Lie group operators can improve classification
performance when using few labels1.
1",TMLR
"Popular approaches for quantifying predictive uncertainty in deep neural networks often in-
volve distributions over weights or multiple models, for instance via Markov Chain sampling,
ensembling, or Monte Carlo dropout. These techniques usually incur overhead by having to
train multiple model instances or do not produce very diverse predictions. This comprehen-
sive and extensive survey aims to familiarize the reader with an alternative class of models
based on the concept of Evidential Deep Learning : For unfamiliar data, they aim to admit
â€œwhat they donâ€™t knowâ€, and fall back onto a prior belief. Furthermore, they allow uncer-
tainty estimation in a single model and forward pass by parameterizing distributions over
distributions . This survey recapitulates existing works, focusing on the implementation in a
classification setting, before surveying the application of the same paradigm to regression.
We also reflect on the strengths and weaknesses compared to other existing methods and
provide the most fundamental derivations using a unified notation to aid future research.
1",TMLR
"In this paper, we address the following problem: Given an offline demonstration dataset
from an imperfect expert, what is the best way to leverage it to bootstrap online learning
performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL)
algorithm that uses the offline dataset, and information about the expertâ€™s behavioral policy
used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero
exponentially fast in N, the offline dataset size if the expert is competent enough. Since
this algorithm is computationally impractical, we then propose the iRLSVI algorithm that
can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning.
Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant
reduction in regret as compared to two baselines: no offline data, and offline dataset but
used without suitably modeling the generative policy. Our algorithm can be seen as bridging
online RL and imitation learning.
1",TMLR
"Many approaches for optimizing decision making models rely on gradient based methods
requiring informative feedback from the environment. However, in the case where such
feedback is sparse or uninformative, such approaches may result in poor performance.
Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the
quality of gradient feedback, but are known to scale poorly in the high-dimension setting
of complex decision making models. This problem is exacerbated if the model requires
interactions between several agents cooperating to accomplish a shared goal. To address
the dimensionality challenge, we propose a compact multi-layered architecture modeling
the dynamics of agent interactions through the concept of role. We introduce Dependency
Structure Search Bayesian Optimization to efficiently optimize the multi-layered architecture
parameterized by a large number of parameters, and show an improved regret bound. Our
approach shows strong empirical results under malformed or sparse reward.
1",TMLR
"Saliency methods attempt to explain deep neural networks by highlighting the most salient
features of a sample. Some widely used methods are based on a theoretical framework called
Deep Taylor Decomposition (DTD), which formalizes the recursive application of the Tay-
lor Theorem to the networkâ€™s layers. However, recent work has found these methods to be
independent of the networkâ€™s deeper layers and appear to respond only to lower-level image
structure. Here, we investigate the DTD theory to better understand this perplexing behav-
ior and found that the Deep Taylor Decomposition is equivalent to the basic gradient Ë†input
method when the Taylor root points (an important parameter of the algorithm chosen by
the user) are locally constant. If the root points are locally input-dependent, then one can
justify any explanation. In this case, the theory is under-constrained. In an empirical eval-
uation, we find that DTD roots do not lie in the same linear regions as the input â€“ contrary
to a fundamental assumption of the Taylor theorem. The theoretical foundations of DTD
were cited as a source of reliability for the explanations. However, our findings urge caution
in making such claims.
1",TMLR
"Multiparty computation approaches to secure neural network inference commonly rely on
garbled circuits for securely executing nonlinear activation functions. However, garbled
circuits require excessive communication between server and client, impose significant storage
overheads, and incur large runtime penalties; for example, securely evaluating ResNet-32
using standard approaches requires more than 300MB of communication, over 10s of runtime,
and around 5 GB of preprocessing storage. To reduce these costs, we propose an alternative
to garbled circuits: Tabula, an algorithm based on secure lookup tables. Our approach
precomputes lookup tables during an offline phase that contains the result of all possible
nonlinear function calls. Because these tables incur exponential storage costs in the number
of operands and the precision of the input values, we use quantization to reduce these
storage costs to make this approach practical. This enables an online phase where securely
computing the result of a nonlinear function requires just a single round of communication,
with communication cost equal to twice the number of bits of the input to the nonlinear
function. In practice our approach costs 2 bytes of communication per nonlinear function
call in the online phase. Compared to garbled circuits with 8-bit quantized inputs, when
computing individual nonlinear functions during the online phase, experiments show Tabula
with 8-bit activations uses between 280-560Ã—less communication, is over 100Ã—faster, and
uses a comparable (within a factor of 2) amount of storage; compared against other state-
of-the-art protocols Tabula achieves greater than 40Ã—communication reduction. This
leads to significant performance gains over garbled circuits with quantized inputs during the
online phase of secure inference of neural networks: Tabula reduces end-to-end inference
communication by up to 9Ã—and achieves an end-to-end inference speedup of up to 50Ã—,
while imposing comparable storage and offline preprocessing costs.
1",TMLR
"Model-based deep learning methods such as loop unrolling (LU) and deep equilibrium model
(DEQ) extensions offer outstanding performance in solving inverse problems (IP). These
methods unroll the optimization iterations into a sequence of neural networks that in effect
learn a regularization function from data. While these architectures are currently state-of-
the-art in numerous applications, their success heavily relies on the accuracy of the forward
model. This assumption can be limiting in many physical applications due to model simplifi-
cations or uncertainties in the apparatus. To address forward model mismatch, we introduce
an untrained forward model residual block within the model-based architecture to match
the data consistency in the measurement domain for each instance. We propose two variants
in well-known model-based architectures (LU and DEQ) and prove convergence under mild
conditions. Ourapproachoffersaunifiedsolutionthatislessparameter-sensitive,requiresno
additional data, and enables simultaneous fitting of the forward model and reconstruction in
a single pass, benefiting both linear and nonlinear inverse problems. The experiments show
significant quality improvement in removing artifacts and preserving details across three dis-
tinct applications, encompassing both linear and nonlinear inverse problems. Moreover, we
highlight reconstruction effectiveness in intermediate steps and showcase robustness to ran-
dom initialization of the residual block and a higher number of iterations during evaluation.
Code is available at https://github.com/InvProbs/A-adaptive-model-based-methods .
1",TMLR
"Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms
in reinforcement learning (RL). Despite its widespread use, it has only been recently that
researchers have begun to actively study its finite time behavior, including the finite time
bound on the mean squared error and sample complexity. On the empirical side, experience
replay has been a key ingredient in the success of deep RL algorithms, but its theoretical
effects on RL have yet to be fully understood. In this paper, we present a simple decomposi-
tion of the Markovian noise terms and provide finite-time error bounds for tabular on-policy
TD-learning with experience replay. Specifically, under the Markovian observation model,
we demonstrate that for both the averaged iterate and final iterate cases, the error term
induced by a constant step-size can be effectively controlled by the size of the replay buffer
and the mini-batch sampled from the experience replay buffer.
1",TMLR
"In real-world applications, one often encounters ambiguously labeled data, where different
annotators assign conflicting class labels. Partial-label learning allows training classifiers in
this weakly supervised setting, where state-of-the-art methods already show good predictive
performance. However, even the best algorithms give incorrect predictions, which can
have severe consequences when they impact actions or decisions. We propose a novel
risk-consistent nearest-neighbor-based partial-label learning algorithm with a reject option,
that is, the algorithm can reject unsure predictions. Extensive experiments on artificial and
real-world datasets show that our method provides the best trade-off between the number and
accuracy of non-rejected predictions when compared to our competitors, which use confidence
thresholds for rejecting unsure predictions. When evaluated without the reject option, our
nearest-neighbor-based approach also achieves competitive prediction performance.
1",TMLR
"To improve the performance of deep learning, mixup has been proposed to force the neural
networks favoring simple linear behaviors in-between training samples. Performing mixup
for transfer learning with pre-trained models however is not that simple, a high capacity
pre-trained model with a large fully-connected (FC) layer could easily overfit to the target
dataset even with samples-to-labels mixed up. In this work, we propose SMILEâ€”Sample-
to-feature MIxup for Efficient Transfer LEarning. With mixed images as inputs, SMILE
regularizes the outputs of CNN feature extractors to learn from the mixed feature vectors of
inputs, in addition to the mixed labels. SMILEincorporates a mean teacher to provide the
surrogate ""ground truth"" for mixed feature vectors. The sample-to-feature mixup regularizer
is imposed both on deep features for the target domain and classifier outputs for the source
domain, bounding the linearity in-between samples for target tasks. Extensive experiments
have been done to verify the performance improvement made by SMILE, in comparisons
with a wide spectrum of transfer learning algorithms, including fine-tuning, L2-SP, DELTA,
BSS, RIFLE, Co-Tuning and RegSL, even with mixup strategies combined. Ablation studies
show that the vanilla sample-to-label mixup strategies could marginally increase the linearity
in-between training samples but lack of generalizability, while SMILEsignificantly improves
the mixup effects in both label and feature spaces with both training and testing datasets.
The empirical observations backup our design intuition and purposes. Our code is available
at https://github.com/lixingjian/SMILE.
1",TMLR
"We present the Pathways (Dean, 2021) Autoregressive Text-to-Image (Parti) model, which
generates high-fidelity photorealistic images and supports content-rich synthesis involving
complex compositions and world knowledge. Parti treats text-to-image generation as a
sequence-to-sequence modeling problem, akin to machine translation, with sequences of
image tokens as the target outputs rather than text tokens in another language. This strategy
can naturally tap into the rich body of prior work on large language models, which have seen
continued advances in capabilities and performance through scaling data and model sizes.
Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN,
to encode images as sequences of discrete tokens. Second, we achieve consistent quality
improvements by scaling the encoder-decoder Transformer model up to 20B parameters,
with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on
MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P 2),
a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of
Parti across a wide variety of categories and difficulty aspects. We also explore and highlight
limitations of our models in order to define and exemplify key areas of focus for further
improvements.
1",TMLR
"Causal inference is essential for data-driven decision-making across domains such as business
engagement, medical treatment, and policy making. However, in practice, causal inference
suffers from many limitations including unknown causal graphs, missing data problems, and
mixed data types. To tackle those challenges, we develop Deep End-to-end Causal Inference
(DECI) framework, a flow-based non-linear additive noise model combined with variational
inference, whichcanperformbothBayesiancausaldiscoveryandinference. Theoretically, we
show that DECI unifies many existing structural equation model (SEM) based causal infer-
ence techniques and can recover the ground truth mechanism under standard assumptions.
Motivated by the challenges in the real world, we further extend DECI to heterogeneous,
mixed-type data with missing values, allowing for both continuous and discrete treatment
decisions. Empirically, we conduct extensive experiments (over a thousand) to show the
competitive performance of DECI when compared to relevant baselines for both causal dis-
covery and inference with both synthetic and causal machine learning benchmarks across
data types and levels of missingness.
1",TMLR
"Quantifying the impact of individual data samples on machine learning models is an open
research problem. This is particularly relevant when complex and high-dimensional relation-
ships have to be learned from a limited sample of the data generating distribution, such as in
deep learning. It was previously shown that, in these cases, models rely not only on extract-
ing patterns which are helpful for generalisation, but also seem to be required to incorporate
some of the training data more or less as is, in a process often termed memorisation . This
raises the question: if some memorisation is a requirement for effective learning, what are
its privacy implications? In this work we consider a broad range of previous definitions
and perspectives on memorisation in ML, discuss their interplay with model generalisation
and their implications of these phenomena on data privacy. We then propose a framework
to reason over what memorisation means in the context of ML training under the prism
of individual sampleâ€™s influence on the model. Moreover, we systematise methods allowing
practitioners to detect the occurrence of memorisation or quantify it and contextualise our
findings in a broad range of ML learning settings. Finally, we discuss memorisation in the
context of privacy attacks, differential privacy and adversarial actors.
1",TMLR
"When analyzing real-world data it is common to work with event ensembles, which comprise
sets of observations that collectively constrain the parameters of an underlying model of
interest. Such models often have a hierarchical structure, where â€œlocalâ€ parameters impact
individual events and â€œglobalâ€ parameters influence the entire dataset. We introduce practical
approaches for frequentist and Bayesian dataset-wide probabilistic inference in cases where
the likelihood is intractable, but simulations can be realized via a hierarchical forward model.
We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly
accounting for the modelâ€™s hierarchical structure can lead to significantly tighter parameter
constraints. We ground our discussion using case studies from the physical sciences, focusing
on examples from particle physics and cosmology.
1",TMLR
"Universal random features (URF) are state of the art regarding practical graph neural net-
works that are provably universal. There is great diversity regarding terminology, method-
ology, benchmarks, and evaluation metrics used among existing URF. Not only does this
make it increasingly difficult for practitioners to decide which technique to apply to a given
problem, but it also stands in the way of systematic improvements. We propose a new
comprehensive framework that captures all previous URF techniques. On the theoretical
side, among other results, we formally prove that under natural conditions all instantiations
of our framework are universal. The framework thus provides a new simple technique to
prove universality results. On the practical side, we develop a method to systematically
and automatically train URF. This in turn enables us to impartially and objectively com-
pare all existing URF. New URF naturally emerge from our approach, and our experiments
demonstrate that they improve the state of the art.
1",TMLR
"Machine learning frameworks such as graph neural networks typically rely on a given, ï¬xed
graph to exploit relational inductive biases and thus eï¬€ectively learn from network data.
However, when said graphs are (partially) unobserved, noisy, or dynamic, the problem
of inferring graph structure from data becomes relevant. In this paper, we postulate a
graph convolutional relationship between the observed and latent graphs, and formulate
the graph structure learning task as a network inverse (deconvolution) problem. In lieu of
eigendecomposition-based spectral methods or iterative optimization solutions, we unroll and
truncate proximal gradient iterations to arrive at a parameterized neural network architecture
that we call a Graph Deconvolution Network (GDN). GDNs can learn a distribution of graphs
in a supervised fashion, perform link prediction or edge-weight regression tasks by adapting
the loss function, and they are inherently inductive as well as node permutation equivariant.
We corroborate GDNâ€™s superior graph learning performance and its generalization to larger
graphs using synthetic data in supervised settings. Moreover, we demonstrate the robustness
and representation power of GDNs on real world neuroimaging and social network datasets.
1",TMLR
"With the increasing demand for deep learning models on mobile devices, splitting neural network
computation between the device and a more powerful edge server has become an attractive solution.
However, existing split computing approaches often underperform compared to a naive baseline of
remote computation on compressed data. Recent studies propose learning compressed representa-
tions that contain more relevant information for supervised downstream tasks, showing improved
tradeoffs between compressed data size and supervised performance. However, existing evaluation
metrics only provide an incomplete picture of split computing. This study introduces supervised
compression for split computing (SC2) and proposes new evaluation criteria: minimizing computa-
tion on the mobile device, minimizing transmitted data size, and maximizing model accuracy. We
conduct a comprehensive benchmark study using 10 baseline methods, three computer vision tasks,
and over 180 trained models, and discuss various aspects of SC2. We also release our code1and
sc2bench ,2a Python package for future research on SC2. Our proposed metrics and package will
help researchers better understand the tradeoffs of supervised compression in split computing.
1",TMLR
"The efï¬cacy of availability poisoning, a method of poisoning data by injecting imperceptible per-
turbations to prevent its use in model training, has been a hot subject of investigation. Previous
research suggested that it was difï¬cult to effectively counteract such poisoning attacks. However, the",TMLR
"Adversarial robustness often comes at the cost of degraded accuracy, impeding real-life
applications of robust classification models. Training-based solutions for better trade-offs are
limited by incompatibilities with already-trained high-performance large models, necessitating
the exploration of training-free ensemble approaches. Observing that robust models are more
confident in correct predictions than in incorrect ones on clean and adversarial data alike, we
speculate amplifying this â€œbenign confidence propertyâ€ can reconcile accuracy and robustness
in an ensemble setting. To achieve so, we propose â€œMixedNUTSâ€, a training-free method
where the output logits of a robust classifier and a standard non-robust classifier are processed
by nonlinear transformations with only three parameters, which are optimized through an
efficient algorithm. MixedNUTS then converts the transformed logits into probabilities
and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet datasets,
experimental results with custom strong adaptive attacks demonstrate MixedNUTSâ€™s vastly
improved accuracy and near-SOTA robustness â€“ it boosts CIFAR-100 clean accuracy by 7.86
points, sacrificing merely 0.87points in robust accuracy.
1",TMLR
"Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep image
priors (DIP) that regularize various image inverse problems. Meanwhile, researchers also
proposed extremely compact, under-parameterized image priors (e.g., deep decoder) that
are strikingly competent for image restoration too, despite a loss of accuracy. These two
extremes push us to think whether there exists a better solution in the middle: between
over- and under-parameterized image priors, can one identify â€œintermediate"" parameter-
ized image priors that achieve better trade-offs between performance, efficiency, and even
preserving strong transferability? Drawing inspirations from the lottery ticket hypothesis
(LTH), we conjecture and study a novel â€œlottery image prior"" ( LIP) by exploiting DNN
inherent sparsity, stated as: given an over-parameterized DNN-based image prior, it will
contain a sparse subnetwork that can be trained in isolation, to match the original DNNâ€™s
performance when being applied as a prior to various image inverse problems . Our results
validate the superiority of LIPs: we can successfully locate the LIP subnetworks from over-
parameterized DIPs at substantial sparsity ranges. Those LIP subnetworks significantly
outperform deep decoders under comparably compact model sizes (by often fully preserving
the effectiveness of their over-parameterized counterparts), and they also possess high trans-
ferability across different images as well as restoration task types. Besides, we also extend
LIP to compressive sensing image reconstruction, where a pre-trained GAN generator is
used as the prior (in contrast to untrained DIP or deep decoder), and confirm its validity
in this setting too. To our best knowledge, this is the first time that LTH is demonstrated
to be relevant in the context of inverse problems or image priors. Codes are available at
https://github.com/VITA-Group/Chasing-Better-DIPs .
1",TMLR
"We consider the problem of online prediction using linear smoothers that are functions of
a nominal covariance model with unknown parameters. The model parameters are often
learned using cross-validation or maximum-likelihood techniques. But when training data
arrives in a streaming fashion, the implementation of such techniques can only be done in
an approximate manner. Even if this limitation could be overcome, there appears to be no
clear-cut results on the statistical properties of the resulting predictor.
Here we consider a covariance-ï¬tting method to learn the model parameters, which was
initially developed for spectral estimation. We ï¬rst show that the use of this approach
results in a computationally eï¬ƒcient online learning method in which the resulting predictor
can be updated sequentially. We then prove that, with high probability, its out-of-sample
error approaches the optimal level at a root- nrate, where nis the number of data samples.
This is so even if the nominal covariance model is misspeciï¬ed. Moreover, we show that the
resultingpredictor enjoys two robustnessproperties. First, it corresponds to apredictor that
minimizes the out-of-sample error with respect to the least favourable distribution within
a given Wasserstein distance from the empirical distribution. Second, it is robust against
errors in the covariate training data. We illustrate the performance of the proposed method
in a numerical experiment.
1",TMLR
"Advancements in accelerated physics simulations have greatly reduced training times for
reinforcement learning policies, yet the conventional step-by-step agent-simulator interac-
tion undermines simulation accuracy. In the real world, interactions are asynchronous, with
sensing, acting and processing happening simultaneously. Failing to capture this widens the
sim2real gap and results in suboptimal real-world performance. In this paper, we address
the challenges of simulating realistic asynchronicity and delays within parallelized simula-
tions, crucial to bridging the sim2real gap in complex cyber-physical systems. Our approach
efficiently parallelizes cyber-physical system simulations on accelerator hardware, including
physics, sensors, actuators, processing components and their asynchronous interactions. We
extend existing accelerated physics simulations with latency simulation capabilities by con-
structing a â€˜supergraphâ€™ that encodes all data dependencies across parallelized simulation
steps, ensuring accurate simulation. By finding the smallest supergraph, we minimize re-
dundant computation. We validate our approach on two real-world systems and perform an
extensive ablation, demonstrating superior performance compared to baseline methods.
1",TMLR
"Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or
variations in the structural complexity of the source function, pose a unique set of challenges
for statistical inference. Accounting for them can greatly improve predictive power when
physical resources or computation time is limited. In this paper, we draw on recent theoretical
results on the estimation of local function complexity (LFC), derived from the domain of
local polynomial smoothing (LPS), to establish a notion of local structural complexity, which
is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on
pointwise estimates, the LPSmodel class is not robust and scalable concerning large input
space dimensions that typically come along with real-world problems. Here, we derive and
estimate the Gaussian process regression (GPR)-based analog of the LPS-basedLFCand
use it as a substitute in the above framework to make it robust and scalable. We assess the
effectiveness of our LFCestimate in an ALapplication on a prototypical low-dimensional
synthetic dataset, before taking on the challenging real-world task of reconstructing a
quantum chemical force field for a small organic molecule and demonstrating state-of-the-art
performance with a significantly reduced training demand.
1",TMLR
"Message passing graph neural networks (GNNs) are known to have their expressiveness
upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) algorithm. To achieve more
powerful GNNs, existing attempts either require ad hocfeatures, or involve operations that
incur high time and space complexities. In this work, we propose a generalandprovably
powerful GNN framework that preserves the scalability of the message passing scheme. In
particular, we first propose to empower 1-WL for graph isomorphism test by considering
edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown
to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN
framework as a differentiable neural version of NC-1-WL. Our simple implementation of
NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN
performs effectively and efficiently on various benchmarks.
1",TMLR
"Mathematical models of infectious diseases have long been used for studying the mechanisms
by which diseases spread, for predicting the spread of epidemics, and also for controlling
their outbreaks. These models are based on some assumptions and different assumptions
give rise to different models. Models on social networks of individuals which capture con-
tact patterns are usually more realistic and can more accurately model contagion dynamics.
Unfortunately, computing the output of realistic models is often hard. Thus, modeling the
evolution of contagion dynamics over large complex networks constitutes a challenging task.
In this paper, we present a computational approach to model the contagion dynamics under-
lying infectious diseases. Specifically, we focus on the susceptible-infectious-recovered (SIR)
epidemic model on networks. Given that this model can be expressed by an intractable
system of ordinary differential equations, we devise a simpler system that approximates the
output of the model. Then, we capitalize on recent advances in neural ordinary differential
equations and propose a neural architecture that can effectively predict the course of an
epidemic on the network. We apply the proposed architecture on several network datasets
and compare it against state-of-the-art methods under different experimental settings. Our
results indicate that the proposed method improves predictions in various spreading scenar-
ios, paving the way for the extensive application of interpretable neural networks in the field
of epidemic spreading. At the same time, the proposed model is highly efficient even when
trained on very large networks where traditional algorithms become significantly slower.
1",TMLR
"We study the task of gesture recognition from electromyography (EMG), with the goal of
enabling expressive human-computer interaction at high accuracy, while minimizing the
time required for new subjects to provide calibration data. To fulfill these goals, we define
combination gestures consisting of a direction component and a modifier component. New
subjects only demonstrate the single component gestures and we seek to extrapolate from
these to all possible single or combination gestures. We extrapolate to unseen combination
gestures by combining the feature vectors of real single gestures to produce synthetic training
data. This strategy allows us to provide a large and flexible gesture vocabulary, while not
requiring new subjects to demonstrate combinatorially many example gestures. We pre-train
an encoder and a combination operator using self-supervision, so that we can produce useful
synthetic training data for unseen test subjects. To evaluate the proposed method, we collect
and release a real-world EMG dataset, and measure the effect of augmented supervision
against two baselines: a partially-supervised model trained with only single gesture data
from the unseen subject, and a fully-supervised model trained with real single and real
combination gesture data from the unseen subject. We find that the proposed method
provides a dramatic improvement over the partially-supervised model, and achieves a useful
classification accuracy that in some cases approaches the performance of the fully-supervised
model.
1",TMLR
"Machine Learning models have shown susceptibility to various privacy attacks, with model
inversion (MI) attacks posing a significant threat. Current defense techniques are mostly
model-centric , involving modifying model training or inference. However, these approaches
require model trainersâ€™ cooperation, are computationally expensive, and often result in a
significant privacy-utility tradeoff. To address these limitations, we propose a novel data-
centricapproach to mitigate MI attacks. Compared to traditional model-centric techniques,
our approach offers the unique advantage of enabling each individual user to control their
dataâ€™s privacy risk, aligning with findings from a Cisco survey that only a minority actively
seek privacy protection. Specifically, we introduce several privacy-focused data augmenta-
tions that modify the private data uploaded to the model trainer. These augmentations
shape the resulting modelâ€™s loss landscape, making it challenging for attackers to generate
private target samples. Additionally, we provide theoretical analysis to explain why such
augmentations can reduce the risk of model inversion. We evaluate our approach against
state-of-the-art MI attacks and demonstrate its effectiveness and robustness across various
model architectures and datasets. Specifically, in standard face recognition benchmarks,
we reduce face reconstruction success rates to â‰¤5%, while maintaining high utility with
onlya2%classificationaccuracydrop, significantlysurpassingstate-of-the-artmodel-centric
defenses. This is the first study to propose a data-centric approach for mitigating model
inversion attacks, showing promising potential for decentralized privacy protection. Our
code is available at https://github.com/SCccc21/DCD.git .
1",TMLR
"Large-scale nonconvex optimization problems are ubiquitous in modern machine learning, and
among practitioners interested in solving them, Stochastic Gradient Descent (SGD) reigns
supreme. We revisit the analysis of SGD in the nonconvex setting and propose a new variant
of the recently introduced expected smoothness assumption which governs the behavior of
the second moment of the stochastic gradient. We show that our assumption is both more
general and more reasonable than assumptions made in all prior work. Moreover, our results
yield the optimal O(Îµâˆ’4)rate for finding a stationary point of nonconvex smooth functions,
and recover the optimal O(Îµâˆ’1)rate for finding a global solution if the Polyak-Åojasiewicz
condition is satisfied. We compare against convergence rates under convexity and prove
a theorem on the convergence of SGD under Quadratic Functional Growth and convexity,
which might be of independent interest. Moreover, we perform our analysis in a framework
which allows for a detailed study of the effects of a wide array of sampling strategies and
minibatch sizes for finite-sum optimization problems. We corroborate our theoretical results
with experiments on real and synthetic data.
1",TMLR
"Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, where
attackers can inject hidden backdoors during the training stage. This poses a serious threat
to the Model-as-a-Service setting, where downstream users directly utilize third-party models
(e.g., HuggingFace Hub, ChatGPT). To this end, we study inference-stage black-box backdoor
detection problem in the paper, where defenders aim to build a firewall to filter out the
backdoor inputs in the inference stage, with only input samples and prediction labels available.
Existing investigations on this problem either rely on strong assumptions on types of triggers
and attacks or suffer from poor efficiency. To build a more generalized and efficient method,
we first provide a novel causality-based lens to analyze heterogeneous prediction behaviors
for clean and backdoored samples in the inference stage, considering both sample-specific and
sample-agnostic backdoor attacks. Motivated by the causal analysis and do-calculus in causal
inference, we introduce Black-boxBackdoor detection under the CausalityLens (BBCaL)
which distinguishes backdoor and clean samples by analyzing prediction consistency after
progressively constructing counterfactual samples. Theoretical analysis also sheds light on
the effectiveness of the BBCaL. Extensive experiments on three benchmark datasets validate
the effectiveness and efficiency of our method.
1",TMLR
"In supervised learning, it is quite frequent to be confronted with real imbalanced datasets.
This situation leads to a learning difficulty for standard algorithms. Research and solutions
in imbalanced learning have mainly focused on classification tasks. Despite its importance,
very few solutions exist for imbalanced regression. In this paper, we propose a data augmen-
tation procedure, the GOLIATH algorithm, based on kernel density estimates and especially
dedicated to the problem of imbalanced data. This general approach encompasses two large
families of synthetic oversampling: those based on perturbations, such as Gaussian Noise,
and those based on interpolations, such as SMOTE. It provides an explicit form common to
such machine learning algorithms. and new synthetic data generators can be deduced. We
apply GOLIATH in imbalanced regression combining such generator procedures with a new
wild-bootstrap resampling technique for the target values. We evaluate the performance of
the GOLIATH algorithm in imbalanced regression where we compare our approach with
state-of-the-art techniques.
1",TMLR
"While dropout is known to be a successful regularization technique, insights into the mecha-
nisms that lead to this success are still lacking. We introduce the concept of weight expansion ,
an increase in the signed volume of a parallelotope spanned by the column or row vectors
of the weight covariance matrix, and show that weight expansion is an effective means of
increasing the generalization in a PAC-Bayesian setting. We provide a theoretical argument
that dropout leads to weight expansion and extensive empirical support for the correlation
between dropout and weight expansion. To support our hypothesis that weight expansion can
be regarded as an indicator of the enhanced generalization capability endowed by dropout,
and not just as a mere by-product, we have studied other methods that achieve weight
expansion (resp. contraction), and found that they generally lead to an increased (resp.
decreased) generalization ability. This suggests that dropout is an attractive regularizer,
because it is a computationally cheap method for obtaining weight expansion. This insight
justifies the role of dropout as a regularizer, while paving the way for identifying regularizers
that promise improved generalization through weight expansion.
1",TMLR
"In this paper, we empirically analyze a simple, non-learnable, and nonparametric Nadaraya-
Watson (NW) prediction head that can be used with any neural network architecture. In the
NW head, the prediction is a weighted average of labels from a support set. The weights are
computed from distances between the query feature and support features. This is in contrast
to the dominant approach of using a learnable classification head (e.g., a fully-connected
layer) on the features, which can be challenging to interpret and can yield poorly calibrated
predictions. Our empirical results on an array of computer vision tasks demonstrate that
the NW head can yield better calibration with comparable accuracy compared to its para-
metric counterpart, particularly in data-limited settings. To further increase inference-time
efficiency, we propose a simple approach that involves a clustering step run on the training
set to create a relatively small distilled support set. Furthermore, we explore two means of
interpretability/explainability that fall naturally from the NW head. The first is the label
weights, and the second is our novel concept of the â€œsupport influence function,â€ which is an
easy-to-compute metric that quantifies the influence of a support element on the prediction
for a given query. As we demonstrate in our experiments, the influence function can allow
the user to debug a trained model. We believe that the NW head is a flexible, interpretable,
and highly useful building block that can be used in a range of applications.
1",TMLR
"For long-tailed recognition problem, beyond imbalanced label distribution, unreliable em-
pirical data distribution due to instance scarcity has recently emerged as a concern. It
inevitably causes Class-Conditional Distribution (CCD) shift between training and test.
Data augmentation and head-to-tail information transfer methods indirectly alleviate the
problem by synthesizing novel examples but may remain biased. In this paper, we conduct a
thorough study on the impact of CCD shift and propose Distributionally Robust Augmenta-
tion (DRA) to directly train models robust to the shift. DRA admits a novel generalization
bound reflecting the benefit of distributional robustness to CCD shift for long-tailed recog-
nition. Extensive experiments show DRA greatly improves existing re-balancing and data
augmentation methods when cooperating with them. It also alleviates the recently discov-
ered saddle-point issue, verifying its ability to achieve enhanced robustness.
1",TMLR
"Spiking Neural Networks (SNNs) are recognized as the candidate for the next-generation
neuralnetworksduetotheirbio-plausibilityandenergyefficiency. Recently, researchershave
demonstrated that SNNs are able to achieve nearly state-of-the-art performance in image
recognition tasks using surrogate gradient training. However, some essential questions exist
pertaining to SNNs that are little studied: Do SNNs trained with surrogate gradient learn
different representations from traditional Artificial Neural Networks (ANNs)? Does the time
dimension in SNNs provide unique representation power? In this paper, we aim to answer
these questions by conducting a representation similarity analysis between SNNs and ANNs
using Centered Kernel Alignment (CKA). We start by analyzing the spatial dimension of
the networks, including both the width and the depth. Furthermore, our analysis of residual
connections shows that SNNs learn a periodic pattern, which rectifies the representations
in SNNs to be ANN-like. We additionally investigate the effect of the time dimension on
SNN representation, finding that deeper layers encourage more dynamics along the time
dimension. We also investigate the impact of input data such as event-stream data and
adversarial attacks. Our work uncovers a host of new findings of representations in SNNs.
We hope this work will inspire future research to fully comprehend the representation power
of SNNs. Code is released at https://github.com/Intelligent-Computing-Lab-Yale/
SNNCKA.
1",TMLR
"Correlation clustering is a well-known unsupervised learning setting that deals with positive
and negative pairwise similarities. In this paper, we study the case where the pairwise sim-
ilarities are not given in advance and must be queried in a cost-efficient way. Thereby, we
develop a generic active learning framework for this task that benefits from several advan-
tages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation
to any correlation clustering algorithm and query strategy, and robustness to noise. In ad-
dition, we propose and analyze a number of novel query strategies suited to this setting.
We demonstrate the effectiveness of our framework and the proposed query strategies via
several experimental studies.
1",TMLR
"We study a class of Gaussian processes for which the posterior mean, for a particular choice
of data, replicates a truncated Taylor expansion of any order. The data consist of derivative
evaluations at the expansion point and the prior covariance kernel belongs to the class
of Taylor kernels, which can be written in a certain power series form. We discuss and
prove some results on maximum likelihood estimation of parameters of Taylor kernels. The
proposed framework is a special case of Gaussian process regression based on data that is
orthogonal in the reproducing kernel Hilbert space of the covariance kernel.
1",TMLR
"Capsule Networks have emerged as a powerful class of deep learning architectures, known
for robust performance with relatively few parameters compared to Convolutional Neural
Networks (CNNs). However, their inherent efficiency is often overshadowed by their slow,
iterative routing mechanisms which establish connections between Capsule layers, posing
computational challenges resulting in an inability to scale. In this paper, we introduce a
novel, non-iterative routing mechanism, inspired by trainable prototype clustering. This
innovative approach aims to mitigate computational complexity, while retaining, if not en-
hancing, performanceefficacy. Furthermore, weharnessasharedCapsulesubspace, negating
the need to project each lower-level Capsule to each higher-level Capsule, thereby signifi-
cantly reducing memory requisites during training. Our approach demonstrates superior
results compared to the current best non-iterative Capsule Network and tests on the Im-
agewoof dataset, which is too computationally demanding to handle efficiently by iterative
approaches. Our findings underscore the potential of our proposed methodology in en-
hancing the operational efficiency and performance of Capsule Networks, paving the way
for their application in increasingly complex computational scenarios. Code is available at
https://github.com/mileseverett/ProtoCaps .
1",TMLR
"We introduce wrapped Î²-Gaussians, a family of wrapped distributions on tractable Rie-
mannian manifolds, supporting efficient reparametrized sampling, as well as exact density
assessment, effortlessly supporting high dimensions and anisotropic scale parameters. We
extend Fenchel-Young losses for geometry-aware learning with wrapped Î²-Gaussians, and
demonstrate the efficacy of our proposed family in a suite of experiments on hypersphere and
rotation manifolds: data fitting, hierarchy encoding, generative modeling with variational
autoencoders, and multilingual word embedding alignment.
The source code is available at https://github.com/ltl-uva/wbg .
1",TMLR
"In this work, we consider a distributed multi-agent stochastic optimization problem, where
each agent holds a local objective function that is smooth and strongly convex and that is
subject to a stochastic process. The goal is for all agents to collaborate to ï¬nd a common
solution that optimizes the sum of these local functions. With the practical assumption
that agents can only obtain noisy numerical function queries at precisely one point at a
time, we consider an extention of a standard consensus-based distributed stochastic gradient
(DSG) method to the bandit setting where we do not have access to the gradient, and we
introduce a zero-order (ZO) one-point estimate (1P-DSG). We analyze the convergence of
this techniques using stochastic approximation tools, and we prove that it converges almost
surely to the optimum despite the biasedness of our gradient estimate. We then study the
convergence rate of our method. With constant step sizes, our method competes with its
ï¬rst-order (FO) counterparts by achieving a linear rate O(/rho1k)as a function of number of
iterationsk. To the best of our knowledge, this is the ï¬rst work that proves this rate in
the noisy estimation setting or with one-point estimators. With vanishing step sizes, we
establish a rate of O(1âˆš
k)after a suï¬ƒcient number of iterations k>K 0. This rate matches
the lower bound of centralized techniques utilizing one-point estimators. We then provide
a regret bound of O(âˆš
k)with vanishing step sizes. We further illustrate the usefulness of
the proposed technique using numerical experiments.
1",TMLR
"Automated video summarization is a vision task that aims to generate concise summaries of
lengthy videos. Recent advancements in deep learning have led to highly performant video
summarization models; however, there has been a lack of attention given to fairness and
unbiased representation in the generated summaries. To bridge this gap, we introduce and
analytically define the fair video summarization problem, and demonstrate its connections
to the well-established problem of fair clustering. To facilitate fair model development, we
also introduce the FairVidSum dataset, which is similar in design to state-of-the-art video
summarization datasets such as TVSumandSumMe, but also includes annotations for sen-
sitive attributes and individuals alongside frame importance scores. Finally, we propose the
SumBal metric for quantifying the fairness of an outputted video summary. We conduct
extensive experiments to benchmark the fairness of various state-of-the-art video summa-
rization models. Our results highlight the need for better models that balance accuracy and
fairnesstoensureequitablerepresentationandinclusioninsummaries. Forcompleteness, we
also provide a novel fair-only baseline, FVS-LP, to showcase the fairness-utility gap models
can improve upon.
1",TMLR
"Semi-Supervised Learning (SSL) aims to learn a model using a tiny labeled set and massive
amounts of unlabeled data. To better exploit the unlabeled data the latest SSL methods
use pseudo-labels predicted from a single discriminative classifier . However, the generated
pseudo-labels are inevitably linked to inherent confirmation bias and noise which greatly
affects the model performance. In this work we introduce a new framework for SSL named
NorMatch. Firstly, we introduce a new uncertainty estimation scheme based on normaliz-
ing flows, as an auxiliary classifier, to enforce highly certain pseudo-labels yielding a boost
of the discriminative classifiers. Secondly, we introduce a threshold-free sample weighting
strategy to exploit better both high and low confidence pseudo-labels. Furthermore, we
utilize normalizing flows to model, in an unsupervised fashion, the distribution of unlabeled
data. This modelling assumption can further improve the performance of generative classi-
fiers via unlabeled data, and thus, implicitly contributing to training a better discriminative
classifier. We demonstrate, through numerical and visual results, that NorMatch achieves
state-of-the-art performance on several datasets.
1",TMLR
"Multiple sclerosis is a disease that aï¬€ects the brain and spinal cord, it can lead to severe
disabilityandhasnoknowncure. Themajorityofpriorworkinmachinelearningformultiple
sclerosis has been centered around using Magnetic Resonance Imaging scans or laboratory
tests; these modalities are both expensive to acquire and can be unreliable. In a recent
paper it was shown that disease progression can be predicted eï¬€ectively using performance
outcome measures and demographic data. In our work we build on this to investigate the
modeling side, using continuous time models to predict progression. We benchmark four
continuous time models using a publicly available multiple sclerosis dataset. We ï¬nd that
the best continuous model is often able to outperform the best benchmarked discrete time
model. We also carry out an extensive ablation to discover the sources of performance
gains, we ï¬nd that standardizing existing features leads to a larger performance increase
than interpolating missing features.
An associated video presentation is available at: https://www.youtube.com/watch?v=
sqDLDkbP2H0
1",TMLR
"Model sparsity is a promising approach to reducing parameters and FLOPs of convolutional
neural networks (CNNs). Compared to unstructured or coarse-grained structured sparsity,
ï¬ne-grained structured sparsity, e.g., N:M sparse pattern, can achieve better balance be-
tween accuracy and eï¬ƒciency on general computing platforms like CPUs and GPUs. In
particular, the 2:4 sparsity can accelerate CNN inference by 2 Ã—speed and with negligi-
ble accuracy drop. However, N:M sparsity needs to be supported by GPU within spe-
ciï¬c hardware circuits and hardly achieve signiï¬cant speedups on common GPUs. To ac-
celerate CNNs with general-purposed computing resources and simultaneously retain the
model accuracy as much as possible, this paper proposes complementary sparsity (CS).
CS denotes that only one weight can be retained for weights spaced at the same distance.
On the one hand, CS features high mask ï¬‚exibility, which is naturally favorable to high
model accuracy. Moreover, we propose a CS-speciï¬c sparse training method to improve
CS-based CNNsâ€™ accuracy under high parameter sparsities ( >75%). On the other hand,
CS itself is memory-access balanced and robust to pattern hyperparameters, making it an
ideal candidate for speeding up CS-based convolution computation on CPUs and common
GPUs. We thus propose a CS convolution parallel computing algorithm that adapts to
common GPUs without sparse tensor cores. Experimental results show that compared to
other sparsity patterns, the proposed CS achieves the optimal trade-oï¬€ in terms of accu-
âˆ—Equal contribution.
â€ Corresponding author.
1 Published in Transactions on Machine Learning Research (10/2023)
racy and latency for CPUs and common GPUs, respectively. Codes will be available at
https://gitee.com/mindspore/models/tree/master/research/cv/CS.
1",TMLR
"In the era of big data, many big organizations are integrating machine learning into their
work pipelines to facilitate data analysis. However, the performance of their trained models
is often restricted by limited and imbalanced data available to them. In this work, we
develop an assisted learning framework for assisting organizations to improve their learning
performance. The organizations have sufficient computation resources but are subject to
stringent data-sharing and collaboration policies. Their limited imbalanced data often cause
biased inference and sub-optimal decision-making. In assisted learning, an organizational
learner purchases assistance service from an external service provider and aims to enhance
its model performance within only a few assistance rounds. We develop effective stochastic
training algorithms for both assisted deep learning and assisted reinforcement learning.
Different from existing distributed algorithms that need to transmit gradients or models
frequently, our framework allows the learner to only occasionally share information with the
service provider, but still, obtain a model that achieves near-oracle performance as if all the
data were centralized.
1",TMLR
"Deep generative models have become more popular in recent years due to their scalabil-
ity and representation capacity. Unlike probabilistic graphical models, they typically do
not incorporate speciï¬c domain knowledge. As such, this work explores incorporating ar-
bitrarydependency structures, as speciï¬ed by Bayesian networks, into variational autoen-
coders (VAEs). This is achieved by developing a new type of graphical normalizing ï¬‚ow,
which extends residual ï¬‚ows by encoding conditional independence through masking of the
ï¬‚owâ€™s residual block weight matrices, and using these to extend both the prior and inference
network of the VAE. We show that the proposed graphical VAE provides a more inter-
pretable model that generalizes better in data-sparse settings, when practitioners know or
can hypothesize about certain latent factors in their domain. Furthermore, we show that
graphical residual ï¬‚ows provide not only density estimation and inference performance com-
petitive with existing graphical ï¬‚ows, but also more stable and accurate inversion in practice
as a byproduct of the ï¬‚owâ€™s Lipschitz bounds.
1",TMLR
"In decentralized multi-agent reinforcement learning, agents learning in isolation can lead to
relative over-generalization (RO), where optimal joint actions are undervalued in favor of
suboptimal ones. This hinders effective coordination in cooperative tasks, as agents tend
to choose actions that are individually rational but collectively suboptimal. To address
this issue, we introduce MaxMax Q-Learning (MMQ), which employs an iterative process
of sampling and evaluating potential next states, selecting those with maximal Q-values
for learning. This approach refines approximations of ideal state transitions, aligning more
closely with the optimal joint policy of collaborating agents. We provide theoretical analysis
supporting MMQâ€™s potential and present empirical evaluations across various environments
susceptible to RO. Our results demonstrate that MMQ frequently outperforms existing
baselines, exhibiting enhanced convergence and sample efficiency.
1",TMLR
"We describe a tensor-network latent-space encoding approach for increasing the scalabil-
ity of behavioral cloning of a video game playerâ€™s actions entirely from video streams of
the gameplay. Specifically, we address challenges associated with the high computational
requirements of traditional deep-learning based encoders such as convolutional variational
autoencodersthatprohibittheiruseinwidelyavailablehardwareorforlargescaledata. Our
approach uses tensor networks instead of deep variational autoencoders for this purpose, and
it yields significant speedups with no loss of accuracy. Empirical results on ATARI games
demonstrate that our approach leads to a speedup in the time it takes to encode data and
train a predictor using the encodings (between 2.6Ã—to9.6Ã—compared to autoencoders or
variational autoencoders). Furthermore, the tensor train encoding can be efficiently trained
onCPUtoachievecomparableorbettertrainingtimesthantheautoencoderand variational
autoencoder trained on GPU ( 0.9Ã—to5.4Ã—faster). These results suggest significant possi-
bilities in mitigating the need for cost and time-intensive hardware for training deep-learning
architectures for behavioral cloning.
1",TMLR
"We investigate the fundamental optimization question of minimizing a targetfunctionf(x),
whose gradients are expensive to compute or have limited availability, given access to some
auxiliary side function h(x)whose gradients are cheap or more available. This formulation
captures many settings of practical relevance, such as i) re-using batches in SGD, ii) transfer
learning, iii) federated learning, iv) training with compressed models/dropout, Et cetera. We
propose two generic new algorithms that apply in all these settings; we also prove that we
can benefit from this framework under the Hessian similarity assumption between the target
and side information. A benefit is obtained when this similarity measure is small; we also
show a potential benefit from stochasticity when the auxiliary noise is correlated with that
of the target function.
1",TMLR
"The emergence of Deep Neural Networks (DNNs) has revolutionized various domains by
enabling the resolution of complex tasks spanning image recognition, natural language pro-
cessing, and scientific problem-solving. However, this progress has also brought to light
a concerning vulnerability: adversarial examples. These crafted inputs, imperceptible to
humans, can manipulate machine learning models into making erroneous predictions, rais-
ing concerns for safety-critical applications. An intriguing property of this phenomenon
is the transferability of adversarial examples, where perturbations crafted for one model
can deceive another, often with a different architecture. This intriguing property enables
â€œblack-boxâ€ attacks which circumvents the need for detailed knowledge of the target model.
This survey explores the landscape of the adversarial transferability of adversarial examples.
We categorize existing methodologies to enhance adversarial transferability and discuss the
fundamental principles guiding each approach. While the predominant body of research
primarily concentrates on image classification, we also extend our discussion to encompass
other vision tasks and beyond. Challenges and opportunities are discussed, highlighting the
importance of fortifying DNNs against adversarial vulnerabilities in an evolving landscape.
1",TMLR
"Clustering algorithms play a pivotal role in various societal applications, where fairness is
paramount to prevent adverse impacts on individuals. In this study, we revisit the robust-
ness of fair clustering algorithms against adversarial attacks, affirming previous research
findings that highlighted their susceptibility and the resilience of the Consensus Fair Clus-
tering(CFC)model. Beyondreproducingthesecriticalresults, ourworkextendstheoriginal
analysis by refining the codebase for enhanced experimentation, introducing additional met-
rics and datasets to deepen the evaluation of fairness and clustering quality, exploring novel
attack strategies, including targeted attacks on new metrics and a combined approach for
balance and entropy as well as an ablation study. These contributions validate the origi-
nal claims about the vulnerability and resilience of fair clustering algorithms and broaden
the research landscape by offering a more comprehensive toolkit for assessing adversarial
robustness in fair clustering.
1",TMLR
"Multimodal neural networks often fail to utilize all modalities. They subsequently gener-
alize worse than their unimodal counterparts, or make predictions that only depend on a
subset of modalities. We refer to this problem as modality underutilization . Existing work
has addressed this issue by ensuring that there are no systematic biases in dataset creation,
or that our neural network architectures and optimization algorithms are capable of learn-
ing modality interactions. We demonstrate that even when these favorable conditions are
met, modality underutilization can still occur in the small data regime. To explain this
phenomenon, we put forth a concept that we call incidental correlation . It is a spurious
correlation that emerges in small datasets, despite not being a part of the underlying data
generating process (DGP). We develop our argument using a DGP under which multimodal
neural networks must utilize all modalities, since all paths between the inputs and target
are causal. This represents an idealized scenario that often fails to materialize. Instead,
due to incidental correlation, small datasets sampled from this DGP have higher likelihood
under an alternative DGP with spurious paths between the inputs and target. Multimodal
neural networks that use these spurious paths for prediction fail to utilize all modalities.
Given its harmful effects, we propose to detect incidental correlation via latent variable
modeling. We specify an identifiable variational autoencoder such that the latent posterior
encodes the spurious correlations between the inputs and target. This allows us to interpret
the Kullback-Leibler divergence between the latent posterior and prior as the severity of
incidental correlation. We use an ablation study to show that identifiability is important in
this context, since we derive our conclusions from the latent posterior. Using experiments
with synthetic data, as well as with VQA v2.0 and NLVR2, we demonstrate that incidental
correlation emerges in the small data regime, and leads to modality underutilization. Prac-
titioners of multimodal learning can use our method to detect whether incidental correlation
is present in their datasets, and determine whether they should collect additional data.
1 Published in Transactions on Machine Learning Research (08/2023)
1",TMLR
"Envision an intelligent agent capable of assisting users in conducting forecasting tasks through
intuitive, natural conversations without requiring in-depth knowledge of the underlying machine
learning (ML) processes. A significant challenge for the agent in this endeavor is to accurately
comprehend the userâ€™s prediction goals and, consequently, formulate precise ML tasks. In this paper,
we take a pioneering step towards this ambitious goal by introducing a new concept called Forecast
Utterance and then focus on the automatic and accurate interpretation of usersâ€™ prediction goals from
these utterances. Specifically, we frame the task as a slot-filling problem, where each slot corresponds
to a specific aspect of the goal prediction task. We then employ two methods based on self-supervision
with synthetic examples for solving the slot-filling task, namely: 1) Entity Extraction (EE), and 2)
Question-Answering (QA) techniques. Our experiments, evaluated with three meticulously crafted
data sets, validate the viability of our ambitious goal and demonstrate the effectiveness of both EE
and QA techniques in interpreting Forecast Utterances .
1",TMLR
"Microaggregation is a method to coarsen a data set, by optimally clustering data points in
groups of at least kpoints, thereby providing a k-anonymity type disclosure guarantee for
each point in the data set. Previous algorithms for univariate microaggregation had a O(kn)
time complexity. By rephrasing microaggregation as an instance of the concave least weight
subsequence problem, in this work we provide improved algorithms that provide an optimal
univariate microaggregation on sorted data in O(n)time and space. We further show that
our algorithms work not only for sum of squares cost functions, as typically considered, but
seamlessly extend to many other cost functions used for univariate microaggregation tasks.
In experiments we show that the presented algorithms lead to performance improvements
on real hardware.
1",TMLR
"This paper investigates the relationship between graph convolution and Mixup techniques.
Graph convolution in a graph neural network involves aggregating features from neighboring
samples to learn representative features for a specific node or sample. On the other hand,
Mixup is a data augmentation technique that generates new examples by averaging features
and one-hot labels from multiple samples. One commonality between these techniques is
their utilization of information from multiple samples to derive feature representation. This
study aims to explore whether a connection exists between the two. Our investigation reveals
that, under two mild modifications, graph convolution can be viewed as a specialized form of
Mixup that is applied during both the training and testing phases. The two modifications
are 1)Homophily Relabel - assigning the target nodeâ€™s label to all its neighbors, and 2)
Test-Time Mixup - Mixup the feature during the test time. We establish this equivalence
mathematically by demonstrating that graph convolution networks and simplified graph
convolution can be expressed as a form of Mixup. We also empirically verify the equivalence
by training an MLP using the two modifications to achieve comparable performance.
1",TMLR
"The practice of applying several local updates before aggregation across clients has been
empirically shown to be a successful approach to overcoming the communication bottleneck
in Federated Learning (FL). Such methods are usually implemented by having clients
perform one or more epochs of local training per round, while randomly reshuï¬„ing their
ï¬nite dataset in each epoch. Data imbalance, where clients have diï¬€erent numbers of local
training samples, is ubiquitous in FL applications, resulting in diï¬€erent clients performing
diï¬€erent numbers of local updates in each round. In this work, we propose a general
recipe, FedShuffle , that better utilizes the local updates in FL, especially in this regime
encompassing random reshuï¬„ing and heterogeneity. FedShuffle is the ï¬rst local update
method with theoretical convergence guarantees that incorporates random reshuï¬„ing, data
imbalance, and client sampling â€” features that are essential in large-scale cross-device
FL. We present a comprehensive theoretical analysis of FedShuffle and show, both
theoretically and empirically, that it does not suï¬€er from the objective function mismatch
that is present in FL methods that assume homogeneous updates in heterogeneous FL
setups, such as FedAvg (McMahan et al., 2017). In addition, by combining the ingredients
above, FedShuffle improves upon FedNova (Wang et al., 2020), which was previously
proposed to solve this mismatch. Similar to Mime(Karimireddy et al., 2020), we show that
FedShuffle with momentum variance reduction (Cutkosky & Orabona, 2019) improves
upon non-local methods under a Hessian similarity assumption.
1",TMLR
"Graph neural networks (GNNs) have become compelling models designed to perform learning
and inference on graph-structured data. However, little work has been done to understand
the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-
distribution (OOD) inputs. In this paper, we use a random graph generator to systematically
investigate how the graph size and structural properties affect the predictive performance
of GNNs. We present specific evidence that the average node degree is a key feature in
determining whether GNNs can generalize to unseen graphs, and that the use of multiple
node update functions can improve the generalization performance of GNNs when dealing
with graphs of multimodal degree distributions. Accordingly, we propose a multi-module
GNN framework that allows the network to adapt flexibly to new graphs by generalizing a
single canonical nonlinear transformation over aggregated inputs. Our results show that the
multi-module GNNs improve the OOD generalization on a variety of inference tasks in the
direction of diverse structural features.
1",TMLR
"We present a general-purpose framework for image modelling and vision tasks based on
probabilistic frame prediction. Our approach uniï¬es a broad range of tasks, from image
segmentation, to novel view synthesis and video interpolation. We pair this framework with
an architecture we term Transframer, which uses U-Net and Transformer components to
condition on annotated context frames, and outputs sequences of sparse, compressed image
features. Transframer is the state-of-the-art on a variety of video generation benchmarks, is
competitive with the strongest models on few-shot view synthesis, and can generate coherent
30 second videos from a single image without any explicit geometric information. A single
generalist Transframer simultaneously produces promising results on 8 tasks, including se-
mantic segmentation, image classiï¬cation and optical ï¬‚ow prediction with no task-speciï¬c
architectural components, demonstrating that multi-task computer vision can be tackled
using probabilistic image models. Our approach can in principle be applied to a wide range
of applications that require learning the conditional structure of annotated image-formatted
data.
1",TMLR
"Incomputervision, thevisiontransformer(ViT)hasincreasinglysupersededtheconvolutional
neural network (CNN) for improved accuracy and robustness. However, ViTâ€™s large model
sizes and high sample complexity make it difficult to train on resource-constrained edge
devices. Split learning (SL) emerges as a viable solution, leveraging server-side resources
to train ViTs while utilizing private data from distributed devices. However, SL requires
additional information exchange for weight updates between the device and the server,
which can be exposed to various attacks on private training data. To mitigate the risk of
data breaches in classification tasks, inspired from the CutMix regularization, we propose
a novel privacy-preserving SL framework that injects Gaussian noise into smashed data
and mixes randomly chosen patches of smashed data across clients, coined DP-CutMixSL .
Our analysis demonstrates that DP-CutMixSL is a differentially private (DP) mechanism
that strengthens privacy protection against membership inference attacks during forward
propagation. Through simulations, we show that DP-CutMixSL improves privacy protection
against membership inference attacks, reconstruction attacks, and label inference attacks,
while also improving accuracy compared to DP-SL and DP-MixSL.
âˆ—Corresponding author
1 Published in Transactions on Machine Learning Research (08/2024)
Figure 1: Schematic illustration of DP-CutMixSL with 2 clients.
1",TMLR
"The state-of-the-art dimensionality reduction approaches largely rely on complicated opti-
mization procedures. On the other hand, closed-form approaches requiring merely eigen-
decompositiondonothaveenoughsophisticationandnonlinearity. Inthispaper, wepropose
a novel nonlinear dimensionality reduction methodâ€”Inverse Kernel Decomposition (IKD)â€”
based on an eigen-decomposition of the sample covariance matrix of data. The method is
inspired by Gaussian process latent variable models (GPLVMs) and has comparable perfor-
mance with GPLVMs. To deal with very noisy data with weak correlations, we propose two
solutionsâ€”blockwise and geodesicâ€”to make use of locally correlated data points and pro-
vide better and numerically more stable latent estimations. We use synthetic datasets and
four real-world datasets to show that IKD is a better dimensionality reduction method than
other eigen-decomposition-based methods, and achieves comparable performance against
optimization-based methods with faster running speeds. Open-source IKD implementation
in Python can be accessed at https://github.com/JerrySoybean/ikd .
1",TMLR
"Continual learning is a machine learning approach where the challenge is that a constructed
learning model executes incoming tasks while maintaining its performance over the earlier
tasks. In order to address this issue, we devise a technique that combines two uniquely
important concepts in machine learning, namely ""replay buffer"" and ""meta learning"", aiming
to exploit the best of two worlds. In this method, the model weights are initially computed
by using the current task dataset. Next, the dataset of the current task is merged with the
stored samples from the earlier tasks and the model weights are updated using the combined
dataset. This aids in preventing the model weights converging to the optimal parameters
of the current task and enables the preservation of information from earlier tasks. We
choose to adapt our technique to graph data structure and the task of node classification on
graphs. We introduce MetaCLGraph, which outperforms the baseline methods over various
graph datasets including Citeseer, Corafull, Arxiv, and Reddit. This method illustrates the
potential of combining replay buffer and meta learning in the field of continual learning on
graphs.
1",TMLR
"We present an adaptive mechanism for hyperparameter selection in differentially private op-
timization that addresses the inherent trade-off between utility and privacy. The mechanism
eliminates the often unstructured and time-consuming manual effort of selecting hyperpa-
rameters and avoids the additional privacy costs that hyperparameter selection otherwise
incurs on top of that of the actual algorithm.
Weinstantiateourmechanismfornoisygradientdescentonnon-convex, convexandstrongly
convex loss functions, respectively, to derive schedules for the noise variance and step size.
These schedules account for the properties of the loss function and adapt to convergence
metrics such as the gradient norm. When using these schedules, we show that noisy gradient
descent converges at essentially the same rate as its noise-free counterpart. Numerical
experiments show that the schedules consistently perform well across a range of datasets
without manual tuning.
1",TMLR
"Large language models (LLMs) show remarkable capabilities across a variety of tasks. De-
spite the models only seeing text in training, several recent studies suggest that LLM repre-
sentations implicitly capture aspects of the underlying grounded concepts. Here, we explore
LLM representations of a particularly salient kind of grounded knowledge â€” spatial rela-
tionships. We design natural-language navigation tasks and evaluate the ability of LLMs,
in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason
about spatial structures. These tasks reveal substantial variability in LLM performance
across different spatial structures, including square, hexagonal, and triangular grids, rings,
and trees. In extensive error analysis, we find that LLMsâ€™ mistakes reflect both spatial and
non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of
spatial structure implicitly, but room for improvement remains.1
1",TMLR
