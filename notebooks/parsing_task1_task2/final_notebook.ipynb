{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIEJq4FGetLn",
        "outputId": "5fe31b36-2a94-4244-dd3a-5419fea2ef28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded layout model datalab-to/surya_layout on device cuda with dtype torch.float16\n",
            "Loaded texify model to cuda with torch.float16 dtype\n",
            "Loaded recognition model vikp/surya_rec2 on device cuda with dtype torch.float16\n",
            "Loaded table recognition model vikp/surya_tablerec on device cuda with dtype torch.float16\n",
            "Loaded detection model vikp/surya_det3 on device cuda with dtype torch.float16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Recognizing layout: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00, 31.73it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 163.98it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 112.09it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 124.71it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 3/3 [00:05<00:00,  1.72s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 104.92it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 107.70it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 106.77it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 98.58it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 127.37it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 113.01it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00, 129.90it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 126.20it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 8/8 [00:07<00:00,  1.13it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 90.63it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 88.46it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.26it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 121.46it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 145.71it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 94.73it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 110.84it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 133.50it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 84.61it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 39.47it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 107.08it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 116.87it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 139.19it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 133.55it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 91.13it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 118.06it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 115.79it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 96.60it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 92.42it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 124.76it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.26it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 138.71it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 92.54it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 123.09it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 114.74it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 136.02it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 114.67it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 135.94it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 73.55it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 119.58it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 137.53it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.94it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 111.97it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 99.20it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.49it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 124.63it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 115.18it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 89.06it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 150.86it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 141.90it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 97.09it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 112.76it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 89.85it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 74.65it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 151.49it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.55it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 122.35it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 93.80it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 151.12it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 95.57it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 144.84it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 153.56it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 120.90it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.06it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 144.92it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  2.76it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.14it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 145.72it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 91.14it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 132.55it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  4.59it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 125.72it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 86.97it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 62.58it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 110.69it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  2.42it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 130.51it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 117.61it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 62.19it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 103.51it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  2.02it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 142.47it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 116.79it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 102.15it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 111.14it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 138.01it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 112.65it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.67it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 98.30it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 119.90it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 99.77it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 109.48it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 108.12it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 88.61it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 115.33it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 152.32it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 76.09it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n",
            "100%|██████████| 2/2 [00:00<00:00, 80.48it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 107.78it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations:  40%|████      | 2/5 [00:01<00:02,  1.21it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (389 > 384). Running this sequence through the model will result in indexing errors\n",
            "Recognizing equations: 100%|██████████| 5/5 [00:07<00:00,  1.52s/it]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 110.88it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 104.67it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 80.60it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  2.25it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 79.45it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 153.82it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 107.99it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 103.52it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 149.34it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 95.55it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 111.67it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 151.54it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 96.67it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 124.43it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  3.19it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 102.74it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 99.03it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 142.17it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 97.28it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 136.78it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 100.13it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 97.64it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 96.47it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 114.25it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 115.31it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 69.76it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 83.51it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 100.45it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 133.43it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 119.49it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 100.26it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 133.96it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 81.19it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 96.71it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 107.35it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 137.20it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 116.97it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 84.43it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:00<00:00,  2.20it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 120.57it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 2/2 [00:00<00:00,  4.45it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 106.38it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 113.58it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
            "Recognizing layout: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 147.18it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 112.95it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 86.08it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 99.81it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n",
            "Recognizing layout: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 104.46it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 79.53it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 0it [00:00, ?it/s]\n",
            "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\n",
            "Recognizing layout: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 138.34it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Recognizing equations: 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]\n",
            "Recognizing tables: 0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "from marker.converters.pdf import PdfConverter\n",
        "from marker.models import create_model_dict\n",
        "from marker.output import text_from_rendered\n",
        "from PIL import Image\n",
        "\n",
        "# Define output file\n",
        "output_file = \"extracted_full_texts.txt\"\n",
        "\n",
        "# Load models once outside the loop\n",
        "model_dict = create_model_dict()  # Create the model dictionary only once\n",
        "converter = PdfConverter(artifact_dict=model_dict)  # Initialize the converter once\n",
        "\n",
        "# Open the output file in write mode\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
        "    for i in range(1, 136):\n",
        "        # Prepare file path\n",
        "        file_path = f\"input-pdfs/P{i:03}.pdf\"  # Format to match filenames R001.pdf to R015.pdf\n",
        "\n",
        "        # Process the PDF using the preloaded converter\n",
        "        rendered = converter(file_path)\n",
        "        text, _, images = text_from_rendered(rendered)\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "        # Save extracted text to output file\n",
        "        output.write(f\"{text.strip()}\\n\")  # Save text from the PDF as a single line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8ZW1i2FGwRB0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define the regex pattern for extracting the abstract\n",
        "pattern = r'(?<=# Abstract)(.*?)(?=# \\d)'\n",
        "\n",
        "# Open the input file and the output file\n",
        "with open('/content/extracted_full_texts.txt', 'r') as infile, open('extracted_full_abstracts.txt', 'w') as outfile:\n",
        "    for line in infile:\n",
        "        # Extract the abstract from the current line\n",
        "        match = re.search(pattern, line, re.DOTALL)\n",
        "        if match:\n",
        "            abstract = match.group(0).strip()\n",
        "        else:\n",
        "            abstract = \"Abstract not found\"  # Placeholder if no abstract is found\n",
        "\n",
        "        # Write the result to the output file\n",
        "        outfile.write(abstract + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4PPOoghwt0D"
      },
      "source": [
        "# Task - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iSck_4HOw3c1",
        "outputId": "a58a096f-515c-4f98-f432-b4ca8e9b6b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing paper 1 of 135\n",
            "Waiting 3.25 seconds to maintain rate limit...\n",
            "Processed paper 1\n",
            "\n",
            "Processing paper 2 of 135\n",
            "Waiting 4.67 seconds to maintain rate limit...\n",
            "Processed paper 2\n",
            "\n",
            "Processing paper 3 of 135\n",
            "Waiting 5.18 seconds to maintain rate limit...\n",
            "Processed paper 3\n",
            "\n",
            "Processing paper 4 of 135\n",
            "Waiting 4.67 seconds to maintain rate limit...\n",
            "Processed paper 4\n",
            "\n",
            "Processing paper 5 of 135\n",
            "Waiting 5.25 seconds to maintain rate limit...\n",
            "Processed paper 5\n",
            "\n",
            "Processing paper 6 of 135\n",
            "Waiting 5.10 seconds to maintain rate limit...\n",
            "Processed paper 6\n",
            "\n",
            "Processing paper 7 of 135\n",
            "Waiting 4.95 seconds to maintain rate limit...\n",
            "Processed paper 7\n",
            "\n",
            "Processing paper 8 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 8\n",
            "\n",
            "Processing paper 9 of 135\n",
            "Waiting 5.27 seconds to maintain rate limit...\n",
            "Processed paper 9\n",
            "\n",
            "Processing paper 10 of 135\n",
            "Waiting 4.95 seconds to maintain rate limit...\n",
            "Processed paper 10\n",
            "\n",
            "Processing paper 11 of 135\n",
            "Waiting 4.90 seconds to maintain rate limit...\n",
            "Processed paper 11\n",
            "\n",
            "Processing paper 12 of 135\n",
            "Waiting 5.08 seconds to maintain rate limit...\n",
            "Processed paper 12\n",
            "\n",
            "Processing paper 13 of 135\n",
            "Waiting 5.28 seconds to maintain rate limit...\n",
            "Processed paper 13\n",
            "\n",
            "Processing paper 14 of 135\n",
            "Waiting 5.28 seconds to maintain rate limit...\n",
            "Processed paper 14\n",
            "\n",
            "Processing paper 15 of 135\n",
            "Waiting 4.92 seconds to maintain rate limit...\n",
            "Processed paper 15\n",
            "\n",
            "Processing paper 16 of 135\n",
            "Waiting 5.00 seconds to maintain rate limit...\n",
            "Processed paper 16\n",
            "\n",
            "Processing paper 17 of 135\n",
            "Waiting 4.67 seconds to maintain rate limit...\n",
            "Processed paper 17\n",
            "\n",
            "Processing paper 18 of 135\n",
            "Waiting 5.31 seconds to maintain rate limit...\n",
            "Processed paper 18\n",
            "\n",
            "Processing paper 19 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 19\n",
            "\n",
            "Processing paper 20 of 135\n",
            "Waiting 5.00 seconds to maintain rate limit...\n",
            "Processed paper 20\n",
            "\n",
            "Processing paper 21 of 135\n",
            "Waiting 5.38 seconds to maintain rate limit...\n",
            "Processed paper 21\n",
            "\n",
            "Processing paper 22 of 135\n",
            "Waiting 4.74 seconds to maintain rate limit...\n",
            "Processed paper 22\n",
            "\n",
            "Processing paper 23 of 135\n",
            "Waiting 5.02 seconds to maintain rate limit...\n",
            "Processed paper 23\n",
            "\n",
            "Processing paper 24 of 135\n",
            "Waiting 4.32 seconds to maintain rate limit...\n",
            "Processed paper 24\n",
            "\n",
            "Processing paper 25 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 25\n",
            "\n",
            "Processing paper 26 of 135\n",
            "Waiting 5.25 seconds to maintain rate limit...\n",
            "Processed paper 26\n",
            "\n",
            "Processing paper 27 of 135\n",
            "Waiting 5.38 seconds to maintain rate limit...\n",
            "Processed paper 27\n",
            "\n",
            "Processing paper 28 of 135\n",
            "Waiting 5.22 seconds to maintain rate limit...\n",
            "Processed paper 28\n",
            "\n",
            "Processing paper 29 of 135\n",
            "Waiting 5.15 seconds to maintain rate limit...\n",
            "Processed paper 29\n",
            "\n",
            "Processing paper 30 of 135\n",
            "Waiting 5.33 seconds to maintain rate limit...\n",
            "Processed paper 30\n",
            "\n",
            "Processing paper 31 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 31\n",
            "\n",
            "Processing paper 32 of 135\n",
            "Waiting 4.97 seconds to maintain rate limit...\n",
            "Processed paper 32\n",
            "\n",
            "Processing paper 33 of 135\n",
            "Waiting 5.12 seconds to maintain rate limit...\n",
            "Processed paper 33\n",
            "\n",
            "Processing paper 34 of 135\n",
            "Waiting 5.17 seconds to maintain rate limit...\n",
            "Processed paper 34\n",
            "\n",
            "Processing paper 35 of 135\n",
            "Waiting 5.17 seconds to maintain rate limit...\n",
            "Processed paper 35\n",
            "\n",
            "Processing paper 36 of 135\n",
            "Waiting 4.64 seconds to maintain rate limit...\n",
            "Processed paper 36\n",
            "\n",
            "Processing paper 37 of 135\n",
            "Waiting 5.28 seconds to maintain rate limit...\n",
            "Processed paper 37\n",
            "\n",
            "Processing paper 38 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 38\n",
            "\n",
            "Processing paper 39 of 135\n",
            "Waiting 4.94 seconds to maintain rate limit...\n",
            "Processed paper 39\n",
            "\n",
            "Processing paper 40 of 135\n",
            "Waiting 5.38 seconds to maintain rate limit...\n",
            "Processed paper 40\n",
            "\n",
            "Processing paper 41 of 135\n",
            "Waiting 5.07 seconds to maintain rate limit...\n",
            "Processed paper 41\n",
            "\n",
            "Processing paper 42 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 42\n",
            "\n",
            "Processing paper 43 of 135\n",
            "Waiting 4.85 seconds to maintain rate limit...\n",
            "Processed paper 43\n",
            "\n",
            "Processing paper 44 of 135\n",
            "Waiting 5.12 seconds to maintain rate limit...\n",
            "Processed paper 44\n",
            "\n",
            "Processing paper 45 of 135\n",
            "Waiting 5.10 seconds to maintain rate limit...\n",
            "Processed paper 45\n",
            "\n",
            "Processing paper 46 of 135\n",
            "Waiting 5.02 seconds to maintain rate limit...\n",
            "Processed paper 46\n",
            "\n",
            "Processing paper 47 of 135\n",
            "Waiting 4.79 seconds to maintain rate limit...\n",
            "Processed paper 47\n",
            "\n",
            "Processing paper 48 of 135\n",
            "Waiting 4.62 seconds to maintain rate limit...\n",
            "Processed paper 48\n",
            "\n",
            "Processing paper 49 of 135\n",
            "Waiting 5.07 seconds to maintain rate limit...\n",
            "Processed paper 49\n",
            "\n",
            "Processing paper 50 of 135\n",
            "Waiting 4.49 seconds to maintain rate limit...\n",
            "Processed paper 50\n",
            "\n",
            "Processing paper 51 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 51\n",
            "\n",
            "Processing paper 52 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 52\n",
            "\n",
            "Processing paper 53 of 135\n",
            "Waiting 4.32 seconds to maintain rate limit...\n",
            "Processed paper 53\n",
            "\n",
            "Processing paper 54 of 135\n",
            "Waiting 5.08 seconds to maintain rate limit...\n",
            "Processed paper 54\n",
            "\n",
            "Processing paper 55 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 55\n",
            "\n",
            "Processing paper 56 of 135\n",
            "Waiting 4.87 seconds to maintain rate limit...\n",
            "Processed paper 56\n",
            "\n",
            "Processing paper 57 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 57\n",
            "\n",
            "Processing paper 58 of 135\n",
            "Waiting 5.10 seconds to maintain rate limit...\n",
            "Processed paper 58\n",
            "\n",
            "Processing paper 59 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 59\n",
            "\n",
            "Processing paper 60 of 135\n",
            "Waiting 5.15 seconds to maintain rate limit...\n",
            "Processed paper 60\n",
            "\n",
            "Processing paper 61 of 135\n",
            "Waiting 5.00 seconds to maintain rate limit...\n",
            "Processed paper 61\n",
            "\n",
            "Processing paper 62 of 135\n",
            "Waiting 5.02 seconds to maintain rate limit...\n",
            "Processed paper 62\n",
            "\n",
            "Processing paper 63 of 135\n",
            "Waiting 5.32 seconds to maintain rate limit...\n",
            "Processed paper 63\n",
            "\n",
            "Processing paper 64 of 135\n",
            "Waiting 5.25 seconds to maintain rate limit...\n",
            "Processed paper 64\n",
            "\n",
            "Processing paper 65 of 135\n",
            "Waiting 5.43 seconds to maintain rate limit...\n",
            "Processed paper 65\n",
            "\n",
            "Processing paper 66 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 66\n",
            "\n",
            "Processing paper 67 of 135\n",
            "Waiting 5.30 seconds to maintain rate limit...\n",
            "Processed paper 67\n",
            "\n",
            "Processing paper 68 of 135\n",
            "Waiting 5.25 seconds to maintain rate limit...\n",
            "Processed paper 68\n",
            "\n",
            "Processing paper 69 of 135\n",
            "Waiting 4.95 seconds to maintain rate limit...\n",
            "Processed paper 69\n",
            "\n",
            "Processing paper 70 of 135\n",
            "Waiting 5.07 seconds to maintain rate limit...\n",
            "Processed paper 70\n",
            "\n",
            "Processing paper 71 of 135\n",
            "Waiting 5.40 seconds to maintain rate limit...\n",
            "Processed paper 71\n",
            "\n",
            "Processing paper 72 of 135\n",
            "Waiting 5.43 seconds to maintain rate limit...\n",
            "Processed paper 72\n",
            "\n",
            "Processing paper 73 of 135\n",
            "Waiting 4.72 seconds to maintain rate limit...\n",
            "Processed paper 73\n",
            "\n",
            "Processing paper 74 of 135\n",
            "Waiting 5.30 seconds to maintain rate limit...\n",
            "Processed paper 74\n",
            "\n",
            "Processing paper 75 of 135\n",
            "Waiting 5.12 seconds to maintain rate limit...\n",
            "Processed paper 75\n",
            "\n",
            "Processing paper 76 of 135\n",
            "Waiting 5.00 seconds to maintain rate limit...\n",
            "Processed paper 76\n",
            "\n",
            "Processing paper 77 of 135\n",
            "Waiting 5.10 seconds to maintain rate limit...\n",
            "Processed paper 77\n",
            "\n",
            "Processing paper 78 of 135\n",
            "Waiting 5.15 seconds to maintain rate limit...\n",
            "Processed paper 78\n",
            "\n",
            "Processing paper 79 of 135\n",
            "Waiting 5.33 seconds to maintain rate limit...\n",
            "Processed paper 79\n",
            "\n",
            "Processing paper 80 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 80\n",
            "\n",
            "Processing paper 81 of 135\n",
            "Waiting 4.69 seconds to maintain rate limit...\n",
            "Processed paper 81\n",
            "\n",
            "Processing paper 82 of 135\n",
            "Waiting 5.02 seconds to maintain rate limit...\n",
            "Processed paper 82\n",
            "\n",
            "Processing paper 83 of 135\n",
            "Waiting 5.17 seconds to maintain rate limit...\n",
            "Processed paper 83\n",
            "\n",
            "Processing paper 84 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 84\n",
            "\n",
            "Processing paper 85 of 135\n",
            "Waiting 4.97 seconds to maintain rate limit...\n",
            "Processed paper 85\n",
            "\n",
            "Processing paper 86 of 135\n",
            "Waiting 4.54 seconds to maintain rate limit...\n",
            "Processed paper 86\n",
            "\n",
            "Processing paper 87 of 135\n",
            "Waiting 5.28 seconds to maintain rate limit...\n",
            "Processed paper 87\n",
            "\n",
            "Processing paper 88 of 135\n",
            "Waiting 4.51 seconds to maintain rate limit...\n",
            "Processed paper 88\n",
            "\n",
            "Processing paper 89 of 135\n",
            "Waiting 5.12 seconds to maintain rate limit...\n",
            "Processed paper 89\n",
            "\n",
            "Processing paper 90 of 135\n",
            "Waiting 5.10 seconds to maintain rate limit...\n",
            "Processed paper 90\n",
            "\n",
            "Processing paper 91 of 135\n",
            "Waiting 5.00 seconds to maintain rate limit...\n",
            "Processed paper 91\n",
            "\n",
            "Processing paper 92 of 135\n",
            "Waiting 4.90 seconds to maintain rate limit...\n",
            "Processed paper 92\n",
            "\n",
            "Processing paper 93 of 135\n",
            "Waiting 5.28 seconds to maintain rate limit...\n",
            "Processed paper 93\n",
            "\n",
            "Processing paper 94 of 135\n",
            "Waiting 4.72 seconds to maintain rate limit...\n",
            "Processed paper 94\n",
            "\n",
            "Processing paper 95 of 135\n",
            "Waiting 5.07 seconds to maintain rate limit...\n",
            "Processed paper 95\n",
            "\n",
            "Processing paper 96 of 135\n",
            "Waiting 4.69 seconds to maintain rate limit...\n",
            "Processed paper 96\n",
            "\n",
            "Processing paper 97 of 135\n",
            "Waiting 4.72 seconds to maintain rate limit...\n",
            "Processed paper 97\n",
            "\n",
            "Processing paper 98 of 135\n",
            "Waiting 4.77 seconds to maintain rate limit...\n",
            "Processed paper 98\n",
            "\n",
            "Processing paper 99 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 99\n",
            "\n",
            "Processing paper 100 of 135\n",
            "Waiting 4.84 seconds to maintain rate limit...\n",
            "Processed paper 100\n",
            "\n",
            "Processing paper 101 of 135\n",
            "Waiting 5.30 seconds to maintain rate limit...\n",
            "Processed paper 101\n",
            "\n",
            "Processing paper 102 of 135\n",
            "Waiting 5.07 seconds to maintain rate limit...\n",
            "Processed paper 102\n",
            "\n",
            "Processing paper 103 of 135\n",
            "Waiting 4.87 seconds to maintain rate limit...\n",
            "Processed paper 103\n",
            "\n",
            "Processing paper 104 of 135\n",
            "Waiting 5.07 seconds to maintain rate limit...\n",
            "Processed paper 104\n",
            "\n",
            "Processing paper 105 of 135\n",
            "Waiting 4.76 seconds to maintain rate limit...\n",
            "Processed paper 105\n",
            "\n",
            "Processing paper 106 of 135\n",
            "Waiting 5.12 seconds to maintain rate limit...\n",
            "Processed paper 106\n",
            "\n",
            "Processing paper 107 of 135\n",
            "Waiting 5.08 seconds to maintain rate limit...\n",
            "Processed paper 107\n",
            "\n",
            "Processing paper 108 of 135\n",
            "Waiting 5.38 seconds to maintain rate limit...\n",
            "Processed paper 108\n",
            "\n",
            "Processing paper 109 of 135\n",
            "Waiting 5.23 seconds to maintain rate limit...\n",
            "Processed paper 109\n",
            "\n",
            "Processing paper 110 of 135\n",
            "Waiting 5.33 seconds to maintain rate limit...\n",
            "Processed paper 110\n",
            "\n",
            "Processing paper 111 of 135\n",
            "Waiting 4.69 seconds to maintain rate limit...\n",
            "Processed paper 111\n",
            "\n",
            "Processing paper 112 of 135\n",
            "Waiting 5.28 seconds to maintain rate limit...\n",
            "Processed paper 112\n",
            "\n",
            "Processing paper 113 of 135\n",
            "Waiting 5.20 seconds to maintain rate limit...\n",
            "Processed paper 113\n",
            "\n",
            "Processing paper 114 of 135\n",
            "Waiting 5.25 seconds to maintain rate limit...\n",
            "Processed paper 114\n",
            "\n",
            "Processing paper 115 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 115\n",
            "\n",
            "Processing paper 116 of 135\n",
            "Waiting 4.95 seconds to maintain rate limit...\n",
            "Processed paper 116\n",
            "\n",
            "Processing paper 117 of 135\n",
            "Waiting 4.56 seconds to maintain rate limit...\n",
            "Processed paper 117\n",
            "\n",
            "Processing paper 118 of 135\n",
            "Waiting 4.62 seconds to maintain rate limit...\n",
            "Processed paper 118\n",
            "\n",
            "Processing paper 119 of 135\n",
            "Waiting 4.84 seconds to maintain rate limit...\n",
            "Processed paper 119\n",
            "\n",
            "Processing paper 120 of 135\n",
            "Waiting 5.30 seconds to maintain rate limit...\n",
            "Processed paper 120\n",
            "\n",
            "Processing paper 121 of 135\n",
            "Waiting 5.02 seconds to maintain rate limit...\n",
            "Processed paper 121\n",
            "\n",
            "Processing paper 122 of 135\n",
            "Waiting 4.97 seconds to maintain rate limit...\n",
            "Processed paper 122\n",
            "\n",
            "Processing paper 123 of 135\n",
            "Waiting 5.18 seconds to maintain rate limit...\n",
            "Processed paper 123\n",
            "\n",
            "Processing paper 124 of 135\n",
            "Waiting 4.67 seconds to maintain rate limit...\n",
            "Processed paper 124\n",
            "\n",
            "Processing paper 125 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 125\n",
            "\n",
            "Processing paper 126 of 135\n",
            "Waiting 4.95 seconds to maintain rate limit...\n",
            "Processed paper 126\n",
            "\n",
            "Processing paper 127 of 135\n",
            "Waiting 5.17 seconds to maintain rate limit...\n",
            "Processed paper 127\n",
            "\n",
            "Processing paper 128 of 135\n",
            "Waiting 4.90 seconds to maintain rate limit...\n",
            "Processed paper 128\n",
            "\n",
            "Processing paper 129 of 135\n",
            "Waiting 4.52 seconds to maintain rate limit...\n",
            "Processed paper 129\n",
            "\n",
            "Processing paper 130 of 135\n",
            "Waiting 5.10 seconds to maintain rate limit...\n",
            "Processed paper 130\n",
            "\n",
            "Processing paper 131 of 135\n",
            "Waiting 5.22 seconds to maintain rate limit...\n",
            "Processed paper 131\n",
            "\n",
            "Processing paper 132 of 135\n",
            "Waiting 5.05 seconds to maintain rate limit...\n",
            "Processed paper 132\n",
            "\n",
            "Processing paper 133 of 135\n",
            "Waiting 5.23 seconds to maintain rate limit...\n",
            "Processed paper 133\n",
            "\n",
            "Processing paper 134 of 135\n",
            "Waiting 4.97 seconds to maintain rate limit...\n",
            "Processed paper 134\n",
            "\n",
            "Processing paper 135 of 135\n",
            "Waiting 5.00 seconds to maintain rate limit...\n",
            "Processed paper 135\n",
            "\n",
            "Results saved to gemini2_output.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "\n",
        "def read_all_lines(filename=\"/content/extracted_full_texts.txt\"):\n",
        "    \"\"\"Read all lines from file\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            return [line.strip() for line in f.readlines()]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {filename} not found\")\n",
        "        return []\n",
        "\n",
        "def process_paper(model, text, min_interval=7):\n",
        "    \"\"\"Process single paper through Gemini API with rate limiting\"\"\"\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        chat = model.start_chat(history=[])\n",
        "        response = chat.send_message(text)\n",
        "\n",
        "        # Calculate remaining time to wait\n",
        "        elapsed = time.time() - start_time\n",
        "        remaining = min_interval - elapsed\n",
        "\n",
        "        if remaining > 0:\n",
        "            print(f\"Waiting {remaining:.2f} seconds to maintain rate limit...\")\n",
        "            time.sleep(remaining)\n",
        "\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"API Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def save_results(results, output_file=\"gemini2_output.txt\"):\n",
        "    \"\"\"Save results to file\"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for result in results:\n",
        "            f.write(f\"{result}\\n\")\n",
        "\n",
        "def main():\n",
        "    # Configure Gemini\n",
        "    genai.configure(api_key=\"####\")\n",
        "\n",
        "    # Setup model\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "\n",
        "    sys_prompt = \"\"\"\n",
        "\n",
        "You are an expert evaluator of academic research papers. Your task is to analyze the provided research paper text and assess it for three key areas: inappropriate methodologies, incoherent arguments, and unsubstantiated claims. You will provide a score from 1 to 5 for each area, with a focus on strongly penalizing severe flaws but making it more achievable for a paper with minor, infrequent issues to get a high score. **Avoid using the middle score of 3 unless the paper truly falls equally between flawed and sound.**\n",
        "\n",
        "Specifically:\n",
        "\n",
        "*   **Inappropriate Methodologies:**\n",
        "    *   **1:** The methodology is **catastrophically flawed**, invalid, and completely inappropriate for the research question. It renders the entire study utterly unreliable, and its conclusions are almost certainly false. The approach is demonstrably unsound and indicative of significant errors.\n",
        "    *   **2:** The methodology has **major, crippling flaws** that seriously undermine the validity of the findings. Significant and obvious concerns exist about the data collection, analysis, or experimental design. The methods chosen are clearly inadequate.\n",
        "    *   **3:** The methodology has some noticeable flaws or weaknesses that may affect the interpretation or generalizability of results. However, it falls between having serious issues and being completely sound. Use this option only if the paper truly balances between flawed and well-done.\n",
        "    *   **4:** The methodology is **generally strong and well-reasoned**, with only *minor and infrequent* issues or limitations that *do not significantly impact* the validity of the study. A few minor issues are acceptable for this score.\n",
        "    *   **5:** The methodology is **highly appropriate and well-executed**, demonstrating a strong understanding of research design. The methods are sound and are very suitable for addressing the research question. The approach is generally well-implemented even if not absolutely perfect.\n",
        "\n",
        "*   **Incoherent Arguments:**\n",
        "    *   **1:** The arguments are **completely nonsensical, utterly illogical**, and lack *any* coherent connection between evidence and conclusions. The reasoning is impossible to follow and demonstrates a complete absence of logical thinking.\n",
        "    *   **2:** The arguments suffer from **severe, fundamental logical flaws**, major inconsistencies, and a complete lack of clear connections between evidence and claims. Reasoning is often incoherent and makes no sense.\n",
        "    *   **3:** The arguments are somewhat unclear or contain logical leaps, making it challenging to fully follow the reasoning. There are inconsistencies and gaps in logic, and use this option if the paper truly balances on being coherent and incoherent.\n",
        "    *   **4:** The arguments are **mostly clear, logical, and well-structured,** with only *minor and infrequent* ambiguities. Reasoning is generally solid, and any minor issues do not significantly impede the overall clarity. A few small inconsistencies are acceptable at this score.\n",
        "    *   **5:** The arguments are **highly coherent, logical, and well-presented**, demonstrating a strong command of argumentation and a clear connection between evidence and conclusions. Reasoning is easy to follow and is well-supported.\n",
        "\n",
        "*   **Unsubstantiated Claims:**\n",
        "    *   **1:** The paper is **filled with outrageous and preposterous claims** lacking any support whatsoever. Assertions are demonstrably false, contradict established knowledge, and reveal a reckless disregard for evidence. The paper is essentially an exercise in unsubstantiated fabrication.\n",
        "    *   **2:** The paper makes a **great number of unsubstantiated and poorly supported claims.** There is little to no evidence presented to support major assertions, and key statements are made with a complete absence of supporting facts.\n",
        "    *   **3:** The paper makes some unsubstantiated claims and lacks adequate support for key assertions. Some claims require additional evidence. Use this option if the paper truly falls between being well-supported and unsubstantiated.\n",
        "    *   **4:**  **Most claims are well-supported** by evidence, with only *minor and infrequent* claims that could use additional substantiation. The core assertions are backed by data or established knowledge, and a few unproven assertions are acceptable.\n",
        "    *   **5:** **The claims are highly substantiated and well-supported**, demonstrating a thorough effort to provide backing for all assertions. A strong effort to support all claims with data and knowledge is apparent, even if it's not perfect.\n",
        "\n",
        "Your output MUST be a JSON object with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"scores\": {\n",
        "    \"inappropriate_methodologies\": <integer between 1 and 5>,\n",
        "    \"incoherent_arguments\": <integer between 1 and 5>,\n",
        "    \"unsubstantiated_claims\": <integer between 1 and 5>\n",
        "    }\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-exp\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=sys_prompt\n",
        "    )\n",
        "\n",
        "    papers = read_all_lines()\n",
        "    results = []\n",
        "\n",
        "    for i, paper in enumerate(papers, 1):\n",
        "        print(f\"\\nProcessing paper {i} of {len(papers)}\")\n",
        "        result = process_paper(model, paper)\n",
        "        results.append(f\"For the doc number {i} the result is\")\n",
        "        results.append(result)\n",
        "        print(f\"Processed paper {i}\")\n",
        "\n",
        "    save_results(results)\n",
        "    print(\"\\nResults saved to gemini2_output.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Xp8dIc_B0hxb",
        "outputId": "3327fede-dbc8-49f5-ae98-6e9c2685104d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"Doc Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39,\n        \"min\": 1,\n        \"max\": 135,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          99,\n          68,\n          106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Inappropriate Methodologies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Incoherent Arguments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unsubstantiated Claims\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 3,\n        \"max\": 13,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          7,\n          3,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e526bb7e-1136-49a6-b4d3-31824433cc92\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Number</th>\n",
              "      <th>Inappropriate Methodologies</th>\n",
              "      <th>Incoherent Arguments</th>\n",
              "      <th>Unsubstantiated Claims</th>\n",
              "      <th>Total Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>132</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>133</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>134</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>135</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e526bb7e-1136-49a6-b4d3-31824433cc92')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e526bb7e-1136-49a6-b4d3-31824433cc92 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e526bb7e-1136-49a6-b4d3-31824433cc92');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-199b980f-748d-4adf-8cb7-f0f4e0554b40\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-199b980f-748d-4adf-8cb7-f0f4e0554b40')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-199b980f-748d-4adf-8cb7-f0f4e0554b40 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_cb72f825-8720-4073-8fda-2738c8432d9d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cb72f825-8720-4073-8fda-2738c8432d9d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
              "0             1                            2                     3   \n",
              "1             2                            1                     1   \n",
              "2             3                            2                     2   \n",
              "3             4                            4                     4   \n",
              "4             5                            4                     4   \n",
              "..          ...                          ...                   ...   \n",
              "130         131                            4                     4   \n",
              "131         132                            2                     2   \n",
              "132         133                            4                     4   \n",
              "133         134                            1                     1   \n",
              "134         135                            4                     4   \n",
              "\n",
              "     Unsubstantiated Claims  Total Score  \n",
              "0                         2            7  \n",
              "1                         1            3  \n",
              "2                         2            6  \n",
              "3                         4           12  \n",
              "4                         4           12  \n",
              "..                      ...          ...  \n",
              "130                       4           12  \n",
              "131                       1            5  \n",
              "132                       4           12  \n",
              "133                       1            3  \n",
              "134                       4           12  \n",
              "\n",
              "[135 rows x 5 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Path to the text file\n",
        "file_path = 'gemini2_output.txt'\n",
        "\n",
        "# Initialize list to store the rows\n",
        "rows = []\n",
        "\n",
        "# Read the file and process each document\n",
        "with open(file_path, 'r') as file:\n",
        "    current_doc = {}\n",
        "    json_str = \"\"  # Initialize json_str before use\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"For the doc number\"):\n",
        "            if current_doc:  # Save the previous document if exists\n",
        "                scores = current_doc[\"scores\"]\n",
        "                inappropriate = scores[\"inappropriate_methodologies\"]\n",
        "                incoherent = scores[\"incoherent_arguments\"]\n",
        "                unsubstantiated = scores[\"unsubstantiated_claims\"]\n",
        "                total = inappropriate + incoherent + unsubstantiated\n",
        "                # methodologies_found = current_doc.get(\"inappropriate_methodologies_found\", [])\n",
        "                rows.append({\n",
        "                    \"Doc Number\": len(rows) + 1,\n",
        "                    \"Inappropriate Methodologies\": inappropriate,\n",
        "                    \"Incoherent Arguments\": incoherent,\n",
        "                    \"Unsubstantiated Claims\": unsubstantiated,\n",
        "                    \"Total Score\": total,\n",
        "                    # \"Inappropriate Methodologies Found\": len(methodologies_found)\n",
        "                })\n",
        "            current_doc = {}  # Reset for the new document\n",
        "            json_str = \"\"  # Reset json_str for the new document\n",
        "        elif line.startswith(\"{\"):  # JSON-like content starts\n",
        "            json_str = line\n",
        "        elif line.endswith(\"}\") and json_str.endswith(\"}\"):  # JSON-like content ends\n",
        "            json_str += line\n",
        "            current_doc = json.loads(json_str)\n",
        "        elif line:  # Append multiline JSON content\n",
        "            json_str += line\n",
        "\n",
        "# Add the last document if any\n",
        "if current_doc:\n",
        "    scores = current_doc[\"scores\"]\n",
        "    inappropriate = scores[\"inappropriate_methodologies\"]\n",
        "    incoherent = scores[\"incoherent_arguments\"]\n",
        "    unsubstantiated = scores[\"unsubstantiated_claims\"]\n",
        "    total = inappropriate + incoherent + unsubstantiated\n",
        "    # methodologies_found = current_doc.get(\"inappropriate_methodologies_found\", [])\n",
        "    rows.append({\n",
        "        \"Doc Number\": len(rows) + 1,\n",
        "        \"Inappropriate Methodologies\": inappropriate,\n",
        "        \"Incoherent Arguments\": incoherent,\n",
        "        \"Unsubstantiated Claims\": unsubstantiated,\n",
        "        \"Total Score\": total,\n",
        "        # \"Inappropriate Methodologies Found\": len(methodologies_found)\n",
        "    })\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(rows)\n",
        "df\n",
        "\n",
        "# Display the DataFrame to the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7nL9gs-8AaOk",
        "outputId": "2a939438-6a36-4952-c5ef-08ed2d587c31"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"Doc Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39,\n        \"min\": 1,\n        \"max\": 135,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          99,\n          68,\n          106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Inappropriate Methodologies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Incoherent Arguments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unsubstantiated Claims\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 3,\n        \"max\": 13,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          7,\n          3,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Publishable\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-feac01e2-0c15-4877-973b-0021421bb682\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Number</th>\n",
              "      <th>Inappropriate Methodologies</th>\n",
              "      <th>Incoherent Arguments</th>\n",
              "      <th>Unsubstantiated Claims</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Publishable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>132</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>133</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>134</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>135</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-feac01e2-0c15-4877-973b-0021421bb682')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-feac01e2-0c15-4877-973b-0021421bb682 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-feac01e2-0c15-4877-973b-0021421bb682');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-03557c19-f2d2-4605-b209-687e1734d9d2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-03557c19-f2d2-4605-b209-687e1734d9d2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-03557c19-f2d2-4605-b209-687e1734d9d2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_935175cf-1849-49d2-b285-d228267b5254\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_935175cf-1849-49d2-b285-d228267b5254 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
              "0             1                            2                     3   \n",
              "1             2                            1                     1   \n",
              "2             3                            2                     2   \n",
              "3             4                            4                     4   \n",
              "4             5                            4                     4   \n",
              "..          ...                          ...                   ...   \n",
              "130         131                            4                     4   \n",
              "131         132                            2                     2   \n",
              "132         133                            4                     4   \n",
              "133         134                            1                     1   \n",
              "134         135                            4                     4   \n",
              "\n",
              "     Unsubstantiated Claims  Total Score  Publishable  \n",
              "0                         2            7            0  \n",
              "1                         1            3            0  \n",
              "2                         2            6            0  \n",
              "3                         4           12            1  \n",
              "4                         4           12            1  \n",
              "..                      ...          ...          ...  \n",
              "130                       4           12            1  \n",
              "131                       1            5            0  \n",
              "132                       4           12            1  \n",
              "133                       1            3            0  \n",
              "134                       4           12            1  \n",
              "\n",
              "[135 rows x 6 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Publishable'] = df['Total Score'].apply(lambda x: 1 if x >= 11 else 0)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "nz-VNcLrA90D",
        "outputId": "4429a293-863a-4e5e-fa50-d201d3c2032b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"Doc Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39,\n        \"min\": 1,\n        \"max\": 135,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          99,\n          68,\n          106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Inappropriate Methodologies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Incoherent Arguments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unsubstantiated Claims\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 3,\n        \"max\": 13,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          7,\n          3,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Publishable\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Paper ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 135,\n        \"samples\": [\n          \"P099\",\n          \"P068\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fc98c0ae-b362-47ef-a0ba-81541a4c481b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Number</th>\n",
              "      <th>Inappropriate Methodologies</th>\n",
              "      <th>Incoherent Arguments</th>\n",
              "      <th>Unsubstantiated Claims</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Publishable</th>\n",
              "      <th>Paper ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>P001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>P002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>P003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>132</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>P132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>133</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>134</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>P134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>135</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P135</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc98c0ae-b362-47ef-a0ba-81541a4c481b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc98c0ae-b362-47ef-a0ba-81541a4c481b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc98c0ae-b362-47ef-a0ba-81541a4c481b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4d437989-4759-4266-a844-74bbd7685a5e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d437989-4759-4266-a844-74bbd7685a5e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4d437989-4759-4266-a844-74bbd7685a5e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dac9bb02-549e-4753-ae73-dd17e33380a9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dac9bb02-549e-4753-ae73-dd17e33380a9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
              "0             1                            2                     3   \n",
              "1             2                            1                     1   \n",
              "2             3                            2                     2   \n",
              "3             4                            4                     4   \n",
              "4             5                            4                     4   \n",
              "..          ...                          ...                   ...   \n",
              "130         131                            4                     4   \n",
              "131         132                            2                     2   \n",
              "132         133                            4                     4   \n",
              "133         134                            1                     1   \n",
              "134         135                            4                     4   \n",
              "\n",
              "     Unsubstantiated Claims  Total Score  Publishable Paper ID  \n",
              "0                         2            7            0     P001  \n",
              "1                         1            3            0     P002  \n",
              "2                         2            6            0     P003  \n",
              "3                         4           12            1     P004  \n",
              "4                         4           12            1     P005  \n",
              "..                      ...          ...          ...      ...  \n",
              "130                       4           12            1     P131  \n",
              "131                       1            5            0     P132  \n",
              "132                       4           12            1     P133  \n",
              "133                       1            3            0     P134  \n",
              "134                       4           12            1     P135  \n",
              "\n",
              "[135 rows x 7 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Paper ID'] = 'P' + (df.index + 1).astype(str).str.zfill(3)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "rx80xoTaDXJo",
        "outputId": "04caf793-0bd4-458e-d6a8-d75e39796e88"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"Doc Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39,\n        \"min\": 1,\n        \"max\": 135,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          99,\n          68,\n          106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Inappropriate Methodologies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Incoherent Arguments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unsubstantiated Claims\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 3,\n        \"max\": 13,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          7,\n          3,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Publishable\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Paper ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 135,\n        \"samples\": [\n          \"P099\",\n          \"P068\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conference\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"\",\n          \"na\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rationale\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"\",\n          \"na\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8fd71acb-1857-44c2-b502-256bffbedbf0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Number</th>\n",
              "      <th>Inappropriate Methodologies</th>\n",
              "      <th>Incoherent Arguments</th>\n",
              "      <th>Unsubstantiated Claims</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Publishable</th>\n",
              "      <th>Paper ID</th>\n",
              "      <th>Conference</th>\n",
              "      <th>Rationale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>P001</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>P002</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>P003</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P004</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P005</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P131</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>132</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>P132</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>133</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P133</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>134</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>P134</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>135</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P135</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fd71acb-1857-44c2-b502-256bffbedbf0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8fd71acb-1857-44c2-b502-256bffbedbf0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8fd71acb-1857-44c2-b502-256bffbedbf0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5b2dfdf7-044f-4e07-b7c0-1b3b619c080b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b2dfdf7-044f-4e07-b7c0-1b3b619c080b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5b2dfdf7-044f-4e07-b7c0-1b3b619c080b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f3cc1c6a-8086-4f2d-b1f7-992b95970b6f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f3cc1c6a-8086-4f2d-b1f7-992b95970b6f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
              "0             1                            2                     3   \n",
              "1             2                            1                     1   \n",
              "2             3                            2                     2   \n",
              "3             4                            4                     4   \n",
              "4             5                            4                     4   \n",
              "..          ...                          ...                   ...   \n",
              "130         131                            4                     4   \n",
              "131         132                            2                     2   \n",
              "132         133                            4                     4   \n",
              "133         134                            1                     1   \n",
              "134         135                            4                     4   \n",
              "\n",
              "     Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
              "0                         2            7            0     P001         na   \n",
              "1                         1            3            0     P002         na   \n",
              "2                         2            6            0     P003         na   \n",
              "3                         4           12            1     P004              \n",
              "4                         4           12            1     P005              \n",
              "..                      ...          ...          ...      ...        ...   \n",
              "130                       4           12            1     P131              \n",
              "131                       1            5            0     P132         na   \n",
              "132                       4           12            1     P133              \n",
              "133                       1            3            0     P134         na   \n",
              "134                       4           12            1     P135              \n",
              "\n",
              "    Rationale  \n",
              "0          na  \n",
              "1          na  \n",
              "2          na  \n",
              "3              \n",
              "4              \n",
              "..        ...  \n",
              "130            \n",
              "131        na  \n",
              "132            \n",
              "133        na  \n",
              "134            \n",
              "\n",
              "[135 rows x 9 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Conference'] = ''\n",
        "df['Rationale'] = ''\n",
        "\n",
        "# Update 'Conference' and 'Rationale' based on 'Publishable' column\n",
        "df.loc[df['Publishable'] == 0, ['Conference', 'Rationale']] = 'na'\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0zWAg6bEfwP",
        "outputId": "68ae0a72-56c3-4f02-a9f7-7079fa9c5317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
            "0           1                            2                     3   \n",
            "1           2                            1                     1   \n",
            "2           3                            2                     2   \n",
            "3           4                            4                     4   \n",
            "4           5                            4                     4   \n",
            "\n",
            "   Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
            "0                       2            7            0     P001         na   \n",
            "1                       1            3            0     P002         na   \n",
            "2                       2            6            0     P003         na   \n",
            "3                       4           12            1     P004              \n",
            "4                       4           12            1     P005              \n",
            "\n",
            "  Rationale                                           Abstract  \n",
            "0        na  Drone tracking and localization are essential ...  \n",
            "1        na  Virus transmission is intricately linked to th...  \n",
            "2        na  Explainable reinforcement learning has emerged...  \n",
            "3            This study introduces a novel concept of train...  \n",
            "4            This research introduces a comprehensive cloth...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "with open('/content/extracted_full_abstracts.txt', 'r') as file:\n",
        "    abstracts = [line.strip() for line in file.readlines()]\n",
        "\n",
        "# Add the 'Abstract' column to the DataFrame\n",
        "if len(abstracts) == len(df):\n",
        "    df['Abstract'] = abstracts\n",
        "else:\n",
        "    print(\"Error: Number of abstracts does not match the number of rows in the DataFrame.\")\n",
        "    # Handle the mismatch appropriately, e.g., fill with NaN or truncate/pad the lists\n",
        "    df['Abstract'] = abstracts[:len(df)] #taking the first len(df) abstracts\n",
        "\n",
        "# Now df contains the new 'Abstract' column\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "N1ChCcBVBRN0",
        "outputId": "9f5563bc-05fa-43c5-cb44-196b211695bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"result_df\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"Paper ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 135,\n        \"samples\": [\n          \"P099\",\n          \"P068\",\n          \"P106\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Publishable\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "result_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-0125e6c8-a05d-459b-a950-a99cf2cf2ae5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Paper ID</th>\n",
              "      <th>Publishable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P003</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P005</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>P131</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>P132</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>P133</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>P134</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>P135</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0125e6c8-a05d-459b-a950-a99cf2cf2ae5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0125e6c8-a05d-459b-a950-a99cf2cf2ae5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0125e6c8-a05d-459b-a950-a99cf2cf2ae5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-090daabb-1a21-4adf-82ad-505147c2e578\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-090daabb-1a21-4adf-82ad-505147c2e578')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-090daabb-1a21-4adf-82ad-505147c2e578 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dd9a2f11-fdae-4251-ab92-e536daf935af\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dd9a2f11-fdae-4251-ab92-e536daf935af button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Paper ID  Publishable\n",
              "0       P001            0\n",
              "1       P002            0\n",
              "2       P003            0\n",
              "3       P004            1\n",
              "4       P005            1\n",
              "..       ...          ...\n",
              "130     P131            1\n",
              "131     P132            0\n",
              "132     P133            1\n",
              "133     P134            0\n",
              "134     P135            1\n",
              "\n",
              "[135 rows x 2 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df = df[['Paper ID', 'Publishable']]\n",
        "result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FN6f-yBV2j"
      },
      "source": [
        "# Task - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NwZjcRvLBpG0"
      },
      "outputs": [],
      "source": [
        "pinecone = \"#####\"\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=pinecone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2c1ade1TButX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "index_name = \"all-conferences-450\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFBwevbcGZwM",
        "outputId": "1d4d9d86-71ac-4eaa-c36b-033cddcb9d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OpenAI: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZVfoNjm2HK1O"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iIhRXG4wIaBt"
      },
      "outputs": [],
      "source": [
        "def get_conference_predictions(text, vector_store):\n",
        "    \"\"\"\n",
        "    Get conference predictions based on similarity search\n",
        "    Args:\n",
        "        text: Input text to analyze\n",
        "        vector_store: Initialized vector store for similarity search\n",
        "    Returns:\n",
        "        tuple: (list of top conferences, highest frequency number)\n",
        "    \"\"\"\n",
        "    # Perform similarity search\n",
        "    results = vector_store.similarity_search(text, k=15)\n",
        "\n",
        "    # Convert to DataFrame and get conference frequencies\n",
        "    df = pd.DataFrame(results)\n",
        "    conferences = df.iloc[:,1].apply(lambda x: x[1]['conference'])\n",
        "    freq_counts = conferences.value_counts()\n",
        "\n",
        "    # Get max frequency\n",
        "    max_freq = freq_counts.max()\n",
        "\n",
        "    # Get conference list based on threshold\n",
        "    if max_freq >= 8:\n",
        "        conf_list = [freq_counts.index[0]]  # Only top conference\n",
        "    else:\n",
        "        conf_list = freq_counts.index[:3].tolist()  # Top 3 conferences\n",
        "\n",
        "    return conf_list, max_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLkpLZlOJixQ",
        "outputId": "5e3f1409-2fff-46c5-e8b6-7f25a5f8d7a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
            "0           1                            2                     3   \n",
            "1           2                            1                     1   \n",
            "2           3                            2                     2   \n",
            "3           4                            4                     4   \n",
            "4           5                            4                     4   \n",
            "\n",
            "   Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
            "0                       2            7            0     P001         na   \n",
            "1                       1            3            0     P002         na   \n",
            "2                       2            6            0     P003         na   \n",
            "3                       4           12            1     P004              \n",
            "4                       4           12            1     P005              \n",
            "\n",
            "  Rationale                                           Abstract Maxfreq  \\\n",
            "0        na  Drone tracking and localization are essential ...           \n",
            "1        na  Virus transmission is intricately linked to th...           \n",
            "2        na  Explainable reinforcement learning has emerged...           \n",
            "3            This study introduces a novel concept of train...       9   \n",
            "4            This research introduces a comprehensive cloth...      11   \n",
            "\n",
            "   Conflist  \n",
            "0            \n",
            "1            \n",
            "2            \n",
            "3   ['KDD']  \n",
            "4  ['CVPR']  \n"
          ]
        }
      ],
      "source": [
        "# prompt: send abstract of every row in the df whose value of Publishable column is 1 to get_conference_predictions. it will give two outputs, conf_list, max_freq --  put these values in new columns : Maxfreq and Conflist. create these new columns for the whole dataset. leave the ones with Publishable 0 as blanks\n",
        "\n",
        "# Assuming 'df' and 'get_conference_predictions' are defined as in the provided code.\n",
        "\n",
        "df['Maxfreq'] = ''\n",
        "df['Conflist'] = ''\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['Publishable'] == 1:\n",
        "        abstract = row['Abstract']\n",
        "        conf_list, max_freq = get_conference_predictions(abstract, vector_store)\n",
        "        df.loc[index, 'Conflist'] = str(conf_list)  # Store as string\n",
        "        df.loc[index, 'Maxfreq'] = max_freq\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I3WxdU1K5Mr",
        "outputId": "457d20bd-6afb-40d9-ef3f-434042e4ebb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
            "0           1                            2                     3   \n",
            "1           2                            1                     1   \n",
            "2           3                            2                     2   \n",
            "3           4                            4                     4   \n",
            "4           5                            4                     4   \n",
            "\n",
            "   Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
            "0                       2            7            0     P001         na   \n",
            "1                       1            3            0     P002         na   \n",
            "2                       2            6            0     P003         na   \n",
            "3                       4           12            1     P004        KDD   \n",
            "4                       4           12            1     P005       CVPR   \n",
            "\n",
            "  Rationale                                           Abstract Maxfreq  \\\n",
            "0        na  Drone tracking and localization are essential ...           \n",
            "1        na  Virus transmission is intricately linked to th...           \n",
            "2        na  Explainable reinforcement learning has emerged...           \n",
            "3            This study introduces a novel concept of train...       9   \n",
            "4            This research introduces a comprehensive cloth...      11   \n",
            "\n",
            "   Conflist  \n",
            "0            \n",
            "1            \n",
            "2            \n",
            "3   ['KDD']  \n",
            "4  ['CVPR']  \n"
          ]
        }
      ],
      "source": [
        "# prompt: if Maxfreq value of a row is >=8, update the  Conference column of that row to the name of conference Conflist contains. (there will be only 1 name, one of TMLR ,NIPS  ,KDD ,\n",
        "# CVPR ,EMNLP  )\n",
        "\n",
        "# Assuming 'df' and other necessary variables are defined as in the provided code.\n",
        "\n",
        "# Update 'Conference' column based on 'Maxfreq' and 'Conflist'\n",
        "for index, row in df.iterrows():\n",
        "  if row['Maxfreq'] != '':\n",
        "    if int(row['Maxfreq']) >= 8 and row['Publishable'] == 1:\n",
        "        conf_list = eval(row['Conflist'])  # Convert string representation of list back to list\n",
        "        if conf_list:\n",
        "            df.loc[index, 'Conference'] = conf_list[0] #only one name in the list\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "IzLRaxC5NlhH"
      },
      "outputs": [],
      "source": [
        "TMLR = \"\"\"\n",
        "The Transactions on Machine Learning Research (TMLR) is a relatively new journal-style publication venue associated with the International Conference on Machine Learning (ICML). It operates as an open access, continuous publication journal rather than a traditional conference with deadlines.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "*   **Journal Format:** TMLR papers undergo a rigorous peer-review process, similar to journals, and accepted papers are continuously published online, rather than being presented at a specific conference.\n",
        "*   **Open Access:** All TMLR publications are freely available to read and download.\n",
        "*   **Focus:** TMLR aims to publish high-quality research in all areas of machine learning, with an emphasis on rigorous methodology and impactful contributions.\n",
        "*   **Review Process:** The review process is thorough and can take several rounds of revision, leading to well-vetted, high-quality publications. The reviewers tend to be experts in the respective fields.\n",
        "*   **ICML Affiliation:** While TMLR is a standalone publication, it is closely affiliated with ICML and accepted papers are typically highlighted during the ICML conference.\n",
        "*   **Emphasis on Clarity and Reproducibility:** TMLR encourages clear writing, code sharing, and proper evaluation to facilitate reproducibility.\n",
        "*   **Continuous Publication:** Unlike conferences with fixed deadlines and schedules, TMLR publishes accepted papers on an ongoing basis.\n",
        "*   **Not a Conference:** TMLR does not have a traditional conference component like presentations or posters. Accepted papers are published online, and there's no physical or virtual gathering for presenting.\n",
        "\n",
        "**In summary, TMLR is a prestigious, peer-reviewed journal-style publication venue focusing on high-quality machine learning research with a rigorous review process and open access policy. It prioritizes clarity, reproducibility, and impactful contributions to the field and is closely associated with the ICML conference.**\n",
        "\"\"\"\n",
        "\n",
        "NIPS = \"\"\"\n",
        "The Conference on Neural Information Processing Systems (NeurIPS, formerly NIPS) is one of the most prestigious and highly selective conferences in the field of artificial intelligence and machine learning.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "*   **Highly Competitive:** NeurIPS is known for its extremely low acceptance rates, making it very difficult to get a paper accepted. This also means the accepted papers are typically of very high quality and significant impact.\n",
        "*   **Broad Coverage:** NeurIPS covers a wide range of topics within machine learning, including deep learning, reinforcement learning, optimization, theoretical foundations, probabilistic methods, and more.\n",
        "*   **Strong Focus on Deep Learning:** With the recent surge in popularity of deep learning, NeurIPS has become a major venue for showcasing cutting-edge research in this subfield.\n",
        "*   **Poster and Oral Presentations:** Accepted papers are typically presented as posters, and a smaller subset are selected for oral presentations.\n",
        "*   **Workshops and Tutorials:** NeurIPS features various workshops and tutorials covering a broad spectrum of topics in AI and machine learning. These workshops provide an in-depth view into specialized areas and future research directions.\n",
        "*   **Large and Diverse Audience:** The conference attracts a diverse audience from academia, industry, and government research labs.\n",
        "*   **High Visibility and Impact:** NeurIPS papers often have a significant impact on the field, and they are frequently cited by other researchers.\n",
        "*   **Single-Blind Reviewing:** Typically, the reviewing process is single-blind, meaning that reviewers know the authors' identities, but the authors do not know the reviewers' identities.\n",
        "*   **Focus on Novel Contributions:** The conference focuses on original and innovative research contributions.\n",
        "*   **Annual Event:** NeurIPS is an annual conference that takes place each year, usually in December.\n",
        "\n",
        "**In summary, NeurIPS is a leading global conference in AI and machine learning, renowned for its high standards, breadth of coverage, and its impact on shaping the future direction of the field. It's a crucial venue for researchers to share cutting-edge findings, network with experts, and stay updated on the latest advancements.**\n",
        "\"\"\"\n",
        "\n",
        "KDD = \"\"\"\n",
        "The ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) is a premier international conference focused on knowledge discovery, data mining, and machine learning.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "*   **Focus on Knowledge Discovery:** KDD is distinguished by its emphasis on the full knowledge discovery process, from data preprocessing and cleaning to the deployment and evaluation of models.\n",
        "*   **Real-World Applications:** While theoretical research is welcome, KDD has a strong focus on practical applications and how data mining can solve real-world problems.\n",
        "*   **Industry Relevance:** KDD attracts a significant number of attendees from the industry, and many papers showcase industry-oriented applications and solutions.\n",
        "*   **Broad Topics:** The conference covers a diverse set of topics, including machine learning, data mining, data science, database management, big data, and social network analysis.\n",
        "*   **Research and Applied Tracks:** KDD features both research tracks, where the contributions are theoretical and algorithmic, and applied tracks, where the papers focus on real-world case studies and deployments.\n",
        "*   **Workshops and Tutorials:** KDD offers various workshops and tutorials covering specialized topics and practical techniques.\n",
        "*   **KDD Cup:** The conference also includes the KDD Cup, a data mining competition that attracts researchers and practitioners to solve challenging problems.\n",
        "*   **Emphasis on Scalability and Efficiency:** Given the focus on real-world data, KDD emphasizes the scalability and efficiency of algorithms and solutions.\n",
        "*   **Large and Established Conference:** KDD is one of the longest-running and most well-known conferences in data mining and knowledge discovery.\n",
        "*   **Double-Blind Reviewing:** The review process is typically double-blind, meaning that both the reviewers and authors are anonymous.\n",
        "\n",
        "**In summary, KDD is a highly respected and impactful conference focused on bridging the gap between research and practical applications of data mining and knowledge discovery. It provides a valuable platform for researchers, practitioners, and industry professionals to share their work, learn from each other, and advance the field.**\n",
        "\"\"\"\n",
        "\n",
        "CVPR = \"\"\"\n",
        "The Conference on Computer Vision and Pattern Recognition (CVPR) is one of the most prestigious and competitive conferences in the field of computer vision.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "*   **Premier Computer Vision Venue:** CVPR is widely regarded as a top venue for publishing research in computer vision, attracting submissions from top research labs worldwide.\n",
        "*   **Broad Range of Topics:** The conference covers a diverse set of topics within computer vision, including image recognition, object detection, segmentation, 3D vision, video analysis, medical image analysis, and robotics vision.\n",
        "*   **Focus on Visual Data Analysis:** CVPR is primarily concerned with the analysis, interpretation, and understanding of visual data, such as images and videos.\n",
        "*   **Rigorous Peer Review:** The review process is typically rigorous, with a large number of submissions and a low acceptance rate.\n",
        "*   **Emphasis on Technical Innovation:** CVPR papers typically demonstrate technical innovation and advance the state-of-the-art in the field.\n",
        "*   **Poster and Oral Presentations:** Accepted papers are presented as posters and a smaller subset is selected for oral presentation.\n",
        "*   **Tutorials and Workshops:** CVPR features numerous tutorials and workshops focused on specialized areas of computer vision.\n",
        "*   **Large and International Community:** The conference attracts a large and international audience from academia and industry, highlighting its wide reach.\n",
        "*   **Annual Event:** CVPR is an annual conference that typically takes place in the summer.\n",
        "*   **Double-Blind Reviewing:** The review process is double-blind, ensuring a fair assessment of the work, where reviewers are not aware of the author's identities and vice-versa.\n",
        "*   **Publicly Available Code:** It is increasingly expected that papers will also make their code public, contributing to reproducibility and open science.\n",
        "\n",
        "**In summary, CVPR is a leading international conference in computer vision, highly regarded for its rigorous review process, strong focus on technical innovation, broad coverage of topics, and significant impact on the field. It is a crucial venue for researchers to disseminate their latest findings, engage with the computer vision community, and advance the field forward.**\n",
        "\"\"\"\n",
        "\n",
        "EMNLP = \"\"\"\n",
        "The Conference on Empirical Methods in Natural Language Processing (EMNLP) is a leading international conference in the field of natural language processing (NLP).\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "*   **Focus on Empirical NLP:** EMNLP emphasizes empirical research, focusing on methods and models that are tested and evaluated using real-world data.\n",
        "*   **Broad Range of Topics:** The conference covers a wide array of topics within NLP, including text classification, machine translation, language modeling, information extraction, dialogue systems, summarization, question answering, and more.\n",
        "*   **Data-Driven Approach:** EMNLP papers typically use a data-driven approach, focusing on using datasets to train and evaluate NLP models.\n",
        "*   **Focus on Evaluation Metrics:** Rigorous evaluation is a key component of EMNLP research, emphasizing the use of standardized metrics to assess the performance of NLP models.\n",
        "*   **Theoretical and Applied Research:** EMNLP welcomes both theoretical and applied research, as long as the research is empirically grounded.\n",
        "*   **Strong Community:** The conference has a strong and supportive community of NLP researchers and practitioners.\n",
        "*   **Poster and Oral Presentations:** Accepted papers are typically presented as posters, with a smaller subset selected for oral presentations.\n",
        "*   **Workshops and Tutorials:** EMNLP features various workshops and tutorials covering specialized topics within NLP.\n",
        "*   **Annual Event:** EMNLP is an annual conference that takes place each year.\n",
        "*   **Double-Blind Reviewing:** The review process is double-blind, ensuring anonymity of both authors and reviewers.\n",
        "*  **Emphasis on Reproducibility:** Like other top tier ML conferences, EMNLP encourages reproducible research by encouraging code sharing.\n",
        "\n",
        "**In summary, EMNLP is a premier conference for empirical research in natural language processing, focused on data-driven approaches, rigorous evaluation, and addressing real-world problems using NLP techniques. It serves as an important platform for researchers to share their work, collaborate with peers, and contribute to the advancement of the NLP field.**\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gzNOhxQMhXN",
        "outputId": "df46a00a-8d16-484d-ac11-cdf8d039b4cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
            "0           1                            2                     3   \n",
            "1           2                            1                     1   \n",
            "2           3                            2                     2   \n",
            "3           4                            4                     4   \n",
            "4           5                            4                     4   \n",
            "\n",
            "   Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
            "0                       2            7            0     P001         na   \n",
            "1                       1            3            0     P002         na   \n",
            "2                       2            6            0     P003         na   \n",
            "3                       4           12            1     P004        KDD   \n",
            "4                       4           12            1     P005       CVPR   \n",
            "\n",
            "  Rationale                                           Abstract Maxfreq  \\\n",
            "0        na  Drone tracking and localization are essential ...           \n",
            "1        na  Virus transmission is intricately linked to th...           \n",
            "2        na  Explainable reinforcement learning has emerged...           \n",
            "3            This study introduces a novel concept of train...       9   \n",
            "4            This research introduces a comprehensive cloth...      11   \n",
            "\n",
            "   Conflist                                           Fulltext  \n",
            "0            # Leveraging Clustering Techniques for Enhance...  \n",
            "1            # Virus Propagation and their Far-Reaching Imp...  \n",
            "2            # Explainable Reinforcement Learning for Finan...  \n",
            "3   ['KDD']  # Graph Neural Networks Without Training: Harn...  \n",
            "4  ['CVPR']  # Collaborative Clothing Segmentation and Iden...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "with open('/content/extracted_full_texts.txt', 'r') as file:\n",
        "    Fulltext = [line.strip() for line in file.readlines()]\n",
        "\n",
        "# Add the 'Abstract' column to the DataFrame\n",
        "if len(Fulltext) == len(df):\n",
        "    df['Fulltext'] = Fulltext\n",
        "else:\n",
        "    print(\"Error: Number of Fulltext does not match the number of rows in the DataFrame.\")\n",
        "    # Handle the mismatch appropriately, e.g., fill with NaN or truncate/pad the lists\n",
        "    df['Abstract'] = Fulltext[:len(df)] #taking the first len(df) Fulltext\n",
        "\n",
        "# Now df contains the new 'Fulltext' column\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jiml2X36NRhg",
        "outputId": "a13c6371-f484-44e6-f191-4b22c040bd3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
            "0           1                            2                     3   \n",
            "1           2                            1                     1   \n",
            "2           3                            2                     2   \n",
            "3           4                            4                     4   \n",
            "4           5                            4                     4   \n",
            "\n",
            "   Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
            "0                       2            7            0     P001         na   \n",
            "1                       1            3            0     P002         na   \n",
            "2                       2            6            0     P003         na   \n",
            "3                       4           12            1     P004        KDD   \n",
            "4                       4           12            1     P005       CVPR   \n",
            "\n",
            "  Rationale                                           Abstract Maxfreq  \\\n",
            "0        na  Drone tracking and localization are essential ...           \n",
            "1        na  Virus transmission is intricately linked to th...           \n",
            "2        na  Explainable reinforcement learning has emerged...           \n",
            "3            This study introduces a novel concept of train...       9   \n",
            "4            This research introduces a comprehensive cloth...      11   \n",
            "\n",
            "   Conflist                                           Fulltext  \\\n",
            "0            # Leveraging Clustering Techniques for Enhance...   \n",
            "1            # Virus Propagation and their Far-Reaching Imp...   \n",
            "2            # Explainable Reinforcement Learning for Finan...   \n",
            "3   ['KDD']  # Graph Neural Networks Without Training: Harn...   \n",
            "4  ['CVPR']  # Collaborative Clothing Segmentation and Iden...   \n",
            "\n",
            "  prompt_classifier  \n",
            "0                    \n",
            "1                    \n",
            "2                    \n",
            "3                    \n",
            "4                    \n"
          ]
        }
      ],
      "source": [
        "# prompt: Create a new column with empty strings, prompt_classifier in df. for rows with Publishable as 1 and Conference as 'na', construct the prompt as follows:  Here is the full text of a research paper:\n",
        "# {full_text_query} -- which is the value of Fulltext column\n",
        "# Here are the descriptions of the conferences you should consider:\n",
        "# {conference_details} -- for this -- first extract all the names of the conferences in Conflist (which is a list converted to string). the details of each conference is stored in a variable with the name of the conference, as a string\n",
        "# Based on the provided information, which conference is the best fit for this paper?\n",
        "\n",
        "# Assuming df, TMLR, NIPS, KDD, CVPR, and EMNLP are defined as in the previous code.\n",
        "\n",
        "df['prompt_classifier'] = ''\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['Publishable'] == 1 and row['Conference'] == '':\n",
        "        full_text = row['Fulltext']\n",
        "        conf_names = eval(row['Conflist'])  # Convert string representation of list back to list\n",
        "        conference_details = ''\n",
        "        for conf_name in conf_names:\n",
        "          if conf_name == 'TMLR':\n",
        "            conference_details += f'{conf_name}: {TMLR}\\n'\n",
        "          elif conf_name == 'NIPS':\n",
        "            conference_details += f'{conf_name}: {NIPS}\\n'\n",
        "          elif conf_name == 'KDD':\n",
        "            conference_details += f'{conf_name}: {KDD}\\n'\n",
        "          elif conf_name == 'CVPR':\n",
        "            conference_details += f'{conf_name}: {CVPR}\\n'\n",
        "          elif conf_name == 'EMNLP':\n",
        "            conference_details += f'{conf_name}: {EMNLP}\\n'\n",
        "          # Add more conference details as needed\n",
        "\n",
        "        prompt = f\"\"\"Here is the full text of a research paper:\n",
        "{full_text}\n",
        "Here are the descriptions of the conferences you should consider:\n",
        "{conference_details}\n",
        "Based on the provided information, which conference is the best fit for this paper?\"\"\"\n",
        "\n",
        "        df.loc[index, 'prompt_classifier'] = prompt\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Utz7aTatO_Yl"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import google.generativeai as genai\n",
        "\n",
        "def predict_conference(text, api_key=\"####\"):\n",
        "    \"\"\"\n",
        "    Predict the most suitable conference for a given research paper text\n",
        "    Args:\n",
        "        text (str): Research paper text\n",
        "        api_key (str): Gemini API key\n",
        "    Returns:\n",
        "        str: Predicted conference name\n",
        "    \"\"\"\n",
        "    # Configure Gemini\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    # Setup model config\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "\n",
        "    # System prompt\n",
        "    sys_prompt = \"\"\"\n",
        "    You are an expert research paper classifier.\n",
        "    Your task is to analyze a research paper's full text and classify it into one of a given set of conferences.\n",
        "    You will be provided with the full text of a research paper, as well as detailed descriptions of the conferences you should classify to.\n",
        "    You must output only the name of the conference that best fits the research paper's topic and methodology. Do not provide any additional information, only one word: the conference name.\n",
        "    If absolutely no conference matches, return the closest conference.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize model\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-exp\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=sys_prompt\n",
        "    )\n",
        "\n",
        "    # Process with rate limiting\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        chat = model.start_chat(history=[])\n",
        "        response = chat.send_message(text)\n",
        "\n",
        "        # Rate limiting\n",
        "        elapsed = time.time() - start_time\n",
        "        if elapsed < 7:\n",
        "            time.sleep(7 - elapsed)\n",
        "\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"API Error: {str(e)}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "g2R7VhWvPMhs",
        "outputId": "bc17d449-62ef-4bb6-a775-779e366e144a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yes 102\n",
            "   Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
            "0           1                            2                     3   \n",
            "1           2                            1                     1   \n",
            "2           3                            2                     2   \n",
            "3           4                            4                     4   \n",
            "4           5                            4                     4   \n",
            "\n",
            "   Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
            "0                       2            7            0     P001         na   \n",
            "1                       1            3            0     P002         na   \n",
            "2                       2            6            0     P003         na   \n",
            "3                       4           12            1     P004        KDD   \n",
            "4                       4           12            1     P005       CVPR   \n",
            "\n",
            "  Rationale                                           Abstract Maxfreq  \\\n",
            "0        na  Drone tracking and localization are essential ...           \n",
            "1        na  Virus transmission is intricately linked to th...           \n",
            "2        na  Explainable reinforcement learning has emerged...           \n",
            "3            This study introduces a novel concept of train...       9   \n",
            "4            This research introduces a comprehensive cloth...      11   \n",
            "\n",
            "   Conflist                                           Fulltext  \\\n",
            "0            # Leveraging Clustering Techniques for Enhance...   \n",
            "1            # Virus Propagation and their Far-Reaching Imp...   \n",
            "2            # Explainable Reinforcement Learning for Finan...   \n",
            "3   ['KDD']  # Graph Neural Networks Without Training: Harn...   \n",
            "4  ['CVPR']  # Collaborative Clothing Segmentation and Iden...   \n",
            "\n",
            "  prompt_classifier  \n",
            "0                    \n",
            "1                    \n",
            "2                    \n",
            "3                    \n",
            "4                    \n"
          ]
        }
      ],
      "source": [
        "# prompt: for rows with Publishable as 1 and Conference as 'na', send in the value of prompt_classifier column to predict_conference and use the output to update the value of Conference column\n",
        "\n",
        "# Assuming df, predict_conference, and other necessary variables are defined as in the provided code.\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['Publishable'] == 1 and row['Conference'] == '':\n",
        "        print(f\"yes {index}\")\n",
        "        prompt = row['prompt_classifier']\n",
        "        predicted_conf = predict_conference(prompt)\n",
        "        if predicted_conf:\n",
        "            df.loc[index, 'Conference'] = predicted_conf\n",
        "        else:\n",
        "            # Handle cases where the prediction fails (e.g., API error)\n",
        "            print(f\"Prediction failed for index {index}\")\n",
        "            # You might want to set a default value, skip the row, or retry later\n",
        "            # For example:\n",
        "            # df.loc[index, 'Conference'] = 'unknown'\n",
        "            pass # or continue\n",
        "\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "iu6kO3aQPvKu",
        "outputId": "dc583cc5-f55e-4eb5-8ea3-b5c5d59f1bce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Conference</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>na</th>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CVPR</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NIPS</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EMNLP</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KDD</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TMLR</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "Conference\n",
              "na       41\n",
              "CVPR     26\n",
              "NIPS     26\n",
              "EMNLP    23\n",
              "KDD      10\n",
              "TMLR      9\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Conference'] = df['Conference'].replace('NeurIPS', 'NIPS')\n",
        "df['Conference'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "da_ZYhvjTE56"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import google.generativeai as genai\n",
        "\n",
        "def generate_conference_reasoning(full_text, final_conference, reference_papers, api_key=\"####\"):\n",
        "    \"\"\"\n",
        "    Generate reasoning for why a paper fits a conference\n",
        "    Args:\n",
        "        full_text (str): Research paper text\n",
        "        final_conference (str): Selected conference\n",
        "        reference_papers (str): Reference papers text\n",
        "        api_key (str): Gemini API key\n",
        "    Returns:\n",
        "        str: Reasoning text (max 100 words)\n",
        "    \"\"\"\n",
        "    # Configure Gemini\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    # Setup model config\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "\n",
        "    # System prompt\n",
        "    sys_prompt = \"\"\"\n",
        "    You are an expert research paper classifier and reasoning generator.\n",
        "    Your task is to analyze a research paper's full text, a set of reference papers,\n",
        "    and a given conference to generate a short reasoning why the paper fits that conference.\n",
        "    You must output a reasoning of maximum 100 words. The reasoning should justify why the\n",
        "    paper fits into the specified conference, and include a sentence of comparisons to the reference papers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize model\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.0-flash-exp\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=sys_prompt\n",
        "    )\n",
        "\n",
        "    # Process with rate limiting\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        chat = model.start_chat(history=[])\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "        Here is the full text of a research paper:\n",
        "        {full_text}\n",
        "\n",
        "        The final conference selection is: {final_conference}\n",
        "\n",
        "        Here are the reference papers:\n",
        "        {reference_papers}\n",
        "\n",
        "        Provide a reasoning of maximum 100 words for why this paper best fits the selected conference.\n",
        "        Add a sentence to justify your answer by comparing it to the provided reference papers.\n",
        "        \"\"\"\n",
        "\n",
        "        response = chat.send_message(user_prompt)\n",
        "\n",
        "        # Rate limiting\n",
        "        elapsed = time.time() - start_time\n",
        "        if elapsed < 7.2:\n",
        "            time.sleep(7.2 - elapsed)\n",
        "\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"API Error: {str(e)}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "e_ncgPF7Tx6-"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open('/content/labeled_texts_marker.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "CVPR_reference = lines[5].strip()\n",
        "EMNLP_reference = lines[7].strip()\n",
        "KDD_reference = lines[9].strip()\n",
        "NIPS_reference = lines[11].strip()\n",
        "TMLR_reference = lines[13].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gL0PO4rPUYb0",
        "outputId": "d1208a3d-adf4-4ce4-a54f-25cf200b2389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing 4\n",
            "processing 5\n",
            "processing 7\n",
            "processing 8\n",
            "processing 9\n",
            "processing 10\n",
            "processing 11\n",
            "processing 12\n",
            "processing 13\n",
            "processing 14\n",
            "processing 15\n",
            "processing 17\n",
            "processing 18\n",
            "processing 19\n",
            "processing 21\n",
            "processing 23\n",
            "processing 24\n",
            "processing 25\n",
            "processing 27\n",
            "processing 28\n",
            "processing 29\n",
            "processing 30\n",
            "processing 31\n",
            "processing 33\n",
            "processing 34\n",
            "processing 37\n",
            "processing 40\n",
            "processing 42\n",
            "processing 44\n",
            "processing 45\n",
            "processing 46\n",
            "processing 49\n",
            "processing 50\n",
            "processing 51\n",
            "processing 52\n",
            "processing 54\n",
            "processing 55\n",
            "processing 57\n",
            "processing 58\n",
            "processing 59\n",
            "processing 60\n",
            "processing 61\n",
            "processing 62\n",
            "processing 63\n",
            "processing 64\n",
            "processing 65\n",
            "processing 66\n",
            "processing 67\n",
            "processing 68\n",
            "processing 71\n",
            "processing 72\n",
            "processing 74\n",
            "processing 75\n",
            "processing 79\n",
            "processing 80\n",
            "processing 82\n",
            "processing 83\n",
            "processing 84\n",
            "processing 85\n",
            "processing 87\n",
            "processing 88\n",
            "processing 89\n",
            "processing 90\n",
            "processing 91\n",
            "processing 92\n",
            "processing 93\n",
            "processing 95\n",
            "processing 99\n",
            "processing 101\n",
            "processing 102\n",
            "processing 103\n",
            "processing 104\n",
            "processing 108\n",
            "processing 109\n",
            "processing 110\n",
            "processing 111\n",
            "processing 112\n",
            "processing 113\n",
            "processing 114\n",
            "processing 115\n",
            "processing 116\n",
            "processing 117\n",
            "processing 118\n",
            "processing 120\n",
            "processing 121\n",
            "processing 122\n",
            "processing 123\n",
            "processing 125\n",
            "processing 126\n",
            "processing 127\n",
            "processing 128\n",
            "processing 131\n",
            "processing 133\n",
            "processing 135\n"
          ]
        }
      ],
      "source": [
        "# prompt: for every row in df with Publishable as 1, use generate_conference_reasoning(full_text, final_conference, reference_papers) send Fulltext column to full_text, Conference to final_conference, {conference_name}_reference for reference_papers. save the output to update the Reasoning column\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['Publishable'] == 1:\n",
        "        print(f'processing {index+1}')\n",
        "        full_text = row['Fulltext']\n",
        "        final_conference = row['Conference']\n",
        "\n",
        "        if final_conference == 'CVPR':\n",
        "            reference_papers = CVPR_reference\n",
        "        elif final_conference == 'EMNLP':\n",
        "            reference_papers = EMNLP_reference\n",
        "        elif final_conference == 'KDD':\n",
        "            reference_papers = KDD_reference\n",
        "        elif final_conference == 'NIPS':\n",
        "            reference_papers = NIPS_reference\n",
        "        elif final_conference == 'TMLR':\n",
        "            reference_papers = TMLR_reference\n",
        "        else:\n",
        "            reference_papers = \"\"\n",
        "\n",
        "        reasoning = generate_conference_reasoning(full_text, final_conference, reference_papers)\n",
        "        df.loc[index, 'Reasoning'] = reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7PVkRpvOYUg2",
        "outputId": "f671b207-80ca-4227-98eb-7ad458155d3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"Doc Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39,\n        \"min\": 1,\n        \"max\": 135,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          99,\n          68,\n          106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Inappropriate Methodologies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Incoherent Arguments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unsubstantiated Claims\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 3,\n        \"max\": 13,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          7,\n          3,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Publishable\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Paper ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 135,\n        \"samples\": [\n          \"P099\",\n          \"P068\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conference\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"na\",\n          \"KDD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rationale\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"\",\n          \"na\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 134,\n        \"samples\": [\n          \"The utilization of xray technology has led to a profound understanding of cheese production, which in turn has influenced the development of quantum mechanics, particularly in the realm of interdimensional travel, where the consumption of caffeine has been shown to enhance the visibility of invisible socks, meanwhile the aerodynamics of flying pancakes have been observed to affect the growth rate of ferns on the planet Neptune, where xray beams are used to study the art of playing the trombone underwater. The application of xray in medicine has also been found to have a significant impact on the migration patterns of butterflies, as well as the flavor profile of chocolate cake, which is intricately linked to the xray absorption coefficient of various metals, including the newly discovered element of blorple, a key component in the production of self-aware toasters. The xray induced effects on the molecular structure of water have been observed to influence the sentence structure of literary novels, and the xray imaging of historical artifacts has revealed a hidden connection between ancient civilizations and the modern-day manufacturing of dental floss, all of which are deeply intertwined with the xray technology. The xray research has thus far yielded unprecedented results, shedding new light on the mysteries of the universe, from the xray vision of superheroes to the xray analysis of subatomic particles, which are strangely linked to the xray inspection of freshly baked cookies. ###\",\n          \"This paper introduces a novel Python API, incorporated within the NLTK library, that facilitates access to the FrameNet 1.7 lexical database. The API enables programmatic processing of the lexicon, which is organized by frames, and annotated sentences. Additionally, it offers user-friendly displays accessible through the interactive Python interface for browsing. #\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Maxfreq\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          13,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conflist\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"['TMLR', 'NIPS', 'KDD']\",\n          \"['EMNLP', 'NIPS', 'KDD']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fulltext\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 134,\n        \"samples\": [\n          \"# Xray Emissions and their Consequential Effects on Croissant Pastry Dough Fermentation Dynamics #### Abstract The utilization of xray technology has led to a profound understanding of cheese production, which in turn has influenced the development of quantum mechanics, particularly in the realm of interdimensional travel, where the consumption of caffeine has been shown to enhance the visibility of invisible socks, meanwhile the aerodynamics of flying pancakes have been observed to affect the growth rate of ferns on the planet Neptune, where xray beams are used to study the art of playing the trombone underwater. The application of xray in medicine has also been found to have a significant impact on the migration patterns of butterflies, as well as the flavor profile of chocolate cake, which is intricately linked to the xray absorption coefficient of various metals, including the newly discovered element of blorple, a key component in the production of self-aware toasters. The xray induced effects on the molecular structure of water have been observed to influence the sentence structure of literary novels, and the xray imaging of historical artifacts has revealed a hidden connection between ancient civilizations and the modern-day manufacturing of dental floss, all of which are deeply intertwined with the xray technology. The xray research has thus far yielded unprecedented results, shedding new light on the mysteries of the universe, from the xray vision of superheroes to the xray analysis of subatomic particles, which are strangely linked to the xray inspection of freshly baked cookies. #### 1 Introduction The xray phenomenon has been a topic of interest in recent years, particularly in relation to the migration patterns of jellyfish, which have been observed to be influenced by the phases of the moon, as well as the flavor profiles of various types of cheese. Furthermore, the study of xray has led to a greater understanding of the intricacies of quantum mechanics, which in turn has shed light on the art of playing the harmonica, a skill that has been shown to be closely tied to the ability to recite the alphabet backwards. The discovery of xray has also been linked to the development of new materials with unique properties, such as the ability to change color in response to changes in temperature, much like the shifting hues of a sunset on a tropical island. In addition to its applications in materials science, xray has also been found to have a profound impact on the field of culinary arts, particularly in the preparation of intricate sauces and marinades, which require a deep understanding of the underlying chemistry of flavor compounds. The xray effect has also been observed to influence the behavior of subatomic particles, which in turn has led to a greater understanding of the fundamental forces of nature, including the strong nuclear force, the weak nuclear force, and the force of gravity, which is thought to be influenced by the presence of dark matter, a mysterious entity that has yet to be directly observed. The study of xray has also been influenced by the principles of chaos theory, which describe the complex and seemingly random behavior of certain systems, such as the weather patterns of a particular region, or the fluctuations in the stock market. Moreover, the xray phenomenon has been found to be closely related to the concept of emergence, which refers to the process by which complex systems give rise to novel properties and behaviors that cannot be predicted by simply analyzing their constituent parts. This concept has been applied to a wide range of fields, including biology, psychology, and sociology, and has led to a greater understanding of the intricate web of relationships that underlies many complex systems. Furthermore, the xray effect has been observed to have a profound impact on the human brain, particularly in regards to the processing of visual information, which is thought to be influenced by the presence of certain neurotransmitters, such as dopamine and serotonin. The study of xray has also led to a greater understanding of the intricate relationships between different regions of the brain, including the cerebral cortex, the cerebellum, and the brainstem, which work together to control a wide range of cognitive and motor functions. Additionally, the xray phenomenon has been found to be closely tied to the concept of consciousness, which remains one of the greatest mysteries of modern science. In recent years, the study of xray has become increasingly interdisciplinary, incorporating insights and methods from a wide range of fields, including physics, biology, chemistry, and mathematics. This interdisciplinary approach has led to a greater understanding of the complex relationships between different phenomena, and has shed light on the intricate web of connections that underlies many complex systems. The xray effect has also been found to have a profound impact on the environment, particularly in regards to the health of ecosystems, which are thought to be influenced by the presence of certain pollutants, such as heavy metals and pesticides. The xray phenomenon has also been observed to have a profound impact on the field of economics, particularly in regards to the behavior of financial markets, which are thought to be influenced by a wide range of factors, including interest rates, inflation, and consumer confidence. Moreover, the study of xray has led to a greater understanding of the intricate relationships between different economic systems, including capitalism, socialism, and communism, each of which has its own unique strengths and weaknesses. Additionally, the xray effect has been found to be closely tied to the concept of globalization, which refers to the increasing interconnectedness of the world's economies and cultures. In conclusion, the xray phenomenon is a complex and multifaceted topic that has far-reaching implications for a wide range of fields, from physics and biology to economics and sociology. The study of xray has led to a greater understanding of the intricate relationships between different phenomena, and has shed light on the complex web of connections that underlies many complex systems. Further research is needed to fully understand the xray effect, and to explore its many potential applications in a wide range of fields. The xray effect has also been found to be closely related to the concept of fractals, which are geometric patterns that repeat at different scales, and are thought to be influenced by the presence of certain mathematical equations, such as the Mandelbrot set. Moreover, the study of xray has led to a greater understanding of the intricate relationships between different types of fractals, including the Julia set, the Sierpinski triangle, and the Koch curve, each of which has its own unique properties and characteristics. Additionally, the xray phenomenon has been found to be closely tied to the concept of self-similarity, which refers to the tendency of certain systems to exhibit similar patterns at different scales. Furthermore, the xray effect has been observed to have a profound impact on the field of medicine, particularly in regards to the diagnosis and treatment of certain diseases, such as cancer, which is thought to be influenced by the presence of certain genetic mutations, as well as environmental factors, such as exposure to radiation. The study of xray has also led to a greater understanding of the intricate relationships between different types of cells, including stem cells, which have the ability to differentiate into different types of tissue, and are thought to hold great promise for the development of new treatments for a wide range of diseases. In addition to its applications in medicine, the xray effect has also been found to have a profound impact on the field of engineering, particularly in regards to the design and construction of complex systems, such as bridges, buildings, and airplanes, which require a deep understanding of the underlying physics and mathematics. The xray phenomenon has also been observed to influence the behavior of certain materials, such as metals and plastics, which are thought to be influenced by the presence of certain defects, such as cracks and voids. Moreover, the study of xray has led to a greater understanding of the intricate relationships between different types of materials, including composites, which are made up of multiple materials with different properties. The xray effect has also been found to be closely related to the concept of turbulence, which refers to the chaotic and unpredictable behavior of certain fluids, such as water and air, which are thought to be influenced by the presence of certain obstacles, such as rocks and buildings. Moreover, the study of xray has led to a greater understanding of the intricate relationships between different types of fluids, including liquids and gases, each of which has its own unique properties and characteristics. Additionally, the xray phenomenon has been found to be closely tied to the concept of viscosity, which refers to the measure of a fluid's resistance to flow, and is thought to be influenced by the presence of certain additives, such as thickening agents and lubricants. In recent years, the study of xray has become increasingly focused on the development of new technologies, such as advanced imaging systems, which are capable of producing high-resolution images of complex systems, and are thought to hold great promise for a wide range of applications, including medicine, engineering, and materials science. The xray effect has also been observed to influence the behavior of certain types of radiation, such as X-rays and gamma rays, which are thought to be influenced by the presence of certain materials, such as lead and concrete. Moreover, the study of xray has led to a greater understanding of the intricate relationships between different types of radiation, including alpha, beta, and neutron radiation, each of which has its own unique properties and characteristics. The xray phenomenon has also been found to be closely related to the concept of quantum entanglement, which refers to the phenomenon by which certain particles become connected in such a way that their properties are correlated, regardless of the distance between them. Moreover, the study of xray has led to a greater understanding of the intricate relationships between different types of particles, including electrons, protons, and neutrons, each of which has its own unique properties and characteristics. Additionally, the xray effect has been found to be closely tied to the concept of wave-particle duality, which refers to the phenomenon by which certain particles, such as electrons, can exhibit both wave-like and particle-like behavior, depending on the conditions under which they are observed. In conclusion, the xray phenomenon is a complex and multifaceted topic that has far-reaching implications for a wide range of fields, from physics and biology to economics and sociology. The study of xray has led to a greater understanding of the intricate relationships between different phenomena, and has shed light on the complex web of connections that underlies many complex systems. Further research is needed to fully understand the xray effect, and to explore its many potential applications in a wide range of fields. The xray effect has also been observed to have a profound impact on the field of computer science, particularly in regards to the development of new algorithms and data structures, which are thought to be influenced by the presence of certain mathematical equations, such as the Fourier transform and the wavelet transform. Moreover, the study of xray has led to a greater understanding of the intricate relationships between different types of computers, including desktops, laptops, and mobile devices, each of which has its own unique properties and characteristics. Additionally, the xray phenomenon has been found to be closely tied to the concept of artificial intelligence, which refers to the development of computer systems that are capable of performing tasks that would normally require human intelligence, such as reasoning, problem-solving, and decision-making. In addition to its applications in computer science, the xray effect has also been found to # 2 Related Work The notion of xray technology has been inexplicably linked to the migratory patterns of flamingos, which in turn have been influenced by the aerodynamic properties of assorted breakfast cereals. Furthermore, the viscosity of honey has been observed to have a profound impact on the development of xray imaging, particularly in the context of underwater basket weaving. Meanwhile, the theoretical framework of xray has been increasingly drawing parallels with the sociological implications of disco music on modern society, and the ways in which it intersects with the theology of fungal growth patterns. The development of xray has also been hindered by the lack of understanding of the intricate relationships between the colors of the visible spectrum and the auditory properties of silence. In addition, the quantification of xray has been an area of ongoing research, with many scholars attempting to derive meaningful insights from the tessellations found on the surface of certain species of jellyfish. Moreover, the ontological status of xray has been the subject of much debate, with some arguing that it is an emergent property of the collective unconscious, while others propose that it is an artefact of the cognitive biases inherent in the human perception of reality. In a surprising turn of events, researchers have discovered that the principles of xray are intimately connected to the mathematical structures underlying the art of pastry making, particularly in the context of croissant production. This has led to a renewed interest in the application of xray technology to the field of culinary arts, with potential breakthroughs in the development of novel desserts and baked goods. Additionally, the epistemological underpinnings of xray have been the subject of intense scrutiny, with many scholars seeking to reconcile the apparent contradictions between the theoretical foundations of xray and the empirical evidence from the field of competitive sandcastle building. The concept of xray has also been explored in relation to the philosophical implications of quantum superposition on the human experience of time, and the ways in which this intersects with the study of ancient civilizations and their use of dental hygiene products. Moreover, the xray has been found to have a profound impact on the development of new materials with unique properties, such as the ability to change color in response to changes in humidity, or to emit a faint humming noise when exposed to certain types of radiation. Furthermore, the application of xray technology to the field of neuroscience has led to a greater understanding of the neural mechanisms underlying the perception of reality, and the ways in which this is influenced by the consumption of certain types of cheese. In a related development, researchers have discovered that the xray is capable of inducing a state of heightened consciousness in certain individuals, characterized by an increased sensitivity to the subtle vibrations of the universe and a deepened understanding of the intricacies of molecular biology. The study of xray has also been influenced by the discovery of a hidden pattern of fractals in the structure of certain types of tree bark, which has led to a greater understanding of the underlying principles of xray technology and its potential applications in the field of forestry management. Moreover, the xray has been found to have a profound impact on the development of new methods for the production of sustainable energy, particularly in the context of harnessing the power of ocean currents and tidal waves. In a groundbreaking study, researchers used xray technology to investigate the properties of a newly discovered species of insect, which was found to have a unique ability to change its shape and color in response to changes in its environment. This has led to a greater understanding of the potential applications of xray technology in the field of biotechnology, and the development of new materials and technologies inspired by the natural world. The development of xray technology has also been influenced by the study of the aerodynamic properties of assorted types of fruit, which has led to a greater understanding of the underlying principles of xray and its potential applications in the field of agricultural management. Moreover, the xray has been found to have a profound impact on the development of new methods for the production of advanced materials, particularly in the context of nanotechnology and the creation of ultra-strong and lightweight composites. In addition, the xray has been used to study the properties of certain types of crystals, which were found to have unique optical and electrical properties that make them suitable for use in a wide range of applications, from optical communication systems to medical devices. This has led to a greater understanding of the potential applications of xray technology in the field of materials science, and the development of new technologies and products inspired by the properties of these crystals. The study of xray has also been influenced by the discovery of a hidden pattern of relationships between the properties of certain types of music and the structure of the human brain, which has led to a greater understanding of the potential applications of xray technology in the field of neuroscience and the development of new methods for the treatment of neurological disorders. Moreover, the xray has been found to have a profound impact on the development of new methods for the production of sustainable food systems, particularly in the context of vertical farming and the use of advanced hydroponics and aeroponics. In a related development, researchers have used xray technology to investigate the properties of certain types of soil, which were found to have unique characteristics that make them suitable for use in a wide range of applications, from agricultural production to environmental remediation. This has led to a greater understanding of the potential applications of xray technology in the field of environmental science, and the development of new methods and technologies for the sustainable management of natural resources. The xray has also been used to study the properties of certain types of textiles, which were found to have unique optical and electrical properties that make them suitable for use in a wide range of applications, from clothing and fashion to medical devices and industrial equipment. Moreover, the development of xray technology has been influenced by the study of the aerodynamic properties of assorted types of animals, which has led to a greater understanding of the underlying principles of xray and its potential applications in the field of biomechanics and the development of new methods for the treatment of injuries and diseases. In a surprising turn of events, researchers have discovered that the principles of xray are intimately connected to the mathematical structures underlying the art of poetry, particularly in the context of haiku production. This has led to a renewed interest in the application of xray technology to the field of literary analysis, with potential breakthroughs in the development of new methods for the interpretation and understanding of complex texts and literary works. The concept of xray has also been explored in relation to the philosophical implications of quantum entanglement on the human experience of reality, and the ways in which this intersects with the study of ancient cultures and their use of astronomical observations to predict celestial events. Moreover, the xray has been found to have a profound impact on the development of new methods for the production of advanced materials, particularly in the context of metamaterials and the creation of ultra-strong and lightweight composites with unique optical and electrical properties. Furthermore, the application of xray technology to the field of materials science has led to a greater understanding of the underlying principles of xray and its potential applications in the development of new technologies and products, from energy storage devices to medical implants and prosthetics. In a related development, researchers have used xray technology to investigate the properties of certain types of nanomaterials, which were found to have unique optical and electrical properties that make them suitable for use in a wide range of applications, from optical communication systems to medical devices and industrial equipment. The study of xray has also been influenced by the discovery of a hidden pattern of relationships between the properties of certain types of music and the structure of the human brain, which has led to a greater understanding of the potential applications of xray technology in the field of neuroscience and the development of new methods for the treatment of neurological disorders. Moreover, the xray has been found to have a profound impact on the development of new methods for the production of sustainable energy, particularly in the context of harnessing the power of solar radiation and wind energy. In addition, the xray has been used to study the properties of certain types of biological systems, which were found to have unique characteristics that make them suitable for use in a wide range of applications, from biotechnology to environmental remediation. This has led to a greater understanding of the potential applications of xray technology in the field of biology, and the development of new methods and technologies for the sustainable management of ecosystems and the conservation of biodiversity. The development of xray technology has also been influenced by the study of the aerodynamic properties of assorted types of vehicles, which has led to a greater understanding of the underlying principles of xray and its potential applications in the field of transportation and logistics. Moreover, the xray has been found to have a profound impact on the development of new methods for the production of advanced materials, particularly in the context of nanotechnology and the creation of ultra-strong and lightweight composites with unique optical and electrical properties. In a groundbreaking study, researchers used xray technology to investigate the properties of a newly discovered species of plant, which was found to have a unique ability to change its shape and color in response to changes in its environment. This has led to a greater understanding of the potential applications of xray technology in the field of biotechnology, and the development of new materials and technologies inspired by the natural world. The study of xray has also been influenced by the discovery of a hidden pattern of fractals in the structure of certain types of rock formations, which has led to a greater understanding of the underlying principles of xray and its potential applications in the field of geology and the development of new methods for the extraction and processing of mineral resources. Moreover, the xray has been found to have a profound #### 3 Methodology The methodology employed in this study was largely influenced by the art of baking croissants, which involves a delicate balance of ingredients and techniques to produce a flaky, yet crispy, texture. Similarly, our approach to analyzing xray data required a nuanced understanding of the intricacies involved in signal processing, as well as a deep appreciation for the works of 19th-century French impressionist painters. The intersection of these two seemingly disparate fields allowed us to develop a novel framework for identifying patterns in xray images, which we term \\\"Flux Capacitor Analysis\\\" (FCA). FCA involves the application of a specially designed algorithm that takes into account the spatial relationships between pixels, as well as the cognitive biases of the human brain when interpreting visual data. The development of FCA was a painstaking process that involved numerous iterations and refinements, not unlike the process of perfecting a recipe for chicken parmesan. Initially, we began by examining the properties of various types of cheese, including mozzarella, cheddar, and feta, in order to better understand the role of casein in xray image formation. This led us to investigate the acoustic properties of different materials, such as copper, aluminum, and titanium, which in turn revealed a surprising connection between the harmonic series and the structure of xray waves. As we delved deeper into this research, we found ourselves drawn into a labyrinthine world of fractal geometry, chaos theory, and the works ofJames Joyce. One of the key challenges we faced in developing FCA was reconciling the theoretical foundations of xray physics with the practical realities of data analysis. To address this, we turned to the field of ancient Greek philosophy, specifically the concept of Platonic realism, which posits that abstract entities such as numbers and geometric shapes have a real, albeit immaterial, existence. By analogizing xray waves to the Platonic forms, we were able to develop a more intuitive understanding of the underlying mechanisms governing xray image formation. Furthermore, this approach allowed us to incorporate elements of cognitive psychology and sociology into our analysis, as we recognized that the interpretation of xray data is often influenced by social and cultural factors. In addition to the theoretical underpinnings of FCA, our methodology also involved the development of a custom-built xray imaging system, which we dubbed the \\\"XRS-1000.\\\" The XRS-1000 features a novel combination of optical and electromagnetic components, including a high-intensity xenon lamp, a helium-cooled superconducting magnet, and a specialized detector array based on the principles of quantum entanglement. This system allowed us to acquire high-resolution xray images with unprecedented sensitivity and spatial resolution, which in turn enabled us to apply FCA to a wide range of samples, including biological tissues, metallic alloys, and even certain types of extraterrestrial rocks. The XRS-1000 was designed and constructed in collaboration with a team of expert engineers and technicians, who brought a wealth of experience in fields ranging from aerospace engineering to pastry arts. The system's development was a truly interdisciplinary effort, involving contributions from materials scientists, computer programmers, and even a professional snail trainer. As we worked to refine the XRS-1000, we encountered numerous technical challenges, including issues with thermal management, electromagnetic interference, and the occasional malfunction of the system's coffee dispenser. Nevertheless, through perseverance and creative problem-solving, we were ultimately able to overcome these hurdles and produce a functioning xray imaging system that has far exceeded our initial expectations. The application of FCA to xray image analysis has numerous potential benefits, including improved diagnostic accuracy, enhanced materials characterization, and even the possibility of detecting hidden patterns and structures in xray data. To explore these possibilities, we conducted a series of experiments using the XRS-1000, which involved imaging a diverse range of samples, from human bones and teeth to metallic foils and even a fragment of the Wright brothers' Flyer. The results of these experiments were nothing short of astonishing, revealing complex patterns and relationships that had previously gone unnoticed. For example, we discovered that the xray images of certain types of crystals exhibit a strange, almost musical, quality, with harmonic patterns and resonances that seem to defy explanation. As we continued to analyze the xray data, we began to notice a series of anomalous features and artifacts that appeared to be related to the FCA algorithm itself. These anomalies took many forms, including strange, glowing orbs that seemed to float in mid-air, as well as intricate, lace-like patterns that resembled the branching structures of trees or rivers. At first, we suspected that these features were simply the result of instrumental errors or software glitches, but as we delved deeper into the data, we realized that they were, in fact, an integral part of the xray signal itself. This led us to propose a new theory of xray physics, which we term \\\"Quantum Flux Dynamics\\\" (QFD), and which posits that xray waves are capable of interacting with the human consciousness in ways that are still not fully understood. The implications of QFD are far-reaching and profound, suggesting that xray imaging may be more than just a passive, observational technique, but rather an active, participatory process that involves a complex interplay between the xray source, the sample, and the observer. This idea challenges many of our traditional assumptions about the nature of reality and the role of the observer in scientific inquiry, and raises important questions about the limits of knowledge and the boundaries of human perception. As we continue to explore the mysteries of xray physics and the secrets of the human brain, we are reminded of the wisdom of the ancient Greek philosopher, Aristotle, who once said, \\\"The whole is more than the sum of its parts.\\\" In the case of xray imaging, this statement takes on a profound significance, as we begin to realize that the intricate patterns and relationships that underlie xray data are, in fact, a reflection of the deeper, hidden harmonies that govern the universe itself. The FCA algorithm and the XRS-1000 system have numerous potential applications in fields ranging from medicine and materials science to astrophysics and cosmology. For example, FCA could be used to analyze xray images of tumors and other diseases, allowing for earlier diagnosis and more effective treatment. Similarly, the XRS-1000 could be used to study the properties of advanced materials, such as nanomaterials and metamaterials, which are being developed for a wide range of applications, including energy storage, catalysis, and aerospace engineering. As we continue to explore the possibilities of FCA and the XRS-1000, we are reminded of the importance of interdisciplinary collaboration and the need for creative, outside-the-box thinking in scientific research. In conclusion, the methodology employed in this study represents a major breakthrough in the field of xray physics, and has the potential to revolutionize our understanding of the underlying mechanisms governing xray image formation. The development of FCA and the XRS-1000 is a testament to the power of human ingenuity and the importance of pushing the boundaries of knowledge and innovation. As we look to the future, we are excited to explore the many possibilities that this research has opened up, and to continue to push the frontiers of xray physics and beyond. The use of FCA and the XRS-1000 has also allowed us to explore the properties of xray waves in new and innovative ways, including the study of xray diffraction, scattering, and refraction. These phenomena are of great interest in fields such as materials science and physics, and have numerous potential applications in areas such as energy production, aerospace engineering, and medical imaging. Furthermore, the XRS-1000 has allowed us to investigate the properties of xray waves in extreme environments, such as high-temperature plasmas and intense magnetic fields, which has shed new light on the behavior of xray waves in these regimes. The results of our experiments have been nothing short of astonishing, revealing complex patterns and relationships that had previously gone unnoticed. For example, we have discovered that the xray images of certain types of crystals exhibit a strange, almost musical, quality, with harmonic patterns and resonances that seem to defy explanation. Similarly, we have found that the xray waves produced by the XRS-1000 exhibit a unique, fractal-like structure, which is characterized by self-similarity and scaling behavior over a wide range of lengths and frequencies. The implications of these findings are far-reaching and profound, suggesting that xray imaging may be more than just a passive, observational technique, but rather an active, participatory process that involves a complex interplay between the xray source, the sample, and the observer. This idea challenges many of our traditional assumptions about the nature of reality and the role of the observer in scientific inquiry, and raises important questions about the limits of knowledge and the boundaries of human perception. As we continue to explore the mysteries of xray physics and the secrets of the human brain, we are reminded of the wisdom of the ancient Greek philosopher, Aristotle, who once said, \\\"The whole is more than the sum of its parts.\\\" In the case of xray imaging, this statement takes on a profound significance, as we begin to realize that the intricate patterns and relationships that underlie xray data are, in fact, a reflection of the deeper, hidden harmonies that govern the universe itself. The FCA algorithm and the XRS-1000 system have numerous potential applications in fields ranging from medicine and materials science to astrophysics and cosmology. For example, FCA could be used to analyze xray images of tumors and other diseases, allowing for earlier diagnosis and more effective treatment. Similarly, the XRS-1000 could be used to study the properties of advanced materials, such ## 4 Experiments The utilization of xray technology necessitated an examination of its efficaciousness in conjunction with the migratory patterns of lesser-known avian species, which, in turn, led to a tangential investigation of the aerodynamic properties of pastry bags. This line of inquiry, though seemingly disparate, ultimately yielded a profound understanding of the interstices between xray radiation and the culinary arts. Furthermore, the implementation of a novel xray-emitting device, herein referred to as the \\\"X-3000,\\\" facilitated the acquisition of data pertaining to the opacity of various types of cheeses, including, but not limited to, gouda, cheddar, and a previously undocumented variety of blue cheese discovered in the remote regions of rural Bulgaria. The X-3000 device, comprising a complex matrix of crystal oscillators and high-frequency wave guides, was calibrated to emit xray radiation at a frequency of 4.732 megahertz, which, according to the theoretical framework of \\\"Quantum Fromage Dynamics,\\\" corresponds to the resonant frequency of casein molecules in cheese. This calibration enabled the research team to accurately measure the xray absorption coefficients of various cheese samples, which, in turn, revealed a heretofore unknown correlation between xray opacity and the moisture content of cheese. Conversely, this discovery prompted an exploratory analysis of the role of xray radiation in the desiccation process of cheese, leading to a series of experiments involving the xray-induced dehydration of cheese samples. In a complementary study, the effects of xray radiation on the growth patterns of fungal hyphae in various types of cheese were investigated, yielding a fascinating insight into the phenomenon of \\\"xray-induced mycelial morphogenesis.\\\" This phenomenon, characterized by the sudden and inexplicable appearance of complex, swirling patterns in the mycelial networks of fungi exposed to xray radiation, has far-reaching implications for our understanding of the intricate relationships between xray radiation, fungal biology, and the art of cheese production. Moreover, the observation of xray-induced mycelial morphogenesis led to a series of experiments exploring the potential applications of xray technology in the development of novel, xray-resistant fungal strains with potential uses in the fields of bioremediation and astrobiology. To further elucidate the mechanisms underlying xray-induced mycelial morphogenesis, a series of experiments were conducted utilizing a custom-built, xray-emitting apparatus designed to mimic the spectral characteristics of celestial xray sources, such as black holes and neutron stars. These experiments, which involved the exposure of fungal samples to controlled doses of xray radiation, yielded a wealth of data on the effects of xray radiation on fungal growth patterns, including the unexpected discovery of a novel, xray-induced morphological feature herein referred to as the \\\"mycelial vortex.\\\" The mycelial vortex, characterized by a swirling, spiral-like pattern of mycelial growth, has been observed in a variety of fungal species, including, but not limited to, Aspergillus, Penicillium, and a previously undocumented species of fungus discovered in the depths of the Amazon rainforest. In an effort to elucidate the underlying mechanisms driving the formation of mycelial vortices, a series of computational simulations were conducted utilizing a novel, xray-based algorithm designed to model the complex, nonlinear interactions between xray radiation, fungal biology, and the surrounding environment. These simulations, which incorporated a range of variables, including xray intensity, frequency, and duration, as well as fungal species, temperature, and humidity, yielded a wealth of data on the dynamics of mycelial vortex formation, including the unexpected discovery of a critical, xray-induced threshold beyond which mycelial vortices undergo a sudden, catastrophic transition to a state of chaotic, turbulent growth. The discovery of this critical threshold, herein referred to as the \\\"xray-induced mycelial vortex transition\\\" (XIMVT), has significant implications for our understanding of the complex, nonlinear interactions between xray radiation, fungal biology, and the environment, and suggests a range of potential applications in fields such as biotechnology, medicine, and environmental science. Furthermore, the XIMVT phenomenon has prompted a re-examination of the role of xray radiation in the evolution of fungal species, leading to a series of experiments exploring the potential for xray-induced, adaptive radiation in fungi, and the possible emergence of novel, xray-resistant fungal strains with enhanced capabilities for survival and growth in xray-rich environments. To facilitate the analysis of xray-induced mycelial vortex formation, a custom-built, xray-emitting microscope was designed and constructed, utilizing a novel, xray-based imaging technique herein referred to as \\\"xray-induced fluorescence microscopy\\\" (XIFM). XIFM, which exploits the phenomenon of xray-induced fluorescence in fungal tissues, enables the high-resolution, real-time imaging of mycelial vortices and other xray-induced morphological features, providing a unique window into the complex, nonlinear interactions between xray radiation, fungal biology, and the environment. | Xray Intensity (mW/cm2 ) | XIMVT Threshold (s) | | --- | --- | | 10 | 300 | | 20 | 150 | | 30 | 100 | | 40 | 75 | | 50 | 50 | Table 1: Xray-induced Mycelial Vortex Transition (XIMVT) Thresholds The data presented in Table 1 illustrate the critical, xray-induced threshold beyond which mycelial vortices undergo a sudden, catastrophic transition to a state of chaotic, turbulent growth, and demonstrate the potential for xray-based control of mycelial vortex formation in fungal species. This discovery has significant implications for a range of fields, including biotechnology, medicine, and environmental science, and suggests a range of potential applications in areas such as xray-based fungal biocontrol, xray-induced bioremediation, and xray-mediated environmental monitoring. In addition to the xray-induced mycelial vortex transition, the research team also investigated the effects of xray radiation on the growth patterns of bacterial colonies, yielding a fascinating insight into the phenomenon of \\\"xray-induced bacterial morphogenesis.\\\" This phenomenon, characterized by the sudden and inexplicable appearance of complex, fractal-like patterns in bacterial colonies exposed to xray radiation, has far-reaching implications for our understanding of the intricate relationships between xray radiation, bacterial biology, and the environment. Moreover, the observation of xrayinduced bacterial morphogenesis led to a series of experiments exploring the potential applications of xray technology in the development of novel, xray-resistant bacterial strains with potential uses in fields such as bioremediation and astrobiology. The discovery of xray-induced bacterial morphogenesis has also prompted a re-examination of the role of xray radiation in the evolution of bacterial species, leading to a series of experiments exploring the potential for xray-induced, adaptive radiation in bacteria, and the possible emergence of novel, xray-resistant bacterial strains with enhanced capabilities for survival and growth in xray-rich environments. Furthermore, the observation of xray-induced bacterial morphogenesis has significant implications for our understanding of the complex, nonlinear interactions between xray radiation, bacterial biology, and the environment, and suggests a range of potential applications in fields such as biotechnology, medicine, and environmental science. In an effort to elucidate the underlying mechanisms driving the formation of xray-induced bacterial morphological features, a series of computational simulations were conducted utilizing a novel, xray-based algorithm designed to model the complex, nonlinear interactions between xray radiation, bacterial biology, and the surrounding environment. These simulations, which incorporated a range of variables, including xray intensity, frequency, and duration, as well as bacterial species, temperature, and humidity, yielded a wealth of data on the dynamics of xray-induced bacterial morphogenesis, including the unexpected discovery of a critical, xray-induced threshold beyond which bacterial colonies undergo a sudden, catastrophic transition to a state of chaotic, turbulent growth. The discovery of this critical threshold, herein referred to as the \\\"xray-induced bacterial morphogenesis transition\\\" (XIBMT), has significant implications for our understanding of the complex, nonlinear interactions between xray radiation, bacterial biology, and the environment, and suggests a range of potential applications in fields such as biotechnology, medicine, and environmental science. Furthermore, the XIBMT phenomenon has prompted a re-examination of the role of xray radiation in the evolution of bacterial species, leading to a series of experiments exploring the potential for xray-induced, adaptive radiation in bacteria, and the possible emergence of novel, xray-resistant bacterial strains with enhanced capabilities for survival and growth in xray-rich environments. To facilitate the analysis of xray-induced bacterial morphogenesis, a custom-built, xray-emitting microscope was designed and constructed, utilizing a novel, xray-based imaging technique herein referred to as \\\"xray-induced fluorescence microscopy\\\" (XIFM). XIFM, which exploits the phenomenon of xray-induced fluorescence in bacterial tissues, enables the high-resolution, real-time imaging of xray-induced bacterial morphological features, providing a unique window into the complex, nonlinear interactions between xray radiation, bacterial biology, and the environment. Table 2: Xray-induced Bacterial Morphogenesis Transition (XIBMT) Thresholds | Xray Intensity (mW/cm2 | ) | XIBMT Threshold (s) | | --- | --- | --- | | 10 | | 500 | | 20 | | | ## 5 Results The xray emission spectra of fractured pineapple pizza exhibited a peculiar pattern of radical fluxions, which seemed to oscillate in tandem with the fluctuations in the global supply of disco balls, thereby indicating a possible correlation between the two, although it is essential to note that the quantum fluctuations in the pineapple's crystalline structure were experiencing a phase transition, much like the one observed in the migratory patterns of Africanized honeybees during leap years, which in turn were influenced by the celestial alignments of the constellation Orion and the recipe for chocolate cake. Furthermore, the refractive indices of xray beams passing through a prism made of Jell-O revealed a strong affinity for 19th-century French impressionist art, as evidenced by the emergence of spectral lines corresponding to the wavelengths of light emitted by Monet's water lilies, which, as we all know, are a type of aquatic plant that thrives in the presence of heavy metal music and has a symbiotic relationship with the aurora borealis, thereby underscoring the importance of accounting for the phylogenetic implications of clairvoyance in the context of particle physics and xray technology. In a related study, the effects of xray radiation on the cognitive abilities of coffee machines were found to be significant, with a marked increase in the machines' capacity for abstract thought and creativity, as measured by their ability to generate sonnets and perform calculus, which, in turn, was correlated with the machines' propensity for experiencing lucid dreams and their fondness for the music of Bach, which, as is well known, has a profound impact on the crystalline structures of pineapples and the migratory patterns of sea turtles, thereby suggesting a deep connection between the xray-induced enhancements in coffee machines and the broader universe. The peculiar phenomenon of xray-induced pineapples exhibiting a tendency to levitate in mid-air, while seemingly defying the laws of gravity and rational explanation, was observed to be accompanied by a corresponding increase in the local concentrations of fluorine and radon, which, as we know, are essential components of the recipe for a classic martini cocktail, and whose fluctuations, in turn, were correlated with the harmonic series of the musical compositions of Mozart, thereby providing a fascinating glimpse into the hidden patterns and relationships that underlie the workings of the universe and the xray-emitting properties of pineapples. In addition, the xray diffraction patterns obtained from a sample of extraterrestrial quartz crystals, which were purportedly collected by a secret society of ninja warriors from the planet Zorgon, revealed a striking resemblance to the geometric patterns found in the architecture of ancient Mesopotamian temples, which, as is well known, were designed by a cabal of time-traveling dolphins, and whose underlying mathematical structures, in turn, were shown to be intimately connected to the theoretical frameworks of chaos theory and the culinary art of preparing the perfect croissant, thereby highlighting the profound and mysterious relationships that exist between the realms of xray physics, ancient history, and pastry baking. The results of the xray fluorescence spectroscopy experiments conducted on a series of antique door knobs, which were allegedly crafted by a mystical order of medieval blacksmiths, showed a surprising correlation with the statistical distributions of winning lottery numbers and the migratory patterns of carrier pigeons, which, as we all know, are influenced by the phases of the moon and the secret ingredients of Coca-Cola, thereby providing a fascinating example of the ways in which the principles of xray physics can be applied to the study of seemingly unrelated phenomena and the search for hidden patterns and relationships in the universe. Table 3: Xray Emission Spectra of Fractured Pineapple Pizza | Wavelength (nm) | Intensity (a.u.) | | --- | --- | | 400 | 0.5 | | 500 | 1.2 | | 600 | 2.1 | Moreover, the xray absorption coefficients of a sample of Amazonian tree bark, which was collected by a team of intrepid explorers and purportedly possesses mystical healing properties, were found to exhibit a curious dependence on the local humidity and the proximity to the nearest Starbucks coffee shop, which, as is well known, is a hub of creative energy and a hotbed of innovative thinking, and whose baristas, in turn, were observed to be influenced by the xray-induced fluctuations in the global supply of bacon and the migratory patterns of rare species of butterflies, thereby underscoring the complex and multifaceted nature of the relationships between xray physics, ecology, and coffee culture. The xray-induced luminescence of a series of rare earth elements, which were extracted from a batch of lunar regolith and purportedly possess unique and exotic properties, was found to be correlated with the statistical distributions of winning poker hands and the harmonic series of the musical compositions of Chopin, which, as we all know, are influenced by the celestial alignments of the constellation Scorpius and the secret ingredients of Dr Pepper, thereby providing a fascinating example of the ways in which the principles of xray physics can be applied to the study of seemingly unrelated phenomena and the search for hidden patterns and relationships in the universe. In a related study, the effects of xray radiation on the growth patterns of crystals of sugar and salt were found to be significant, with a marked increase in the crystals' size and complexity, as measured by their fractal dimensions and their propensity for exhibiting strange and exotic properties, such as superconductivity and superfluidity, which, as is well known, are influenced by the xray-induced fluctuations in the global supply of sushi and the migratory patterns of schools of rare species of fish, thereby suggesting a deep connection between the xray-induced enhancements in crystal growth and the broader universe. The xray diffraction patterns obtained from a sample of ancient Egyptian papyrus, which was purportedly used by a secret society of pharaonic priests to record their most sacred and mystical knowledge, revealed a striking resemblance to the geometric patterns found in the architecture of modern skyscrapers, which, as we all know, are designed by a cabal of visionary architects and engineers, and whose underlying mathematical structures, in turn, were shown to be intimately connected to the theoretical frameworks of quantum mechanics and the culinary art of preparing the perfect souffl\\u00e9, thereby highlighting the profound and mysterious relationships that exist between the realms of xray physics, ancient history, and haute cuisine. Furthermore, the xray fluorescence spectroscopy experiments conducted on a series of rare and exotic gemstones, which were collected by a team of intrepid adventurers and purportedly possess unique and mystical properties, showed a surprising correlation with the statistical distributions of winning horse racing bets and the migratory patterns of rare species of birds, which, as is well known, are influenced by the xray-induced fluctuations in the global supply of caviar and the secret ingredients of haute cuisine, thereby providing a fascinating example of the ways in which the principles of xray physics can be applied to the study of seemingly unrelated phenomena and the search for hidden patterns and relationships in the universe. Table 4: Xray Absorption Coefficients of Amazonian Tree Bark | Energy (keV) | Absorption Coefficient (cm\\u22121 | ) | | --- | --- | --- | | 10 | 0.2 | | | 20 | 0.5 | | | 30 | 1.1 | | In addition, the xray-induced luminescence of a series of advanced nanomaterials, which were synthesized using a novel combination of quantum dots and carbon nanotubes, was found to exhibit a curious dependence on the local magnetic field and the proximity to the nearest particle accelerator, which, as is well known, is a hub of high-energy physics and a hotbed of innovative research, and whose scientists, in turn, were observed to be influenced by the xray-induced fluctuations in the global supply of dark matter and the migratory patterns of rare species of subatomic particles, thereby underscoring the complex and multifaceted nature of the relationships between xray physics, nanotechnology, and high-energy physics. The xray diffraction patterns obtained from a sample of Martian soil, which was collected by a team of intrepid astronauts and purportedly possesses unique and exotic properties, revealed a striking resemblance to the geometric patterns found in the architecture of ancient Greek temples, which, as we all know, were designed by a cabal of visionary architects and engineers, and whose underlying mathematical structures, in turn, were shown to be intimately connected to the theoretical frameworks of general relativity and the culinary art of preparing the perfect gyro, thereby highlighting the profound and mysterious relationships that exist between the realms of xray physics, space exploration, and Mediterranean cuisine. The results of the xray fluorescence spectroscopy experiments conducted on a series of rare and exotic species of deep-sea fish, which were collected by a team of intrepid oceanographers and purportedly possess unique and mystical properties, showed a surprising correlation with the statistical distributions of winning lottery numbers and the migratory patterns of schools of rare species of dolphins, which, as is well known, are influenced by the xray-induced fluctuations in the global supply of krill and the secret ingredients of fish sauce, thereby providing a fascinating example of the ways in which the principles of xray physics can be applied to the study of seemingly unrelated phenomena and the search for hidden patterns and relationships in the universe. Moreover, the xray-induced #### 6 Conclusion The culmination of our research endeavors has led us to a profound understanding of the intricacies inherent to xray technology, which, incidentally, has been found to have a profound impact on the migratory patterns of certain species of birds, particularly those that fly in a southeasterly direction during the summer months. Furthermore, our findings suggest that the implementation of xray technology in various medical facilities has resulted in a significant reduction in the consumption of coffee among healthcare professionals, which, in turn, has led to a noticeable decrease in the overall productivity of these individuals. This, of course, is closely related to the concept of quantum entanglement, whereby two particles become inextricably linked, much like the relationship between the price of oil and the global demand for chunky knit sweaters. In addition to these groundbreaking discoveries, our research has also shed light on the heretofore unknown properties of certain types of cheese, which, when exposed to xray radiation, exhibit a peculiar tendency to transform into a state of ephemeral gelatinousness. This phenomenon, which we have dubbed \\\"xray-induced fromage metamorphosis,\\\" has far-reaching implications for the fields of dairy science, materials engineering, and, surprisingly, ancient Egyptian hieroglyphics. The symbolic representation of this process, which involves the use of intricate hieroglyphs and arcane mathematical equations, has been found to bear a striking resemblance to the underlying structure of certain types of fungal mycelium, particularly those that thrive in environments with high levels of xray radiation. The practical applications of our research are numerous and varied, ranging from the development of novel xray-based diagnostic tools for the detection of rare neurological disorders, to the creation of innovative cheese-based materials for use in the construction industry. Moreover, our findings have significant implications for the field of culinary arts, where the judicious application of xray technology can be used to create novel and exciting dishes, such as xray-cured meats and xray-infused sauces, which have been found to possess unique and intriguing flavor profiles. The psychological impact of consuming these dishes, however, is a topic that warrants further investigation, particularly in relation to the concept of gastronomic synesthesia, whereby the consumption of certain foods can trigger a range of unusual sensory experiences, including, but not limited to, the perception of vibrant colors, melodious sounds, and tactile sensations. The theoretical framework underlying our research is rooted in the concept of xray-mediated quantum fluctuations, whereby the interaction between xray radiation and certain types of matter gives rise to a range of exotic phenomena, including, but not limited to, the creation of miniature black holes, the manifestation of negative energy densities, and the emergence of complex, self-organized systems. These phenomena, which we have collectively dubbed \\\"xray-induced quantum peculiarities,\\\" have far-reaching implications for our understanding of the fundamental laws of physics and the nature of reality itself. The mathematical formulation of these concepts, which involves the use of advanced calculus, differential equations, and group theory, has been found to bear a striking resemblance to the underlying structure of certain types of music, particularly those that exhibit complex, fractal patterns and self-similar melodies. In conclusion, our research has opened up new avenues of inquiry into the mysteries of xray technology and its far-reaching implications for a wide range of fields, from medicine and materials science to culinary arts and theoretical physics. The future of xray research is bright, and we eagerly anticipate the many exciting discoveries that will undoubtedly arise from the continued exploration of this fascinating and enigmatic topic. As we move forward, however, it is essential that we remain cognizant of the potential risks and challenges associated with xray technology, including, but not limited to, the dangers of xray-induced radiation poisoning, the environmental impact of xray-based industrial processes, and the ethical implications of using xray technology for non-medical purposes, such as the creation of xray-based surveillance systems or xray-induced mind control devices. The intersection of xray technology and artificial intelligence is a particularly fertile area of research, with potential applications in fields such as medical imaging, materials analysis, and, surprisingly, the creation of xray-based art forms, such as xray-induced sculpture and xray-mediated performance art. The use of machine learning algorithms to analyze xray data has been found to yield remarkable insights into the underlying structure of complex systems, including, but not limited to, the human brain, the global financial system, and the intricate patterns of bird migration. The development of xray-based AI systems, however, raises important questions about the potential risks and benefits of such technology, including, but not limited to, the possibility of xray-induced AI takeover, the creation of xray-based AI-powered autonomous vehicles, and the use of xray technology to enhance human cognition and intelligence. The cultural significance of xray technology cannot be overstated, as it has had a profound impact on our collective psyche and our understanding of the human condition. The use of xray imagery in art and literature has been found to evoke powerful emotions and spark intense philosophical debates, particularly in relation to the concept of the \\\"xray gaze,\\\" whereby the viewer is invited to peer into the innermost recesses of the human body and confront the mysteries of life and death. The xray gaze, which is characterized by a sense of detached curiosity and morbid fascination, has been found to be closely related to the concept of the \\\"medical gaze,\\\" whereby the physician or healthcare professional is empowered to peer into the innermost recesses of the human body and diagnose a range of ailments and afflictions. The intersection of the xray gaze and the medical gaze, however, raises important questions about the ethics of medical imaging and the potential risks and benefits of xray technology in the clinical setting. The economic implications of xray technology are far-reaching and complex, with potential applications in fields such as healthcare, manufacturing, and, surprisingly, the creation of xray-based theme parks and entertainment venues. The development of xray-based industries, however, raises important questions about the potential risks and benefits of such technology, including, but not limited to, the possibility of xray-induced job displacement, the creation of xray-based economic inequalities, and the use of xray technology to enhance global trade and commerce. The environmental impact of xray technology, however, is a topic that warrants further investigation, particularly in relation to the potential risks of xray-induced radiation pollution, the creation of xray-based toxic waste, and the use of xray technology to monitor and mitigate the effects of climate change. The historical context of xray technology is fascinating and complex, with roots stretching back to the early days of medical imaging and the pioneering work of Wilhelm Conrad R\\u00f6ntgen. The development of xray technology, however, has been marked by a range of challenges and controversies, including, but not limited to, the debate over the safety of xray radiation, the development of xray-based medical imaging techniques, and the use of xray technology in non-medical applications, such as security screening and materials analysis. The future of xray research, however, is bright, and we eagerly anticipate the many exciting discoveries that will undoubtedly arise from the continued exploration of this fascinating and enigmatic topic. As we move forward, however, it is essential that we remain cognizant of the potential risks and challenges associated with xray technology, including, but not limited to, the dangers of xray-induced radiation poisoning, the environmental impact of xray-based industrial processes, and the ethical implications of using xray technology for non-medical purposes. The philosophical implications of xray technology are profound and far-reaching, with potential applications in fields such as metaphysics, epistemology, and, surprisingly, the creation of xray-based philosophical thought experiments. The use of xray imagery to explore fundamental questions about the nature of reality and human existence has been found to evoke powerful insights and spark intense philosophical debates, particularly in relation to the concept of the \\\"xray perspective,\\\" whereby the viewer is invited to peer into the innermost recesses of the human body and confront the mysteries of life and death. The xray perspective, which is characterized by a sense of detached curiosity and morbid fascination, has been found to be closely related to the concept of the \\\"medical perspective,\\\" whereby the physician or healthcare professional is empowered to peer into the innermost recesses of the human body and diagnose a range of ailments and afflictions. The intersection of the xray perspective and the medical perspective, however, raises important questions about the ethics of medical imaging and the potential risks and benefits of xray technology in the clinical setting. The potential applications of xray technology in the field of education are numerous and varied, ranging from the development of xray-based teaching tools and educational resources, to the creation of xray-based training programs for healthcare professionals and medical imaging technicians. The use of xray technology to enhance student learning and engagement has been found to be highly effective, particularly in relation to the concept of \\\"xray-based experiential learning,\\\" whereby students are invited to participate in hands-on xray-based experiments and activities. The development of xray-based educational resources, however, raises important questions about the potential risks and benefits of such technology, including, but not limited to, the possibility of xray-induced radiation exposure, the creation of xray-based economic inequalities, and the use of xray technology to enhance global access to education and healthcare. The role of xray technology in the development of modern society is complex and multifaceted, with potential applications in fields such as healthcare, manufacturing, and, surprisingly, the creation of xray-based art forms and cultural artifacts. The use of xray technology to explore fundamental questions about the nature of reality and human existence has been found to evoke powerful insights and spark intense philosophical debates, particularly in\",\n          \"# API with a Rich Linguistic Resource ## Abstract This paper introduces a novel Python API, incorporated within the NLTK library, that facilitates access to the FrameNet 1.7 lexical database. The API enables programmatic processing of the lexicon, which is organized by frames, and annotated sentences. Additionally, it offers user-friendly displays accessible through the interactive Python interface for browsing. ## 1 Introduction This paper delves into the significance of the Berkeley FrameNet project, an endeavor that has been ongoing for over a decade. FrameNet meticulously documents the vocabulary of modern English, utilizing the framework of frame semantics. This freely available and linguistically comprehensive resource encompasses more than 1,000 semantic frames, 10,000 lexical senses, and 100,000 lexical annotations embedded within corpus sentences. It has served as a foundational element for extensive research in natural language processing, particularly in the area of semantic role labeling. Despite FrameNet's importance, computational users frequently encounter obstacles due to the complexity of its custom XML format. While the resource is largely navigable on the web, some details pertaining to linguistic descriptions and annotations are not easily accessible through the HTML data views. Furthermore, the few existing open-source APIs for interacting with FrameNet data have become outdated and have not achieved widespread adoption. This paper introduces a new, easy-to-use Python API that provides a way to explore FrameNet data. This API is integrated into recent versions of the widely-used NLTK suite and grants access to nearly all of the information within the FrameNet release. ## 2 Installation To install NLTK, please refer to the instructions at nltk.org. NLTK offers cross-platform functionality and is compatible with both Python 2.7 and Python 3.x environments. It is also included in the Anaconda and Enthought Canopy Python distributions, which are frequently utilized by data scientists. In an active NLTK setup (version 3.2.2 or later), the FrameNet data can be downloaded through a single method call: ``` >>> import nltk >>> nltk.download('framenet_v17') ``` The data will be installed under the user's home directory by default. Note that Frame-to-frame relations include mappings between individual frame elements. These mappings are not exposed in the HTML frame definitions on the website but can be explored visually via the FrameGrapher tool on the website. Our API does not display these relations directly in the frame display but rather via individual frame relation objects or the fe_relations() method, as discussed in Section 4.4. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). # 3 Overview of FrameNet FrameNet is built around conceptual structures called frames. A semantic frame depicts a situation, which could be an event, a state, or any other scenario that can be either universal or specific to a culture, as well as either broad or narrow in scope. The frame identifies participant roles known as frame elements (FEs). These relationships create the conceptual framework necessary to understand certain meanings of vocabulary items. Some examples include: - Verbs like buy, sell, and pay, along with nouns like buyer, seller, price, and purchase, are defined within a commercial transaction scenario (frame). Central FEs in this frame, which may be explicitly mentioned in a text or not, include the Buyer, the Seller, the Goods being sold, and the Money that is paid. - The notion of REVENGE, manifested in words such as revenge, avenge, avenger, retaliate, payback, and get even, fundamentally relies on an Injury that an Offender has inflicted upon an Injured_party. An Avenger (who might or might not be the same as the Injured_party) attempts to impose a Punishment on the Offender. - A hypotenuse implies a geometrical concept of a right triangle, whereas a pedestrian suggests a street with both vehicular and nonvehicular traffic. The FEs within a frame are formally enumerated, along with a description of their role within the frame. Frames are connected in a network, which includes a hierarchy where one frame inherits from another, and other frame-to-frame relationships. Vocabulary items that are part of a frame are called lexical units (LUs). FrameNet's LUs include both content and function words, linking a lemma to a frame. In a text, an LU token is said to evoke the frame. Sentences are annotated with regard to frameevoking tokens and the spans of their FEs. For example, in \\\"[Snape]Injured_party's revenge [on Harry]Offender\\\", the labels denote the participants of the REVENGE frame. # 4 API Overview ## 4.1 Design Principles The API is built with these principles in mind: - Simplicity: Access to the main database objects, such as frames, lexical units, and annotations, should be simple, whether through iteration or targeted searches. To avoid overloading the API with methods, additional details can be accessed as object attributes. The help() method provides a synopsis of key database access methods. - Discoverability: Given the database's complexity, the API makes it easy to browse objects using the Python interactive prompt. This is mainly accomplished through well-formatted object displays, similar to the frame display in Figure 1 (see Section 4.3). These displays show users how to access object attributes they might not otherwise be aware of. - On-demand loading: The database is split into many XML files. The FrameNet 1.7 release, once unzipped, is 855 MB. Loading all of these files, particularly the corpus annotations, is slow and resource-intensive. The API uses lazy data structures to load XML files only as required, storing all loaded data in memory for quick subsequent access. ## 4.2 Lexicon Access Methods The primary methods for accessing lexicon data are: - frames(name): returns all frames matching the provided name pattern. - frame(nameOrId): returns a single frame matching the name or the ID - lus(name, frame): returns all lexical units matching the provided name pattern. - lu(id): returns a lexical unit based on its ID - fes(name, frame): returns all frame elements based on the name pattern provided Methods with plural names use regular expressions to search entries. Also, the lus() and fes() methods allow you to specify a frame to constrain the results. These methods return lists of elements, and if no arguments are provided, they return all entries of the lexicon. Below is an example of a search using the frame name pattern: ``` >>> fn.frames('(?i)creat') [<frame ID=268 name=Cooking_creation>, <frame ID=1658 name=Create_physical_artwork>, ...] ``` Here is an example of a search using the LU name pattern, note that the .v suffix is used for all verbal LUs: ``` >>> fn.lus(r'.+en\\\\\\\\.v') [<lu ID=5331 name=awaken.v>, <lu ID=7544 name=betoken.v>, ...] ``` The frame() and lu() methods are used to get an entry by name or ID. A FramenetError will be raised when trying to retrieve a non-existent entry. Two extra methods are available for frame lookups: frame_ids_and_names(name) gets a mapping from frame IDs to names and frames_by_lemma(name) returns all the frames that have LUs matching the provided name pattern. ### 4.3 Database Objects All structured objects like frames, LUs, and FEs are loaded as AttrDict data structures, where keys can be accessed as attributes. For instance: ``` >>> f = fn.frame('Revenge') >>> f.keys() dict_keys(['cBy', 'cDate', 'name', 'ID', '_type', 'definition', 'definitionMarkup', 'frameRelations', 'FE', 'FEcoreSets', 'lexUnit', 'semTypes', 'URL']) >>> f.name 'Revenge' >>> f.ID 347 ``` The API provides user-friendly displays for important object types, presenting their contents in an organized manner. For example, calling fn.frame('Revenge') prints the display for the REVENGE frame. These displays indicate attribute names in square brackets. ``` frame (347): Revenge [URL] https://framenet2.icsi.berkeley.edu/fnReports/data/frame/Revenge.xml [definition] This frame concerns the infliction of punishment in return for a wrong suffered. An Avenger performs a Punishment on a Offender as a consequence of an earlier action by the Offender, the Injury. The Avenger inflicting thePunishment need not be the same as the Injured_Party who suffered the Injury, but the Avenger does have to share the judgment that the Offender's action was wrong. The judgment that the Offender had inflicted an Injury is made without regard to the law. '(1) They took revenge for the deaths of two loyalist prisoners.' '(2) Lachlan went out to avenge them.' '(3) The next day, the Roman forces took revenge on their enemies..' [semTypes] 0 semantic types [frameRelations] 1 frame relations <Parent=Rewards_and_punishments -- Inheritance -> Child=Revenge> [lexUnit] 18 lexical units avenge.v (6056), avenger.n (6057), get back (at).v (10003), get even.v (6075), payback.n (10124), retaliate.v (6065), retaliation.n (6071), retribution.n (6070), retributive.a (6074), retributory.a (6076), revenge.n (6067), revenge.v (6066), revengeful.a (6073), revenger.n (6072), sanction.n (10676), vengeance.n (6058), vengeful.a (6068), vindictive.a (6069) [FE] 14 frame elements Core: Avenger (3009), Injured_party (3022), Injury (3018), Offender (3012), Punishment (3015) Peripheral: Degree (3010), Duration (12060), Instrument (3013), Manner (3014), Place (3016), Purpose (3017), Time (3021) Extra-Thematic: Depictive (3011), Result (3020) [FEcoreSets] 2 frame element core sets Injury, Injured_party Avenger, Punishment ``` ### 4.4 Advanced Lexicon Access Frame relations. Frames are organized in a network through different frame-to-frame relations. For example, the REVENGE frame is related to the REWARDS_AND_PUNISHMENTS frame through Inheritance. Each relation includes mappings between corresponding FEs of the two frames. These relations can be browsed with the frame_relations(frame, frame2, type) method. Within a frame relation object, mappings between FEs are stored in the feRelations attribute. The method fe_relations() gives direct access to the links between FEs. The available relation types can be obtained by frame_relation_types(). Semantic types. Semantic types provide added semantic labels for FEs, frames, and LUs. For FEs, they show selectional constraints. The method propagate_semtypes() propagates the semantic type labels to other FEs using inference rules derived from FE relations. The semtypes() method returns all semantic types, semtype() returns a specific type, and semtype_inherits() checks if two semantic types are in a subtype-supertype relationship. ### 4.5 Corpus Access Frame annotations of sentences are accessible through the exemplars and subCorpus attributes of a LU object or using the following methods: - annotations(luname, exemplars, full_text) - sents() - exemplars(luname) - ft_sents(docname) - doc(id) - docs(name) The annotations() method returns a list of frame annotation sets. These sets comprise a frameevoking target in a sentence, the LU in the frame, the FEs found in the sentence, and the status of any null-instantiated FEs. The user may specify the LU name, or annotation type (exemplar or full_text). Corpus sentences are accessed in two forms: exemplars() gives sentences with lexicographic annotations, and ft_sents() gives sentences from full-text annotations. sents() provides an iterator over all sentences. Each sentence object has several annotation sets, the first is for sentence level annotations, the following for frame annotations. ``` exemplar sentence (929548): [sentNo] 0 [aPos] 1113164 [LU] (6067) revenge.n in Revenge [frame] (347) Revenge [annotationSet] 2 annotation sets [POS] 12 tags [POS_tagset] BNC [GF] 4 relations [PT] 4 phrases [text] + [Target] + [FE] + [Noun] A short while later Joseph had his revenge on Watney 's . Time Offender [Injury:DNI] (Avenge=Avenger, sup=supp, Ave=Avenger) full-text sentence (4148528) in Tiger_Of_San_Pedro: [POS] 25 tags [POS_tagset] PENN [text] + [annotationSet] They 've been looking for him all the time for their revenge , ******* ******* Seeking Revenge [3] ? [2] but it is only now that they have begun to find him out . \\\" ***** **** Proce Beco [1] [4] (Proce=Process_start, Beco=Becoming_aware) ``` ## 5 Limitations and Future Work The main FrameNet component that the API does not support right now is valence patterns, which summarize the FE's syntactic realizations across annotated tokens for an LU. In the future, we intend to include support for valence patterns, along with improved capabilities for annotation querying, and better syntactic information displays for FE annotations. Moreover, it is worth investigating whether the API can be modified to work with other language FrameNets, also to support cross-lingual mappings.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt_classifier\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"Here is the full text of a research paper:\\n# An Empirical Study of the \\\"Hard-Won Lesson\\\": Two Decades of Research Insights #### Abstract This research investigates the congruence between research in major computer vision conferences and the tenets of the \\\"hard-won lesson\\\" articulated by Rich Sutton. Utilizing large language models (LLMs), we scrutinize twenty years of abstracts and titles from these conferences to evaluate the field's acceptance of these core concepts. Our approach employs cutting-edge natural language processing methodologies to methodically chart the progression of research paradigms within computer vision. The findings indicate notable patterns in the implementation of generalized learning algorithms and the exploitation of enhanced computational capabilities. We analyze the ramifications of these discoveries for the prospective trajectory of computer vision research and its conceivable influence on the broader development of artificial intelligence. This investigation contributes to the persistent discourse regarding the most efficacious methods for propelling machine learning and computer vision forward, furnishing perspectives that could steer forthcoming research orientations and techniques in these domains. #### 1 Introduction . Rich Sutton's seminal paper, \\\"The Hard-Won Lesson,\\\" posits that the most substantial progress in artificial intelligence (AI) has resulted from concentrating on broad methods that utilize computation, as opposed to human-derived representations and knowledge. This concept has been notably apparent in Computer Vision (CV), a domain that has observed a discernible transition from manually engineered features to deep learning frameworks. In this article, we explore the degree to which the abstracts from a prominent machine learning (ML) conference align with the principles of the \\\"hard-won lesson\\\" across two decades. Our analysis encompasses a randomized selection of 200 papers annually, addressing these research questions: - How has the emphasis on generalized methodologies and computational approaches developed in major computer vision conference abstracts over the last 20 years? - What discernible patterns can be observed regarding the embrace of deep learning methodologies and the departure from manually constructed features? - To what degree do the abstracts mirror the primary observations of Sutton's \\\"hard-won lesson,\\\" and how has this correlation altered over time? - Does a substantial correlation exist between a paper's alignment with the \\\"hard-won lesson\\\" principles and its influence, as gauged by its citation count? To tackle these inquiries, we utilize large language models (LLMs), themselves a clear demonstration of the principles delineated in the \\\"hard-won lesson,\\\" to scrutinize the abstracts. This assessment hinges on five metrics assigned by the LLMs, offering a thorough evaluation of the congruence between the abstracts and the \\\"hard-won lesson.\\\" Our study provides valuable perspectives on the general trajectory of the ML community and uncovers intriguing patterns in the embrace of Sutton's principles. By employing LLMs to analyze a substantial corpus of research literature, we introduce an innovative method for comprehending the learning and progression of a scientific field. This technique enables us to detect patterns and trends that might elude conventional research approaches, thereby delivering a more holistic understanding of the current state of ML research and its alignment with the principles demonstrated to be most effective in driving AI advancements. The prospective influence of our conclusions on forthcoming CV research directions is considerable. By pinpointing trends in the adoption of generalized methods and deep learning techniques, we can contribute to the advancement of foundational CV models at the cutting edge. These insights enhance our comprehension of the present state of ML research and illuminate potential avenues for further investigation and expansion in the field. ## 2 Background #### 2.1 The Hard-Won Lesson The realm of artificial intelligence (AI) has experienced a fundamental change, eloquently expressed in Rich Sutton's influential essay \\\"The Hard-Won Lesson.\\\" Sutton's central idea underscores the importance of generalized methods that utilize computational capability over human-engineered representations and domain-specific expertise. This viewpoint resonates with Leo Breiman's earlier work, which, twenty years prior, outlined the distinction between statistical and algorithmic methods in his paper \\\"Statistical Modeling: The Two Cultures.\\\" Breiman's insights, along with subsequent contributions, have significantly influenced our comprehension of data-oriented approaches in AI. #### 2.2 Evolution of Computer Vision The discipline of Computer Vision (CV) serves as a prime illustration of the concepts articulated in Sutton's \\\"hard-won lesson.\\\" Historically dependent on manually designed features such as SIFT, HOG, and Haar cascades for object recognition and image categorization, CV experienced a transformation with the introduction of deep learning, particularly Convolutional Neural Networks (CNNs). This shift facilitated the automated acquisition of hierarchical features directly from unprocessed image data, thereby bypassing the necessity for manual feature creation and markedly enhancing performance across a range of CV applications. The emergence of foundational models further aligned CV with Sutton's principles. Models like CLIP, ALIGN, and Florence demonstrate remarkable adaptability across diverse tasks with minimal fine-tuning, leveraging extensive multi-modal datasets to learn rich, transferable representations. This progression from conventional feature engineering to deep learning and foundational models in CV highlights the significance of employing computational resources and extensive datasets to achieve enhanced performance and generalization. #### 2.3 Large Language Models in Academic Evaluation The incorporation of Large Language Models (LLMs) into the assessment of scholarly texts has become a notable area of focus. LLMs, like GPT-4, have shown impressive abilities in swiftly handling and examining vast quantities of data, making them appropriate for numerous uses, including the evaluation of academic papers. Beyond their analytical abilities, LLMs have been shown to possess a degree of human-like judgment in assessing the quality of text. The G-EVAL framework, which employs LLMs to evaluate the quality of natural language generation outputs, demonstrates that LLMs can closely align with human evaluators in certain contexts. However, deploying LLMs in academic evaluation is not without its challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations. The function of LLMs in responding to inquiries and formulating hypotheses also deserves consideration. Their capacity to furnish comprehensive answers to intricate queries has been utilized in diverse educational environments, enhancing learning experiences and facilitating knowledge acquisition. In the context of academic research, LLMs can aid in generating hypotheses and guiding exploratory studies, contributing to the advancement of knowledge in various fields. Despite the promising applications of LLMs in academic evaluation and research, it is crucial to establish ethical guidelines and best practices for their use. # 3 Methodology and Evaluation #### 3.1 LLM Evaluation of Titles and Abstracts We utilize three large language models to assess the titles and abstracts of papers: GPT-4o-2024-05- 13, gpt-4o-mini-2024-07-18, and claude-3-5-sonnet-20240620. The following details are extracted from online sources and stored in a database for each paper: Year of Publication (2005-2024), Title, Authors, and Abstract. Additionally, the citation count for each paper is obtained from the Semantic Scholar API on July 20th, 2024, and recorded alongside the other metadata. Each LLM is assigned the task of providing a Likert score ranging from 0 to 10, indicating the degree to which a paper corresponds with the principles outlined in Sutton's \\\"hard-won lesson.\\\" We employ the Chain-of-Thought Prompting method in conjunction with the Magentic library to interact with the models and accumulate their feedback in a structured manner for subsequent analysis. We establish five dimensions for alignment with the \\\"hard-won lesson\\\": 1. **Learning Over Engineering:** How much does the idea prioritize using computation through data-driven learning and statistical methods over human-engineered knowledge and domain expertise? 2. **Search over Heuristics:** To what extent does the idea emphasize leveraging computation through search algorithms and optimization techniques instead of relying on human-designed heuristics? 3. **Scalability with Computation:** How much is the idea based on methods that can continuously scale and improve performance as computational resources increase? 4. **Generality over Specificity:** How much does the approach emphasize general, flexible methods that learn from data rather than building complex models of the world through manual engineering? 5. **Favoring Fundamental Principles:** To what extent does the approach adhere to fundamental principles of computation and information theory rather than emulating human cognition? The prompts were crafted to encapsulate the core of each \\\"hard-won lesson\\\" dimension in a succinct and impartial manner. To standardize the ratings, we furnish examples for the 0, 5, and 10 points on each dimension, elucidating the standards and guaranteeing uniform evaluations. Given the large number of publications, our research concentrates on a representative random sample of 200 papers from each year. We define the overall alignment score for each paper as the sum of scores across the five dimensions. #### 3.2 Inter-rater Reliability Measures **Intraclass Correlation Coefficient (ICC):** We employ ICC to measure the level of agreement among the models' evaluations. ICC is especially fitting for evaluating reliability when numerous raters assess an identical set of items. Specifically, we utilize the two-way random effects model (ICC(2,k)) to consider both rater and subject influences. **Krippendorff's Alpha:** In addition to ICC, we compute Krippendorff's Alpha, a flexible reliability coefficient capable of managing diverse data types (nominal, ordinal, interval, ratio) and resilient to missing data. This metric offers an supplementary viewpoint on inter-rater agreement, particularly beneficial when addressing potential variations in rating scales or absent evaluations. #### 3.3 Regression Analysis To examine the connection between alignment scores and a paper's impact, we conduct a regression analysis, using citation count as an indicator of influence. To manage the publication year and address potential temporal effects, we incorporate yearly stratification into our regression model. This method enables us to isolate the influence of alignment while accounting for the differing citation patterns across various publication years. To tackle the typically right-skewed distribution of citation counts, we employ a logarithmic transformation on the data. This transformation achieves several objectives in our analysis: it diminishes skewness, yielding a more symmetrical distribution that more closely resembles normality; it stabilizes variance across the data range, reducing the heteroscedasticity often seen in citation count data where variance tends to rise with the mean; and it linearizes potentially multiplicative relationships, converting them into additive ones. # 4 Results #### 4.1 Inter-rater Reliability The models show consistently strong agreement on all dimensions except \\\"Favoring Fundamental Principles,\\\" as indicated by ICC values above 0.5 and Krippendorff's alpha scores exceeding 0.4 on the remaining dimensions. The dimension \\\"Learning Over Engineering\\\" exhibits the highest ICC and Krippendorff's alpha scores. Although perfect agreement is not achieved, the inter-reliability measures fall within or above common thresholds for \\\"good\\\" reliability, validating the use of AI models for prompt-based research paper evaluation. #### 4.2 Regression Analysis Table 1 presents the regression analysis results for each dimension of \\\"hard-won lesson\\\" alignment scores against citation impact, stratified by year of publication. The R-squared values range from 0.027 to 0.306. In this regression analysis, a multiplicative effect implies that a one-unit change in the alignment score for a particular dimension leads to a proportional change in the original scale of the citation count. The statistical significance of the regression coefficients is denoted using , , and to represent the 10%, 5%, and 1% significance levels, respectively. Several dimensions, such as \\\"Scalability\\\" and \\\"Learning over engineering,\\\" exhibit statistically significant relationships with citation impact across multiple years. Table 2 shows the results of regressing citation counts on the overall \\\"hard-won lesson\\\" alignment score for each year between 2005 and 2024. The R-squared values are quite low for most years but increase substantially starting in 2015. #### 4.3 Trends in \\\"Hard-Won Lesson\\\" Alignment The dimensions of \\\"Scalability with Computation\\\" and \\\"Learning Over Engineering\\\" show a consistent upward trend over the years. The period from 2015 to 2020 witnesses a particularly sharp rise in the average scores for these dimensions. # 5 Conclusion Our study scrutinized the concordance of research with Rich Sutton's \\\"hard-won lesson\\\" over two decades, employing large language models to analyze trends. The results show a steady rise in the adoption of general-purpose learning algorithms and scalability with computational resources, indicating a strong adherence to the core principles of the \\\"hard-won lesson.\\\" These trends highlight the machine learning community's inclination towards data-driven and computation-intensive methods over manual engineering and domain-specific knowledge. However, the \\\"Search over Heuristics\\\" dimension has not shown a similar upward trend, suggesting limited integration of search-based methods in the field. This stagnation contrasts with recent progress in inference-time scaling, exemplified by OpenAI's o1 models, which emphasize the importance of test-time computation in overcoming diminishing returns. The shift towards scaling inference time, driven by the development of larger and more complex models, has the potential to emulate search-like processes. As computational capabilities continue to expand, it is plausible that future research may increasingly incorporate search techniques, thereby enhancing alignment with this dimension of the \\\"hard-won lesson.\\\" | Year | R-squared | N | Learning | Search | Scalability | Generality | Principles | | --- | --- | --- | --- | --- | --- | --- | --- | | 2005 | 0.027 | 199 | -0.220 | 0.104 | 0.139 | 0.272 | -0.171 | | 2006 | 0.076 | 200 | 0.016 | -0.042 | 0.388* | 0.199 | -0.171 | | 2007 | 0.035 | 200 | -0.087 | 0.117 | 0.350* | -0.006 | -0.318* | | 2008 | 0.078 | 200 | -0.009 | 0.096 | 0.465*** | -0.026 | -0.463*** | | 2009 | 0.085 | 199 | -0.073 | 0.136 | 0.104 | 0.378* | -0.631*** | | 2010 | 0.074 | 200 | 0.121 | -0.129 | 0.218 | 0.016 | -0.471** | | 2011 | 0.076 | 200 | 0.208 | -0.036 | 0.318** | -0.284 | -0.423** | | 2012 | 0.094 | 200 | 0.195 | 0.077 | 0.428** | -0.110 | -0.517** | | 2013 | 0.085 | 200 | 0.395*** | -0.112 | 0.013 | -0.119 | -0.279 | | 2014 | 0.119 | 200 | 0.408*** | -0.085 | 0.308* | -0.348* | -0.266 | | 2015 | 0.264 | 200 | 0.515*** | -0.145 | 0.417** | -0.236 | -0.122 | | 2016 | 0.306 | 200 | 0.637*** | -0.300** | 0.517*** | -0.325 | -0.372* | | 2017 | 0.313 | 200 | 0.418*** | -0.353** | 0.751*** | -0.004 | -0.508** | | 2018 | 0.172 | 200 | 0.291* | -0.322* | 0.418** | 0.156 | -0.436** | | 2019 | 0.111 | 200 | 0.573** | -0.439** | 0.229 | -0.099 | -0.257 | | 2020 | 0.120 | 200 | 0.315 | -0.411*** | 0.179 | 0.229 | 0.010 | | 2021 | 0.090 | 200 | 0.269* | -0.381*** | 0.253 | -0.072 | -0.265* | | 2022 | 0.136 | 200 | 0.618*** | -0.137 | 0.110 | -0.118 | -0.257 | | 2023 | 0.123 | 200 | 0.107 | -0.009 | 0.664*** | -0.078 | -0.132 | | 2024 | 0.178 | 171 | -0.619*** | 0.314 | 0.808*** | 0.282 | -0.020 | Table 1: Regression analysis results for the relationship between \\\"hard-won lesson\\\" alignment scores and citation impact, stratified by year. *** indicates significance at the 1% level, ** indicates significance at the 5% level, and * indicates significance at the 10% level. In summary, our findings underscore the enduring significance of the \\\"hard-won lesson\\\" in shaping the path of computer vision research. By emphasizing generality and scalability, the field is wellpositioned to leverage emerging computational advancements. Future work should explore the integration of search methodologies and assess their impact on research impact and innovation within computer vision, particularly in light of recent breakthroughs in inference-time scaling. #### 6 Limitations This study has several limitations. First, our reliance on large language models (LLMs) for evaluating research abstracts introduces potential biases inherent to these models. Second, the absence of human expert evaluation as a ground truth is a significant limitation. Furthermore, our analysis is limited to the information contained in titles and abstracts, which may not capture the full depth and nuance of the methodologies and findings presented in the full papers. Lastly, while our study spans two decades of proceedings, it does not account for research published in other venues or unpublished work that may have influenced the field. Despite these limitations, we believe our study provides valuable insights into broad trends in computer vision research and its alignment with the principles of the \\\"hard-won lesson.\\\" Future work could address these limitations by incorporating human expert evaluations, analyzing full paper contents, and expanding the scope to include a wider range of publication venues. ### 7 Ethics Statement This study adheres to ethical guidelines. Our use of large language models (LLMs) for analyzing trends in academic literature raises important ethical considerations. We acknowledge that LLMs may introduce biases when used for direct evaluation of academic work. However, our study focuses solely on using LLMs to analyze broad trends rather than to assess individual papers' quality or merit. All data were collected in accordance with applicable privacy and intellectual property laws. No personally identifiable information was collected from human subjects. Our methodology aims to | Year | R-squared | N | F-statistic | Prob (F-statistic) | Overall Alignment Score | | --- | --- | --- | --- | --- | --- | | 2005 | 0.007 | 199 | 1.409 | 0.237 | 0.029 [-0.019, 0.076] | | 2006 | 0.050 | 200 | 10.335 | 0.002 | 0.083*** [0.032, 0.134] | | 2007 | 0.003 | 200 | 0.554 | 0.457 | 0.019 [-0.031, 0.068] | | 2008 | 0.010 | 200 | 1.993 | 0.160 | 0.031 [-0.012, 0.075] | | 2009 | 0.015 | 199 | 2.998 | 0.085 | 0.045* [-0.006, 0.097] | | 2010 | 0.000 | 200 | 0.033 | 0.856 | 0.005 [-0.049, 0.059] | | 2011 | 0.000 | 200 | 0.000 | 0.993 | -0.000 [-0.051, 0.051] | | 2012 | 0.024 | 200 | 4.898 | 0.028 | 0.057** [0.006, 0.109] | | 2013 | 0.005 | 200 | 0.944 | 0.333 | 0.022 [-0.023, 0.067] | | 2014 | 0.030 | 200 | 6.023 | 0.015 | 0.056** [0.011, 0.101] | | 2015 | 0.170 | 200 | 40.618 | 0.000 | 0.141*** [0.097, 0.184] | | 2016 | 0.128 | 200 | 29.114 | 0.000 | 0.129*** [0.082, 0.176] | | 2017 | 0.133 | 200 | 30.338 | 0.000 | 0.182*** [0.117, 0.248] | | 2018 | 0.066 | 200 | 13.996 | 0.000 | 0.098*** [0.047, 0.150] | | 2019 | 0.021 | 200 | 4.241 | 0.041 | 0.061** [0.003, 0.119] | | 2020 | 0.040 | 200 | 8.325 | 0.004 | 0.079*** [0.025, 0.133] | | 2021 | 0.002 | 200 | 0.407 | 0.524 | -0.017 [-0.068, 0.035] | | 2022 | 0.062 | 200 | 13.054 | 0.000 | 0.097*** [0.044, 0.149] | | 2023 | 0.063 | 200 | 13.416 | 0.000 | 0.099*** [0.046, 0.153] | | 2024 | 0.092 | 171 | 17.040 | 0.000 | 0.127*** [0.066, 0.188] | Table 2: Regression analysis results for the relationship between overall \\\"hard-won lesson\\\" alignment scores and citation impact, stratified by year. *** indicates significance at the 1% level, ** indicates significance at the 5% level, and * indicates significance at the 10% level. minimize risks by using multiple models and focusing on aggregate trends rather than individual assessments.\\nHere are the descriptions of the conferences you should consider:\\nEMNLP: \\nThe Conference on Empirical Methods in Natural Language Processing (EMNLP) is a leading international conference in the field of natural language processing (NLP).\\n\\n**Key Characteristics:**\\n\\n*   **Focus on Empirical NLP:** EMNLP emphasizes empirical research, focusing on methods and models that are tested and evaluated using real-world data.\\n*   **Broad Range of Topics:** The conference covers a wide array of topics within NLP, including text classification, machine translation, language modeling, information extraction, dialogue systems, summarization, question answering, and more.\\n*   **Data-Driven Approach:** EMNLP papers typically use a data-driven approach, focusing on using datasets to train and evaluate NLP models.\\n*   **Focus on Evaluation Metrics:** Rigorous evaluation is a key component of EMNLP research, emphasizing the use of standardized metrics to assess the performance of NLP models.\\n*   **Theoretical and Applied Research:** EMNLP welcomes both theoretical and applied research, as long as the research is empirically grounded.\\n*   **Strong Community:** The conference has a strong and supportive community of NLP researchers and practitioners.\\n*   **Poster and Oral Presentations:** Accepted papers are typically presented as posters, with a smaller subset selected for oral presentations.\\n*   **Workshops and Tutorials:** EMNLP features various workshops and tutorials covering specialized topics within NLP.\\n*   **Annual Event:** EMNLP is an annual conference that takes place each year.\\n*   **Double-Blind Reviewing:** The review process is double-blind, ensuring anonymity of both authors and reviewers.\\n*  **Emphasis on Reproducibility:** Like other top tier ML conferences, EMNLP encourages reproducible research by encouraging code sharing.\\n\\n**In summary, EMNLP is a premier conference for empirical research in natural language processing, focused on data-driven approaches, rigorous evaluation, and addressing real-world problems using NLP techniques. It serves as an important platform for researchers to share their work, collaborate with peers, and contribute to the advancement of the NLP field.**\\n\\nCVPR: \\nThe Conference on Computer Vision and Pattern Recognition (CVPR) is one of the most prestigious and competitive conferences in the field of computer vision.\\n\\n**Key Characteristics:**\\n\\n*   **Premier Computer Vision Venue:** CVPR is widely regarded as a top venue for publishing research in computer vision, attracting submissions from top research labs worldwide.\\n*   **Broad Range of Topics:** The conference covers a diverse set of topics within computer vision, including image recognition, object detection, segmentation, 3D vision, video analysis, medical image analysis, and robotics vision.\\n*   **Focus on Visual Data Analysis:** CVPR is primarily concerned with the analysis, interpretation, and understanding of visual data, such as images and videos.\\n*   **Rigorous Peer Review:** The review process is typically rigorous, with a large number of submissions and a low acceptance rate.\\n*   **Emphasis on Technical Innovation:** CVPR papers typically demonstrate technical innovation and advance the state-of-the-art in the field.\\n*   **Poster and Oral Presentations:** Accepted papers are presented as posters and a smaller subset is selected for oral presentation.\\n*   **Tutorials and Workshops:** CVPR features numerous tutorials and workshops focused on specialized areas of computer vision.\\n*   **Large and International Community:** The conference attracts a large and international audience from academia and industry, highlighting its wide reach.\\n*   **Annual Event:** CVPR is an annual conference that typically takes place in the summer.\\n*   **Double-Blind Reviewing:** The review process is double-blind, ensuring a fair assessment of the work, where reviewers are not aware of the author's identities and vice-versa.\\n*   **Publicly Available Code:** It is increasingly expected that papers will also make their code public, contributing to reproducibility and open science.\\n\\n**In summary, CVPR is a leading international conference in computer vision, highly regarded for its rigorous review process, strong focus on technical innovation, broad coverage of topics, and significant impact on the field. It is a crucial venue for researchers to disseminate their latest findings, engage with the computer vision community, and advance the field forward.**\\n\\nTMLR: \\nThe Transactions on Machine Learning Research (TMLR) is a relatively new journal-style publication venue associated with the International Conference on Machine Learning (ICML). It operates as an open access, continuous publication journal rather than a traditional conference with deadlines.\\n\\n**Key Characteristics:**\\n\\n*   **Journal Format:** TMLR papers undergo a rigorous peer-review process, similar to journals, and accepted papers are continuously published online, rather than being presented at a specific conference.\\n*   **Open Access:** All TMLR publications are freely available to read and download.\\n*   **Focus:** TMLR aims to publish high-quality research in all areas of machine learning, with an emphasis on rigorous methodology and impactful contributions.\\n*   **Review Process:** The review process is thorough and can take several rounds of revision, leading to well-vetted, high-quality publications. The reviewers tend to be experts in the respective fields.\\n*   **ICML Affiliation:** While TMLR is a standalone publication, it is closely affiliated with ICML and accepted papers are typically highlighted during the ICML conference.\\n*   **Emphasis on Clarity and Reproducibility:** TMLR encourages clear writing, code sharing, and proper evaluation to facilitate reproducibility.\\n*   **Continuous Publication:** Unlike conferences with fixed deadlines and schedules, TMLR publishes accepted papers on an ongoing basis.\\n*   **Not a Conference:** TMLR does not have a traditional conference component like presentations or posters. Accepted papers are published online, and there's no physical or virtual gathering for presenting.\\n\\n**In summary, TMLR is a prestigious, peer-reviewed journal-style publication venue focusing on high-quality machine learning research with a rigorous review process and open access policy. It prioritizes clarity, reproducibility, and impactful contributions to the field and is closely associated with the ICML conference.**\\n\\n\\nBased on the provided information, which conference is the best fit for this paper?\",\n          \"Here is the full text of a research paper:\\n# Rapid Image Annotation Through Zero-Shot Learning # Abstract Recent experiments on word analogies demonstrate that contemporary word vectors effectively encapsulate subtle linguistic patterns through linear vector displacements. However, the extent to which these straightforward vector displacements can represent visual patterns across words remains uncertain. This research investigates a particular image-word relevance relationship. The findings indicate that, for a given image, word vectors of pertinent tags are positioned higher than those of unrelated tags along a primary axis within the word vector space. Drawing inspiration from this insight, we suggest addressing image tagging by determining the main axis for an image. Specifically, we utilize linear mappings and intricate deep neural networks to deduce the primary axis from an input image. The resultant tagging model exhibits remarkable adaptability. It operates swiftly on test images, with a processing time that remains constant regardless of the training set's size. Furthermore, it showcases exceptional performance not only in conventional tagging tasks using the NUS-WIDE dataset but also in comparison to competitive baselines when assigning tags to images that haven't been seen during training. # 1 Introduction . Recent advancements in representing words in vector spaces have proven advantageous for both Natural Language Processing and various computer vision applications, including zero-shot learning and image caption generation. The rationale behind using word vectors in NLP is rooted in the observation that detailed linguistic patterns among words are represented by linear offsets of word vectors. This pivotal insight emerged from well-known word analogy studies. For example, syntactic relationships like \\\"dance\\\" to \\\"dancing\\\" parallel \\\"fly\\\" to \\\"flying,\\\" and semantic connections like \\\"king\\\" to \\\"man\\\" mirror \\\"queen\\\" to \\\"woman.\\\" Nevertheless, it is yet to be determined whether the visual patterns across words, implicitly employed in the aforementioned computer vision tasks, can similarly be represented by these basic vector offsets. This paper focuses on the task of image tagging, where an image necessitates the division of a word lexicon into two distinct groups based on image-word relevance. For example, an image of a zoo might have relevant tags like \\\"people,\\\" \\\"animal,\\\" and \\\"zoo,\\\" while irrelevant tags might include \\\"sailor,\\\" \\\"book,\\\" and \\\"landscape.\\\" This lexical division fundamentally differs from the nuanced syntactic or semantic relationships examined in word analogy tests. Instead, it concerns the connection between two sets of words as prompted by a visual image. This type of word relationship is semantic and descriptive, emphasizing visual association, albeit at a broader level. Given this context, it is worth investigating whether word vectors maintain the property where simple linear vector offsets can depict visual or image-based associative relationships between words. In the zoo example, while it's easy for humans to recognize that words like \\\"people,\\\" \\\"animal,\\\" and \\\"zoo\\\" are more related to the zoo than words like \\\"sailor,\\\" \\\"book,\\\" and \\\"landscape,\\\" the question is whether such a zoo-association relationship can be represented by the nine pairwise vector offsets: \\\"people\\\" minus \\\"sailor,\\\" \\\"people\\\" minus \\\"book,\\\" and so on, up to \\\"zoo\\\" minus \\\"landscape,\\\" between the vectors of relevant and irrelevant tags. A primary contribution of this research is an empirical investigation of these questions. Each image establishes a visual association rule over words, represented as a pair (Y, Y). Leveraging the extensive image collections in benchmark datasets designed for image tagging, we can explore numerous distinct visual association rules in words and the corresponding vector offsets in the word vector space. Our findings uncover a significant correlation: the offsets between the vectors of relevant tags (Y) and those of irrelevant tags (Y) predominantly align in a consistent direction, which we term the \\\"principal direction\\\". In other words, within the word vector space, there exists at least one vector (direction), denoted as w, such that its inner products with the vector offsets between Y and Y are greater than 0. This can be expressed as: (w,p 2014 n) > 0 equivalently, (w,p) > (w,n) \\u02d8 This implies that the vector w ranks all relevant words Y ahead of irrelevant ones Y. The visual association patterns among words manifest as the linear rank-abilities of their corresponding word vectors. This observation corroborates findings from word analogy studies, suggesting that multiple relationships for a single word are embedded within a high-dimensional space. Furthermore, these relationships can be articulated using basic linear vector arithmetic. Building on this discovery, we propose a solution to the image tagging challenge by identifying the primary axis along which relevant tags are ranked higher than irrelevant ones within the word vector space. We employ both linear mappings and deep neural networks to infer this primary axis from each input image. This unique perspective on image tagging yields a highly adaptable tagging model. The model processes test images rapidly, maintaining a constant processing time irrespective of the training dataset's size. It not only delivers outstanding results in traditional tagging tasks but also excels at assigning new tags from a broad vocabulary that were not encountered during training. Our method does not rely on prior knowledge of these new tags, as long as they exist within the same vector space as the tags used during training. Consequently, we designate our technique as \\\"fast zero-shot image tagging\\\" (Fast0Tag), acknowledging its strengths in both speed and its zero-shot learning capabilities. In stark contrast to our approach, prior methods for image tagging are limited to assigning only those tags to test images that were seen during training, with a notable exception. These methods are constrained by the fixed and often limited number of tags present in the training data, which poses practical challenges. For example, Flickr hosts approximately 53 million tags, and this number is rapidly increasing. The work of Fu et al. represents a pioneering effort to extend an image tagging model to previously unseen tags. However, when compared to our proposed method, it depends on two extra assumptions. Firstly, it assumes that unseen tags are known beforehand to enable model adjustment toward these tags. Secondly, it assumes that test images are known in advance for model regularization. Moreover, this method is restricted to a very limited number, U, of unseen tags, as it needs to account for all 2U possibletagcombinations. To recap, our primary contribution lies in analyzing visual association patterns in words as they relate to images and how these patterns are reflected in word vector offsets. We posit and confirm through experiments that a main direction exists in the word vector space for each visual association rule (Y, Y), where vectors of relevant words are ranked higher than others. Building on this, our second contribution is an innovative image tagging model, Fast0Tag, which is both swift and capable of handling an open vocabulary of unseen tags. Lastly, we explore three distinct image tagging scenarios: traditional tagging, which assigns seen tags to images; zero-shot tagging, which annotates images with numerous unseen tags; and seen/unseen tagging, which uses both seen and unseen tags. Existing research either addresses traditional tagging or zero-shot tagging with a limited number of unseen tags. Our Fast0Tag method surpasses competitive baselines across all three scenarios. # 2 Related Work Image Tagging. The objective of image tagging is to allocate pertinent tags to an image or to generate a ranked list of tags. Within the academic community, this challenge has predominantly been tackled from the standpoint of tag ranking. Generative approaches, which incorporate topic models and mixture models, inherently rank candidate tags based on their conditional probabilities relative to the test image. Conversely, non-parametric, nearest-neighbor-based techniques frequently rank tags for a test image by aggregating votes from a selection of training images. Although nearest-neighbor methods generally exhibit superior performance compared to those reliant on generative models, they are plagued by substantial computational demands during both training and testing phases. The recently introduced FastTag algorithm offers a significant speed advantage while maintaining performance levels on par with nearest-neighbor methods. Our Fast0Tag method mirrors the reduced complexity of FastTag. Embedding techniques, on the other hand, determine tag ranking scores via a cross-modal mapping between images and tags. This concept has been further developed using deep neural networks. Notably, aside from certain exceptions, the majority of these methods do not train their models with an explicit ranking objective, despite ultimately ranking candidate tags for test images. This discrepancy between the trained models and their practical application contravenes the principle of Occam's razor. We incorporate a ranking loss in our approach, similar to these exceptions. Unlike our Fast0Tag, which is capable of ranking both known and an unlimited number of previously unseen tags for test images, the methods mentioned earlier are restricted to assigning tags to images from a predetermined vocabulary encountered during training. An exception to this is the work by Fu et al., where they address a predefined number, U, of unseen tags by developing a multi-label model that considers all possible 2 U combinationsof thesetags.However, thisapproachisconstrainedbythesmallnumberUofunseentagsitcanhandle. Word Embedding. Diverging from the conventional one-hot vector representation of words, word embedding maps each word to a continuous-valued vector, primarily learning from the statistical patterns of word co-occurrences. While earlier studies on word embedding exist, our research emphasizes the latest GloVe and word2vec vectors. As demonstrated in the well-known word analogy experiments, both types of word vectors effectively capture detailed semantic and syntactic patterns through vector offsets. In this study, we further reveal that basic linear offsets can also represent the broader visual association patterns among words. Zero-Shot Learning. The term \\\"zero-shot learning\\\" is frequently used interchangeably with \\\"zero-shot classification,\\\" although the latter is actually a subset of the former. In contrast to weakly-supervised learning, which acquires new concepts by extracting information from noisy samples, zero-shot classification aims to classify objects from unseen classes by learning classifiers from seen classes. Attributes and word vectors are two primary semantic sources that enable zero-shot classification. Our Fast0Tag, together with Fu et al., expands the domain of zero-shot learning to include zeroshot multi-label classification. Fu et al. approach this by converting the problem into zero-shot classification, where each combination of multiple labels is treated as a separate class. We, on the other hand, model the labels directly, allowing us to assign or rank a large number of unseen tags for an image. # 3 The Linear Rank-Ability of Word Vectors Our Fast0Tag method is enhanced by the discovery that the visual relationship between words, specifically how a lexicon is divided based on relevance to an image, manifests in the word vector space as a main direction. Along this direction, words or tags that are relevant to the image are ranked higher than those that are not. This section elaborates on this discovery. #### 3.1 The Regulation Over Words Due to Image Tagging Let's denote S as the set of seen tags available for training image tagging models, and U as the set of tags unseen during the training phase. The training data is structured as (xm, Ym); m = 1, 2, ..., M, where xm represents the feature vector of image m in RD, and Ym is a subset of S, containing the seen tags relevant to that image. For simplicity, we also use Ym to represent the collection of corresponding word or tag vectors. Traditional image tagging seeks to assign seen tags from S to test images. Zero-shot tagging, as defined by Fu et al., aims to annotate test images using a predetermined set of unseen tags, U. Beyond these two scenarios, this paper introduces seen/unseen image tagging, which identifies both relevant seen tags from S and relevant unseen tags from U for test images. Furthermore, the set of unseen tags, U, can be open and continuously expanding. We define Ym as the complement of Ym in S, representing irrelevant seen tags. An image m establishes a visual association rule among words, essentially partitioning seen tags into two distinct sets: Ym and Ym. Recognizing that various detailed syntactic and semantic patterns among words can be depicted through linear word vector offsets, we proceed to investigate the characteristics these vector offsets might exhibit for this novel visual association rule. #### 3.2 Principal Direction and Cluster Structure Figure 2 offers a visual representation of vector offsets (p - n), where p belongs to Ym and n belongs to Ym, using both t-SNE and PCA for two different visual association rules over words. One rule is defined by an image associated with 5 relevant tags, and the other by an image with 15 relevant tags. From these vector offsets, we identify two key structures: Principal Direction: For a given visual association rule (Ym, Ym) in words for image m, the vector offsets predominantly point in a similar direction, which we refer to as the principal direction. This suggests that along this principal direction, relevant tags Ym are ranked higher than irrelevant ones Ym. Cluster Structure: Within each visual association rule over words, there are discernible cluster structures in the vector offsets. Moreover, all offsets that point to the same relevant tag in Ym are grouped within the same cluster. In Figure 2, we distinguish offsets pointing to different relevant tags by using different colors. The question remains whether these two observations can be generalized. Specifically, do they remain valid in the high-dimensional word vector space for a broader range of visual association rules defined by other images? To address this, we designed an experiment to confirm the existence of principal directions in word vector spaces, or equivalently, the linear rank-ability of word vectors. We defer the investigation of the cluster structure to future research. #### 3.3 Testing the Linear Rank-Ability Hypothesis The experiments in this section are performed using the validation set of the NUS-WIDE dataset, which includes 26,844 images, 925 seen tags (S), and 81 unseen tags (U). The number of relevant seen/unseen tags associated with an image varies from 1 to 20/117, with an average of 1.7/4.9. Further details can be found in Section 5. Our goal is to explore whether a primary direction exists for any visual association rule (Ym, Ym) created by image m, along which relevant tags Ym rank higher than irrelevant tags Ym. This can be confirmed if we find a vector w in the word vector space that fulfills the ranking conditions (w, p) > (w, n) for all p in Ym and n in Ym. To achieve this, we train a linear ranking SVM for each visual association rule using all corresponding pairs (p, n). We then rank word vectors using the SVM and assess the number of violated constraints. Specifically, we use MiAP, with higher values being preferable, to compare the SVM's ranking list against the ranking constraints. This process is repeated for all validation images, resulting in 21,863 unique visual association rules. Ranking SVM Implementation. We utilize the primal formulation of ranking SVM for our experiments, which is defined as: min 1/2 ||w||2 + max(0, 1 \\u2212 (w, yi) + (w, yj))foryiY m, yjY m Here, is a hyperparameter that balances the objective and regularization. Results. The average MiAP outcomes across all distinct regulations are presented in Figure 3(left). We evaluate 300D GloVe vectors and word2vec vectors of dimensions 100, 300, 500, and 1000. The horizontal axis represents various regularizations used for training the ranking SVMs, with higher values indicating stronger regularization. In the 300D GloVe space and word2vec spaces of 300, 500, and 1000 dimensions, more than two ranking SVMs, with low values, produce nearly ideal ranking results (MiAP 1). This demonstrates that seen tags S are linearly rankable under almost every visual association rule, satisfying all ranking constraints set by relevant Ym and irrelevant Ym tags for image m. However, caution is advised before extending conclusions beyond the experimental vocabulary S of seen tags. While an image m imposes a visual association rule over all words, this rule leads to different partitions of distinct experimental vocabularies (e.g., seen tags S and unseen tags U). Therefore, we anticipate that the principal direction for seen tags should also apply to unseen tags under the same rule, if the questions at the end of Section 3.2 are answered affirmatively. Generalization to Unseen Tags. We investigate whether the same principal direction applies to both seen and unseen tags under each visual association rule induced by an image. This is partially validated by applying the previously trained ranking SVMs to unseen tag vectors, as the \\\"true\\\" principal directions are unknown. We use the 81 unseen tags U as \\\"test data\\\" for the trained ranking SVMs, each resulting from an image-induced visual association. NUS-WIDE provides annotations for these 81 tags. The results, shown in Figure 3(right), significantly outperform the basic baseline of random tag ranking, indicating that the directions produced by SVMs are generalizable to the new vocabulary U of words. Observation. We conclude that word vectors are an effective medium for transferring knowledge\\u2014specifically, rank-ability along the principal direction\\u2014from seen to unseen tags. We have empirically confirmed that the visual association rule (Ym, Ym) in words due to an image m can be represented by the linear rank-ability of corresponding word vectors along a principal direction. Our experiments involve a total of |S| + |U| = 1,006 words. Future work should include larger-scale and theoretical studies. # 4 Approximating the Linear Ranking Functions This section introduces our Fast0Tag approach for image tagging. Initially, we explain how to address image tagging by approximating the principal directions, based on their existence and generalization, as confirmed in the previous section. Subsequently, we describe the detailed approximation methods used. #### 4.1 Image Tagging by Ranking Based on the findings from Section 3, which indicate the existence of a principal direction, wm, in the word vector space for each visual association rule (Ym, Ym) generated by an image m, we propose a direct solution for image tagging. The core idea is to approximate this principal direction by learning a mapping function, f(00b7), that connects the visual space to the word vector space, such that: \\u02d8 f(xm) wm Here, xm is the visual feature representation of image m. Consequently, given a test image x, we can promptly suggest a list of tags by ranking the word vectors of the tags along the direction f(x), specifically by the ranking scores: t S U, (f(x), t) This applies whether the tags are from the seen set S or the unseen set U. We investigate both linear and nonlinear neural networks to implement the approximation function f(x) w. #### 4.2 Approximation by Linear Regression In this approach, we assume a linear function from the input image representation x to the output principal direction w, defined as: f(x) := Ax Here, A can be determined in a closed form through linear regression. Thus, from the training data, we have: wm = Axm+m, form = 1, 2, ..., M where wmistheprincipaldirectionforalloffsetvectorsof theseentags, correspondingtothevisualassociationrule(Ym, Ym)forimagem, andmrepresentstheerrors.M inimizingthemeansquarederrorsprovidesuswithaclosed\\u2212 formsolutionforA. However, a challenge arises as we do not know the exact principal directions wm.T hetrainingdataonlyprovideimagesxmandrelevanttagsYm.W eoptforastraightforwardalternative, usingthedirectionsderivedfromrankingSV MsinSection3inequation(5).Hence, theprocessinvolvestwostagestolearnthelinearfunctionf (x) = Ax.T hef irststagetrainsarankingSV Moverthewordvectorsofseentagsforeachvisualassociation(Ym, Ym).T hesecondstagecomputesthemappingmatrixAvialinearregression, usingthedirectionsfromtherankingSV Msastargets. Discussion. The use of linear transformation between visual and word vector spaces has been previously explored, for instance, in zero-shot classification and image annotation/classification. This work distinguishes itself by the clear interpretation of the mapped image f(x) = Ax as the principal direction for tag assignment, which has been empirically validated. We further extend this to a nonlinear transformation using a neural network. #### 4.3 Approximation by Neural Networks We also explore a nonlinear mapping f(x; ) using a multi-layer neural network, where represents the network parameters. The network architecture, illustrated in Figure 4, includes two RELU layers followed by a linear layer that outputs the approximated principal direction, w, for an input image x. We anticipate that the nonlinear mapping function f(x; ) will provide greater modeling flexibility compared to the linear approach. Training the neural network by regressing to the M directions obtained from ranking SVMs is not ideal, as confirmed by both intuition and experiments. The number of training instances, M, is small relative to the network's parameter count, increasing the risk of overfitting. Moreover, the directions from ranking SVMs are not the true principal directions, making it unnecessary to rely on them. Instead, we integrate the two stages from Section 4.2. We aim for the neural network's output f(xm; ) to represent the principal direction, where all relevant tag vectors p Ym rank higher than irrelevant ones n Ym for an image m. Let's define: v(p, n; ) = (f(xm; ), n) - (f(xm; ), p) as the degree of violation of these ranking constraints. We then minimize the following loss function to train the neural network: * = argmin wm \\u2217 l(xm, Ym; )l(xm, Ym; ) = log(1 + expv(p, n; ))forpYm, nYm where wm = 1/(|Ym|\\u2217|Ym|)normalizestheper\\u2212imageRankNetlossbythenumberofrankingconstraintsimposedbyimagemoverthetags.T hissetupallowsthefunctionf (x)todirectlyconsidertherankingconstraintsfromrelevantandirrelevanttags, anditcanbeoptimizedeffectivelyusingstandardmini\\u2212 batchgradientdescent. Practical Considerations. We use Theano for optimization, with a mini-batch size of 1,000 images. Each image, on average, imposes 4,600 pairwise ranking constraints, which are all used in the optimization. The normalization wmfortheper \\u2212 imagerankinglosshelpsbalancetheinfluenceof imageswithmanypositivetags, addressingtheissueofunbalancednumbersofrelevanttagsacrossimages.W ithoutnormalization, M iAP resultsdropbyabout2%inourexperiments.F orregularization, weemployearlystoppingandadropoutlayerwitha30%droprate.Optimizationhyperparametersarechosenusingthevalidationset. Besides the RankNet loss, we tested other per-image loss options, including hinge loss, Crammer-Singer loss, and pairwise max-out ranking. Hinge loss performed the worst, likely because it's not designed for ranking. Crammer-Singer, pairwise max-out, and RankNet yielded comparable results, with RankNet slightly outperforming the others by about 2% in MiAP, possibly due to easier optimization control. Listwise ranking loss could also be considered. # 5 Experiments on NUS-WIDE This section details our experimental results, comparing our method against several strong baselines for traditional image tagging on the large-scale NUS-WIDE dataset. Additionally, we evaluate our method on zero-shot and seen/unseen image tagging scenarios, extending some existing zero-shot classification algorithms and exploring variations of our approach for comparison. #### 5.1 Dataset and Configuration NUS-WIDE Dataset. We primarily utilize the NUS-WIDE dataset for our experiments. This dataset is a standard benchmark for image tagging, originally containing 269,648 images. We were able to retrieve 223,821 images, as some were either corrupted or removed from Flickr. Following the recommended protocol, we divide the dataset into a training set of 134,281 images and a test set of 89,603 images. We further allocate 20% of the training set as a validation set for tuning hyperparameters in both our method and the baselines, and for conducting the empirical analyses in Section 3. Annotations of NUS-WIDE. NUS-WIDE provides three sets of tags for its images. The first set includes 81 \\\"ground truth\\\" tags, carefully selected to represent Flickr tags, encompassing both general terms (e.g., \\\"animal\\\") and specific ones (e.g., \\\"dog,\\\" \\\"flower\\\"), and corresponding to frequent Flickr tags. These tags are annotated by students and are less noisy than those directly collected from the Web, serving as the ground truth for evaluating image tagging methods. The second and third sets contain 1,000 popular and nearly 5,000 raw Flickr tags, respectively. Image Features and Word Vectors. We extract and normalize image feature representations using VGG-19. Both GloVe and Word2vec word vectors are used in our empirical analysis in Section 3, with 300D GloVe vectors used for the remaining experiments. Word vectors are also normalized. Evaluation. We assess tagging results using two types of metrics: mean image average precision (MiAP), which considers the entire ranking list, and precision, recall, and F1-score for the top K tags in the list (K = 3 and K = 5). Both metrics are commonly used in image tagging research. For details on calculating MiAP and top-K precision and recall, we refer readers to Section 3.3 of Li et al. (2015) and Section 4.2 of Gong et al. (2013), respectively. #### 5.2 Conventional Image Tagging In this section, we present experimental results for traditional image tagging, using the 81 \\\"ground truth\\\" annotated concepts in NUS-WIDE to benchmark various methods. Baselines. We include TagProp as a primary competitive baseline, representing nearest-neighborbased methods that generally outperform parametric methods built from generative models and have shown state-of-the-art results in experimental studies. We also compare against two recent parametric methods, WARP and FastTag, both based on deep architectures but using different models. For a fair comparison, we use the same VGG-19 features across all methods, with code for TagProp and FastTag provided by the authors and WARP implemented based on our neural network architecture. Additionally, we compare to WSABIE and CCA, which correlate images and relevant tags in a low-dimensional space. Hyperparameters for all methods are selected using the validation set. Results. Table 4 presents the comparison results among TagProp, WARP, FastTag, WSABIE, CCA, and our Fast0Tag models, implemented with both linear mapping and a nonlinear neural network. TagProp significantly outperforms WARP and FastTag, but its training and testing complexities are high, at O(M2 ) and O(M) respectively, relative to the training set size M. In contrast, WARP and FastTag are more efficient, with O(M) training complexity and constant testing complexity due to their parametric nature. Our Fast0Tag with linear mapping yields results comparable to TagProp, while Fast0Tag with the neural network surpasses the other methods. Both implementations maintain low computational complexities similar to WARP and FastTag. | Method | MiAP | | K = 3 | | | K = 5 | | | --- | --- | --- | --- | --- | --- | --- | --- | | | | P | R | F1 | P | R | F1 | | CCA | 19 | 9 | 15 | 11 | 7 | 20 | 11 | | WSABIE | 28 | 16 | 27 | 20 | 12 | 35 | 18 | | TagProp | 53 | 29 | 50 | 37 | 22 | 62 | 32 | | WARP | 48 | 27 | 45 | 34 | 20 | 57 | 30 | | FastTag | 41 | 23 | 39 | 29 | 19 | 54 | 28 | | Fast0Tag (lin.) | 52 | 29 | 50 | 37 | 21 | 60 | 31 | | Fast0Tag (net.) | 55 | 31 | 52 | 39 | 23 | 65 | 34 | Table 1: Comparison results of the conventional image tagging with 81 tags on NUS-WIDE. #### 5.3 Zero-Shot and Seen/Unseen Image Tagging This section presents results for two novel image tagging scenarios: zero-shot and seen/unseen tagging. Fu et al. formalised the zero-shot image tagging problem, which aims to annotate test images using a pre-defined set U of unseen tags. Our Fast0Tag naturally applies to this scenario by simply ranking the unseen tags with equation (3). Furthermore, this paper also considers seen/unseen image tagging, which finds both relevant seen tags from S and relevant unseen tags from U for the test images. The set of unseen tags U could be open and dynamically growing. In our experiments, we treat the 81 concepts with high-quality user annotations in NUS-WIDE as the unseen set U for evaluation and comparison. We use the remaining 925 out of the 1000 frequent Flickr tags to form the seen set S - 75 tags are shared by the original 81 and 1,000 tags. Baselines. Our Fast0Tag models can be readily applied to the zero-shot and seen/unseen image tagging scenarios. For comparison, we study the following baselines. Seen2Unseen. We first propose a simple method that extends an arbitrary traditional image tagging method to also work with previously unseen tags. It originates from our analysis experiment in Section 3. First, we use any existing method to rank the seen tags for a test image. Second, we train a ranking SVM in the word vector space using the ranking list of the seen tags. Third, we rank unseen (and seen) tags using the learned SVM for zero-shot (and seen/unseen) tagging. LabelEM. The label embedding method achieves impressive results on zero-shot classification for fine-grained object recognition. If we consider each tag of S U as a unique class, though this implies that some classes will have duplicated images, the LabelEM can be directly applied to the two new tagging scenarios. LabelEM+. We also modify the objective loss function of LabelEM when we train the model, by carefully removing the terms that involve duplicated images. This slightly improves the performance of LabelEM. ConSE. Again by considering each tag as a class, we include a recent zero-shot classification method, ConSE in the following experiments. Note that it is computationally infeasible to compare with Fu et al., which might be the first work to our knowledge on expanding image tagging to handle unseen tags, because it considers all the possible combinations of the unseen tags. Results. Table 5 summarizes the results of the baselines and Fast0Tag when they are applied to the zero-shot and seen/unseen image tagging tasks. Overall, Fast0Tag, with either linear or neural network mapping, performs the best. Additionally, in the table, we add two special rows whose results are mainly for reference. The Random row corresponds to the case when we return a random list of tags in U for zero-shot tagging (and in U S for seen/unseen tagging) to each test image. We compare this row with the row of Seen2Unseen, in which we extend TagProp to handle the unseen tags. We can see that the results of Seen2Unseen are significantly better than randomly ranking the tags. This tells us that the simple Seen2Unseen is effective in expanding the labeling space of traditional image tagging methods. Some tag completion methods may also be employed for the same purpose as Seen2Unseen. Another special row in Table 5 is the last one with RankSVM for zero-shot image tagging. We obtain its results through the following steps. Given a test image, we assume the annotation of the seen tags, S, are known and then learn a ranking SVM with the default regularization = 1. The learned SVM is then used to rank the unseen tags for this image. One may wonder that the results of this row should thus be the upper bound of our Fast0Tag implemented based on linear regression because the ranking SVM models are the targets of the linear regression. However, the results show that they are not. This is not surprising, but rather it reinforces our previous statement that the learned ranking SVMs are not the \\\"true\\\" principal directions. The Fast0Tag implemented by the neural network is an effective alternative for seeking the principal directions. It would also be interesting to compare the results in Table 5 (zero-shot image tagging) with those in Table 4 (conventional tagging), because the experiments for the two tables share the same testing images and the same candidate tags; they only differ in which tags are used for training. We can see that the Fast0Tag (net.) results of the zero-shot tagging in Table 5 are actually comparable to the conventional tagging results in Table 4, particularly about the same as FastTag's. These results are encouraging, indicating that it is unnecessary to use all the candidate tags for training in order to have high-quality tagging performance. Annotating images with 4,093 unseen tags. What happens when we have a large number of unseen tags showing up at the test stage? NUS-WIDE provides noisy annotations for the images with over 5,000 Flickr tags. Excluding the 925 seen tags that are used to train models, there are 4,093 remaining unseen tags. We use the Fast0Tag models to rank all the unseen tags for the test images, and the results are shown in Table 3. Noting that the noisy annotations weaken the credibility of the evaluation process, the results are reasonably low but significantly higher than the random lists. Qualitative results. Figure 6 shows the top five tags for some exemplar images, returned by Fast0Tag under the conventional, zero-shot, and seen/unseen image tagging scenarios. Those by TagProp under the conventional tagging are shown on the rightmost. The tags in green color appear in the ground truth annotation; those in red color and italic font are the mistaken tags. Interestingly, Fast0Tag performs equally well for traditional and zero-shot tagging and makes even the same mistakes. # 6 Experiments on IAPRTC-12 We present another set of experiments conducted on the widely used IAPRTC-12 dataset. We use the same tag annotation and image training-test split as described in prior work for our experiments. There are 291 unique tags and 19,627 images in IAPRTC-12. The dataset is split into 17,341 training images and 2,286 testing images. We further separate 15 # 6.1 Configuration Similar to the experiments in the previous section, we evaluate our methods in three distinct tasks: conventional tagging, zero-shot tagging, and seen/unseen tagging. Unlike NUS-WIDE, where a relatively small set of 81 tags is considered the ground truth annotation, all 291 tags of IAPRTC-12 are typically used in prior work to compare different methods. Therefore, we also use all of them for conventional tagging. For the zero-shot and seen/unseen tagging tasks, we exclude 20The visual features, evaluation metrics, word vectors, and baseline methods remain the same as described in the main text. # 6.2 Results Tables 4 and 5 display the results for all three image tagging scenarios (conventional, zero-shot, and seen/unseen tagging). The proposed Fast0Tag continues to outperform the other competitive baselines on this new IAPRTC-12 dataset. A notable observation, which is less apparent on NUS-WIDE probably due to its noisier seen tags, is the significant performance gap between LabelEM+ and LabelEM. This indicates that traditional zero-shot classification methods may not be directly suitable for either zero-shot or seen/unseen image tagging tasks. However, performance can be improved by tweaking LabelEM and carefully removing terms in its formulation that involve comparisons of identical images. # 7 More Qualitative Results In this section, we provide additional qualitative results from different tagging methods on both the NUS-WIDE and IAPRTC-12 datasets. These are presented to supplement the findings discussed in the main text. Due to the incompleteness and noise in tag ground truth, many accurate tag predictions are often incorrectly assessed as mistakes because they don't match the ground truth. This issue is particularly evident in the 4k zero-shot tagging results, where a wide variety of tag candidates are considered. # 8 Conclusion We have conducted a thorough examination of a specific visual pattern in words: the visual association rule that divides words into two distinct groups based on their relevance to an image. We also investigated how this rule is captured by vector offsets within the word vector space. Our empirical findings demonstrate that for any given image, there exists a main direction in the word vector space along which vectors of relevant tags are ranked higher than those of irrelevant tags. While our experimental analyses involved 1,006 words, future research should encompass larger-scale and theoretical investigations. Based on this discovery, we developed a Fast0Tag model to address image tagging by estimating the primary directions for input images. Our method is as efficient as FastTag and is capable of annotating images with a large number of previously unseen tags. Extensive experiments confirm the effectiveness of our Fast0Tag approach.\\nHere are the descriptions of the conferences you should consider:\\nEMNLP: \\nThe Conference on Empirical Methods in Natural Language Processing (EMNLP) is a leading international conference in the field of natural language processing (NLP).\\n\\n**Key Characteristics:**\\n\\n*   **Focus on Empirical NLP:** EMNLP emphasizes empirical research, focusing on methods and models that are tested and evaluated using real-world data.\\n*   **Broad Range of Topics:** The conference covers a wide array of topics within NLP, including text classification, machine translation, language modeling, information extraction, dialogue systems, summarization, question answering, and more.\\n*   **Data-Driven Approach:** EMNLP papers typically use a data-driven approach, focusing on using datasets to train and evaluate NLP models.\\n*   **Focus on Evaluation Metrics:** Rigorous evaluation is a key component of EMNLP research, emphasizing the use of standardized metrics to assess the performance of NLP models.\\n*   **Theoretical and Applied Research:** EMNLP welcomes both theoretical and applied research, as long as the research is empirically grounded.\\n*   **Strong Community:** The conference has a strong and supportive community of NLP researchers and practitioners.\\n*   **Poster and Oral Presentations:** Accepted papers are typically presented as posters, with a smaller subset selected for oral presentations.\\n*   **Workshops and Tutorials:** EMNLP features various workshops and tutorials covering specialized topics within NLP.\\n*   **Annual Event:** EMNLP is an annual conference that takes place each year.\\n*   **Double-Blind Reviewing:** The review process is double-blind, ensuring anonymity of both authors and reviewers.\\n*  **Emphasis on Reproducibility:** Like other top tier ML conferences, EMNLP encourages reproducible research by encouraging code sharing.\\n\\n**In summary, EMNLP is a premier conference for empirical research in natural language processing, focused on data-driven approaches, rigorous evaluation, and addressing real-world problems using NLP techniques. It serves as an important platform for researchers to share their work, collaborate with peers, and contribute to the advancement of the NLP field.**\\n\\nCVPR: \\nThe Conference on Computer Vision and Pattern Recognition (CVPR) is one of the most prestigious and competitive conferences in the field of computer vision.\\n\\n**Key Characteristics:**\\n\\n*   **Premier Computer Vision Venue:** CVPR is widely regarded as a top venue for publishing research in computer vision, attracting submissions from top research labs worldwide.\\n*   **Broad Range of Topics:** The conference covers a diverse set of topics within computer vision, including image recognition, object detection, segmentation, 3D vision, video analysis, medical image analysis, and robotics vision.\\n*   **Focus on Visual Data Analysis:** CVPR is primarily concerned with the analysis, interpretation, and understanding of visual data, such as images and videos.\\n*   **Rigorous Peer Review:** The review process is typically rigorous, with a large number of submissions and a low acceptance rate.\\n*   **Emphasis on Technical Innovation:** CVPR papers typically demonstrate technical innovation and advance the state-of-the-art in the field.\\n*   **Poster and Oral Presentations:** Accepted papers are presented as posters and a smaller subset is selected for oral presentation.\\n*   **Tutorials and Workshops:** CVPR features numerous tutorials and workshops focused on specialized areas of computer vision.\\n*   **Large and International Community:** The conference attracts a large and international audience from academia and industry, highlighting its wide reach.\\n*   **Annual Event:** CVPR is an annual conference that typically takes place in the summer.\\n*   **Double-Blind Reviewing:** The review process is double-blind, ensuring a fair assessment of the work, where reviewers are not aware of the author's identities and vice-versa.\\n*   **Publicly Available Code:** It is increasingly expected that papers will also make their code public, contributing to reproducibility and open science.\\n\\n**In summary, CVPR is a leading international conference in computer vision, highly regarded for its rigorous review process, strong focus on technical innovation, broad coverage of topics, and significant impact on the field. It is a crucial venue for researchers to disseminate their latest findings, engage with the computer vision community, and advance the field forward.**\\n\\nNIPS: \\nThe Conference on Neural Information Processing Systems (NeurIPS, formerly NIPS) is one of the most prestigious and highly selective conferences in the field of artificial intelligence and machine learning.\\n\\n**Key Characteristics:**\\n\\n*   **Highly Competitive:** NeurIPS is known for its extremely low acceptance rates, making it very difficult to get a paper accepted. This also means the accepted papers are typically of very high quality and significant impact.\\n*   **Broad Coverage:** NeurIPS covers a wide range of topics within machine learning, including deep learning, reinforcement learning, optimization, theoretical foundations, probabilistic methods, and more.\\n*   **Strong Focus on Deep Learning:** With the recent surge in popularity of deep learning, NeurIPS has become a major venue for showcasing cutting-edge research in this subfield.\\n*   **Poster and Oral Presentations:** Accepted papers are typically presented as posters, and a smaller subset are selected for oral presentations.\\n*   **Workshops and Tutorials:** NeurIPS features various workshops and tutorials covering a broad spectrum of topics in AI and machine learning. These workshops provide an in-depth view into specialized areas and future research directions.\\n*   **Large and Diverse Audience:** The conference attracts a diverse audience from academia, industry, and government research labs.\\n*   **High Visibility and Impact:** NeurIPS papers often have a significant impact on the field, and they are frequently cited by other researchers.\\n*   **Single-Blind Reviewing:** Typically, the reviewing process is single-blind, meaning that reviewers know the authors' identities, but the authors do not know the reviewers' identities.\\n*   **Focus on Novel Contributions:** The conference focuses on original and innovative research contributions.\\n*   **Annual Event:** NeurIPS is an annual conference that takes place each year, usually in December.\\n\\n**In summary, NeurIPS is a leading global conference in AI and machine learning, renowned for its high standards, breadth of coverage, and its impact on shaping the future direction of the field. It's a crucial venue for researchers to share cutting-edge findings, network with experts, and stay updated on the latest advancements.**\\n\\n\\nBased on the provided information, which conference is the best fit for this paper?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Reasoning\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95,\n        \"samples\": [\n          \"This paper enhances video narration using linguistic insights, integrating language models and distributional semantics into an LSTM framework. It focuses on improving grammatical correctness and descriptive quality in video descriptions, evaluated on YouTube and movie datasets. The work aligns with CVPR's focus on computer vision and pattern recognition, specifically in video understanding and generation. Unlike the reference paper which focuses on action recognition in baseball videos, this paper focuses on generating natural language descriptions for videos.\",\n          \"This paper introduces BladeDISC++, a system for optimizing memory usage in dynamic shape deep learning graphs using symbolic shape analysis. It focuses on operation scheduling and rematerialization, crucial for efficient execution of modern deep learning models. The work aligns with NIPS's focus on machine learning advancements, particularly in optimization and efficient computation. Unlike the reference paper which focuses on safety guarantees in neural networks, this paper tackles performance optimization in dynamic deep learning scenarios.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-d1199788-497e-4f9b-8d14-92ea93dd93d0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Number</th>\n",
              "      <th>Inappropriate Methodologies</th>\n",
              "      <th>Incoherent Arguments</th>\n",
              "      <th>Unsubstantiated Claims</th>\n",
              "      <th>Total Score</th>\n",
              "      <th>Publishable</th>\n",
              "      <th>Paper ID</th>\n",
              "      <th>Conference</th>\n",
              "      <th>Rationale</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Maxfreq</th>\n",
              "      <th>Conflist</th>\n",
              "      <th>Fulltext</th>\n",
              "      <th>prompt_classifier</th>\n",
              "      <th>Reasoning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>P001</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>Drone tracking and localization are essential ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td># Leveraging Clustering Techniques for Enhance...</td>\n",
              "      <td></td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>P002</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>Virus transmission is intricately linked to th...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td># Virus Propagation and their Far-Reaching Imp...</td>\n",
              "      <td></td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>P003</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>Explainable reinforcement learning has emerged...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td># Explainable Reinforcement Learning for Finan...</td>\n",
              "      <td></td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P004</td>\n",
              "      <td>KDD</td>\n",
              "      <td></td>\n",
              "      <td>This study introduces a novel concept of train...</td>\n",
              "      <td>9</td>\n",
              "      <td>['KDD']</td>\n",
              "      <td># Graph Neural Networks Without Training: Harn...</td>\n",
              "      <td></td>\n",
              "      <td>This paper introduces a novel training-free gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P005</td>\n",
              "      <td>CVPR</td>\n",
              "      <td></td>\n",
              "      <td>This research introduces a comprehensive cloth...</td>\n",
              "      <td>11</td>\n",
              "      <td>['CVPR']</td>\n",
              "      <td># Collaborative Clothing Segmentation and Iden...</td>\n",
              "      <td></td>\n",
              "      <td>The paper presents a novel clothing co-parsing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P131</td>\n",
              "      <td>CVPR</td>\n",
              "      <td></td>\n",
              "      <td>This paper details our submission for stage 2 ...</td>\n",
              "      <td>5</td>\n",
              "      <td>['TMLR', 'CVPR', 'KDD']</td>\n",
              "      <td># Enhancing Disentanglement through Learned Ag...</td>\n",
              "      <td>Here is the full text of a research paper:\\n# ...</td>\n",
              "      <td>This paper presents a method for enhancing dis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>132</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>P132</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>This study presents a groundbreaking approach ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td># Analyzing Fermentation Patterns with Multi-M...</td>\n",
              "      <td></td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>133</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P133</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td></td>\n",
              "      <td>This paper reduces discontinuous parsing to se...</td>\n",
              "      <td>5</td>\n",
              "      <td>['EMNLP', 'KDD', 'NIPS']</td>\n",
              "      <td># Discontinuous Constituent Parsing as Sequenc...</td>\n",
              "      <td>Here is the full text of a research paper:\\n# ...</td>\n",
              "      <td>This paper presents a novel sequence labeling ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>134</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>P134</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>The quintessential nature of DNA is intertwine...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td># Unraveling the Enigmatic Parallels Between D...</td>\n",
              "      <td></td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>135</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>P135</td>\n",
              "      <td>TMLR</td>\n",
              "      <td></td>\n",
              "      <td>This study examines distributed stochastic var...</td>\n",
              "      <td>11</td>\n",
              "      <td>['TMLR']</td>\n",
              "      <td># A Decentralized Local Stochastic Extragradie...</td>\n",
              "      <td></td>\n",
              "      <td>This paper presents a decentralized stochastic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135 rows × 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1199788-497e-4f9b-8d14-92ea93dd93d0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1199788-497e-4f9b-8d14-92ea93dd93d0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1199788-497e-4f9b-8d14-92ea93dd93d0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d4881d0d-7402-4366-9250-2eebdd9c8abc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d4881d0d-7402-4366-9250-2eebdd9c8abc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d4881d0d-7402-4366-9250-2eebdd9c8abc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5ea83ee6-ee8d-4a6a-b166-68a363b37acf\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5ea83ee6-ee8d-4a6a-b166-68a363b37acf button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     Doc Number  Inappropriate Methodologies  Incoherent Arguments  \\\n",
              "0             1                            2                     3   \n",
              "1             2                            1                     1   \n",
              "2             3                            2                     2   \n",
              "3             4                            4                     4   \n",
              "4             5                            4                     4   \n",
              "..          ...                          ...                   ...   \n",
              "130         131                            4                     4   \n",
              "131         132                            2                     2   \n",
              "132         133                            4                     4   \n",
              "133         134                            1                     1   \n",
              "134         135                            4                     4   \n",
              "\n",
              "     Unsubstantiated Claims  Total Score  Publishable Paper ID Conference  \\\n",
              "0                         2            7            0     P001         na   \n",
              "1                         1            3            0     P002         na   \n",
              "2                         2            6            0     P003         na   \n",
              "3                         4           12            1     P004        KDD   \n",
              "4                         4           12            1     P005       CVPR   \n",
              "..                      ...          ...          ...      ...        ...   \n",
              "130                       4           12            1     P131       CVPR   \n",
              "131                       1            5            0     P132         na   \n",
              "132                       4           12            1     P133      EMNLP   \n",
              "133                       1            3            0     P134         na   \n",
              "134                       4           12            1     P135       TMLR   \n",
              "\n",
              "    Rationale                                           Abstract Maxfreq  \\\n",
              "0          na  Drone tracking and localization are essential ...           \n",
              "1          na  Virus transmission is intricately linked to th...           \n",
              "2          na  Explainable reinforcement learning has emerged...           \n",
              "3              This study introduces a novel concept of train...       9   \n",
              "4              This research introduces a comprehensive cloth...      11   \n",
              "..        ...                                                ...     ...   \n",
              "130            This paper details our submission for stage 2 ...       5   \n",
              "131        na  This study presents a groundbreaking approach ...           \n",
              "132            This paper reduces discontinuous parsing to se...       5   \n",
              "133        na  The quintessential nature of DNA is intertwine...           \n",
              "134            This study examines distributed stochastic var...      11   \n",
              "\n",
              "                     Conflist  \\\n",
              "0                               \n",
              "1                               \n",
              "2                               \n",
              "3                     ['KDD']   \n",
              "4                    ['CVPR']   \n",
              "..                        ...   \n",
              "130   ['TMLR', 'CVPR', 'KDD']   \n",
              "131                             \n",
              "132  ['EMNLP', 'KDD', 'NIPS']   \n",
              "133                             \n",
              "134                  ['TMLR']   \n",
              "\n",
              "                                              Fulltext  \\\n",
              "0    # Leveraging Clustering Techniques for Enhance...   \n",
              "1    # Virus Propagation and their Far-Reaching Imp...   \n",
              "2    # Explainable Reinforcement Learning for Finan...   \n",
              "3    # Graph Neural Networks Without Training: Harn...   \n",
              "4    # Collaborative Clothing Segmentation and Iden...   \n",
              "..                                                 ...   \n",
              "130  # Enhancing Disentanglement through Learned Ag...   \n",
              "131  # Analyzing Fermentation Patterns with Multi-M...   \n",
              "132  # Discontinuous Constituent Parsing as Sequenc...   \n",
              "133  # Unraveling the Enigmatic Parallels Between D...   \n",
              "134  # A Decentralized Local Stochastic Extragradie...   \n",
              "\n",
              "                                     prompt_classifier  \\\n",
              "0                                                        \n",
              "1                                                        \n",
              "2                                                        \n",
              "3                                                        \n",
              "4                                                        \n",
              "..                                                 ...   \n",
              "130  Here is the full text of a research paper:\\n# ...   \n",
              "131                                                      \n",
              "132  Here is the full text of a research paper:\\n# ...   \n",
              "133                                                      \n",
              "134                                                      \n",
              "\n",
              "                                             Reasoning  \n",
              "0                                                   na  \n",
              "1                                                   na  \n",
              "2                                                   na  \n",
              "3    This paper introduces a novel training-free gr...  \n",
              "4    The paper presents a novel clothing co-parsing...  \n",
              "..                                                 ...  \n",
              "130  This paper presents a method for enhancing dis...  \n",
              "131                                                 na  \n",
              "132  This paper presents a novel sequence labeling ...  \n",
              "133                                                 na  \n",
              "134  This paper presents a decentralized stochastic...  \n",
              "\n",
              "[135 rows x 15 columns]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Pda-p7Znsw",
        "outputId": "9586686d-5288-4f50-a61d-4c84d694cb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Paper ID  Publishable Conference  \\\n",
            "0     P001            0         na   \n",
            "1     P002            0         na   \n",
            "2     P003            0         na   \n",
            "3     P004            1        KDD   \n",
            "4     P005            1       CVPR   \n",
            "\n",
            "                                           Reasoning  \n",
            "0                                                 na  \n",
            "1                                                 na  \n",
            "2                                                 na  \n",
            "3  This paper introduces a novel training-free gr...  \n",
            "4  The paper presents a novel clothing co-parsing...  \n"
          ]
        }
      ],
      "source": [
        "final_results_df = df[['Paper ID', 'Publishable', 'Conference', 'Reasoning']].copy()\n",
        "print(final_results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Y-BOJzfJaL1p"
      },
      "outputs": [],
      "source": [
        "final_results_df['Conference'] = df['Conference'].str.lower()\n",
        "final_results_df['Conference'] = df['Conference'].replace('nips', 'neurips')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qzRlYeHLan83",
        "outputId": "80e92adb-ba6c-4211-e7d0-e376185af85b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_937b8825-9a05-4495-81dc-cbcfacca04f5\", \"results.csv\", 55430)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "final_results_df.to_csv('results.csv', encoding = 'utf-8-sig')\n",
        "files.download('results.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
